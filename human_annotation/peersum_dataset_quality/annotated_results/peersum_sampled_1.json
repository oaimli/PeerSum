{"source_documents": ["This paper is about the problem of learning a stochastic policy for generating an object (like a molecular graph) from a sequence of actions, such that the probability of generating an object is proportional to a given positive reward for that object. Whereas standard return maximization tends to converge to a single return-maximizing sequence, there are cases where we would like to sample a diverse set of high-return solutions. These arise, for example, in black-box function optimization when few rounds are possible, each with large batches of queries, where the batches should be diverse, e.g., in the design of new molecules. One can also see this as a problem of approximately converting an energy function to a generative distribution. While MCMC methods can achieve that, they are expensive and generally only perform local exploration. Instead, training a generative policy amortizes the cost of search during training and yields to fast generation.  Using insights from Temporal Difference learning, we propose GFlowNet, based on a view of the generative process as a flow network, making it possible to handle the tricky case where different trajectories can yield the same final state, e.g., there are many ways to sequentially add atoms to generate some molecular graph. We cast the set of trajectories as a flow and convert the flow consistency equations into a learning objective, akin to the casting of the Bellman equations into Temporal Difference methods. We prove that any global minimum of the proposed objectives yields a policy which samples from the desired distribution, and demonstrate the improved performance and diversity of GFlowNet on a simple domain where there are many modes to the reward function, and on a molecule synthesis task.\n", "The paper considers generative processes for which state transitions form a DAG. The authors view such a generative process as a flow in a graph—with a single source and sinks connected to final objects. The authors set objects' probabilities being proportional to the reward function and train a generative model by enforcing flow-matching conditions. The main contribution of the paper is a new generative approach for unnormalized distributions.  The proposed approach seems novel and is well described in the paper. The authors provide theoretical and practical details, as well as intuition behind the method. Experimental evaluation is sufficient, and results are demonstrated on a toy problem as well as on a practical task of molecular property optimization. Provided results in supplementary (Fig. 13 and Fig. 14) demonstrate huge advantage of the proposed approach.\n\nSome comments are provided below:\n1. More details on relation of the proposed approach and Bayesian networks would be profitable. Currently it seems as if model definition using a Bayesian network would yield a similar approach (e.g., flow-matching condition will be the same as total probability formulta).\n2. Experimental results should be provided on standard datasets. For example, optimization of logP and QED as provided in multiple papers, including GrammarVAE (https://arxiv.org/pdf/1703.01925.pdf). Such experiments are necessary to confirm that the proposed approach indeed yields good results against strong baselines.\n3. Diverse generation as mentioned in the title of the paper, is not built in the model and is a side-effect of the approach. I would recommend conducting experiments on the objective functions with high mode imbalance (e.g., 10k molecules with high reward in one mode and 5 molecules with high reward in another).\n\nUpdate: The authors answered my questions, I keep my original score. The authors adequately addressed the limitations.", " Having read the other reviews and comments, I'm inclined to keep my original score and stand by my original review. I believe this paper proposes interesting ideas and strong results, although the clarity could be improved as mentioned by other reviewers too (e.g. Reviewer UVLM) -- but this should not be hard to fix. The authors also state that they will add further experiments to the paper (e.g. a comparison with a molecular autoencoder approach), which should improve the paper further.\n\nOne of the main oddities brought up in the rebuttal was the use of a non-random dataset for the \"random\" molecule dataset described in Section 4.2.  However, with further clarification, I now realize that this is only used for scoring proxy, and the authors say that they will fix the description of this in the paper, meaning that I now consider this concern resolved.", "This paper propose GFlowNet, a generative method that can turn a given positive reward into a generative policy that samples with a probability proportional to the return. \n  The most interesting part is that this novel generative method is based on flow network and local flow-conditions. And then the authors cast the flow consistency equations into an objective just like casting the Bellman equations into TD objectives. Compared with other methods, GFlowNet is designed especially for diverse candidate generation.\n\nOverall, I vote for accepting. This current submission is overall well written, clearly illustrated and appropriately structured. The motivations are well explained and experimental results are strong.\n\n\nQuestion:\n\n* Looking at Figure 4, it seems that MARS has the trend to have a higher average reward if the generative models are trained more than 10^6 molecules. \"For GFLowNet and for MARS, the more molecules are visited, the better they become, with slow converge towards the proxy's max reward\", which method converges slower? Are there any theoretical/experimental results?\n\n* In equation (9), the hyperparameter $\\epsilon$ is added to avoid numerical issues. It would be better if the authors can provide more details on why $\\epsilon$  can trade off how much pressure on large versus small flows. And add a brief description about how to setting this hyperparameter for different cases.\n\nUPDATE:\nI appreciate the authors' response to my questions and concerns.  I will keep my original score. yes", " Thank you for the clarification. It clarified most of my concerns. After reading the comments of reviewer 1AuD and others,  I have a better understanding of the paper.  The idea is interesting and new but I still believe that the paper is relatively confusing for the common readers. Therefore I decided to keep my original score.", " > We hope this clears up any confusion.\n\nYes -- many thanks for the speedy reply!", " > did you mean to use this dataset? \n\nYes, its erroneous description in the paper probably stemmed from our focusing our time on explaining GFlowNet. The dataset is ~80% generated from random walks, the rest is from older iterations of actor-critic and PPO agents (before we invented GFlowNet, PPO was our best method for molecule generation) trained on docking scores. The reasoning for this mix was that populating the dataset with a few more high-scoring molecules (according to the docking oracle) would make for a more interesting generative task. \n\nThe end goal of this project is to do active learning (i.e. query a docking oracle, and eventually even more powerful/precise oracles) with batches from our generative models, and so incrementally build a better and better dataset (and by extension, proxy). Note that the results of Section 4.3 & Figure 6 demonstrate this capability fairly well, starting from only 2000 molecules and interacting directly with the docking oracle.\n\n> Just so I understand correctly: this \"random\" dataset was only used for training the scoring proxy, not the generative models? \n\nThis is correct. None of the generative models have access to the dataset that the proxy is trained with. They have to discover the high-reward parts of the state space on their own.\n\n> The junction tree VAE I mention in section 2.2 would also see the continuous \"goodness function\" in its BayesOpt routine and so should also obtain information from the bad examples\n\nIf our understanding of Jin et al.'s JT-VAE is correct, the VAE is trained on a fixed 250k molecule dataset (a random subset of ZINC), thus the embeddings that it learns may only be able capture the \"positive\" parts of molecule space (ZINC itself is not random, it is a \"database of commercially-available compounds\"). It is true that the BayesOpt part will capture the information of good & bad examples, but it is limited by the learned embedding space which likely only covers the regions around the \"positive\" (ZINC) parts of molecule space. By contrast in our setup, generation is not limited by this embedding space and has access to the entire molecule space spanned by our block vocabulary.\n\nWe hope this clears up any confusion.\n\n\n", " I thank the authors for their rebuttal. I had a few comments/questions:\n\n### 1. The \"random\" molecule dataset (replying to 3b)\n> \"Thanks for this observation, we went back and checked how the initial dataset we're using in the paper has been generated, and it is indeed not entirely random data: it includes a bit of data generated by past RL agents that we have run on this problem (the methods in this paper are used within a larger scope drug-discovery project). We will edit the paper to reflect this. Note that we provide this dataset in the supplementary material, so the exact setup presented in the paper is reproducible.\"  \n\nThis seemed rather strange: did you mean to use this dataset? Just so I understand correctly: this \"random\"dataset was only used for training the scoring proxy, not the generative models? Also, it would be interesting to hear more about these past RL agents, are they better than PPO?\n\n\n### 2. Comparison to standard baselines (replying to 2.2b)\n> \"This is a good point, and we shall add such comparisons. The reason we did not explore in that direction is that in our active learning setting we are not given a set of 'good examples' (the usual approach with generative models like VAEs, GANs, etc): instead we are given a goodness function (the reward), which includes information about 'bad examples' and all the shades in between good and bad. It would seem wasteful to throw away most of the information acquired acquired through active learning and only keep the tail of the distribution of collected experimental results.\"\n\nI'm not completely sure I follow the argument here. The junction tree VAE I mention in section 2.2 would also see the continuous \"goodness function\" in its BayesOpt routine and so should also obtain information from the bad examples. Anyway, I'm glad to hear that you are adding comparisons to such approaches (which seems maybe also of interest to Reviewer 3Yk2)! \n\n\n### 3. Scaling to larger action spaces (replying to 5b)\n> \"This is indeed a limitation, if a state has many parents then it becomes more expensive to compute the loss function. In the case of molecules, this is small enough, but it could become a problem for larger objects. Since submitting the paper, we have found alternative formulations to compute the flow that do not require this sum (and that possibly work in continuous action spaces), but these are different enough that they would warrant a follow-up paper.\"  \n\nThis seems very interesting and reasonable to postpone to future work. I look forward to reading!\n", " \nThank you for your review, we will incorporate these comments to our paper.\n\n> More details on relation of the proposed approach and Bayesian networks would be profitable. Currently it seems as if model definition using a Bayesian network would yield a similar approach (e.g., flow-matching condition will be the same as total probability formulta).\n\nWe had not thought about this analogy, but there certainly are some conceptual links to be made. The nodes in a GFlowNet have a more specialized semantics than those in a Bayes Net, but a GFlowNet can be seen as a special kind of BayesNet, with its DAG structure and random variables at nodes and conditional probabilities linking them (however the parametrization is very different, not based on the conditional of a variable given its parents, but the conditional probability of mutually exclusive children being true given some parent). A GFlowNet node $s$ represents the event \"the outcome $X$ was generated by first constructing $s$\", where $X$ is a sample from the GFlowNet (e.g. a complete molecule) while $s$ can be on the path (the sequence of transformations) which goes from the empty initial state to the construction of $X$. There is an order relation \"$<$\" that relates successive states, i.e., $s_t<s_{t+1}$. These special properties of GFlowNets enable particular algorithms which are not applicable with BayesNets in general.\n\n\n> Experimental results should be provided on standard datasets. For example, optimization of logP and QED as provided in multiple papers, including GrammarVAE (https://arxiv.org/pdf/1703.01925.pdf). Such experiments are necessary to confirm that the proposed approach indeed yields good results against strong baselines.\n\nWe did not optimize for logP and QED because on their own these are fairly easy to maximize.  \nJust to be sure, we ran GFlowNet on logP and QED. For logP we quickly find molecules with a >20 logP, which at this point is biologically uninteresting (for reference, ibuprofen's logP is between 3.5 and 4). For QED, we also quickly find molecules with the maximum possible QED in our action space, which is 0.948 (in fact our top-1000 is >0.94 after 100k molecules seen). We will add those results to the appendix for completeness.\n\n> Diverse generation as mentioned in the title of the paper, is not built in the model and is a side-effect of the approach. I would recommend conducting experiments on the objective functions with high mode imbalance (e.g., 10k molecules with high reward in one mode and 5 molecules with high reward in another).\n\nWe are confident that GFlowNet does diverse generation (see figures 2, 13 and 14), but it would indeed be interesting to see how robust it is to bad reward landscapes. We will try to include such an experiment (and hopefully we can report it before the end of the discussion period).  \nIt might interesting to note here that the molecule landscape is already a bit like this, although not as imbalanced as you suggest testing. For some good molecules, removing or adding an atom can have a big effect and make them terrible molecules, whereas other molecules are robust to being \"edited\". These naturally make for peaky and wide modes respectively.\n", " \nThank you for your review, we will adjust the paper to be more explicit about notation.\n\n> the paper could be made more intuitive and much easy to follow if it uses more direct and intuitive concepts and pushes less hard to connect with concepts in RL.\n\nThis is a very good point, and we are indeed working on a formalization which avoids borrowing the usual semantics of RL, to minimize confusion. Thanks for the suggestion.\n\n> Line 71: The RL “agent” should have large support of what?\n\nHere we mean support over the state space, i.e. have a non-zero probability of reaching states. We will clarify the text.  \nIn terms of off-policy methods, one way to think about it is that if we are estimating a quantity depending on policy $\\pi$ (e.g. $V^\\pi$) while running policy $\\mu$, then for any state which $\\pi$ would visit, $\\mu$ ideally needs to have a non-zero probability of visiting it as well (but these probabilities do not need to be equal). If an RL method is truly off-policy, this should be enough to learn $V^\\pi$, even if $\\pi\\neq\\mu$.\n\n> Figure 3: MARS looks to approximate better the proxy dataset distribution\n\nThe dataset distribution and MARS's (or GFlowNet's) distribution should not match. For a uniform dataset, we should have that $p(x) = 1/|\\mathcal{X}|$, but MARS and GFlowNet should be so that $p(x) = R(x) / Z$ (see Eq. (1)). \n\nAs such, these will not be the same. In fact, the probability of $x$s with a higher $R(x)$ should be increased (intuitively, since it becomes $\\propto R(x)$ rather than $\\propto c$, one can imagine that if $R(x) > c$ then $p(x)$ should be larger in the second distribution). This explains why in Figure 3, the proxy dataset distribution is more to the left than the other distributions.\n\n> Figure3: Didn’t understand the shifting of the density to the right.\n\nWe compare sampling from $p(x) \\propto R(x)^1$ with sampling from $p(x) \\propto R(x)^4$ (raising to power 4). In the second distribution, $x$s with higher $R$s should have more probability. In Figure 3 we show the histogram of $R$, thus if the higher $R$s have more probability, the distribution curve will be higher on the right hand side (since the x-axis goes from 0 to 9). This will visually look like the densities are shifted to the right.\n\nJust to be clear, here's an example. If $\\mathcal{X} = \\{x_1, x_2\\}$, and $R(x_1)=1,R(x_2)=2$. Then for $p(x) \\propto R(x)^1$, $p(x_1)=1/(1+2)=1/3$ and $p(x_2)=2/3$. For $p(x) \\propto R(x)^4$, $p(x_1) = 1/(1^4+2^4) = 1/17$, and $p(x_2)=16/17$. $x_2$ becomes much more likely because it has a higher reward and we are using $\\beta=4$.", " \nThank you for your detailed review!\n\n> how dependent the results presented later are on the value of $\\epsilon$ chosen\n\nIt is true that this hyperparameter is worth discussing more. We will refine our explanation.  \nIdeally, $\\epsilon$ should be set to be about as low as the minimum reward one cares about, or close to the minimum useful reward of the environment. This would help the model spend less capacity on the states with flow lower than that. Empirically we have found that this choice was stable, but did not conduct an extensive investigation.\n\n> One other currently popular way to sample from the distribution over molecules is using autoencoders with autoregressive decoders\n\nThis is a good point, and we shall add such comparisons. The reason we did not explore in that direction is that in our active learning setting we are not given a set of 'good examples' (the usual approach with generative models like VAEs, GANs, etc): instead we are given a goodness function (the reward), which includes information about 'bad examples' and all the shades in between good and bad. It would seem wasteful to throw away most of the information acquired acquired through active learning and only keep the tail of the distribution of collected experimental results.\n\n> section 2 is rather dense and I had to read it twice to understand what was going on\n\nWe will try to clarify and be more explicit about the function of each part.\n\n> ($L_{\\theta^*}(\\theta)$)... should be $L_{\\theta^*}(\\tau)$\n\nYou are right. Thanks for catching this typo. You are also right that in principle we can use any loss function but the ones we defined have value 0 when the flows are matched, which yields the convenient outcome of this theorem (that the global minimizer in function space are guaranteed to match the flows.)\n\n> To convert from $s$ to $x$ you had to add the vector -1\n\nWhen we constructed this environment, we wrote a reward for $[-1,1]^n$, and wanted to subdivide an overlapping grid in $H$ parts and vary $H$, thus our presentation; but you are correct that this distinction is unnecessary. We will simplify this passage.\n\n\n> My understanding is that (if trained/run for long enough) the MCMC baseline and GFlowNet should ultimately sample from the same distribution, i.e. that defined by: \\pi(x) \\propto \\R(x). However, in figure 3 it appears that GFlowNet's distribution is different (the mode is higher and the distribution has wider tails). Does this just mean the MCMC method has not converged? Is there any way to compare against the \"true\" distribution (for instance running the MCMC method for much longer, or using very many random trajectories and importance sampling)?\n\nThis is correct, MCMC and GFlowNet should sample from the same distribution. Here we suspect that MCMC is missing high-reward modes, thus the mismatch in distributions.  \nIn general in molecule space it would take orders of magnitude more compute to converge to (and compute) the \"true\" distribution, which we unfortunately could not afford. In the hypergrid setting, we verify that both methods converge to the true distribution.\n\n\n> I am currently having a bit of trouble understanding figure 4. On lines 330-336 the authors state that when sampling 300k molecules using random trajectories that they obtain over 100 molecules with scores above 8 (233 such molecules to be precise). Looking at figure 4 the top-100 average for MARS is below 8 after visiting 300k molecules. Does this mean that it is worse than random search? Is this expected?\n\nThanks for this observation, we went back and checked how the initial dataset we're using in the paper has been generated, and it is indeed not entirely random data: it includes a bit of data generated by past RL agents that we have run on this problem (the methods in this paper are used within a larger scope drug-discovery project). We will edit the paper to reflect this.  \nNote that we provide this dataset in the supplementary material, so the exact setup presented in the paper is reproducible.\n\n>  For the MCMC methods (e.g. MARS) did the authors consider thinning? For GFlowNet I assume every state is valid? If so I was wondering if the authors ever considered counting the reward for every state visited by GFlowNet rather than just the terminal ones. In some ways can one think of the \"stop action\" as a learnable thinning function?\n\nWe have not used thinning in MCMC methods; for finding the top-k this would not be beneficial: if the chain did not visit enough modes, then subsampling it would not help with getting samples from those unvisited modes. For GFlowNet, your suggestion is a good one which we actually have started implementing since submitting the paper, since every state is valid we can indeed consider a virtual stop action after every state to be used as a learning signal. \n\nOn the analogy between the stop action and thinning: thinning helps to induce iid-ness in a Markov Chain because samples close to each other in a chain are correlated. In GFlowNet, every sample is generated as an independent episode, which the stop action ends. Thus the stop action does not break any correlations. However, you are right that some of the intermediate states could be valid solutions and we could use something inspired like thinning (but no need to subsample) and consider all the subsequences of each terminated sequence as a possible outcome. However, if not done properly, this could break the main property of having sampling probabilities matching the reward function.\n\n\n> I apologize if I missed this but what is the initial state for the molecule tasks?\n\nThe initial state is the \"empty molecule\", to which any block can be attached. We will make this clearer in the text. \n\n\n> the word \"surjective\" is used. I think \"non-injective\" is meant here instead.\n\nIndeed, the emphasis in lines 105 and 138 is to highlight the case when the bijective approach breaks down, so the sentence should say non-injective. However, the mathematical framework we designed includes the bijective case as a special case, which is why we used *surjective* everywhere. But you're right that in these sentences it would be more correct as you pointed out to say *non-injective*. We will fix that.\n\n> The authors briefly talk about the limitations of the approach in section 5. The main limitation they draw attention to is the challenge of moving closer to the local maxima of the reward function in the latter stages of optimization. To resolve this they discuss combining their method with local optimization techniques; however, I wonder whether the temperature approach they discuss in the earlier part of their paper (combined with some annealing scheme) could also be used here?\n\nThis is correct, using a higher temperature is a valid approach. The reasoning for having a separate local optimization is that these can be run in parallel, without affecting GFlowNet's exploration and learning procedure.\n\n> One limitation the authors do not mention is how the method scales in terms of the size of the state and action space. The loss function requires for every current state the sum over all previous states and actions that may have led to the current state (see term 1 of Eq.9). I assume this may become intractable for very large state-action spaces (and the flows one is trying to model become very small). Can one approximate the sum using a subset? Also what about continuous state/action spaces?\n\nThis is indeed a limitation, if a state has many parents then it becomes more expensive to compute the loss function. In the case of molecules, this is small enough, but it could become a problem for larger objects. Since submitting the paper, we have found alternative formulations to compute the flow that do not require this sum (and that possibly work in continuous action spaces), but these are different enough that they would warrant a follow-up paper.", " Thank you for your review, we will incorporate these comments to our paper.\n\n> Looking at Figure 4, it seems that MARS has the trend to have higher average reward if the generative models are trained more than 10^6 molecules. \"For GFLowNet and for MARS, the more molecules are visited, the better they become, with slow converge towards the proxy's max reward\", which method converge slower? Is there any theoretical/experimental results?\n\nFigure 4 suggests that GFlowNet and MARS both appear to be _able_ to converge in terms of the top molecules found, whereas PPO seems to plateau. However, the top-$k$ reward curves across different values of $k$ point to a larger number of high-reward molecules found by GFlowNet. If we were to train for 10x longer (i.e. to $10^7$ mols), just extrapolating from the curve, it seems unlikely that MARS would surpass GFlowNet. At best, MARS would match GFlowNet in terms of the top-$k$ with smaller $k$, but due to the low diversity of MARS (see Fig 13,14) it seems more likely that MARS would not be able to find as many unique top-k molecules.\n\n\n> In equation (9), the hyperparameter \\epsilon is added to avoid numerical issues. It would be better if the authors can provide more details why  can trade off how much pressure on large versus small flows. And add a brief description about how to setting this hyperparameter for different cases.\n\nIt is true that this hyperparameter is worth discussing more. We will refine our explanation.  \nIdeally, $\\epsilon$ should be set to be about as low as the minimum reward one cares about, or close to the minimum useful reward of the environment. This would help the model spend less capacity on the states with flow lower than that. ", "The paper tries to tackle a scenario where the goal is not to learn to generate a sequence of actions that has the highest return but rather to generate a diverse set of trajectories with high returns. They do so by using the given positive return function (which is given by the oracle function) to formulate a generative policy that samples with a probability proportional to the return.  They used a flow network to express the MDP and its value function where the value of a state and action pairs is defined by the flow between the two nodes in the flow network, and finally, the flow consistency equations are used as a learning objective.  The idea of the paper is very interesting but the paper is hard to follow as the paper borrows notations and concepts from RL but not in the exact same setting so it makes it a bit confusing for the readers. I personally feel like the paper could be made more intuitive and much easy to follow if it uses more direct and intuitive concepts and pushes less hard to connect with concepts in RL. \n\nThere are some minor comments and questions:\nComments:\nLine 71: The RL “agent” should have large support of what?\nLine 84: Missing citations of PPO and MCMC\nFigure 3: MARS looks to approximate better the proxy dataset distribution.\nFigure3: Didn’t understand the shifting of the density to the right.\n\n The paper is a bit hard to follow. ", "This paper proposes an approach (called GFlowNet) that uses ideas from reinforcement learning to learn a \"policy\" for sampling from a discrete distribution over a set of objects (\"terminal states\"), where these objects can be traversed between using a sequence of \"actions\". Specifically, the authors consider distributions proportional to some (non-negative) reward function, such that one is more likely to sample better performing points (although I believe the approach is general enough to sample from any unnormalized distribution).\n\nIn the experimental section the authors first consider a toy problem to highlight the advantages of their approach before moving on to consider the problem of designing new molecules, where the \"state\" is the current molecule, and the \"actions\" are defined using a vocabulary of fragments which one can add to the current state (as well as the stop action).  Here, the authors show GFlowNet is able to find  a set of better and more diverse molecules more quickly than previous MCMC and PPO approaches.\n\n\n**More specific details about GFlowNet:**   \nIn GFlowNet, states and actions can be described using a flow network (i.e. a transportation network). Here the states are represented using nodes and the actions as edges (the edge weight, ie flow, describes the probability of taking an action). The authors always consider starting from the same state and actions cannot return to previously seen states (ie fragments can be added to molecules but not taken away). Moreover, one can arrive at the same state with different actions (i.e. different fragments could be used to create the same molecule). Overall, this means that the flow network can be represented using a DAG (directed acyclic graph, see Figure 1). \n\nThe core idea of GFlowNet is then about how to learn the policy over the actions (ie the flows) such that when sampling actions from this policy, you sample from a predefined distribution over terminal states. To this end, the authors derive a flow matching objective (equation 9 -- the flow incoming to a state must match the outgoing flow) which can be differentiated through to learn the parameters parameterizing the policy. The authors prove that this loss can be used off-policy.\n  # 1 Summary of My Review\nI found the approach presented in this paper very interesting and it appears to achieve impressive results. However, on a more negative note, some of the paper was hard to follow and the clarity could be improved in places (see main review below). I believe this would be easy enough to fix, and therefore, I have gone for a higher \"overall score\" and think this paper should be accepted. I have gone with a lower confidence score as I'm less knowledgable about how this idea relates to recent work in RL.\n \n# 2 Main Review\n## 2.1 Originality\nIn the molecule domain, the idea of using RL-based ideas to sample from the distribution of all molecules (weighted by their scores), rather than just find the top molecule, seems novel. Obtaining a set of diverse **and** well-performing molecules is an important problem and the proposed approach is an interesting way to do so.\n\nUnfortunately, I am less familiar with the recent, more general RL literature in this domain so feel less qualified to discuss the paper's originality in this respect, but the approach is fairly general and could hopefully also be of interest here.\n\nThe idea of visited states being equivalent (and so the state-action space being best represented with a DAG rather than a tree) has been considered in the context of Monte Carlo Tree Search, which the authors might briefly want to mention, e.g see the discussion about \"transpositions\" in Section 5.2.4 of:   \n> Browne, C. B., Powley, E., Whitehouse, D., Lucas, S. M., Cowling, P. I., Rohlfshagen, P., Tavener, S., Perez, D., Samothrakis, S. and Colton, S. (2012) ‘A Survey of Monte Carlo Tree Search Methods’, IEEE Transactions on Computational Intelligence in AI and Games, 4(1), pp. 1–43.\n\n## 2.2 Quality\na. As far as I can tell the paper is technically sound (although I did not look through the proofs in the appendix). The approach to make the method computationally tractable (leading to Eq 9) seems reasonable although I would have been interested to hear how dependent the results presented later are on the value of $\\epsilon$ chosen.\n\nb. One other currently popular way to sample from the distribution over molecules is using autoencoders with autoregressive decoders, for instance the junction tree VAE of Jin et al. (2020) for which the fragment-based scheme in this paper is said to be based upon. This seems like an obvious baseline to compare to in the experiments but is omitted.\n\nc. I found some of the experimental evaluation for the molecule task a little confusing: the authors invent a method for sampling from a distribution, yet evaluate it mostly in terms of how well it discovers the modes of the distribution (rather than how well it samples from the distribution as a whole). I realize that this is useful when the distribution is defined by a reward function, but what about when sampling more general distributions? (related to question 3.a. below)\n\n## 2.3 Clarity\n\na. One area for improvement is perhaps the paper's clarity. For instance, section 2 is rather dense and I had to read it twice to understand what was going on. Perhaps adding more signposting to the beginning of this section would allow the reader to follow its various parts.\n\nI also got a bit confused by the notation in this section. For instance $\\tau$ suddenly appears in section 2.2 to denote the trajectory, whereas earlier on in this section I believe $\\vec{a}$ was used instead (eg line 123). (Or do these variables mean different things....?)    \n\nLikewise, I got a bit confused with Equation 10. I think the second part of this should be $L_{\\theta^\\ast}(\\tau) = 0 \\quad \\forall \\tau \\sim P(\\tau)$, i.e. with $\\tau$ in place of $\\theta$? (On a more minor note I was also uncertain as to why $L_{\\theta^\\ast}(\\tau) $ had to equal zero, the approach can also deal with loss functions with any minimum right?)\n\nb. In section 4.1 (ie the toy experiment on the grid domain) I didn't understand why the states $s$ could not live in the same space as $x$ (defining the reward)? To convert from $s$ to $x$ you had to add the vector $-\\mathbf{1}$. This just seemed to add unnecessary complexity to the setup and made it harder to understand?\n\nc. I've only briefly glanced over the code, which hopefully should aid reproducibility. One thing that could be added here though is a description of which scripts to run (including runtime arguments) to actually reproduce the experiments. \n\n\n## 2.4 Significance\nThis paper presents interesting theoretical results as well as strong empirical performance. In the molecule optimization task it is nice to see the focus on obtaining a _diverse set_ of top molecules rather than just the top molecule score (although it would be nice if the top-1 score was also reported somewhere for completeness). The authors consider more interesting (and difficult) molecular optimization tasks (based on docking) than much of the previous work, and it is exciting to see improvements wrt recent (and reasonable) baselines. \n\nI think the ideas that GFlowNet is built around are interesting, particularly of how to sample from a distribution of discrete objects when the transition function (i.e. \"action\" using the paper's terminology) is irreversible. Hopefully this will inspire further work in this direction.\n\n\n# 3 Clarifying Questions\n\na. My understanding is that (if trained/run for long enough) the MCMC baseline and GFlowNet should ultimately sample from the same distribution, i.e. that defined by:\n$\\pi(x) \\propto R(x)$. However, in figure 3 it appears that GFlowNet's distribution is different (the mode is higher and the distribution has wider tails). Does this just mean the MCMC method has not converged? Is there any way to compare against the \"true\" distribution (for instance running the MCMC method for much longer, or using very many random trajectories and importance sampling)?\n\nb I am currently having a bit of trouble understanding figure 4. On lines 330-336 the authors state that when sampling 300k molecules using random trajectories that they obtain over 100 molecules with scores above 8 (233 such molecules to be precise). Looking at figure 4 the top-100 average for MARS is below 8 after visiting 300k molecules. Does this mean that it is worse than random search? Is this expected?  \n\nc For the MCMC methods (e.g. MARS) did the authors consider thinning?\n\nd A related question. For GFlowNet I assume every state is valid? If so I was wondering if the authors ever considered counting the reward for every state visited by GFlowNet rather than just the terminal ones. In some ways can one think of the \"stop action\" as a learnable thinning function?\n\ne I apologize if I missed this but what is the initial state for the molecule tasks?\n\n\n# 4 Very Minor Comments\n\ni. Lines 105, 138 and elsewhere on page 3 the word \"surjective\" is used. I think \"non-injective\" is meant here instead...? \n\nii. I found the figure on the right below line 265 helpful for understanding the toy problem (so thank you to the authors for this). However, its axis is too small and unreadable -- this should be made larger. Similarly, Figure 7 in the appendix could be bigger (especially given that there are no space constraints in that section).\n\niii. There are some weird black horizontal-ish lines on Figure 3 (at least on the two pdf viewers that I tried). Did these mean anything or were they just some left over plotting artifacts?\n # 5 Limitations\n\na. The authors briefly talk about the limitations of the approach in section 5. The main limitation they draw attention to is the challenge of moving closer to the local maxima of the reward function in the latter stages of optimization. To resolve this they discuss combining their method with local optimization techniques; however, I wonder whether the temperature approach they discuss in the earlier part of their paper (combined with some annealing scheme) could also be used here?\n\nb. One limitation the authors do not mention is how the method scales in terms of the size of the state and action space. The loss function requires for every current state the sum over all previous states and actions that may have led to the current state (see term 1 of Eq.9). I assume this may become intractable for very large state-action spaces (and the flows one is trying to model become very small). Can one approximate the sum using a subset? Also what about continuous state/action spaces?\n\n\n\n\n# 6 Societal impact\nThe authors state that they foresee no negative social impacts of their work (line 379). While I do not believe this work has the potential for significant negative social impact (and I'm not quite sure if/how I'm meant to review this aspect of their work), the authors could always mention the social impact of increased automation, or the risks from the dual use of their method, etc.\n"], "review_score_variance": 0.1875, "summary": "After an extensive back-and-forth discussion, all reviewers felt positively about the paper and its contribution, and lean towards acceptance.\n\nHowever, there were issues regarding clarity in the submitted draft — this has been largely addressed through clarifications from the authors, but it is essential that the discussion here is incorporated into the final version of the manuscript (and in particular, the responses to reviewer 1AuD).", "paper_id": "nips_2021_Arn2E4IRjEB", "label": "train", "paper_acceptance": "accept", "anchored_texts": "After an extensive back-and-forth discussion, all reviewers felt positively about the paper and its contribution, and lean towards acceptance.\n\nHowever, there were issues regarding clarity in the submitted draft — this has been largely addressed through clarifications from the authors,"}
{"source_documents": ["Recurrent neural networks (RNNs) allow an agent to construct a state-representation from a stream of experience, which is essential in partially observable problems. However, there are two primary issues one must overcome when training an RNN: the sensitivity of the learning algorithm's performance to truncation length and and long training times. There are variety of strategies to improve training in RNNs, the mostly notably Backprop Through Time (BPTT) and by Real-Time Recurrent Learning. These strategies, however, are typically computationally expensive and focus computation on computing gradients back in time. In this work, we reformulate the RNN training objective to explicitly learn state vectors; this breaks the dependence across time and so avoids the need to estimate gradients far back in time. We show that for a fixed buffer of data, our algorithm---called Fixed Point Propagation (FPP)---is sound: it converges to a stationary point of the new objective. We investigate the empirical performance of our online FPP algorithm, particularly in terms of computation compared to truncated BPTT with varying truncation levels. ", "I'm a bit skeptical of the results mentioned about PTB and Sequential MNIST.\n\nFor PTB as well as sequential MNIST, can authors report their baselines results (i.e when you do full back propagation ?). I just want to make sure, baselines are not \"faulty\". \n\nFor reference, authors can see result in Zoneout (https://arxiv.org/abs/1606.01305) or Recurrent Batch Normalization paper (https://arxiv.org/abs/1603.09025). Thanks.", "I would like to thank authors for their informative response, as well as for improving presentation in the paper. Some of my concerns regarding the baselines, however, remain.\n\nQuoting the authors' response: \n\n> In fact, FPP w/o state updating is like using T-BPTT: T steps of back-prop-through time are computed, without the state-loss and by starting from a given state (in this case, whatever the state was at that time).\n\nIn this is the case, I don't understand by FPP w/o state update is not considered as the main baseline, especially given that, quoting the paper, it is similar to a published \"Stored State T-BPTT\" method. The comparison between FPP and FPP w/o state update take place in separate figure and only using toy task. It is therefore very hard to understand which percentage of the improvement that FPP brings upon T-BPTT comes just from using the buffer, and not from updating the states in it. Besides, in Figure 6 cross-entropies for StochasticWorld are much higher than those in Figure 3(b). What's different between these experiments?\n", "Thanks for running those experiments. This definitely helps to improve the paper. ", "We would like to provide some preliminary results of UORO (memory-1 rank-1 UORO). We use the same experimental setting as Figure 3 in our paper. First, we found that UORO is less sample efficient than FPP. The first column of the table below shows the average performance over 5k steps for CycleWorld and 10k steps for StochasticWorld. The second column shows the average performance over 50k steps in both tasks. In CycleWorld, UORO does not learn well during the first 5k steps, while FPP can achieve reasonable accuracy (see Figure 3). We also found that UORO does not perform well in stochastic tasks (e.g. StochasticWorld).\n\n                               default steps    50k steps\nCycleWorld                  9.17%            3.70%\nStochasticWorld         0.663             0.662\n\nWe will update the results (including Sequential MNIST and PTB) into the final version of our paper.", "The pdf is updated now with the changes we mentioned in the comment! ", "\" [1] provides an interesting approach to obtain an unbiased approximation of the RTRL update, and will be added to the list of other such related approaches. [2] introduces a local, perhaps more biologically plausible variant of RTRL. However, [2] introduces some strong assumptions (like disregarding non-leading order terms, linearity of the RNN) in the analysis of their learning rule, and it is rather unclear if the update rule will lead to a stationary point of any objective. \"\n\nI agree with this.\n\n\"We also chose two real datasets with reasonably long dependencies back in time: Sequential MNIST and character-level prediction of the Penn Treebank. \"\n\nIts hard to actually argue that character level prediction task of PTB actually requires \"long-dependencies\". Probably a better task would be doing character level prediction for Text8 dataset.\n\n\"we thought of it as an apples-to-oranges comparison, UORO would nonetheless provide a baseline comparison to this other class of methods which would absolutely strengthen the results.\"\n\nI appreciate. :)", "We thank Reviewer 3 for pointing out the errata and clarity issues; we will make the necessary corrections. We will update the PDF by November 12. We will add the pseudocode into section 3 (with mini-batch processing), we will make the legend in figure 4 consistent, we will improve the explanation of non-overlapping T-BPTT in section 5, and we will explain the CycleWorld environment in section 5. \n\nThe main concern of Reviewer 3 was about baselines. We would like to clarify that our problem setting is online prediction, where data constantly arrives in a stream, rather than offline training from a fixed batch of data. The T-BPTT and no-overlap T-BPTT algorithms are adapted to our online problem setting: where only the loss from the last time step are back-propagated, which is standard for training RNNs online. Even though language modeling can often be done offline, it nonetheless serves as a useful realistic task to evaluate FPP and other algorithms under online training. \n\n> ”A very interesting and absolutely necessary baseline is FPP without state updates, but for such a baseline the loss comparing s_t and s_{i-T} should be disabled. Was this done?”\nYes, this was done. “FPP without State Updates” does not include the quadratic penalty term that compares \\hat{s}_i and s_{i - T}. \n\n>”the paper must clear show that updating the states in the buffer allows to get same performance with smaller T, compared to the best possible baseline that also uses the buffer but does not update states in it.”\nWe agree. We do believe the comparison to FPP w/o state updating provides this role. Potentially the baseline Reviewer 3 feels is missing is running T-BPTT on the buffer, as if it was a fixed dataset (i.e., offline). In fact, FPP w/o state updating is like using T-BPTT: T steps of back-prop-through time are computed, without the state-loss and by starting from a given state (in this case, whatever the state was at that time). We in fact tried a few other strategies of starting from random states rather than stored states, or periodically updating states in the buffer so that start states were less arbitrary; these choices did not improve performance. It is of course possible that another approach using a buffer, that does not update (or even store) state variables, could be developed that outperforms FPP. However, such an approach is not obvious and would be a novel algorithm. We cannot and do not claim to have definitely demonstrated that one must maintain and update state variables. We do nevertheless claim that the baseline of FPP w/o state updating provides evidence of the importance of state updating. \n\nThere are a couple of other comments about future directions, which we appreciate! One of our goals is to take advantage of parallelization with FPP, and so Reviewer 3’s point about increasing B is well-taken. Reviewer 3 also raises an interesting point about updating intermediate states. This is equivalent to setting T=1 and updating consecutive transitions in one mini-batch. There are multiple ways to sample transitions (e.g. sampling consecutive transitions, prioritized sampling or uniform sampling) and perform updates (e.g. choice of T, B and M), given the sound approach for training an RNN with a buffer. These are promising strategies to try, but are additional after first understanding the basic idea; we therefore picked the simplest strategies to show in this paper. A next step is to further investigate how much we can improve performance by more effective sampling approaches, and by further investigating the effects of T, B, and M. ", "We would like to thank Reviewer 1 for their valuable comments and literature suggestions. We will add discussion of the two referenced papers into our work. [1] provides an interesting approach to obtain an unbiased approximation of the RTRL update, and will be added to the list of other such related approaches. [2] introduces a local, perhaps more biologically plausible variant of RTRL. However, [2] introduces some strong assumptions (like disregarding non-leading order terms, linearity of the RNN) in the analysis of their learning rule, and it is rather unclear if the update rule will lead to a stationary point of any objective. \n\nSimulation Problems: There are potentially two concerns here. The first is the reasons for selecting the tasks, and whether they really test if FPP can perform well. The second is understanding the performance, relative to baselines other than T-BPTT.  We address both below.\n\nOur primary goal in selecting tasks was to allow a controlled experiment where we changed the length of dependencies, to investigate the ability of FPP to capture long-term dependencies in comparison to T-BPTT, on both simpler domains with relatively fewer confounding factors and on realistic datasets. We chose two synthetic problems and two real datasets towards this goal. In CycleWorld, the ability to predict the observation bit is directly linked to how far back the agent is able to remember. StochasticWorld is one level more sophisticated: the agent is required to remember two independent observations from the past which probabilistically influence the present prediction target. \n\nWe also chose two real datasets with reasonably long dependencies back in time: Sequential MNIST and character-level prediction of the Penn Treebank. Our character-level prediction task is similar to the character-level prediction task in the UORO paper, but is performed on a real instead of synthetic dataset. We also chose these two because they are common datasets for testing RNNs (see the citations of the Real Datasets subsection in section 5).\n\nFor the second potential concern, we agree that UORO would be useful  as a baseline for a completely different approach,  based on RTRL. We will first justify why we did not include initially, and then detail our current efforts in implementing the UORO baseline. We had focused our empirical investigation on T-BPTT for two reasons. First, the computation can be made comparable between T-BPTT and FPP, and the methods are similar in their simplicity. One of our primary goals is to develop simple approaches to train RNNs. Second, T-BPTT remains the standard algorithm for training RNNs, and with sufficiently large T can perform very well. For this reason, we could simply increase T to ensure we reached good performance and then compare for that T and smaller. UORO, and other algorithms that approximate RTRL, are typically more complicated and expensive;they are also relatively new and so not yet as standard. \n\nHowever, though we thought of it as an apples-to-oranges comparison, UORO would nonetheless provide a baseline comparison to this other class of methods which would absolutely strengthen the results. We are currently implementing the UORO baseline; we will attempt to include it in the revision for the author rebuttal period, and if we cannot finish it in time will include it in a final paper.  \n\n[1]: Mujika et al., “Approximating Real-Time Recurrent Learning with Random Kronecker Factors”\n[2]: Murray, “Local online learning in recurrent networks with random feedback.” \n", "We would like to thank Reviewer 2 for the comments and questions. The biggest concerns seem to be about (1) the strength of the theoretical results, (2) the choice of the key hyperparameter of FPP, and (3) comparing with the strongest baselines in the field. \n\n(1): It is in general difficult to make claims about the quality of an arbitrary stationary point; this difficulty also applies to the vanilla RNN objective. We do partially characterize the stationary points of the FPP objective in terms of recovering stationary points of the RNN objective, in Theorem 2. There are also some theoretical and empirical results on the ability of SGD to converge to local minima (for example, [1, 2]), and avoid getting stuck in saddle points. Because we use SGD, we can rely on such results to suggest we may similarly converge to local minima. The current theory, though, focuses on the first key claims: that the algorithm converges to stationary points (which is non-trivial and is not obviously true of the T-BPTT algorithm) and that there is a connection between the stationary points of FPP and the original RNN objective. \n\n(2): In the Appendix, we plot the sensitivity curve for lambda (Fig. 6). This figure shows that FPP is not particularly sensitive to lambda, though it does also indicate that we could have tuned lambda and gotten even better performance. We opted to show performance in the main body for this default value of lambda = 1 across all experiments, which we actually chose before even seeing these sensitivity plots. In general, the performance of FPP was insensitive to lambda across all our experiments. This is one of the benefits of FPP: we do not require extensive hyperparameter tuning is and yet we achieve consistent stability benefits compared with T-BPTT. We will refer more explicitly to these sensitivity curves in the main body to clarify this point. \n\n(3): We do not claim to introduce a model that outperforms the state-of-the-art on any particular dataset (e.g. on GLUE). SOTA performance requires SOTA algorithms, SOTA meta-parameter tuning,  SOTA implementations, and at times specialized hardware. This paper is about a new algorithm for training RNNs. Based on our results we expect many high-performance systems could be built on top of FPP (perhaps even SOTA on some of these data-sets). But this is well beyond the scope of this paper. Think of this paper as introducing a new algorithm, neither a complete learning system nor a SOTA claim. Certainly there is room for all three types of papers in ICLR as their contributions are very different. Our main contribution was to introduce a novel approach for RNN training and show that it is more robust than T-BPTT. Future work in adapting FPP to more modern RNN architectures would be interesting, but is not in the scope of this paper. \n\n[1] Choromanska, et al., “The Loss Surfaces of Multilayer Networks”\n[2] Lee et al., “Gradient Descent Only Converges to Minimizers”\n", "\nBackground: The authors consider the problem of training RNNs in an online fashion. The authors note that RNNs are trained using BPTT, which prevents them from being trained in an online fashion.  There have been various approximations which has been proposed which are based on RTRL or approximations to RTRL, as current approximations based on RTRL has high computational complexity. \n\nProposed Method: The authors propose to learn the state of the RNNs explicitly by improving the prediction accuracy at each time step as well as predicting the \"next\" state of the RNN. The authos note that the constraint of predicting the next state  is a fixed-point formula for the states underthe given RNN dynamics.\n\nClarity of the paper: The paper is clearly written. \n\nRelated work : Most of the relevant related work has been covered in the paper and discussed. I like it. These two related work could also be cited. Here, authors approximate the RTRL with random kronecker factors.\nhttps://papers.nips.cc/paper/7894-approximating-real-time-recurrent-learning-with-random-kronecker-factors\nhttps://www.biorxiv.org/content/10.1101/458570v1\n\nExperiment section: The authors evaluate the proposed method on both synthetic as well as real experiments. \n\nSimulation Problems: The authors use simulation problems to note the robustness of the proposed method to increasing termporal delay in online learning. These tasks show the soundness of the proposed method. Its actually difficult to tell how the proposed method is performing because of the selection of tasks. It might be more interesting to choose same tasks as in UORO paper (https://arxiv.org/abs/1702.05043) and it could be another \"baseline\" for the proposed method.\n\nAblations: I liked the fact that the authors consider conducting experiments without state updating, as it could also be  due to using a large buffer rather than explicitly optimizing for the prediction objective.\n\n Positive: The proposed method could be interesting for learning the state representation for policy gradient RL methods (specifically POMDPs) as the proposed method can leverage use of mini-batches, as well as multiple-updates which is a crucial ingredient to make best use of data collected by the agent interacting with the environment. \n", "The paper proposes an alternative to the truncated back-propagation through time (BPTT) algorithm for training RNNs. An online setting is assumed, which I understand as training RNN as data arrives and not storing too much of the arriving data (although notably the proposed method uses a buffer). The proposed Fixed-Point Propagation algorithm works as follows. It maintains a buffer of the last N RNN states that can be updated. From this buffer at every time step it samples two states that are T steps apart from each other (s_i and s_{i - T}). The RNN is run for T steps starting from s_{i - T}. A loss function is constructed that takes into account the output loss at time i as well as the mismatch between s_i and the new state constructed by running the RNN. The states s_i and s_{i-T}, as well as the RNN parameters are updated based on this loss function. \n\nThe novel idea of the paper is therefore a modifiable state buffer for the RNN states. The goal is better computational efficiency than that of T-BPTT. \n\nThe paper is mostly clearly written, but I think it is absolutely necessary to move Algorithm 1 to the main text, as well as to add the mini-batch processing (B) and multiple updates (M) to it. This pseudocode was very instrumental for me to understand the algorithm. I confess that I did not read the theory; I don’t think it’s super relevant because in practice convergence to fixed-point will require too many updates. \n\nThe empirical comparison with T-BPTT is substantial, but the waters are muddied a bit by imprecise presentation of baselines. For example, when T-BPTT is used for e.g. language modelling, it doesn’t make sense for back-propagate the loss from only the last time step, losses from all time-steps can be back-propagated together. Was this done in T-BPTT and/or FPP? Does T-BPTT use the 100 step buffer somehow? NoOverlap T-BPTT is not explained very well. A very interesting and absolutely necessary baseline is FPP without state updates, but for such a baseline the loss comparing s_t and s_{i-T} should be disabled. Was this done?\n\nIn short, the paper must clear show that updating the states in the buffer allows to get same performance with smaller T, compared to the best possible baseline that also uses the buffer but does not update states in it. I am not sure this case is clearly made at the moment.\nOne further direction authors could explore is that using a very small T but larger B could be more computationally inefficient because parallel computations would be used instead of sequential ones. Besides, from a practical viewpoint and I think it could make sense to also update intermediate states, and not just s_i and s_{i-T}. \n\nOther remarks:\n- the legend in Figure 4 is a dashed line, but the curves in the plots are dotted\n- the max. cycle length in CycleWorld is not clearly explained in the text, the name CycleWorld is not properly introduced\n", "In this paper, the authors reformulate the RNN training objective to explicitly learn the state vectors, and propose an algorithm called Fixed Point Propagation (FPP Algorithm 1). The authors motivate the derivation of FPP in Section 3, provide some theoretical convergence results in Section 4, and demonstrate experiment results in Section 5.\n\nIn general, this paper is interesting and well written. The experiment results in Section 5 seem to be very strong. However, I am not familiar with the relevant literature, thus, I am not sure if the authors have compared with the strongest baselines in this field. \n\nI think the paper suffers from the following limitations:\n\n1) Theorem 1 shows that the FPP algorithm converges to a stationary point. However, this result seems to be too weak. Can we say something about the stationary point? Is it a local minimum under some conditions?\n\n2) In the experiments, the authors choose \\lambda=1. My understanding is that \\lambda is a key meta-parameter of FPP. Please do a better job in justifying this choice."], "review_score_variance": 2.0, "summary": "The paper proposes an alternative to BPTT for training recurrent neural networks based on an explicit state variable, which is trained to improve both the prediction accuracy and the prediction of the next state. One of the benefits of the methods is that it can be used for online training, where BPTT cannot be used in its exact form. Theoretical analysis is developed to show that the algorithm converges to a fixed point. Overall, the reviewers appreciate the clarity of the paper, and find the theory and the experimental evaluation to be reasonably well balanced. After a round of discussion, the authors improved the paper according to the reviews. The final assessments are overall positive, and I’m therefore recommending accepting this paper.", "paper_id": "iclr_2020_SJgmR0NKPr", "label": "val", "paper_acceptance": "accept-poster", "anchored_texts": "The paper proposes an alternative to BPTT for training recurrent neural networks based on an explicit state variable, which is trained to improve both the prediction accuracy and the prediction of the next state. One of the benefits of the methods is that it can be used for online training, where BPTT cannot be used in its exact form. Theoretical analysis is developed to show that the algorithm converges to a fixed point. Overall, the reviewers appreciate the clarity of the paper, and find the theory and the experimental evaluation to be reasonably well balanced. After a round of discussion, the authors improved the paper according to the reviews. The final assessments are overall positive,"}
{"source_documents": ["State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss. However, the cross-entropy loss has several shortcomings that can lead to sub-optimal generalization and instability. Driven by the intuition that good generalization requires capturing the similarity between examples in one class and contrasting them with examples in other classes, we propose a supervised contrastive learning (SCL) objective for the fine-tuning stage. Combined with cross-entropy, our proposed SCL loss obtains significant improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in few-shot learning settings, without requiring specialized architecture, data augmentations, memory banks, or additional unsupervised data. Our proposed fine-tuning objective leads to models that are more robust to different levels of noise in the fine-tuning training data, and can generalize better to related tasks with limited labeled data.", "This work adds a SCL (supervised contrastive learning) loss term during the fine-tuning stage of RoBERTa. The results show that the model with the SCL term and cross-entropy (CE) achieve better GLUE scores than the classic baseline that only uses CE loss, especially when the numbers of supervised training data are small and the data is noisy. \n\nPros:\nThe method is simple and the improvement looks significant under various settings\n\nCons:\nIt is not very clear whether and why SCL loss improves the results (see the detailed comments below).\n\n\nClarity: \nThe text is fluent and the paper cites lots of related work, but the paper does not well explain why the method performs well.\n\nOriginality: \nThe SCL is not novel because it comes from computer vision but this is the first paper I have seen that successfully applies SCL in NLP tasks.\n\nSignificance of this work:\nIf the authors can really show that the improvement comes from SCL, it may become a popular tool in the fine-tuning stage.\n\n\nIt is possible that the source of improvement comes from the temperature tau and l2 normalization instead of SCL loss itself. Both of the tricks could be also applied to CE loss. Thus, could you do control experiments that replace the SCL loss with the CE loss but keeping l2 normalization and retune the tau and lambda. You can report it as CE+CE.\n\nAs shown in Figure 1, the authors suggest that the main reason that SCL loss is better because SCL loss tends to encourage the samples belonging to the same class. However, I believe that CE loss could achieve the same goal and maybe more aggressively than SCL loss. My understanding is that while minimizing CE loss, we encourage each class embedding close to all its samples, so the class embedding tends to become the cluster center of all the points in the class. In the meanwhile, we encourage the samples in each class close to its class embedding, so the samples within the same class will also become closer together, right (e.g., in Figure 3, CE loss could also separate the two classes with a large margin)? The difference is that CE loss encourages the samples close to the average of the samples in the same class, but SCL loss encourages the samples close to each sample in the same class. From this perspective, could you tell us why SCL loss is better (or tell me why this perspective is wrong)? In the introduction, you cite several studies that mention the limitations of cross-entropy loss. I think the motivation of the paper will be much stronger if the authors could briefly and intuitively introduce why CE loss is worse (e.g., what does it mean poor margins and why CE leads to them) and why SCL loss could fix that issue. It would be even better if you can design experiments to demonstrate that (e.g., measure and compare the margins of CE+CE with CE+SCL).\n\nThe effect of hyperparameters on performances is not clear. You mention that lambda is tuned in each downstream task. It is possible that in many applications, lambda is 0. Could you show the lambda value for each downstream application? In addition, could you show the final tau value(s) as well? In the experiment setup, you mention that you report the best model out of 10 seeds. Could you also report the average performance? This will tell us more about how stable this SCL method is. In Table 3, could you also report CE only and CE+CE performance?\n\nI believe the issues could be resolved by conducting more controlled experiments and analyses, so I currently vote weak accept. If the concerns are not addressed well during the rebuttal period, I am very likely to change my vote to rejection.\n\nMinor:\n1. In equation (2), I think putting CE loss for multi-class classification would be more general.\n2. Before the \"Relationship to Self-Supervised Contrastive Learning\", you mention lower temperature creates harder negatives. The meaning of negative here is unclear. I think you mean harder examples here.", "By comparing CE+CE with CE+SCL, the authors confirm that the advantage of SCL in few-shot learning settings, so I will change my vote to acceptance. \nAs for the presentation, I still think the revised version still does not intuitively explain why SCL is better than CE. The authors mentioned that previous work shows that CE loss has poor generalization performances in the paper and above response, but I think why that is the case and why SCL can solve that issue to achieve a better generalization performance should be the core of this paper. In the response, the authors seem to claim that CE cannot separate the classes as well as SCL due to CE’s projection step to logits, but why is the projection bad in few shot settings and not so bad when we have many training samples? \nIf the authors cannot intuitively explain these questions, it might mean that the authors haven't understood the reasons behind the observed improvement well enough, which will limit the impact of this work significantly.\n", "We thank all the reviewers for their valuable time and insightful feedback. We are encouraged that they find our results significant (R1), encouraging (R3), and valuable (R4), particularly in the few-shot learning scenario (R2, R3) and robustness across augmented noisy training set scenario (R2); think it has the potential to become a popular tool in the fine-tuning stage of pre-trained language models (R1); find our proposed way of analyzing model robustness interesting (R4); and overall find the paper well-written (R1, R2, R4). Based on the feedback of all reviewers, we have repositioned our work as a solution for low-data regimes instead of a general fine-tuning solution. We address the concerns that are shared across several reviewers below. \n\nSignificance of Results: R2, R3, and R4 all requested to see full dataset results that are more easily comparable to previous work. We updated our full dataset results shown in Table 5 such that they show the test performance on the original GLUE validation set, as presented in the original RoBERTa paper. [3] We sampled training and validation sets from GLUE’s original training set during fine-tuning. Also, R2 and R3 had concerns about the statistical significance of our experiments -- we revised our paper to include p-values for full dataset results in Table 5, and for few-shot learning results in Table 2.\n\nNovelty: R1 and R4 expressed concerns about the novelty of our work stating that supervised contrastive learning comes from computer vision. We consider Khosla et al. (2020) [1] and Liu et al. (2020) [2] concurrent work, and we cite both of them in our paper in related work. We also included a comparison with the method proposed in Khosla et al. (2020) based on the request of R4 in Table 8 in the Appendix for the full dataset results. As pointed out by both R1 and R4, we would like to emphasize that our work is the first successful application of supervised contrastive learning in the context of natural language processing to the best of our knowledge. In addition, unlike previous work in computer vision, our paper’s main focus is few-shot learning, along with robustness across augmented noisy training sets when we have few labeled examples.\n\nWe addressed reviewers’ individual comments including hyperparameter specifications (R1, R3), citations (R3), training speed (R3), and few other specific questions.\n\n[1] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. In NeurIPS, 2020.\n\n[2] Hao Liu and P. Abbeel. Hybrid discriminative-generative training via contrastive learning. ArXiv, abs/2007.09070, 2020.\n\n[3] Y. Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis,Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692, 2019.\n\n[4] Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Weinberger, and Yoav Artzi. Revisiting few-sample bert fine-tuning. ArXiv, abs/2006.05987, 2020.\n", "Thank you for your thorough and helpful feedback -- your suggestions helped us investigate our experimental settings further and strengthen our understanding.\n\nYou insightfully identified that improvement that comes from SCL over CE might be partly attributed to L2 normalization and the temperature scaling parameter tau. We conducted experiments to test this hypothesis and reported few-shot learning results in Table 10 and full dataset results in Table 8 as CE+CE. We observed that indeed CE+CE outperforms CE on the full data experiments as shown in Table 8.  CE+SCL still outperforms both CE and CE+CE on 4 out of 6 tasks, but only the MRPC and QNLI results are statistically significant. On few-shot learning experiments, CE+SCL outperforms both CE and CE+CE across all tasks and data regimes as shown in Table 10. In fact, CE+CE decreases the performance compared to CE for some settings such as the 100 labeled examples case in Table 10 for MNLI and QNLI. Overall, these experiments show that (i) L2 normalization and temperature scaling both matter more than we previously hypothesized, and (ii) few-shot learning improvements are more statistically significant than the full dataset improvements. We have repositioned the work as a solution for low-data regimes instead of a general fine-tuning solution as other reviewers suggested as well.\n\nYou made a comment on the intuitive difference between SCL and CE, stating that CE loss encourages the samples close to the average of the samples in the same class, while SCL loss encourages the samples close to each sample in the same class. We agree that both CE and SCL effectively push embeddings of the examples of the same label closer to each other. However, SCL explicitly pushes embeddings of the examples with the same label close to each other and examples with different labels further away by definition, without CE’s projection step to logits in the end. Hence, it encourages even smaller intra-class distance and larger inter-class distance, as we empirically demonstrate the difference in margins in Figure 3 for few-shot learning settings. Our robustness across augmented noisy training datasets experiments shown in Table 3 are motivated by the previous work that we cite in the introduction that suggested CE loss has poor generalization performance, and we empirically investigate whether SCL loss has improved robustness over CE loss.\n\nBased on your comment on \\lambda and \\tau, in Section 4.1, we discussed how we conduct hyperparameter search and reported hyperparameters for our best performing models for reproducibility. We revised the paper to report average test performance for the full dataset results in all tables, changed CE loss formula to multi-class classification, and included CE+CE results in our ablation for batch size and training speed in Table 9. As a clarification, by “hard negative”, we mean examples of different labels that are hard to separate. For concerns on the novelty of our paper, we would like to refer you to the novelty section in our general response to reviewers.\n", "Thank you for your helpful feedback and kind remarks on the presentation of our paper and our experimental results on few-shot learning. We revised the paper to include p-values for few-shot learning and full dataset experiments to address concerns about the significance of the results. We respond to individual questions below.\n\n“a) : Sampling of validation set and full dataset evaluation”: We previously sampled the validation set as described in order to be consistent with the few-shot learning results. However, as you and other reviewers rightfully pointed out, this raised concerns about the comparability of our method to the previous methods. We revised our full dataset results (Table 5) to show the test performance on the original GLUE validation set, as presented in the original RoBERTa paper. [3] We sampled training and validation sets from GLUE’s original training set during fine-tuning.\n\n“b): Top vs. average accuracy”: Our statement describes how evaluation numbers are reported for a fixed set of hyperparameters. We revised our paper to report the average test accuracies across 10 seeds instead of top model performance.\n\n“c) Relation between batch size and performance”: In our ablation experiment, our goal is to show that there is a correlation between performance and batch size. We revised our ablation study, Table 6, to include MNLI along with a comparison on training speed measured by average updates per second across different batch sizes.\n\n“d) Few-shot results are strong, while full dataset results are modest.”: We completely agree that SCL loss provides more statistically meaningful improvement in the few-shot learning setting compared to the full dataset setting. We unfortunately did not have enough time during the rebuttal period to compare to the other popular few-shot learning methods. We will investigate that direction in our future work -- thank you for your suggestion!\n", "Thank you for your positive comments on our few-shot learning results and your insightful feedback.\n\nBased on your suggestions, we updated our full dataset results in Table 5 such that they show the test performance on the original GLUE validation set, as presented in the original RoBERTa paper [3]. There might still be discrepancies between the original RoBERTa paper results and ours since (i) RoBERTa paper reports the median of 5 seeds while we report the average of 10 seeds, (ii) training and validation dataset samples are different, (iii) small datasets such as RTE, CoLA, and MRPC have inherently high variance during fine-tuning [4], and (iv) CoLA results are not comparable as RoBERTa paper reports Matthews correlation coefficient as their performance metric while we report accuracy.  Also, we included p-values for our few-shot learning experiments in Table 2 and for full dataset experiments in Table 5 to show the statistical significance of our results, and finally in Section 4.1 we discussed how we conduct hyperparameter search and reported hyperparameters for our best performing models for reproducibility. \n\nWe would like to clarify that supervised contrastive loss calculation is negligible given the rest of the model computation, when we are using a batch size of 16 as we do in all of our experiments with RoBERTa_Large due to memory constraints. To support this claim, we added a comparison to Table 6 that shows training speed measured by average updates per second across different batch sizes. Also, we fixed the references that use the arXiv version of published papers -- thanks for pointing that out!\n", "Thank you for your helpful feedback and kind remarks on our experimental results and the organization of our paper.\n\nYou rightfully point out that the supervised contrastive loss was previously proposed by Khosla et al. (2020), although our work is the first to apply to natural language processing tasks. We consider Khosla et al. (2020) as concurrent work, and we cite them in our related work. Based on your suggestion, we include a comparison with their two-stage training procedure for our full dataset experiments in Table 8 in the Appendix. We also would like to emphasize that our paper’s main focus is performance and robustness in the few-shot learning settings, unlike previous work where they have access to a lot of labeled data.\n\nAlso, we updated our full dataset results in Table 5 such that they show the test performance on the original GLUE validation set, as presented in the original RoBERTa paper. [3] This will hopefully make our work more directly comparable with the previous methods. We address your individual questions below:\n\n“Instead of using different original sentences for each T value, it is clear and compact to use the same original sentence for each T value.”: We use the same original sentences for each T value for our robustness experiments presented in Table 3. We include different original sentences in Table 4 to show different qualitative examples.\n\n“Why did you \"sample ... taking the label distribution of the original validation set into account\" in Section 4.1? I am worried that this sampling procedure may make the few-shot task easier.”: We sampled taking the label distribution into account in order to keep the data distributions consistent with the original train and validation datasets. We would appreciate any additional insight on why you think this sampling procedure might make the few-shot task easier.\n", "The paper proposes a new training objective for fine-tuning pre-trained models: a weighted sum of the classical cross-entropy (CE) and a new supervised contrastive learning term (SCP). The latter uses the (negated) softmax over the embedding distances (i.e. dot products) between a training instance and all other instances in the batch with the same label. In contrast to the more traditional self-supervised contrastive learning (where positive pairs are obtained by applying transformations to the original data instance), there is no data augmentation; two examples with the same label constitute a positive pair.\n\nExperiments on the GLUE benchmark compare the baseline (RoBERTa-Large with CE loss) against the proposed objective (RoBERTa-Large with CE+SCP loss). There are 4 sets of experiments:\n1) When training on the full datasets, results are quite modest (+0.4 increase in accuracy on average over 6 GLUE tasks).\n2) In the few-shot setting, CE+SCP does meaningfully better than the baseline (for instance, when fine-tuning on only 20 data points, CE+SCP improves accuracy by more than 10%); these gains decrease as the dataset size increases.\n3) When the datasets are noisy (effect obtained via back-translation), CE+SCP shines again (for instance, when the degree of corruption is very high, MNLI accuracy goes from ~47% up to ~53%).\n4) Finally, the authors look at domain shift; they fine-tune a model on SST-2, then apply few-shot learning on other sentiment classification datasets. This set of experiments has quite high error margins, so I didn't find it as convincing as 2) and 3).\n\nHere are some questions/suggestions for the authors regarding their experiments:\n\na) \"In all our experiments, [...] we sample half of the original validation set of GLUE benchmark and use it as our test set, and sample ~500 examples for our validation set from the original validation set [...]\" -- Evaluating the models on a *subset* of the validation set makes it harder to compare it against other papers that fine-tune RoBERTa-Large. I think that, at least for Table 2, it would be useful for posterity if you could either i) get the true test scores from the GLUE server, or ii) use part of the training set for validation, and then test on the full dev set, which is more standard practice.\n\nb) \"We run each experiment with 10 different seeds, and pick the top model out of 10 seeds based on\nvalidation accuracy and report its corresponding test accuracy\" -- I am assuming this statement describes how evaluation numbers are reported for a fixed set of hyperparameters. Why do you choose to pick the *top* model as opposed to reporting the *average* accuracy across the 10 runs?\n\nc) \"we observe that our proposed method does not lead to improvement on MNLI [...]. We believe\nthis is due to the fact that number of positive example pairs are quite sparse [...] with batch size 16 [...]. We show evidence for this hypothesis in our ablation studies that we show in Table 3\" -- Then why doesn't Table 3 include MNLI? Am I missing something?\n\nd) This method excels in the few-shot setting, at least compared to the CE baseline. So I think it would be a lot more impactful to focus on this particular use case and convince the reader that CE+SCP is better than some other standard few-shot learning baselines (e.g. meta-learning objectives). I do appreciate that the current message of the paper is crystal-clear (adding a SCP term to the loss leads to better fine-tuning), but I also think that the results in Table 2 are too weak for this somewhat general statement. There is quite a bit of real-estate in the paper that could be re-allocated to something more substantive (e.g. Table 1).\n\nStrengths:\n- The presentation of the paper is extremely clean, and the goal is clear.\n- In the few-shot learning scenario, CE+SCP performs meaningfully better than the CE baseline.\n\nWeaknesses:\n- The main weakness is related to my suggestion d) above. I believe marketing CE+SCP as a general fine-tuning solution with somewhat underwhelming results in Table 2 is a missed opportunity to lead with potentially strong results on few-shot learning. I'm calling the results \"underwhelming\" because there is evidence that a thorough hyperparameter sweep can boost fine-tuning accuracy on GLUE by quite a bit. For instance, Dodge et al. [1] show that fine-tuning BERT carefully can increase SST-2 accuracy by ~2% without any changes in the pre-trained model or fine-tuning objective.\n\n[1] Dodge et al., Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping", "The paper proposes using a combination of two losses in the fine-tuning stage when using a pre-trained model: the standard CE one, plus a supervised contrastive loss SCL (combined linearly via a \\lambda hyperparameter). The supervised contrastive loss uses a normalization summation over the batch examples, with a temperature hyperparameter \\theta.\n\nThe empirical results cover nicely three scenarios: (a) the impact of adding the SCL loss, in the presence of all the fine-tuning data; (b) the impact of adding the SCL loss in few-shot learning scenarios; and (c) the impact of SCL in the presence of training noise (induced via back-translation through German, using a standard WMT-trained MT model).\nThe results as presented are encouraging, and support the main hypothesis of the paper, namely that training with the added SCL loss improves performance over all three scenarios mentioned above.\n\nI have a few suggestions that could be seen as minor, and a few observations that are major.\nMinor ones:\n(i) adding the SCL loss clearly impacts the training speed, yet there is no mention of that (especially as a function of the batch size); in particular, Table 3 would offer the perfect place for mentioning how the training time (for a fixed number of training steps) is affected by the increase batch size, so that the reader can understand both the “upsize” (the improved performance)  as well as the “downsize” (training cost).\n(ii) I could see no mention of the settings for the hyperparameters used (\\lambda and \\theta), nor any ablation experiments that would indicate how their values have been chosen; in the interest of both reproducibility and increased understanding of the value of SCL, please add a sub-section that discusses this issue.\n\nMajor ones:\n(i) It appears that the authors have used the Roberta_large model and run their own experiments with fine-tuning, w/o and w/ the SCL loss, with little regard for reporting against the published numbers for the GLUE tasks; for instance, in Table 2, they report with CE-only to have RTE performance at 85.0, while the Roberta paper shows 86.6 for it; in this case, the CE+SCL at 85.6 no longer looks like a convincing win; a bit different but nevertheless troublesome is the result reported for CoLA, at 86.4 (CE-only); the numbers for CoLA are normally much lower than that, eg the Roberta paper shows 68.0 (CE-only); this disparity throws a lot of doubt over the accuracy of the results reported in Table 2.\n(ii) I commend the authors for showing the variance across their results in both Table 3, 4, and 5; however, it is unclear to me that the claims that the CE+SCL approach is better are being supported by the results. It is not like the CE+SCL gives lower variance, as it is clearly the case that sometimes it is higher than the CE alone; and the fact that, under this high variance, the CE method sometimes performs better CE+SCL puts the whole conclusion under doubt from an empirical standpoint.\n\nOne one result that seems to hold strong is the one for few-shot learning, which appears to support the main hypothesis of the paper. However, the main issues mentioned above would need to be addressed in order to have the paper reach the level of clearing the bar for ICLR publication.\n\n\nRe: References A lot of the references use the Arxiv version for papers that have been peer-reviewed and published. Please fix.\n", "Summary\n* For the fine-tuning of pre-trained language models, the authors proposed a supervised learning method that combines cross-entropy loss and contrastive loss.\nExperimental results show that the proposed method improves over cross-entropy loss on several classification tasks of the GLUE benchmark set.\nThe proposed method outperforms cross-entropy loss in few-shot learning tasks and noisy datasets generated by English-German and German-English translation.\n\nStrong points\n\n* The proposed loss function is reasonable and the effect of supervised contrastive learning was not reported for NLP applications before, the experimental results are valuable.\n* The paper is well-organized and well-written.\n* Without using extra datasets for fine-tuning, the proposed method consistently improves the baseline method.\n* The generation of noisy examples using the back-translation model in Section 5.3 is an interesting approach to analyze model robustness.\n\nWeak points\n* Although the supervised contrastive learning term as previously proposed in (Khosla et al. 2020), it is not cited in the section.  \n* The benchmark results in Table 2 are not comparable with conventional methods since the experimental setting does not follow the finetuning procedures from prior work (Devlin et al., 2019) which reports the test set performance obtained from GLUE submissions.\n\nDecision reason\n* The technical contribution of this paper is limited since the proposed method is a rather strait-forward expansion of Khosla et al. 2020.  In addition, although it is novel to apply supervised contrastive learning for NLP applications, the impact of these results is also limited because the experimental results are not directly comparable with previous work.\n\nQuestions\n* Why did you \"sample ... taking the label distribution of the original validation set into account\" in Section 4.1?   I am worried that this sampling procedure may make the few-shot task easier.\n\nAdditional Feedback\n* Since the subtraction between two values in percentage is not a ratio,    the percentage is not an appropriate unit for it.  For example \"1.2% improvement on SST-2\" in Section 5.1 should be \"1.2 point improvement on SST-2\".\n* Since Khosla et al. 2020 proposed a two-stage training procedure, supervised contrastive learning at the first stage and the learning of the output layer at the second stage, I would like to see the qualitative comparison with the proposed joint training procedure.\n* Instead of using different original sentences for each T values, it is clear and compact to use the same original sentence for each T values."], "review_score_variance": 0.5, "summary": "This paper introduces supervised contrastive learning loss on top of typical cross-entropy loss for fine-tuning language model for downstream tasks. While the idea is simple and has been used in vision literature (as pointed out by R1 & R4), its application LM is first introduced in this paper. The experimental gain is small in the regular setting but clearer gains in a few-shot learning setting and noisy training dataset (through back translation) setting. Overall the paper is clearly written and experiments are carefully studied. During the discussion phase, the authors provided results on the full GLUE dataset as well as other ablation studies (e.g., CE+CE recommended by R2), improving the paper.  ", "paper_id": "iclr_2021_cu7IUiOhujH", "label": "test", "paper_acceptance": "poster-presentations", "anchored_texts": "This paper introduces supervised contrastive learning loss on top of typical cross-entropy loss for fine-tuning language model for downstream tasks. While the idea is simple and has been used in vision literature (as pointed out by R1 & R4), its application LM is first introduced in this paper. The experimental gain is small in the regular setting but clearer gains in a few-shot learning setting and noisy training dataset (through back translation) setting. During the discussion phase, the authors provided results on the full GLUE dataset as well as other ablation studies (e.g., CE+CE recommended by R2),"}
{"source_documents": ["The dependency of the generalization error of neural networks on model and dataset size is of critical importance both in practice and for understanding the theory of neural networks. Nevertheless, the functional form of this dependency remains elusive. In this work, we present a functional form which approximates well the generalization error in practice. Capitalizing on the successful concept of model scaling (e.g., width, depth), we are able to simultaneously construct such a form and specify the exact models which can attain it across model/data scales. Our construction follows insights obtained from observations conducted over a range of model/data scales, in various model types and datasets, in vision and language tasks. We show that the form both fits the observations well across scales, and provides accurate predictions from small- to large-scale models and data.", "We would like the thank the reviewers for their helpful comments. We have updated the paper accordingly. Please see detailed responses in the individual comments on each review.", "I would like to thank the authors for updating the results of their error estimation experiments with 10 fold cross validation, which addressed my overfitting and uncertainty estimation concerns. I wish them the best of luck.", "Summary:\nThis paper proposes a functional form to model the dependence of generalization error on a held-out test set on model and dataset size. The functional form is derived based on empirical observations of the generalizing error for various model and dataset sizes (sections O1, O2, and O3) and on certain necessary criteria (C1, C4 and C5). The parameters of the function are then fit using linear regression on observed data. The authors show that the regressed function \\(\\epsilon(m,n)\\) is able to predict the generalization error for various \\(m\\) and \\(n\\) reasonably accurately.\n\nMajor Points:\n- While the current experiments are a good start, I do not think they are extensive enough to count as strong evidence for the  power-law form of \\(\\epsilon(m,n)\\). I would ideally like to see results on more optimizers, at the very least for Adam, even if for fixed hyper-parameters. As far as I understand, this involves only minor changes in the code since reasonable hyperparameters required for the convergence of Adam have been extensively studied. If the form still holds true then the results from this work can be more reliably used for small-scale network development and in making trade-off choices (as discussed in section 8).\n- Given the current form of the paper, the abstract and introduction should be modified to reflect the fact that only limited architectures and optimizers were experimented with, and the claims of the paper are not experimentally validated in general.\n\nMinor Points:\n \n- It would be nice if more network architectures were analysed (such as VGG and DenseNets). \n- It would be nice if different stopping criteria were analysed.\n- It would greatly benefit the reader if eq. 5 were expanded.\n \nOverall, I think this is a well written paper and provides good insight into the behaviour of the error landscape as a function of model and dataset size. The paper’s primary drawback is the restrictive setting under which the experiments are performed. Therefore, I am not convinced that the power-law form of the generalization error would hold when the experimental settings are marginally different (like when using the Adam optimizer or a VGG-like architecture). I think this work would have much greater impact if the authors can show that the power-law form holds for a larger variety of architectures and optimizers thus allowing researchers to more confidently incorporate the results of this work into the design and training deep neural networks.\n\nRebuttal Response\nI would like to thank the authors for their response. The results of additional experiments as described in Section 6.2 and in Figure 5 do indeed provide stronger evidence of the power-law form of the error function. In light of this, I have changed my original rating. \n\n", "Thank you for your thorough and helpful review. We also believe that the criteria we identified will be useful for others in narrowing the search for functions that approximate the generalization error of NNs in realistic settings with no access to the true data distribution. \n\nConcerns regarding overfitting and uncertainty estimation: Given your suggestion, we performed 10-fold cross validation in all tasks and found high quality results and cross-fold consistency. We now report updated cross-val for all results in section 6 including figures 3,4 and in the newly-added figure 5. We believe that this addresses both the overfitting concern and the uncertainty estimation concern. \nAdditional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer. \n\nRegarding the envelope function (equation 5): This form of function is a simple case of the (complex) rational function family (simple pole at $\\eta$, simple zero at the origin in this case). This family arises naturally in transitory systems in control theory and electrical engineering, e.g., when considering the frequency response of systems. It captures naturally powerlaw transitions. With that said, as we stress in the end of section 5, the particular choice of envelope is merely a convenience one and there may be other such functions / refinements. We leave further exploration of this aspect to future work. \n\nWe have fixed the misspelling in “differentiable”. Thanks for pointing this out.  \n", "Thank you for your review.\n\nWe are a bit surprised since the paper provides answers to the exact questions you raised as missing. We are sorry you missed it, and we have cleaned up the presentation so it is hopefully now clear that we do answer these questions and more. The answers, as you pointed out, were much desired and not known before. \n\nBelow are answers resultant from eq. 5 to the specific questions the referee raised, with some added definitions to make them concrete.\n\n1. “how deep should a model be for a classification or regression task? “\n\nWe show in section 6.1 that the dependency of the classification error on the number of layers is also well approximated by eq. 5 (recall $m$ scales linearly with depth).\n\nSo, if we consider some target error $\\epsilon_{target}$, we can solve eq. 5 for m or n given the other or for both, attaining the m,n contour for $\\hat{\\epsilon}(m,n) = \\epsilon_{target}$.\n\n\n2. “What is the minimum/maximum layers of a deep model? “\n\nFor a fixed dataset size, model scaling eventually contributes marginally to error reduction and becomes negligible when $bm^{-\\beta} \\ll n_{lim}^{-\\alpha}$ (Eq. 5).\n\nDefine the relative contribution threshold $T$ as satisfying $ T = \\frac{n^{-\\alpha} }{ bm^{-\\beta}}$. (For example, $T=10$.) Then the maximal useful model size meeting threshold $T$ is:\n$$     m_{max}(T) = \\left(bT\\right)^{1/\\beta} n_{lim}^{\\alpha/\\beta}  $$\n\nAs for minimal depth, here too let’s consider a definition as a working example: what is the minimum depth that could meet a certain error level $\\epsilon_{target}$ (if data is not a limit). \nFor example, when the target error is small relative to the “random guess error” $\\epsilon_0$ (equivalently when $ n^{-\\alpha} + bm^{-\\beta} \\ll \\eta$), by solving eq. 5 for $m$ we have:\n\n$$ m_{min} = \\left(\\frac{b}{\\frac{\\epsilon_{target}}{\\epsilon_0}\\eta-c_\\infty}\\right)^{1/\\beta} $$\n    \n3. “How much data is sufficient for a model to learn? What is the minimum/maximum size of the data set?”\n\nSimilarly to the above: \nMinimum data needed for target error (if model size is not a limit):\n$$ n_{min} = \\left(\\frac{1}{\\frac{\\epsilon_{target}}{\\epsilon_0}\\eta-c_\\infty}\\right)^{1/\\alpha} $$\n\n4. Maximum useful data (in the marginal sense $T$ for a limited size model, as above):\n$$n_{max}(T) = \\left(1/bT\\right)^{1/\\alpha} m_{lim}^{\\beta/\\alpha} $$\nIn particular, note that there is also a minimal amount of data and model size needed for better-than-random-guess error level, characterized by the location of the pole $\\eta$: $n^{-\\alpha}+bm^{-\\beta}< \\eta$\n\n5. “Do we really need a large data set or just a subset that covers the data distribution?”\n\nVia careful dataset sub-sampling (as noted by reviewer 3) we show that indeed more data *is* needed to improve performance (reduce error) while holding the class distribution fixed (in expectation), for a given architecture and scaling policy. For directly viewing the error manifolds decoupling the dependency on model and data size, see figure 1 and in appendix C. \n\n6. “What's the relation between the size of a model and that of a data set? “\n\nThe joint form in Eq. 5 captures the relation between data-size and model-size (and error) completely. \n\n7. “By increasing the depth/width of a neural network, how much new data should be collected for achieving a reasonable performance?” \n\nFor example, from Eq. 5, it is clear that a sweet-spot in terms of balancing the effect of the data/model sizes on limiting the error is $n^{-\\alpha} \\approx bm^{-\\beta}$ . \n\nWhen considering this sweet spot for example, increasing depth/width/both such that the model size $m$ is increased by a factor $f$ to a new size is $m’ = mf$, the corresponding increase in data maintaining the sweet-spot is $n’ = nf^{\\beta/\\alpha}$\n\n8. How about the gain of the task performance?”\n\nThe effect on the performance is given by evaluating Eq.5 for the initial and scaled $m,n$.\n\nFor example, in the powerlaw region ($c_\\infty \\ll n^{-\\alpha} + bm^{-\\beta} \\ll \\eta$):\nThe effect on the performance is $\\epsilon’ = \\epsilon f^{-\\beta}$ ", "Thank you very much for your thoughtful review.\n\nWe would like to point out that our experiments include multiple architectures (WRN and ResNet for image classification, LSTM and transformers for language modeling) and optimizers (SGD for image classification, SGD and Adam for language modeling). These were chosen according to standard implementations in the literature. \n\nHowever, we agree that it is important to demonstrate the results on a greater variety of architectures and optimizers and in particular in a manner that allows to assess the stability with respect to changing them for a specified task. Following your suggestion, we have therefore added experiments with both VGG and DenseNet, each trained with both SGD and Adam, on CIFAR100. The results conform with good agreement to the functional form defined in Eq. 5, with fit quality quantitatively very similar across all the architectures/optimizers settings in these experiments, and in particular reaching small divergences.  We added a new section (6.2) and figure (Fig. 5) for these experiments.  \n\nWe do believe that the variety of architectures/optimizers examined over a variety of tasks (extending to large datasets over both vision and language processing) in this study, augmented with the explicit additions following your valuable feedback, experimentally cover a meaningful chunk of settings, which supports our conclusions. We hope you will reevaluate the paper in light of these additions, and welcome any additional feedback.\n", "This work proposes a functional form for the relationship between <dataset size, model size> and generalization error, and performs an empirical study to validate it. First, it states 5 criteria that such a functional form must take, and proposes one such functional form containing 6 free coefficients that satisfy all these criteria. It then performs a rigorous empirical study consisting of 6 image datasets and 3 text datasets, each with 2 distinct architectures defined at several model scales, and trained with different dataset sizes. This process produces 42-49 data points for each <dataset, architecture> pair, and the 6 coefficients of the proposed functional form are fit to those data points, with < 2% mean deviation in accuracy. It then studies how this functional form performs at extrapolation, and finds that it still performs pretty well, with ~4.5% mean deviation in accuracy, but with additional caveats.\n\nDecision: Accept. This paper states 5 necessary criteria for any functional form for generalization error predictor that jointly considers dataset size and model size, then empirically verifies it with multiple datasets and architectures. These criteria are well justified, and can be used by others to narrow down the search for functions that approximate the generalization error of NNs without access to the true data distribution, which is a significant contribution. The empirical study is carefully done (e.g., taking care to subsample the dataset in a way that preserves the class distribution). I also liked that the paper is candid about its own limitations. A weakness that one might perceive is that the coefficients of the proposed functional form still needs to be fit to 40-ish trained NNs for every dataset and training hyperparameters, but I do not think this should be held against this work, because a generalization error predictor (let alone its functional form) that works for multiple datasets and architecture without training is difficult, and the paper does include several proposals for how this can still be used in practice.\n(Caveat: the use of the envelope function described in equation 5 (page 6) is not something I am familiar with, but seems reasonable.)\n\nIssues to address:\n- Fitting 6 parameters to 42-49 data points raises concerns about overfitting. Consider doing cross validation over those 42-49 data points, and report the mean of deviations computed on the test folds. The extrapolation section did provide evidence that there probably isn't /that/ much overfitting, but cross validation would directly address this concern.\n- In addition, the paper provides the standard deviation for the mean deviations over 100 fits of the function as the measure of its uncertainty, but I suspect that the optimizer converging to different coefficients at different runs isn't the main source of uncertainty. A bigger source of uncertainty is likely due to there being a limited amount of data to fit the coefficients to. Taking the standard deviation over the deviations measured on different folds of the data would be better measure of uncertainty.\n\nMinor issues:\n- Page 8: \"differntiable methods for NAS.\" differentiable is misspelled.", "This paper explores the relation among the generalization error of neural networks and the model and data scales empirically. The topic is interesting, while I was expecting to learn more from the paper, instead of some well-known conclusions. If the paper could provide some guidance for model and data selection, that would be an interesting paper for the ICLR audience.  For instance, how deep should a model be for a classification or regression task? What is the minimum/maximum layers of a deep model? How much data is sufficient for a model to learn? What is the minimum/maximum size of the data set? Do we really need a large data set or just a subset that covers the data distribution? What's the relation between the size of a model and that of a data set? By increasing the depth/width of a neural network, how much new data should be collected for achieving a reasonable performance? How about the gain of the task performance? "], "review_score_variance": 8.666666666666666, "summary": "The paper presents a very interesting idea for estimating the held-out error of deep models as a function of model and data set size. The authors intuit what the shape of the error should be, then they fit the parameters of a function of the desired shape and show that this has predictive power. I find this idea quite refreshing and the paper is well written with good experiments. Please make sure that the final version contains the cross-validation results provided during the rebuttal.", "paper_id": "iclr_2020_ryenvpEKDr", "label": "train", "paper_acceptance": "accept-poster", "anchored_texts": "estimating the held-out error of deep models as a function of model and data set size. The authors intuit what the shape of the error should be, then they fit the parameters of a function of the desired shape and show that this has predictive power. "}
{"source_documents": ["Being uncertain when facing the unknown is key to intelligent decision making. However, machine learning algorithms lack reliable estimates about their predictive uncertainty. This leads to wrong and overly-confident decisions when encountering classes unseen during training. Despite the importance of equipping classifiers with uncertainty estimates ready for the real world, prior work has focused on small datasets and little or no class discrepancy between training and testing data. To close this gap, we introduce UIMNET: a realistic, ImageNet-scale test-bed to evaluate predictive uncertainty estimates for deep image classifiers. Our benchmark provides implementations of eight state-of-the-art algorithms, six uncertainty measures, four in-domain metrics, three out-domain metrics, and a fully automated pipeline to train, calibrate, ensemble, select, and evaluate models. Our test-bed is open-source and all of our results are reproducible from a fixed commit in our repository. Adding new datasets, algorithms, measures, or metrics is a matter of a few lines of code-in so hoping that UIMNET becomes a stepping stone towards realistic, rigorous, and reproducible research in uncertainty estimation. Our results show that ensembles of ERM classifiers as well as single MIMO classifiers are the two best alternatives currently available to measure uncertainty about both in-domain and out-domain classe.", "The paper introduce a benchmark UIMNET to evaluate predictive uncertainty estimates for deep image clasifiers. The authors provides implements of ten state-of-the-art algorithms and six uncertainty measures with four in-domain metrics and three out-domain metrics. For Strengths, the paper provides a solid test-bed for evaluating the uncertainty estimation for deep image classifiers. Also, the paper is well-written and both the data construction and results part are clearly described.\nThe main weakness of the paper is that the uncertainty measurements in Section 4 lack the SOTA uncertainty estimation methods. For example, no method in Section 4 measures the epistemic uncertainty such as deep ensemble/dropout and aleatoric uncertainty [1] explicitly. Also, recent dirichlet-distribution-based approaches [2][3] are not included to measure the evidence-based uncertainty. \n\n[1] Kendall, Alex, and Yarin Gal. \"What uncertainties do we need in bayesian deep learning for computer vision?.\" arXiv preprint arXiv:1703.04977 (2017).\n[2] Sensoy, Murat, Lance Kaplan, and Melih Kandemir. \"Evidential deep learning to quantify classification uncertainty.\" arXiv preprint arXiv:1806.01768 (2018).\n[3] Charpentier, Bertrand, Daniel Zügner, and Stephan Günnemann. \"Posterior network: Uncertainty estimation without ood samples via density-based pseudo-counts.\" arXiv preprint arXiv:2006.09239 (2020).\n Overall, I think the paper is well-written and the benchmark is valuble as testbed for uncertainty estimation. But the uncertainty measurement part lacks some important uncertainty estimation methods, which are highly encouraged to be included in the paper.", "This paper proposes the UIMNET benchmark for uncertainty estimation. This benchmark includes (1) \"ImageNot\", a remix of ImageNet using hierarchical clustering of pairwise distances between features (2) framework for evaluating uncertainty estimation, including a suite of algorithms, metrics, and ablations studies. The authors report empirical results on their dataset using their suite, and make a number of recommendations based on these findings. Reproducible software is also provided. ##########################################################################\n\nPros: \n \n* Uncertainty estimation is an important topic.\n\n* I commend the authors for their efforts on reproducibility.\n\n##########################################################################\n\nCons: \n\n* The novelty of the research is low. The data source, algorithms, and metrics are all prior work. \n\n* What value will the community get from the release of ImageNot? The authors have already reported their results. Can the authors outline some additional use cases? Otherwise there is not much point in branding it as a \"new dataset\".\n\n##########################################################################\n\nQuestions during rebuttal period: \n\n\n* The authors claim in/out domain data should be defined from the same dataset, and that \"this provides realistic out-domain data\", as opposed to the alternatives (different datasets, augmentations). What is the justification for this claim of greater \"realism\"? \n \n* How extensible is the software framework to new datasets? What would I have to implement to run the framework on a new dataset? New empirical benchmark with reproducible software but with unclear research novelty and value to community. ", "This paper proposes a new testbed to test the uncertainty of image classifiers based on a split of ImageNet. Strengths:\n- The paper is well written.\n- The benchmarking is solid and very thorough. Many baselines and metrics have been implemented, and results are presented with confidence intervals.\n\nWeaknesses:\n- Overall, the major weakness is in the design of this benchmark. For a study claiming to be \"the most exhaustive uncertainty estimation benchmark,\" the only test-time distribution shift considered is the presentation of novel classes. The authors do not appear to justify this design decision anywhere in the paper. The authors make multiple appeals to \"providing realistic out-domain data,\" but there are many, many ways out-domain data can be presented in real world applications. This disjoint class assumption is very strong and is not clear why or when this is representative.\n- From reading about the construction of the benchmark, it's not entirely clear why BREEDS wasn't a suitable candidate and why something new had to be created. The authors state that the BREEDS task is to classify super-classes and so the label set remains fixed from train to test - that is true, but could the authors not have just used the same split and kept the leaf node original class labels instead of using the superclass labels?\n- The agglomerative clustering method used to construct the splits seems round-about. The ImageNet hierarchy contains a lot of metadata that could have been used to create the splits - do the authors have any comment on why this wasn't used? Also, the result of this split is that the train-test gap is over object classes to animal classes, which is perhaps (a priori) not as realistic of a setting as in the BREEDS dataset, where the train-test gap is between very similar class categories. This design decision was again never justified. In summary, this study has a large number of design flaws. However, given the benchmark, the authors have done a great job in thoroughly benchmarking existing algorithms."], "review_score_variance": 1.5555555555555554, "summary": "This paper introduces an ImageNet-scale benchmark UIMNET for uncertainty estimation of deep image classifiers and evaluates prior works under the proposed benchmark. Two reviewers suggest reject, and one reviewer does acceptance. In the discussion period, the authors did not provide any response for many concerns of reviewers, e.g., weak baselines, weak novelty, and lack of justification for the current design. Hence, given the current status, AC recommends reject.", "paper_id": "iclr_2022_f9AIc3mEprf", "label": "test", "paper_acceptance": "Reject", "anchored_texts": "This paper introduces an ImageNet-scale benchmark UIMNET for uncertainty estimation of deep image classifiers and evaluates prior works under the proposed benchmark. Two reviewers suggest reject, and one reviewer does acceptance. In the discussion period, the authors did not provide any response for many concerns of reviewers, e.g., weak baselines, weak novelty,"}
{"source_documents": ["Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr, Wendelin Boehmer, Shimon Whiteson", "This paper proposes a new multi-agent deterministic policy gradient method, called FACMAC, which is essentially MADDPG + QMIX. Although the combination is rather trivial, the author claims that they improve MADDPG in the sense that FACMAC optimise in the joint-action space rather than each individual agent's action space as MADDPG do, and they also claim that FACMAC improves QMIX by removing the monotonic constraints on the critic decomposition.   Overall, I have trouble agreeing with the many claims that authors make about the MADDPG and QMIX method. More importantly, the idea of FACMAC =  QMIX + MADDPG is trivial, from both a theoretical perspective and an empirical perspective. I have concerns on the two critical tricks that FACMAC introduce (i.e. the nonmonotonic critic decomposition (Eq. 4) and the policy gradient over joint action space (Eq. 7)). Moreover, I am not convinced why such a combination is an important direction and worth attention from the MARL community, neither does the reported empirical results support that. Therefore, I cannot recommend for acceptance at this stage. \n\n\n1. In line 11, the author mentioned \"FACMAC uses a centralised policy gradient estimator that optimises over the entire joint action space, rather than optimising over each agent’s action space separately as in MADDPG.\". \n\nI believe this shall not be considered as a benefit of FACMAC over MADDPG. The idea of MADDPG to learn an individual policy without the need to learn a big joint Q-network. Following this logic, the centralised Q-learning method should be used in all MARL problems. Also, I don't think it is fair to claim that MADDPG only optimises in each agent's action space, after all the critic value that guides the policy to learn is the centralised critic, so presumably, what MADDPG is doing is searching over a joint-strategy space. \n\nFurthermore, FACMAC cannot optimise in the entire joint action space as the author claim. The main trick that FACMAC introduces is through Equation 7 in which all agents share the same policy parameter (i.e., CPG trick), so updating one agent's policy naturally equals to updating multiple agents' policy. However, this trick only holds for the case when all agents share the same set of policy parameters. Although the author claims that it also appleis in the non-sharing cases, however, if agents each have different set of parameters for its policy, then Equation 7 needs to take derivative at the same time for each agent, which again loops back to the issue of each agent only optimise in its own action space.  And importantly, it is commonly known that taking simultaneous gradients will lead to convergence issue. [On Gradient-Based Learning in Continuous G] https://arxiv.org/pdf/1804.05464.pdf\nMoreover, the author also mentions in the line 187 that FACMAC-vdn is in fact optimising each agent's actions, which self-contradicts what the previous claims are about. \n\nThe effectiveness of CPG is also questionable. Although in the toy example in Figure 2a, the CPG works with MADDPG. However, this could be because of the different exploration strategies in such an toy example, as pointed out by [Multi-agent Soft Q-learning, AAAI]. I suspect even by changing the payoff value of the toy example, it would show a completely different results. Most importantly,   Later in Figure, 7, the CPG trick almost makes no difference whether or not it is activated. \n\n2. In line 7, the author says \"unlike QMIX, there are no inherent constraints on factoring the critic.\" In line 44, the author says \"solve some tasks that cannot be solved with monolithic, or monotonically factored critics.\" I am not convinced how using a neural network-based mixing network in Equation 4 achieve the claim of releasing the monotonic assumption in meeting the individual-global-max condition. To me, FACMAC, which is QMIX+MADDPG, cannot claim anything beyond what QMIX has claimed. \n\n3. I also have major concerns on the reported results.\n\nFirst, the SMAC tasks and Mujoco tasks share different baseline algorithms, even in the same task different scenarios, baseline methods are different. For example, the Fig 4c and Fig 5 c have different citations in [5] and [7], both of which although are irrelevant to the COMIX, COVDN, IDDPG methods. \n\nSecondly, across almost all results on MUJoCO, there is no evidence showing that FACMAC is in fact outperforming other baselines. \nSimilar observations also hold on Fig6 on SMAC tasks. Either all baselines converge to 100% winning rate, or, the baseline methods are not trained to the best of the other reported results [cross reference the results in MAPPO]. \n\nMoreover, the SMAC task has many maps, can the author explain why the six specific maps have been chosen among the many possibilities, especially under these maps, the performance is not really discriminative. I would like to see the performance on the super-hard maps if the author tries to claim the state-of-the-art.\n\n\n4. In line 75, \"the first multi-agent actor-critic method to outperform state-of-the-art valued-based methods on SMAC.\". This is an incorrect claim given the IPPO and MAPPO work. \n\nIPPO: [Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?]\nMAPPO: [the surprising effectiveness of Mappo in cooperative, multi-agent games]\n\n5. In line 160, it is unclear to me what is the motivation behind developing FACMAC-vdn and FACMAC-vdn-s, apart from trial-and-error testing different MARL architectures. \n\n6. The gumbel-max trick is notoriously hard to tune during many empirical studies, I was wondering if the author meets similar issues, and if so, mroe details need to be provided for future result replication. \n\n> ***Post-rebuttal***: I read all reviews, and the rebuttal to my review. I am not convinced by most of the questions, and during the rebuttal phase I found more problems in the replication of the results by running the submitted code. See my detailed comments.  I will maintain my score of strong rejection. Scores are adjusted post-rebuttal.\n\n The claim for MADDPG and QMIX need changing. \n\nThe baselines need to stay consistent throughout different tasks (I am actually surprised why a naive TRPO/PPO is not a baseline in your mujoco tasks. I understand you design a multi-agent mujoco setting, but what is the point of using FACMAC if it cannot outperform TRPO. Maybe on many agent swimmer, TRPO would fail due to scalability).\n\nThe selected tasks cannot show the effectiveness of FACMAC (Figure 4 5 6 fail to show at all). On both Mujoco and SMAC, more solid results need providing.  \n\n", "This paper proposes to apply the idea of value function factorization from recent value-based multi-agent RL work (e.g., VDN, QTRAN, QMIX) and apply it in the multi-agent actor-critic setting. Specifically, the paper proposes to use a centralized factorized critic, with either a monotonic (as in QMIX) or non-monotonic mixing of the individual value terms. The proposed approach applies the deterministic policy gradient method and is applicable to both discrete and continuous action spaces. The approach is compared to baselines in a series of particle world and SMAC benchmarks, and a new multi-agent MuJoCo benchmark proposed in the paper. Ablation studies reveal that scalability to more agents or actions is improved over MADDPG, and that nonmonotonic mixing of the value terms can be useful in some domains.  The main novelty and potentially most significant insight of the paper in my opinion is the concept of non-monotonic mixing. I have some concerns on originality, and some minor technical questions. Overall, the empirical results are thorough, and the paper is very clearly written.\n\nStrengths:\nThe paper convincingly argues that non-monotonic mixing is applicable in a centralized critic in an actor-critic setting, as the monotonicity constraint required in value-based methods is no longer necessary. This is empirically demonstrated to be quite useful in some domains (Fig. 8). The empirical evaluation is thorough. The MAMuJoCo benchmark is a nice additional contribution, complementing the continuous particle environments proposed in the literature so far.\n\nLimitations:\nThe idea of a factorized critic proposed in the paper is certainly reasonable, but I am not sure about the novelty. In [1], a similar critic factorization approach is investigated in the actor-critic setting. This paper should make clear what the similarities and differences to [1] are, and provide appropriate empirical comparisons. This being said, it does seem to me that the non-monotonicity of mixing proposed in this paper has not been considered before.\n\nThe paper identifies some domains where the non-monotonic mixing is useful. One limitation however is that the paper is a bit light on theory or insights about what such domains are like, and when one could expect non-monotonic critic factorization to be useful. The empirical results on non-monotonic mixing also have a minor role in the paper.\n\nIf the concerns above could be addressed, I am happy to revise my score.\n\n[1] Jianyu Su, Stephen C. Adams, Peter A. Beling. Value-Decomposition Multi-Agent Actor-Critics. AAAI 2021\n\n\nDetailed comments:\nI have a technical concern regarding the centralized gradient estimator (Equation (7)). As far as I understand, the two gradients on the right hand side will be vectors of possibly different dimensions. How is it possible to multiply them as presented?\n\nIn several places in the paper, sampling from the policy is mentioned (e.g., Fig. 1, Lines 129-130, 174, 182). This is somewhat confusing, given that the paper learns deterministic policies. This should be clarified overall.\n\nMinor:\nLine 116-117: I don't think MADDPG learns reward functions.\nFig. 4-8: consider removing or explaining meaning of numbers in square brackets in the legend\nFig. 5 (right): legend placement is a bit unfortunate with overlap with the plots.\n\nPost-rebuttal: I thank the authors for their detailed rebuttal. It addresses most of my concerns and I am increasing the overall score to reflect this. The rebuttal has made clear what the difference to Su et al. is. I hope to see a discussion and the empirical results in a revised version of the paper. I also hope to see future work providing actionable insights about which kinds of domains indeed have non-monotonic value functions fitting for the proposed method. Having briefly reviewed the provided source code, I would suggest to include specific commands and configuration files to replicate the experimental results provided in the paper. Yes", " **NOTE: there are *no* problems with the code that affect our experimental results. The problems encountered by the reviewer are due to setting some (hyper-)parameters to different or invalid values.**  \n\nThank you for your comments. \n\n”Plus, I have spent several hours running the code in the supplementary material, and find troubles repeating the results (even for the baselines)” \\\n\\> The code we have provided is exactly the same code we used to run the experiments. We can investigate this issue if you provide a specific example of the command you ran and the results you could not reproduce.\n\n“It becomes even unclear what is the motivation and findings of the whole paper after reading the rebuttal.” \\\n\\> Our motivations are clearly stated in the introduction. The ‘basket of algorithms’ you mention are all introduced for a reason, and are not the result of a trial and error search. \\\nAs mentioned in our earlier response, FACMAC-vdn and FACMAC-vdn-s are introduced to see what kind of a factorisation is necessary. The motivations for using a centralised policy gradient (CPG) are clearly outlined in Section 3.2. FACMAC (without CPG) is an ablation to study the importance of our proposed CPG. We clearly demonstrate both theoretical and empirical improvements when using CPG, as mentioned in our earlier response.\n\n“Although this paper introduces an interesting setting of multi-agent Mujoco, but the author seems to be against adding comparisons even against the most naive TRPO/PPO method.” \\\n\\> Multi-agent MuJoCo is a benchmark for continuous control in the Dec-POMDP setting. In order to compare against TRPO/PPO we would need to formulate it as a single-agent task that is not relevant to the subject of the paper, which is multi-agent RL.\n\nBug1 and Bug2 \\\n\\> This has *no* effect on our experiments, which were all run with batch_size_run=1. All the config files (in src/config/algs/) we have provided for running our experiments use batch_size_run=1. We will update the code to prevent issues should any future experiments use larger values. \n\nBug3 \\\n\\> Again, this has *no* effect on our experiments. In PyMARL batch.max_seq_length cannot be 1. Even in an environment which terminates after a single timestep, the observations and states at the next timestep will be stored making max_seq_length > 1 always. Hence, even in the single-step matrix game experiments there are no problems.\n\n“Although multi-threaded sampling is used in the code” \\\n\\> PyMARL provides the ‘parallel’ runner which uses multi-*processing* in order to run multiple environments in parallel. We did not use this functionality for our experiments. Importantly, PyMARL's parallel runner makes a copy of the environment, one for each parallel process so that any given environment need not be aware it is being run in parallel.\n\n“...the sampling efficiency is not improved...” \\\n\\> Gathering experiences from multiple environments in parallel (in the manner suggested) is extremely unlikely to improve sample efficiency (performance compared to the total number of environmental interactions used). Hence, we only use a single environment to gather experiences.\n\n“...the training time is too long.” \\\n\\> We do not see why the speed of the simulator should be used as a criterion to evaluate our method. We already clearly demonstrate that our method is more sample efficient than competing approaches when considering the performance against the total number of training timesteps, which is an extremely common criterion used to evaluate deep MARL algorithms. \n\n“Let alone the reported results are much worse than the [MAPPO] and [IPPO] across all SMAC tasks.” \\\n\\> As mentioned in our earlier response, MAPPO and IPPO use very different experimental setups. As such, you cannot directly compare the results, especially if the SC2 version differs (which it most likely does).\n\n“Although the author insists that their method does not need a QMIX asssumption on the monotonic value function. However, by using a neural network-based mixing network in Equation 4, I don't see how the claim is solved. In fact, there are tons of MARL work that try to release this assumption, but in this particular work, there shows no new insights of how this can be addressed.” \\\n\\> As mentioned in our earlier response, we are using an actor-critic setup and therefore there are *no* restrictions on the form of the critic. If there is a particular theoretical argument the reviewer is referencing, please mention it so that we can respond with the relevant context.", " I would like to thank the author team to provide a detailed rebuttal to my questions. However, I am not convinced for most of the questions I raised. Plus, I have spent several hours running the code in the supplementary material, and find troubles repeating the results (even for the baselines). I believe there is still a substantial amount of work that needs improving. Therefore, I would remain my score of *strong rejection*. Here are more details:\n\n1. I am not convinced where the novelty of this paper lies. This paper essentially combines QMIX + MADDPG, which are two well-known baselines in MARL domain. With all respect, this paper is more like a technical report of experiments results. There is neither new theoretical insights nor substantial empirical performance improvement that are introduced on top of either QMIX or MADDPG. Especially, this paper has tried to introduce a basket of algorithms (e.g., FACMAC-vdn, FACMAC-vdn-s, ..FACMAC-CPG, FACMAC-w/o CPG, ...) by combining different components from QMIX and MADDPG, this makes me even concerned if there is any insight apart from trials and errors on different tricks. It becomes even unclear what is the motivation and findings of the whole paper after reading the rebuttal. \n\n2. Although this paper introduces an interesting setting of multi-agent Mujoco, but the author seems to be against adding comparisons even against the most naive TRPO/PPO method. \n\n\n3. Since this paper mainly sells its empirical performance, I spent 10 hours looking into the code that the author provides, but unfortunately, it is rather far away from being able to replicate the reported results, even I cannot replicate the MADDPG baseline results.    Overall, I am quite suspicious of the performance boost claimed in the paper.  Here is subtlety:\n\n    - The code is ***not bug-free***, as follows: \n\n        - bug1: In src/controller/cqmix_controller.py line24, the parameter of self.forward  hidden_states=self.hidden_states[bs] is passed, but line114 still uses self.hidden_states in the forward function, and does not use the passed-in parameter hidden_states, which will cause the problem that the dimension of the hidden_state of the input agent does not match the dimension of the input when batch_size_run>1.\n\n        - bug2: After fixing bug1, a RuntimeError: unsupported operation: more than one element of the written-to tensor refers to a single memory location. Please clone() the tensor before performing the operation.\nbug3: src/learners/facmac_learner.py line 68-72 did not deal with the case of batch.max_seq_length==1. In fact, it will sample the sample of max_seq_length=1, and will report RuntimeError: stack expects a non-empty TensorList.\n\n    - Although multi-threaded sampling is used in the code, the provided gym environment can only handle a single action meanwhile, the sampling efficiency is not improved, and the training time is too long.\n\n    - The network structure diagram in the text should be a vector diagram.\n\n\n\n4. In terms of experiment settings, the author seems to be selective on the map. For example, on SMAC tasks, they choose maps “MMM2” and “27m_vs_30m” for super-hard task, with no reasons explained why these two chosen among the many. I believe to report the SOTA results, a comprehensive list of experiments are needed rather than cheery picking. See similar practise in [MAPPO] and [IPPO] paper. Let alone the reported results are much worse than the [MAPPO] and [IPPO] across all SMAC tasks.\n\n5. Although the author insists that their method does not need a QMIX asssumption on the monotonic value function. However, by using a neural network-based mixing network in Equation 4,  I don't see how the claim is solved. In fact, there are tons of MARL work that try to release this assumption, but in this particular work, there shows no new insights of how this can be addressed. \n\n\n\n", " Thank you for your review. \n\n“This paper should make clear what the similarities and differences to [1] are, and provide appropriate empirical comparisons. This being said, it does seem to me that the non-monotonicity of mixing proposed in this paper has not been considered before.” \\\n\\> We will include a discussion of this in a future revision as well as an empirical comparison. To summarise, the main differences between FACMAC and VDAC are: 1) FACMAC uses *deterministic* policy gradients and follows our novel centralised policy gradient to learn policies, while VDAC uses per-agent *stochastic* policy gradients, 2) FACMAC factors the joint action-value function $Q_{tot}$, while VDAC factors the joint value function $V_{tot}$, 3) FACMAC is off-policy (and hence avoids problems related to non-stationarity when training on older data), while VDAC is on-policy, and 4) FACMAC works for both discrete and continuous cooperative tasks, while VDAC only works for discrete cooperative tasks. \\\nIn addition, our FACMAC framework allows for a more flexible factorisation of the joint-action $Q$. We have explored a new non-monotonic factorisation and demonstrated its advantages in some tasks as shown in Figure 8 to take advantage of this, whereas [6] only consider monotonic factorisation. We will make this clear in the revised version. \n \nWe have also run VDAC-mix on SMAC. Our results show that FACMAC performs significantly better than VDAC-mix on *all* 6 SMAC maps we tested. We will include these results in a future revision, but cannot update the paper with these new experiments during the review period.\n\n“One limitation however is that the paper is a bit light on theory or insights about what such domains are like, and when one could expect non-monotonic critic factorization to be useful.” \\\n\\> Non-monotonic critic factorisation can be useful in tasks with *non-monotonic* value functions, as demonstrated by our experimental results in Figure 8. Joint action-value functions are characterised as non-monotonic if an agent’s ordering over its own actions depends on other agents’ actions [1] (as discussed in Lines 552-554 in Appendix A).\n\nThe consequences of a monotonic factorisation, including theoretical insights and discussions are included in other works: [1, 2, 3, 4]. We will highlight this in future revisions.\n\n“The empirical results on non-monotonic mixing also have a minor role in the paper.” \\\n\\> Extensively exploring the use of non-monotonic critics is an interesting avenue for future work. In this paper we first wanted to establish the benefits of a centralised policy-gradient estimator and to demonstrate that factored critics can achieve great performance in many tasks both discrete and continuous.\n\n“I have a technical concern regarding the centralized gradient estimator (Equation (7)). As far as I understand, the two gradients on the right hand side will be vectors of possibly different dimensions. How is it possible to multiply them as presented?” \\\n\\> This equation follows the standard deterministic policy gradient equation, an application of the chain rule, in which $\\mathbf{\\mu}$ is the policy. [5] provides details on its derivation.\n\n“In several places in the paper, sampling from the policy is mentioned (e.g., Fig. 1, Lines 129-130, 174, 182). This is somewhat confusing, given that the paper learns deterministic policies. This should be clarified overall.” \\\n\\> Figure 1 features an explicit sampling step because we sample from the categorical distribution when using discrete actions (described in Lines 218-220). Additionally, during training we need a sampling step to allow for exploration, e.g., using Gaussian noise for continuous actions, or epsilon-greedy for discrete actions. We will clarify this in future revisions.\n\n“Line 116-117: I don't think MADDPG learns reward functions.” \\\n\\> Thanks for pointing this out, we will fix it. We meant to say that each agent can have its own arbitrary reward function.\n\n“Fig. 5 (right): legend placement is a bit unfortunate with overlap with the plots.” \\\n\\> We will utilise the extra page allowed for accepted papers in order to improve the presentation of our figures, specifically the overlaps in the legends you mentioned. \n\n[1] MAVEN: Multi-Agent Variational Exploration. Mahajan et al. NeurIPS 2019. \\\n[2] QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning. Son et al. ICML 2019. \\\n[3] QPLEX: Duplex Dueling Multi-Agent Q-Learning. Wang et al.  ICLR 2021. \\\n[4] Weighted QMIX: Expanding Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning. Rashid et al. NeurIPS 2020. \\\n[5] Deterministic Policy Gradient Algorithms. Silver et al. ICML 2014. \\\n[6] Value-Decomposition Multi-Agent Actor-Critics. Su et al. AAAI 2021.", " “The gumbel-max trick is notoriously hard to tune during many empirical studies, I was wondering if the author meets similar issues, and if so, mroe details need to be provided for future result replication.” \\\n\\> We will include the values of the temperature hyper-parameter setting we used in our gumbel-softmax sampling. We did not encounter any specific difficulties utilising it. In our experiments, it is trivial to switch between continuous and discrete action spaces for FACMAC, which is another strength of our method. We have provided significant additional details in the full code for reproducing all experiments. Additionally, Section 3.3 provides all of the details (once we include the temperature hyper-parameter, thank you for pointing this out) for independent reproduction.\n\n“The baselines need to stay consistent throughout different tasks (I am actually surprised why a naive TRPO/PPO is not a baseline in your mujoco tasks. I understand you design a multi-agent mujoco setting, but what is the point of using FACMAC if it cannot outperform TRPO. Maybe on many agent swimmer, TRPO would fail due to scalability).” \\\n\\> Our Multi-Agent MuJoCo benchmark is different to the standard MuJoCo benchmark that is commonly used in single-agent continuous control benchmarks. The purpose is not to outperform a single-agent setup, its purpose is to introduce a benchmark for continuous control \nin the multi-agent setting.  \n\n[1] Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning. Rashid et al. JMLR 2020. ", " Thank you for your review.\n\n“Moreover, I am not convinced why such a combination is an important direction and worth attention from the MARL community, neither does the reported empirical results support that.” \\\n\\> We believe that the factorisation of the critic and the centralised policy gradient estimator are worthy of attention in the MARL community, particularly to those researchers focused on the CTDE cooperative setting. First, we demonstrate in our results the factorisation of the critic can be crucial to ensuring good performance, which is definitely of interest to the Deep MARL community. Second, the centralised policy gradient estimator is necessary to fully reap the benefits of a centralised critic in our cooperative setting, which again is of interest to Deep MARL researchers.\n\n“In line 11, the author mentioned \"FACMAC uses a centralised policy gradient estimator that optimises over the entire joint action space, rather than optimising over each agent’s action space separately as in MADDPG.\" I believe this shall not be considered as a benefit of FACMAC over MADDPG.” \\\n\\> Section 3.2 discusses the benefits of our centralised policy gradient estimator in detail, and our empirical results in Figures 2(a) and 7 demonstrate that it improves performance on three different domains (continuous matrix game, SMAC, and MAMuJoCo). Thus, we see no reason to discredit its benefits.\n\n“Also, I don't think it is fair to claim that MADDPG only optimises in each agent's action space, after all the critic value that guides the policy to learn is the centralised critic, so presumably, what MADDPG is doing is searching over a joint-strategy space.” \\\n\\> MADDPG uses a per-agent policy gradient (Eqn 3) in which the policy of each agent is optimised *individually* assuming that the other agents’ policies are fixed. We demonstrate a simple failure case for this in Figure 2, which clearly shows MADDPG without the centralised policy gradient is *not* optimising over the joint-action policy as a whole.\n\n“Furthermore, FACMAC cannot optimise in the entire joint action space as the author claim … However, this trick only holds for the case when all agents share the same set of policy parameters.” \\\n\\> The reviewer is mistaken. Eqn 7 optimises over the entire joint-action space irrespective of whether the agents are sharing parameters. One easy way to see this, is to imagine a $\\mathbf{\\mu}$ as being a single-agent’s policy that has a particular architecture (shown in Figure 1a). Then taking the gradient of the policy’s parameters, $\\theta$, optimises over the policy's entire action space.\n\n“And importantly, it is commonly known that taking simultaneous gradients will lead to convergence issue.” \\\n\\> The paper referenced refers to a non-cooperative setting. In our particular fully cooperative setting, since there is a *single* shared reward function such pathologies do not arise. \n\n“Moreover, the author also mentions in the line 187 that FACMAC-vdn is in fact optimising each agent's actions, which self-contradicts what the previous claims are about.” \\\n\\> There is no contradiction, because this particular claim *only* applies to FACMAC-vdn. If we use another form of factorisation for the critic, as FACMAC does, then it is no longer equivalent. Please see the paragraph starting at Line 187 for the description of this. \n\n“The effectiveness of CPG is also questionable. Although in the toy example in Figure 2a, the CPG works with MADDPG. However, this could be because of the different exploration strategies in such an toy example” \\\n\\> Both methods are using *exactly* the same exploration in these experiments. Figure 2b, clearly demonstrates the differences between a per-agent policy gradient (which MADDPG uses) and our proposed centralised policy gradient. \n\n“Most importantly, Later in Figure, 7, the CPG trick almost makes no difference whether or not it is activated.” \\\n\\> Figure 7 clearly shows that FACMAC significantly outperforms FACMAC (without CPG) on SMAC map MMM2, and has lower variance across seeds on 2c_vs_64zg. Furthermore, on ManyAgent Swimmer with 10 agents, FACMAC performs significantly better than FACMAC (without CPG). The only difference between these 2 methods is the use of our centralised policy gradient, so it clearly shows that CPG does significantly affect the performance. \n\n“I am not convinced how using a neural network-based mixing network in Equation 4 achieve the claim of releasing the monotonic assumption in meeting the individual-global-max condition.” \\\n\\> The reviewer is mistaken. We do not claim to enforce consistency between the argmax of the centralised critic, and our greedy agent policies (which you refer to as the IGM condition). In fact, a huge benefit of our method, that we emphasise repeatedly, is that we do *not* need to enforce this consistency in the same way that QMIX does. Thus, we are free to factorise our critic in any manner we choose without restriction. \n\n“First, the SMAC tasks and Mujoco tasks share different baseline algorithms, even in the same task different scenarios, baseline methods are different. For example, the Fig 4c and Fig 5 c have different citations in [5] and [7], both of which although are irrelevant to the COMIX, COVDN, IDDPG methods.” \\\n\\> The reviewer is severely mistaken. In our figures the number in square brackets is the number of random seeds. This is explained in the captions for Figures 4 and 5, that you mentioned. Some baseline algorithms are different on SMAC and MAMuJoCo since SMAC features discrete action spaces while MAMuJoCo features continuous action spaces. This is also why we introduce two new baselines COVDN and COMIX, to compare FACMAC against value-based approaches in continuous cooperative tasks. \n\n“Secondly, across almost all results on MUJoCO, there is no evidence showing that FACMAC is in fact outperforming other baselines.” \\\n\\> The Multi-Agent MuJoCo benchmark, which we introduce in this paper, demonstrates the importance of factorising the critic. In these experiments, FACMAC is one of the best performing algorithms on *every* tested scenario. It performs significantly better than FACMAC-vdn, MADDPG, COVDN, and IDDPG. \n\n“Moreover, the SMAC task has many maps, can the author explain why the six specific maps have been chosen among the many possibilities, especially under these maps, the performance is not really discriminative.” \\\n\\> The SMAC maps can be grouped into three categories including Easy, Hard, and Super-Hard, based on the overall performance difference of all algorithms [1]. In our paper, we consider 2 Esay maps (2s3z, MMM), 2 Hard maps (2c_vs_64_zg, bane_vs_bane), and 2 Super-Hard maps (MMM2, 27m_vs_30m), which cover *all* different difficulty levels of the maps on SMAC. \n\n“I would like to see the performance on the super-hard maps if the author tries to claim the state-of-the-art” \\\n\\> We already present results on 2 super-hard maps in Figure 6, for maps “MMM2” and “27m_vs_30m” (and we mention they are super-hard maps in the caption of Figure 6 and Lines 307-308).\n\n“Either all baselines converge to 100% winning rate, or, the baseline methods are not trained to the best of the other reported results [cross reference the results in MAPPO]” \\\n\\> The paper referenced utilises a significantly different training setup and performs extensive hyperparameter tuning for their results. Nonetheless, our results are comparable at the 2 million timestep mark. Additionally, we utilise SC2 version 4.10 in our experiments which is not directly comparable to results obtained using 4.6. The paper referenced does not specify which version they use. We utilise the PyMARL framework for all of our experiments, and run all of our lines ourselves since it is extremely important to maintain a consistent experimental setup when comparing methods. Furthermore, the point of these experiments is to produce relative comparisons between methods and not merely present a higher bolded number in a table. \n\n“In line 75, \"the first multi-agent actor-critic method to outperform state-of-the-art valued-based methods on SMAC.\". This is an incorrect claim given the IPPO and MAPPO work.” \\\n\\> This specific claim is not from our paper, it is from DOP (and we make this clear in Lines 73-75).\n\n“In line 160, it is unclear to me what is the motivation behind developing FACMAC-vdn and FACMAC-vdn-s, apart from trial-and-error testing different MARL architectures.” \\\n\\> This is done in order to “better understand the advantages of factoring a centralised critic”, as was remarked in the previous sentence on Line 159. These different critic architectures are important in order to better understand what kind of a factorisation is necessary or required. Moreover, these kinds of factorisations are extremely prevalent throughout the literature on value-based methods and have not been merely conjured up through trial-and-error testing. ", " Thank you for your review.\n\n“I’m curious as to why QTran (Son et al., 2019) was not also included as a baseline?” \\\n\\> QTran was not included since it is almost always outperformed by the other baselines on SMAC, especially more recent methods such as QPLEX which we do compare to. \n\n“In line 292 … It is true that FACMAC converges faster, but so does MADDPG and DOP, so this seems more a reflection on the policy gradient approach in general and not something specific to FACMAC.” \\\n\\> Even though FACMAC and QMIX use exactly the same type of architecture for representing the joint-action Q, FACMAC is able to easily solve the task whereas QMIX struggles. Since the major difference between the 2 methods is in the difference between a policy gradient based approach and Q-learning, this demonstrates that a policy gradient based approach can better take advantage of the same factored architecture. \\\nIn this particular map (bane_vs_bane) although factoring the architecture isn’t required (since MADDPG performs well), our other experiments demonstrate its benefits and show that in general it is very beneficial to factorise the critic. And so it is important to see that our method is able to perform more consistently than QMIX across maps. \\\nWe do not think that the performance in bane_vs_bane is solely due to the use of a policy gradient based approach, since both COMA and Central-V do not perform well here. We will make this clearer. \n\n“Regarding the results in Figure 8 for the matrix game.” \\\n\\> In this small environment, the extra architectural complexity of FACMAC-nonmonotonic can indeed slow down the speed of convergence compared to the much simpler MADDPG with CPG. Given enough time we would expect FACMAC to also solve the task, however due to its monotonic factorisation it cannot exactly represent the matrix game which then makes convergence to the optimal solution more difficult.", "The paper proposes a new algorithm for cooperative multi-agent reinforcement learning (MARL) applicable in both discrete and continuous settings. The main innovation is an adaptation of multi-agent deep deterministic policy gradient (MADDPG). MADDPG trains decentralised policies using a centralised critic for each agent in the centralised training with decentralised execution (CTDE) paradigm. In this work, the authors specifically introduce a version of MADDPG with a centralised critic that learns a factored value function. Each factor is an agent-specific utility function with learned weights determined by a mixing network, as is done in popular value factorisation methods in MARL such as VDN and QMIX. In addition, the approach makes use of a centralised policy gradient estimator, where all agent’s current policy actions are considered during gradient updates to guard against pathologies such as over-generalisation. The authors refer to their algorithm as factorised multi-agent centralised policy gradients, or FACMAC for short. Extensive experimental results are presented to showcase the effectiveness of the proposed algorithm, along with multi-agent MuJoCo, a new multi-agent environment suite for continuous control, and ablation studies highlighting the specific benefits of value function factorisation, centralised policy gradients and the expressiveness of the factorisation function when applied to MADDPG. \n  **Originality**: In essence, the work represents a novel combination of MADDPG (Lowe et al., 2017) and QMIX (Rashid et al., 2018) with the addition of the idea of using centralised policy gradients. Combining both innovations to leverage their strength and applicability to discrete and continuous tasks is shown to be useful in settings not previously tested, using a large number of agents. The introduced multi-agent MuJoCo environment for continuous control builds on the single-agent MuJoCo engine, but is new and seems promising and useful as a benchmark for future research in continuous MARL.\n\n**Quality**: The work is technically sound. Most of the claims in the paper are supported by a decent set of experimental results. Experiments are done over multiple seeds and include a wide variety of baselines and environments. The author’s motivation for their approach, citing pathologies such as over-generalisation, could perhaps be made more concrete but overall the presentation comes across as honest and the details provided are sufficient for a reader, interested in MARL, to follow and understand the work.\n\n**Clarity**: The paper is well-written and the overall structure of the paper makes it easy to follow. Some minor remarks. Section 3.2 presents some experimental results before the experiments section, which was slightly odd, but I can also understand why the authors thought to present it earlier to motivate their argument for CPG. On a similar note, the authors reference Figure 11 in appendix D for this experiment, but the same figure is already present in the center of Figure 2(b), so they could just refer to that. Also, throughout the paper, the authors use “a” as the index symbol for agents and “u” for actions, whereas it is perhaps more the convention to use “i” for agents and “a” for actions. This change might make it easier for readers familiar with other MARL papers and notation to read the paper. \n\n**Significance**: In my opinion, the work will definitely be of interest to the community. The results seem to indicate that the approach shows significant promise at scale and nicely ties together different streams of investigation in MARL, namely, value decomposition methods and multi-agent policy gradient methods. Furthermore, the newly introduced multi-agent MuJoCo suite could greatly benefit future research as a benchmark for continuous control. \n\n**Questions**:\n\n* The paper references PyMARL for specific implementations. I’m curious as to why QTran (Son et al., 2019) was not also included as a baseline? Since the library already has an implementation. In my opinion, this should not be held against the authors, since I think the number of baselines is already sufficient, especially with the introduction of COVDN and COMIX. It is just out of interest that I ask the question.  \n* In line 292 it is stated: “For instance, on bane_vs_bane, a task with 24 agents, while QMIX struggles to find the optimal policy with 2 million timesteps, FACMAC, with exactly the same value factorisation, can quickly recover the optimal policy and achieve 100% test win rate. This shows the convergence advantages of policy gradient methods in this type of multi-agent settings”. It is true that FACMAC converges faster, but so does MADDPG and DOP, so this seems more a reflection on the policy gradient approach in general and not something specific to FACMAC. Do you think this might be a more honest reflection? Or is the statement more specific to factorisation not being useful in this case, i.e. PG with and without factorisation having no difference which places the focus on QMIX vs FACMAC and FACMAC benefiting from PG? In any case, perhaps worth rephrasing the claim to be less focused on FACMAC. \n* Regarding the results in Figure 8 for the matrix game. It is interesting that in Figure 2(a) it seems MADDPG with CPG can achieve convergence in just 50k steps, whereas FACMAC fails to converge and FACMAC-nonmonotonic takes around 200k steps to converge. Curious to know whether the authors might have any explanation as to why this might be the case? Is it perhaps that FACMAC also has to learn the mixing and hypernetworks, which significantly slows down convergence speed in such a small environment. Also, do the authors expect FACMAC to converge eventually, given enough time? \n\n**References**\n\n- Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems, pages 6379–6390, 2017.\n- Tabish Rashid, Mikayel Samvelyan, Christian Schroeder Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pages 4292–4301, 2018.\n- Son, K., Kim, D., Kang, W.J., Hostallero, D.E. and Yi, Y., 2019, May. Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In International Conference on Machine Learning (pp. 5887-5896). PMLR.\n The authors highlight some cases where their approach fails to outperform specific baselines but makes no noticeable mention of the core limitations of their algorithm. In terms of societal impact and negative consequences, these are also not stated in the main text. However, given the nature of the work, I don’t feel too strongly about these remarks although it might be interesting for the authors to think about the downsides of FACMAC. There is a section in the appendix detailing some of the social impacts that might stem from the multi-agent MuJoCo suite. "], "review_score_variance": 8.0, "summary": "This paper generated an involved discussion between the reviewers and the authors, as well as between the reviewers themselves.\nThe paper essentially combines two well-known baselines in MARL domain, and therefore was judged as a report of experiments for which reproducibility is of particular importance. The reviewers had an intense discussion about reproducibility, two of them tried the code provided by the authors and one of them commented on it based on their experience. The analysis raised significant doubts about reproducibility (it is unfortunate that the authors did not provide configs on their experiments, to enable to rerun them exactly).", "paper_id": "nips_2021_wZYWwJvkneF", "label": "train", "paper_acceptance": "accept", "anchored_texts": "This paper generated an involved discussion between the reviewers and the authors, as well as between the reviewers themselves.\nThe paper essentially combines two well-known baselines in MARL domain, and therefore was judged as a report of experiments for which reproducibility is of particular importance. The reviewers had an intense discussion about reproducibility, two of them tried the code provided by the authors and one of them commented on it based on their experience. The analysis raised significant doubts about reproducibility (it is unfortunate that the authors did not provide configs on their experiments, to enable to rerun them exactly)."}
