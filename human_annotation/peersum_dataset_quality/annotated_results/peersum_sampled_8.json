{"source_documents": ["A foundational issue in deep reinforcement learning (DRL) is that \\textit{Bellman's optimality equation has multiple fixed points}---failing to return a consistent one. A direct evidence is the instability of existing DRL algorithms, namely, the high variance of cumulative rewards over multiple runs. As a fix of this problem, we propose a quantum K-spin Hamiltonian regularization term (H-term) to help a policy network stably find a \\textit{stationary} policy, which represents the lowest energy configuration of a system. First, we make a novel analogy between a Markov Decision Process (MDP) and a \\textit{quantum K-spin Ising model} and reformulate the objective function into a quantum K-spin Hamiltonian equation, a functional of policy that measures its energy. Then, we propose a generic actor-critic algorithm that utilizes the H-term to regularize the policy/actor network and provide Hamiltonian policy gradient calculations. Finally, on six challenging MuJoCo tasks over 20 runs, the proposed algorithm reduces the variance of cumulative rewards by $65.2\\% \\sim 85.6\\%$ compared with those of existing algorithms.", " The authors sincerely thank all reviewers and area chair. The authors enjoy the discussions and are happy that some key points reached a consensus. \n\nTo recap, this work has made the following major contributions.\n1. Per Reviewer 65t2 and Reviewer Reviewer UzCH’s suggestion, the authors added Appx. E to include theoretical analysis of gradient’s variance reduction brought by the proposed H-term.\n1. The authors had an extensive discussion with Reviewer 65t2 about the motivation. This work targets a foundational issue that Bellman’s optimality equation has multiple fixed points—failing to return a consistent one. Such an issue of multiple fixed points is quite common in RL practice, which is suspected to be an unavoidable obstacle when addressing practitioners’ major criticism of the highly unstable performance of existing DRL algorithms.\n1. The authors had a fruitful interaction with Reviewer JucZ. As a result, Section 3.2 has been substantially revised to investigate the Ising formulation of MDP/RL from a glass of Monte Carlo Gradient Estimator.\n1. An interesting point to restate is that under the Ising model (a quite universal model) analogy of MDP/RL, the Hamiltonian equation has a clear physical meaning, namely, it measures the “energy” of a policy. For this physically-inspired H-term, we derived a variant of the policy gradient estimation, as a regulation term (Alg. 1 in Line 15). Such an add-on term turns out to be rather simple to implement and delivers substantial performance improvements.\n\nEven though there are so many great works in the DRL community, the authors are happy to address such a foundational issue and present to the community a simple and effective add-on H-term, inspired by Ising Model and Hamiltonian equation.\n\nNote that the manuscript has been updated in accordance with  the above responses, mainly Section 3.2 and Appx. E.\n", " > Finally, the argument that the Hamiltonian helps reduce the variance is still not convincing to me, given that there is no difference between the Hamiltonian raised in (7) and cumulative rewards. It would be more convincing if the authors could provide a more rigorous mathematical justification.\n\nIf the reviewer agrees with the above responses regarding H-term reformulation and Bellman’s optimality equation and agrees that the Hamiltonian equation in (7) is a novel foundational perspective, the authors would like to reason the impressive variance reduction results in Table 2 as follows:\n1. There are multiple fixed points (policies) in existing DRL algorithms, due to Bellman’s Optimality Equation, as shown by three examples in Fig. 1 (the case of $\\gamma = 1$) and Fig. 5 (the case of $\\gamma \\in (0, 1)$) which is further supported by the empirical experiments in Fig. 2.\n1. The conventional PPO algorithm (note that we also tested other DRL algorithms) randomly converges to one of several policies, resulting in high variance. This is empirically shown in the third column of Table 2, \n1. Ising Model in Table 1 measures the “energy” of a policy, thus the proposed $H$-term helps the policy network stably converge to a physically stationary policy, namely, the lowest-energy configuration of a system.\n\nTherefore, the Alg. 1 will converge to a policy with the smallest $H$-value, thus the variance of multiple trainings will be reduced.\n\nIn Appx. E (newly updated version), we also provide a rigorous mathematical justification that the added H-term will result in a reduced variance of the gradient.", " Next, we would like to provide our response to “the reviewer expects to see a theorem attached to this claim with a mathematical proof of how the issue is fixed under the proposed framework” and “a proof of how the variance issue is addressed by Hamiltonian regularization is needed.”\n\nIn Appx. E (newly updated version), we also provide a rigorous mathematical justification that the added H-term will result in a reduced variance of the gradient. And the authors would like to reason the impressive variance reduction results in Table 2 as follows:\n1. There are multiple fixed points (policies) in existing DRL algorithms, due to Bellman’s Optimality Equation, as shown by three examples in Fig. 1 (the case of $\\gamma = 1$) and Fig. 5 (the case of $\\gamma \\in (0, 1)$) which is further supported by the empirical experiments in Fig. 2.\n1. The conventional PPO algorithm (note that we also tested other DRL algorithms) randomly converges to one of several policies, resulting in high variance. This is empirically shown in the third column of Table 2, \n1. Ising Model in Table 1 measures the “energy” of a policy, thus the proposed H-term helps the policy network stably converge to a physically stationary policy, namely, the lowest-energy configuration of a system.\n1. Therefore, the Alg. 1 will converge to a policy with the smallest $H$-value, thus the variance of multiple trainings will be reduced.\n", " Thanks very much for this detailed clarification of possible confusion. The authors would like to take this opportunity to communicate with the reviewer.\n\nFirst, the authors agree that “the Bellman operators operate on the space of bounded real-valued functions over $S$ or $S\\times A$, not the space of policies.” In Fig. 1 (the case of $\\gamma = 1$) and Fig. 5 (the case of $\\gamma \\in (0, 1)$), we show that there are multiple feasible solutions for the value function, not limiting to the reviewer’s mentioned case “the same optimal value function, e.g., when $Q^*(s,a1)=Q^*(s,a2)$, or $V^\\pi_1(s)=V^\\pi_2(s)=V^*(s)$”,  but different functions $Q$’s or $V$’s.\n\nSecond, the authors are quite aware of the well-known uniqueness result that  “the Bellman optimality operator has a unique fixed point due to the monotonicity and contraction properties, and the Banach fixed-point theorem”. For example, the Bellman optimality operator is a contraction over a complete metric space of real numbers with a metric L-infinity norm. The authors would like to point out that such a result holds under certain sufficient conditions.\n\nThird, the authors would like to list the following evidence that there are multiple policies,\n1. In the three examples of the case of $\\gamma \\in (0, 1)$ in Fig. 5, there are multiple policies with different value functions. More examples can be found in Ch. 3.1 of the Bertsekas ADP textbook [1] (http://web.mit.edu/dimitrib/www/AbstractDP_ED3_TEXT_2021.pdf)\n1. Section 3 Counterexamples and Section 3.1 Multiple Fixed Points of [2] give counter-examples of the uniqueness solution and also examples for multiple fixed points. Also, [3] pointed out that “However, it is not then possible to assure uniqueness of the fixed point on $C(X)$. Also, in this case, a convergence of the successive approximations from an arbitrary element of $C(X)$ can fail.”\n1.  For the Six MuJoCo tasks in Fig. 2 and Table 2 and more tasks in [4][5][6], there is empirical evidence of “a trained agent randomly converges to one of the multiple policies”.\n1.  Besides the above robotic control tasks, the authors observed the multiple policies issue by checking the MDP instances of several NP-hard problems, e.g., Graph MaxCut, Minimum set cover, Mixed integer programming problems (MILP). Actually, the issue of multiple policies is currently a major challenge for DRL solutions that do not always beat commercial solvers (Gurobi and SCIP).\n\nEven though there are so many great works, the authors are happy to address such a foundational issue and present to the community an add-on term to mitigate the highly unstable performance of existing DRL algorithms, which is a major criticism from practitioners. \n\n\n* [1] Bertsekas D. Abstract dynamic programming[M]. Athena Scientific, 2022.\n* [2] Kamihigashi, T. (2012). Existence and uniqueness of a fixed point for the Bellman operator in deterministic dynamic programming (No. DP2012-05).\n* [3] Rincón‐Zapatero, Juan Pablo, and Carlos Rodríguez‐Palmero. \"Existence and uniqueness of solutions to the Bellman equation in the unbounded case.\" Econometrica 71.5 (2003): 1519-1555.\n* [4] Duan, Yan, et al. \"Benchmarking deep reinforcement learning for continuous control.\" International Conference on Machine Learning. PMLR, 2016.\n* [5] Eysenbach, Benjamin, et al. \"Diversity is all you need: Learning skills without a reward function.\" International Conference on Learning Representations. 2018.\n* [6] Recht, Benjamin. \"A tour of reinforcement learning: The view from continuous control.\" Annual Review of Control, Robotics, and Autonomous Systems 2 (2019): 253-279.", " > > > > Thanks very much for your clarification on your question “our motivation and empirical results” and “intuitive motivation”. It is indeed a good question and a good opportunity for the authors to emphasize the foundational contributions. Before replying, the authors made a thorough rechecking of Sutton’s RL book and survey of existing DRL algorithms.\n> > > > \n> > > > The authors would like to state several backgrounds of current deep reinforcement learning algorithms. First, there is a fact that the Optimality Bellman Equation (with a $\\max_a$ operation) is an optimality condition, which originally is a sufficient condition, namely, any optimal policy of MDP and Dynamic Programming problems should satisfy the Optimality Bellman Equation. Note that it is not about algorithm design, however, several (deep) RL algorithms used it, like Q-learning, DQN, etc. Please do not use Q-learning and DQN algorithms as an example of how the Optimality Bellman Equation should be used.\n> > > > \n> > > > Here, the authors would like to point out that the optimality condition does not discuss how an algorithm should be designed or implemented, but a mathematical principle that any optimal policy should satisfy, as long as the target problem space possesses the MDP structure. Even if the $\\max_a$operation is NOT used in an RL algorithm, an optimal policy should satisfy the Optimality Bellman Equation. However, such an optimal policy under the Optimality Bellman Equation is not unique. In practice, an algorithm will randomly converge to one of many policies. This foundational issue of Optimality Bellman Equation is our strong motivation to consider an alternative, Hamiltonian equation, which is universally used in modern physics.\n> > > > \n> > > > Second, the currently most widely used Actor-Critic algorithms (both DDPG and PPO) use Bellman equation (not the optimal one) for training the critic network (for value estimation), and the critic network converges when the Optimality Bellman Equation is satisfied. That is to say, since the Optimality Bellman Equation has multiple fixed points, the obtained critic network will randomly converge to one of many fixed points, thus the trained Actor-Critic agent also randomly converges to one of many fixed points.\n> > > > \n> > > > Third, Fig. 2 and Table. 2 (vanilla PPO) have given empirical verification about the observation that an DRL algorithm will randomly converge to one of many policies. Such an observation is widely recognized by the DRL community, there are YouTube videos (for example, an upside down policy of the HalfCheetah task: https://www.youtube.com/watch?v=qU8Nd9lyxlw). The authors believe that Fig. 1 (and descriptions in Introduction), Fig. 2, and Table. 2 together strongly motivate our work. Even though there are so many great works, the authors are happy to address such a foundational issue and present to the community an add-on term to mitigate the highly unstable performance of existing DRL algorithms, which is a major criticism from practitioners.\n> > > > Fourth, differentiating “the training/learning process” and “the obtained optimal policy” is the key to understanding the novel analogy between MDP and K-spin Ising model. “Spin in quantum physics is either 1 or -1 when being measured”, similarly, an optimal policy assigns either 1 or 0 to each state-action pair, which is obtained when optimality is achieved. Please note that an optimal policy is deterministic, i.e., either 1 or 0 for each state-action pair. The authors believe that this fact may be the cause of the reviewer’s confusion. In other words, during the training process, a non-optimal policy is just like in a quantum superposition state; when the training process ends, an optimal policy is “measured” (when the algorithm is converged). Since both the initialization and the training process are random, it is natural to treat the training process as a quantum superposition state; and an optimal policy after convergence (and the Bellman’s optimality equation is satisfied) is just like being “measured” and becomes deterministic.\n> > > > \n> > > > Furthermore, both the Ising model and lowest-energy state are fundamental in physics. The authors are quite impressed by the fact that the Hamiltonian equation (simple and easy-to-implement) can be used as an add-on term to most actor-critic DRL algorithms (note that we tested over 5 algorithms, i.e., DDPG, PPO, SAC, TD3), and such an add-on term effectively addresses practitioners’ major criticism “unstable”. We are confident that this work will be highly recognized by both NeurIPS community members and industrial practitioners. ", " > Second, PPO is an on-policy PG method that only solves the Bellman equation for the current policy. It does not solve the Bellman optimality equation.\n\nThis is a great question and Reviewer Zi6K raises a very similar one. Therefore, the authors like to refer to the relevant discussions.\n\n> The main motivation is based on that the Bellman’s optimality equation, which is the base of Q-learning-like algorithms such as DDPG, has multiple fixed points. But the algorithm also work well on PPO, which even does not use Bellman equation to learn the value function. Can the authors provide any explanation about why it PPO+H works so well?\n> \n> > The authors believe the reviewer made a factual error comment that “PPO does not use Bellman equation to learn the value function”. PPO is a policy gradient algorithm with advantage function estimation.\n> > \n> > In the following reference [1], it is theoretically clear how a critic is plugged into the policy gradient theorem in equations (8) and (9). Thus, all actor-ciitic DRL algorithms use the Bellman equation to learn the value function.\n> > \n> > As mentioned in 216~218, the authors use GAE [28] for advantage estimation, where a value function is approximated as a baseline and optimized via the Bellman equation. The authors follow several benchmark implementations as in Stable Baseline3, RLlib, Tianshou, etc, which update the value function by minimizing the TD-residual. Therefore, it is reasonable that the H term also works with PPO.\n> > \n> > [1] Wen, Junfeng, et al. \"Characterizing the gap between actor-critic and policy gradient.\" International Conference on Machine Learning. PMLR, 2021.\n> > \n> > > I appreciate the authors' reply to my questions.\n> > > \n> > > First, for PPO, what I meant was that PPO, which is an on-policy algorithm, does not use an Optimality Bellman Equation, i.e., no max_a operation in value learning. However, the multiple fixed points problem only occurs when the max_a operation is used like in Q-learning. This makes PPO failed to be an empirical evidence to support the authors' foundamental motivation (while I agree it is a good to see PPO+H works well). I also checked other reviewer's comments, which also raise question about the correlation between motivation (BE has multiple fixed points) and the empirical results.\n> > > \n> > > Second, for the analogy between policy and spin angular momentum. I am still confused about the analogy. Spin in quantum physics is either 1 or -1 when being measured. The authors said that non-optimal policy is the \"orientation\"--what I understand here is that the non-optimal policy is the spin vector's z-axis component on the bloch sphere, is this what the authors meant? If so, I do not see a intuition here to consider optimal policy as \"being measuted oplicy\". Could the authors explain in more details? I have a certain background in physics but I feel hard to fully grasp the intuition behind the analogy.\n> > > \n> > > Since most of readers of the NeurIPS conference come from computer science and computational neuroscience, and given the fact this work is not quantum RL but physics-inspired conventional deep RL, I think certain efforts need to be made to make the audience understand at least the intuitive motivation.\n> > > ", " > First, in (2) the cumulative reward is the expectation of $Q^{\\pi_\\theta}$ with respect to the initial state distribution, which I believe is exactly the same as (7) except for the negative sign.\n\n\nThe authors realize that the equal sign in (7) may lead to some confusion and like to clarify it as follows. \n\nPlease note that both the optimization objectives (2) and (7) of reinforcement learning are probabilistic functions, and the “Monte Carlo” method (Chapter 5 in [1]; [2]) is broadly used for gradient estimates whose operation involves a significant random component.\n\n* [1] Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.\n* [2] Mohamed, S., Rosca, M., Figurnov, M., & Mnih, A. (2020). Monte Carlo Gradient Estimation in Machine Learning. J. Mach. Learn. Res., 21(132), 1-62.\n\nTherefore, the authors show that (2) and (7) are NOT exactly the same through the glass of Monte Carlo gradient estimators. This explanation is also available in Section 3.2 (the newly updated version) and will be added in the Appendix of the future version.\n\nTo recap, inspired by [17], the authors formally reformulate (2) into a $K$-spin Hamiltonian equation\n\n$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~H(\\theta) \\triangleq -E_{S_0,A_0} [Q^{\\pi_\\theta}(S_0,A_0)],$\n\n$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~=-\\lim_{K \\rightarrow \\infty}\\sum_{k = 0}^{K-1} \\sum_{\\mu_0}^{\\mathcal{S} \\times \\mathcal{A}} \\cdots \\sum_{\\mu_k}^{\\mathcal{S} \\times \\mathcal{A}} L_{\\mu_0, ..., \\mu_k} \\pi_{\\theta}(\\mu_0)\\cdots\\pi_{\\theta}(\\mu_k),$\n\n$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~=- \\lim_{K \\rightarrow \\infty} E_{\\mu_0, \\mu_1, ..., \\mu_K} [\\sum_{k = 0}^{K-1} L_{\\mu_0, ..., \\mu_k}]$,\n\nthe expectation is taken over $S_0\\sim d_0(\\cdot),A_0\\sim\\pi_\\theta(S_0,\\cdot)$, and the density function $L_{\\mu_0, ..., \\mu_k}$ is given in (6).\n\n**Monte Carlo Estimator** [2]: Consider a general probabilistic objective $\\mathcal{F}$ of the form:\n\n$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\mathcal{F} \\triangleq \\mathbb{E}_{p(x;\\theta)}[f(x;\\phi)],$\n\nin which a function $f$ of an input variable $x$ with $\\textit{structural parameters}$ $\\phi$ is evaluated on average with respect to an input distribution $p(x; \\theta)$ with $\\textit{distributional parameters}$ $\\theta$.\n\nA Monte Carlo method evaluates the function by first drawing independent samples $\\hat{x}^{(1)},..., \\hat{x}^{(N)}$ from the distribution $p(x; \\theta)$, and then computing the average:\n\n$\\widehat{\\mathcal{F}}_N = \\frac{1}{N} \\sum_{i=1}^{N} f(\\hat{x}^{(i)}), ~~where~~\\hat{x}^{(i)} \\sim p(x; \\theta) ~for~i=1,...,N.$\n\nThe Monte Carlo estimator for conventional objective (2) is \n\n$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\widehat{J}(\\theta) = \\frac{1}{N}\\sum\\limits_{i=1}^{N} R(\\tau^{(i)})$ where $\\tau^{(i)} \\sim P(\\tau^{(i)} | \\pi_{\\theta})$ for $i=1,...,N,$\nand\n$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~P(\\tau^{(i)} | \\pi_{\\theta}) = d_0(s_0^{(i)}) \\cdot \\prod_{k = 0}^{T} \\mathbb{P}(s_{k + 1}^{(i)}| s_k^{(i)}, a_k^{(i)}) \\pi_{\\theta}(a_k^{(i)}|s_k^{(i)}).$\n\nThe Monte Carlo estimator for the Hamiltonian reformulation is \n\n$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\widehat{H}(\\theta) = \\frac{1}{N'}\\sum\\limits_{i=1}^{N'} \\sum_{k = 0}^{K-1} L_{\\mu_0^{(i)}, ..., \\mu_k^{(i)}},$ for $i=1,...,N',$\nand\n$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~L_{\\mu_0^{(i)}, ..., \\mu_k^{(i)}} = \\gamma^k\\cdot R(\\mu_k^{(i)})\\cdot d_0(s_0^{(i)}) \\cdot \\prod_{\\ell = 0}^{k - 1} \\mathbb{P}(s_{\\ell + 1}^{(i)}|\\mu_{\\ell}^{(i)}).$\n\n$\\textbf{Remark:}$ The above two Monte Carlo estimators are quite different in the simulation process. The conventional Monte Carlo estimator samples a random trajectory by following an environment's stochastic transition and a policy. In contrast, our novel Hamiltonian Monte Carlo estimator measures a random path's discounted reward (the \"energy\") without following any policy, and the Hamiltonian equation combinatorially enumerates all possible paths of length $K$ over the state-action space. In other words, the simulation process of the Hamiltonian term does not rely on any policy. Therefore, the Hamiltonian term is a suitable regularizer for both on-policy and off-policy algorithms.\n\nThis fundamental difference is due to the Ising model in (5), which combinatorially enumerates all paths and separates the environment and the policy.", " Thanks for agreeing to raise the score, and very much appreciate your open attitude toward discussions.\n\nThe authors agree that the above three updates (related work of variance inhibiting in DRL, the difference from [23], and clarifying the confusion) are necessary. The authors can promise those updates appearing in the next version.\n", " Thanks for the response. I still find some points unclear to me. \n\nFirst, in (2) the cumulative reward is the expectation of $Q^{\\pi_\\theta}$ with respect to the initial state distribution, which I believe is exactly the same as (7) except for the negative sign. \n\n\nSecond, PPO is an on-policy PG method that only solves the Bellman equation for the current policy. It does not solve the Bellman optimality equation.\n\n\nFinally, the argument that the Hamiltonian helps reduce the variance is still not convincing to me, given that there is no difference between the Hamiltonian raised in (7) and cumulative rewards. It would be more convincing if the authors could provide a more rigorous mathematical justification.", " Upon reading the authors' response to all the reviews including mine, it seems to me that the paper suffers from a major confusion. The Bellman operators operate on the space of bounded real-valued functions over $\\mathcal{S}$ or $\\mathcal{S} \\times \\mathcal{A}$, **not the space of policies**. Therefore, the fixed point of the Bellman equations (or the Bellman operators) are not policies, but value functions. Said value functions are guaranteed to be unique whenever $\\gamma \\in [0, 1)$. However, multiple distinct policies may have the same optimal value function, e.g., when $Q^*(\\mathbf{s}, \\mathbf{a}_1) = Q^*(\\mathbf{s}, \\mathbf{a}_2)$, we may have $\\pi_1(\\mathbf{s}) = \\mathbf{a}_1$ and $\\pi_2(\\mathbf{s}) = \\mathbf{a}_2$, yet $V^{\\pi_1}(\\mathbf{s}) = V^{\\pi_2}(\\mathbf{s}) = V^*(\\mathbf{s})$. Seeing as the paper confuses the uniqueness of the value function (which is the fixed point of the Bellman operator) with the uniqueness of the policy, and this confusion is highlighted *10 times* throughout the paper, I think that the paper needs a major revision, so I am keeping my score.\n\nI also *disagree* with the authors' argument that the paper is of an empirical nature and therefore a proof of how the variance issue is addressed by Hamiltonian regularization is not needed. I think the paper makes big claims about (i) the high variance issue in RL arising from the non-uniqueness of the policy corresponding to the Bellman fixed-points, (ii) addressing the Bellman issue and consequently the variance issue. However, the empirical evidence provided is rather indirect and not sufficient to convince a wide audience of which I am a member. ", " I appreciate the author's response to my questions. I like the idea that \"during the training process, a non-optimal policy is just like in a quantum superposition state\".The motivation appears much more clear given the authors' explanations. I am happy to raise my score if the authos promise to complement the manuscript with the following updates:\n\n1. Discussion about related work of variance inhibiting in DRL.\n2. Discussion about how this work differs from the paper \"K-spin Hamiltonian for quantum-resolvable markov decision processes\".\n3. Addressing the potentially confusing points appeared in the reviews from me and others.", " Thanks very much for your clarification on your question “our motivation and empirical results” and “intuitive motivation”. It is indeed a good question and a good opportunity for the authors to emphasize the foundational contributions. Before replying, the authors made a thorough rechecking of Sutton’s RL book and survey of existing DRL algorithms.\n\nThe authors would like to state several backgrounds of current deep reinforcement learning algorithms.\nFirst, there is a fact that the Optimality Bellman Equation (with a $\\max_a$ operation) is an optimality condition, which originally is a sufficient condition, namely, any optimal policy of MDP and Dynamic Programming problems should satisfy the Optimality Bellman Equation. Note that it is not about algorithm design, however, several (deep) RL algorithms used it, like Q-learning, DQN, etc. Please do not use Q-learning and DQN algorithms as an example of how the Optimality Bellman Equation should be used.\n\nHere, the authors would like to point out that the optimality condition does not discuss how an algorithm should be designed or implemented, but a mathematical principle that any optimal policy should satisfy, as long as the target problem space possesses the MDP structure. Even if the $\\max_a$operation is NOT used in an RL algorithm, an optimal policy should satisfy the Optimality Bellman Equation. However, such an optimal policy under the Optimality Bellman Equation is not unique. In practice, an algorithm will randomly converge to one of many policies. This foundational issue of Optimality Bellman Equation is our strong motivation to consider an alternative, Hamiltonian equation, which is universally used in modern physics.\n\nSecond, the currently most widely used Actor-Critic algorithms (both DDPG and PPO) use Bellman equation (not the optimal one) for training the critic network (for value estimation), and the critic network converges when the Optimality Bellman Equation is satisfied. That is to say, since the Optimality Bellman Equation has multiple fixed points, the obtained critic network will randomly converge to one of many fixed points, thus the trained Actor-Critic agent also randomly converges to one of many fixed points.\n\nThird, Fig. 2 and Table. 2 (vanilla PPO) have given empirical verification about the observation that an DRL algorithm will randomly converge to one of many policies. Such an observation is widely recognized by the DRL community, there are YouTube videos (for example, an upside down policy of the HalfCheetah task: https://www.youtube.com/watch?v=qU8Nd9lyxlw). The authors believe that Fig. 1 (and descriptions in Introduction), Fig. 2, and Table. 2 together strongly motivate our work. Even though there are so many great works, the authors are happy to address such a foundational issue and present to the community an add-on term to mitigate the highly unstable performance of existing DRL algorithms, which is a major criticism from practitioners.\nFourth, differentiating “the training/learning process” and “the obtained optimal policy” is the key to understanding the novel analogy between MDP and K-spin Ising model. “Spin in quantum physics is either 1 or -1 when being measured”, similarly, an optimal policy assigns either 1 or 0 to each state-action pair, which is obtained when optimality is achieved. Please note that an optimal policy is deterministic, i.e., either 1 or 0 for each state-action pair. The authors believe that this fact may be the cause of the reviewer’s confusion. In other words, during the training process, a non-optimal policy is just like in a quantum superposition state; when the training process ends, an optimal policy is “measured” (when the algorithm is converged). Since both the initialization and the training process are random, it is natural to treat the training process as a quantum superposition state; and an optimal policy after convergence (and the Bellman’s optimality equation is satisfied) is just like being “measured” and becomes deterministic.\n\nFurthermore, both the Ising model and lowest-energy state are fundamental in physics. The authors are quite impressed by the fact that the Hamiltonian equation (simple and easy-to-implement) can be used as an add-on term to most actor-critic DRL algorithms (note that we tested over 5 algorithms, i.e., DDPG, PPO, SAC, TD3), and such an add-on term effectively addresses practitioners’ major criticism “unstable”. We are confident that this work will be highly recognized by both NeurIPS community members and industrial practitioners. \n", " I appreciate the authors' reply to my questions.\n\nFirst, for PPO, what I meant was that PPO, which is an on-policy algorithm, does not use an Optimality Bellman Equation, i.e., no max_a operation in value learning. However, the multiple fixed points problem only occurs when the max_a operation is used like in Q-learning. This makes PPO failed to be an empirical evidence to support the authors' foundamental motivation (while I agree it is a good to see PPO+H works well). I also checked other reviewer's comments, which also raise question about the correlation between motivation (BE has multiple fixed points) and the empirical results. \n\nSecond, for the analogy between policy and spin angular momentum. I am still confused about the analogy.  Spin in quantum physics is either 1 or -1 when being measured. The authors said that non-optimal policy is the \"orientation\"--what I understand here is that the non-optimal policy is the spin vector's z-axis component on the bloch sphere, is this what the authors meant? If so, I do not see a intuition here to consider optimal policy as \"being measuted oplicy\". Could the authors explain in more details? I have a certain background in physics but I feel hard to fully grasp the intuition behind the analogy.\n\nSince most of readers of the NeurIPS conference come from computer science and computational neuroscience, and given the fact this work is not quantum RL but physics-inspired conventional deep RL, I think certain efforts need to be made to make the audience understand at least the intuitive motivation.\n\n\n\n\n", " > How does utilizing the Hamiltonian regularizer resolves the raised challenge that Bellman's optimality equation has multiple fixed points? Conjecturing that minimizing energy improves the stability is not sufficiently convincing as the formulation of Hamiltonian in equation (7) seems to be the same as the cumulative reward function. That said, minimizing the energy appears to be the same as maximizing the cumulative reward, which is also the objective of various existing policy gradient approaches. More importantly, the adopted AC-style approach is a policy gradient method, so it does not attempt to solve Bellman's optimality equation directly. In contrast, it improves the cumulative reward function by updating the policy directly.\n\nThe authors have to defend against this comment for several reasons.\n\n* First, from the above response, it is said that minimizing the energy by the Hamiltonian equation is NOT exactly maximizing the cumulative reward (the objective function of RL). Maybe the word “reformulate” caused some misunderstanding. However, they are quite similar, especially when the proposed H-term is an add-on term to regularize the policy network. \n* Second, taking an optimization perspective, multiple fixed points mean multiple critical points (including saddle points and local minima); And Bellman's optimality equation having this issue means that each run with a different random initialization will randomly converge to one of many critical points, which the author believe is a fundamental cause of high variance (the current highly unstable DRL algorithms). Exploiting a term that measures the “energy” of the policy will provide a guide for the training process, which helps converge to critical points with lower energy.  Then, the problem becomes whether those critical points have a similar energy, or whether the Hamiltonian equation is a good metric. Since the Hamiltonian equation is universal for a lot of physical systems (robotic control, movements in gaming, etc.), the authors are confident.  \n* Some backup information on its ubiquity is:\n    1. We found this phenomenon (randomly converging to one of many critical points, as shown in Fig. 2) in combinatorial search problems, e.g., graph max-cut, mixed integer learning programming, traveler salesman problem, and minimum independent cover;\n    2. We even found it in resource allocation of 5G/6G wireless communication systems, e.g., power allocation beamformer design of MIMO base stations, respectively.\n* Third, the adopted AC-style approach is a policy gradient method, which involves an estimate of Q-value (via Bellman's optimality equation). For RL, the Q-value estimation is a dual problem, while the primary problem is policy optimization (say via policy gradient). One claim of this work is that since Bellman’s optimality equation has the inherent issue of high variance (randomly converges to one of many critical points), we propose an add-on term to regularize the policy network. As shown by the experimental result in Section 5.2, we verify that prioritized experience replay (PER) on the policy network, achieved by the H-term, is better than PER on the critic network.\n* In summary, the dual problem of Q-value estimation via Bellman's optimality equation is problematic itself, thus we are hoping this add-on H-term directly on the policy network can address a fundamental issue of DRL algorithms, namely how to reduce the variance of policies with different random seeds.", " Thank you for your insightful feedback. We would like to address your concerns and answer your questions in the following.\n\n> Is the regularizer term in (7) equivalent to (2), namely, the corresponding cumulative reward function? Why does adding the cumulative reward function as a regularizer improves the stability of the actor-critic? In fact, by rewriting the log of products of policies into the sum of log policies and reorganizing the sum, I find the gradient equivalent with REINFORCE (with a truncation of trajectory to the $k$-th step). Can I understand the gradient of Hamiltonian as a variant of policy gradient estimation?\n\nThe regularizer term in (7) is NOT exactly equivalent to (2) (details can be found in Appx. C, Equation (18)). \n\n* First, (2) is the cumulative reward function (the expectation is taken over trajectories); Equation (7) is derived from (1) which is the expectation of discounted rewards along a trajectory. We should have clearly specified $R(\\tau)$ after (2).\n* Second, Fig. 3 made a comparison between REINFORCE’s policy gradient in (12) and the proposed Hamiltonian gradient in (11). Yes, the Hamiltonian gradient is a variant of policy gradient estimation. \n* Third, as shown in Fig. 3, we used a truncation of K steps in (7), and the discounted reward $L(\\cdot)$ in (6) is calculated through Monte Carlo simulation.\n* An interesting point to make is that under the Ising model analogy of MDP/RL, the Hamiltonian equation has clear physical meaning, namely, it measures the “energy” of a policy. For this physically-inspired H-term, we derived a variant of the policy gradient estimation, as a regulation term (Alg. 1 in Line 15). Such an add-on term turns out to be rather simple to implement and delivers substantial performance improvements. ", " Thank you for your insightful feedback. We would like to address your concerns and answer your questions in the following.\n\n> Rather than an analogy between optimal policy and quantum field, should it be just policy with the quantum field?\n\nBoth optimal policy $\\pi^* \\in$ {-1, 1} and policy $\\pi \\in [0, 1]$ could be naturally mapped to a quantum field (a spin configuration). We agree that there was notation reuse and we did not make it explicit. In physics, a spin orients at an angle $\\in [0, 2\\pi)$ and takes continuous value $\\in [-1, 1]$, while the optimal spin configuration takes discrete value {-1, 1}. Therefore, the optimal policy $\\pi^*$ corresponds to the optimal spin configuration, and the policy π corresponds to the case when spins take continuous values. The authors add a new row in Table 1 to help distinguish the mapping for optimal policy $\\pi^*$ and policy $\\pi$.\n\n\n> How much more computational cost is needed for the additional H term as compared to the baseline methods?\n\nThere is a relatively little computational cost when the Hamiltonian gradient is truncated with a small K, say K=24 in Table 2. The authors provided a complexity analysis in Section 4.2 lines 201~205, in which the additional cost only involves a Hamiltonian gradient computation.\n\nFor the reviewer’s concern about the computational cost, as mentioned in lines 263 and 295~296, the authors foresee potential high computational costs for future works if Bellman equations in RL would be replaced by the Hamiltonian equation. Note that the accuracy of the H term approximation is directly related to the K-truncation. Therefore, future works may require a very accurate estimate and need a larger K, which may experience high computational costs. \n\n\n> if the main point is to make an analogy with physical systems and MDP, why the quantum k-spin Ising model is specifically chosen?\n\nThere are two main motivations behind it. The K-spin Ising model matches the sequential decision-making process and the Hamiltonian equation measures the energy of an Ising model (here our policy network).\n\nOn the other hand, the Ising model is a universal model, e.g., NP-hard problems [1], and iterative optimization algorithms [2].\n\n* [1] Lucas, Andrew. \"Ising formulations of many NP problems.\" Frontiers in physics (2014): 5.\n* [2] Li, Ke, and Jitendra Malik. \"Learning to Optimize.\" ICLR, 2017.\n\n\n\n> In the Broader Impace Statement the authors state that 'bring together the strengths of both approaches and yield new insights in both fields'. However, I'm not sure what this can bring for the quantum community?\n\nThere are two aspects that our work will bring insights to the quantum community. \n\nFirst of all, our work is trying to bring the success of DRL algorithms to the quantum RL field, which is an active research area in the quantum machine learning community. \n\nOn the other hand, RL has been an alternative promising approach for solving quantum physics problems, such as CQ, QC, and QQ problems, depending on whether the agent (first symbol) or the environment (second symbol) are classical (C) or quantum (Q).  One recent breakthrough is using RL to control nuclear fusion [1].  \n\nMoreover, the ML community is also very interested in borrowing quantum mechanisms for two major reasons. One is that quantum mechanisms may deliver quadratic improvements in learning efficiency and exponential improvements in performance over limited time periods [2, 3].\n\n* [1] Degrave, Jonas, et al. \"Magnetic control of tokamak plasmas through deep reinforcement learning.\" Nature 602.7897 (2022): 414-419.\n* [2] Biamonte, Jacob, et al. \"Quantum machine learning.\" Nature 549.7671 (2017): 195-202.\n* [3] Dunjko, Vedran, Jacob M. Taylor, and Hans J. Briegel. \"Quantum-enhanced machine learning.\" Physical review letters 117.13 (2016): 130501.", " > I found the paper to be rather difficult to read. The paper could use copy editing.\n\nThe authors will update the manuscript to improve the readability.\n\n> The paper claims to fix the multiple fixed-point issue with the Hamiltonian regularization scheme, but only shows the effect of its usage for a few pedagogical examples. But I would expect to see a theorem attached to this claim with a mathematical proof of how the issue is fixed under the proposed framework.\n\nIn addition to showing the effectiveness of the H term on three examples in both undiscounted and discounted cases, the authors provide experimental results (with visualization results in supplementary materials) on six MuJoCo tasks. Due to the high-dimensional continuous state and action space, these tasks are widely-recognized challenging tasks in robotic control [1].\n\nThe current work is not theoretical. It provides a physical-inspired algorithm design that is easy to implement, delivering significant improvements in performance. The target issue of instability of DRL algorithms is practically important for RL’s adoption in real-world tasks, say robotic control.\n \n\n> In L73, the discount factor is defined as $\\gamma \\in (0, 1]$. In L127, $\\gamma \\in (0, 1)$. Why the difference?\n\nHere we are discussing a practical case of discounted cumulative rewards. $\\gamma < 1$ is required to guarantee a small approximation error.\n\n\n> L114: there is no summation in (6), so why is this called a cumulative reward?\n\nThanks for the careful reading, and the typo is fixed in the revised version.\n\n> In (7), what does a summation from $\\mu_k$ to $\\mathcal{S} \\times \\mathcal{A}$ mean?\n\nThe summation comes from the standard Hamiltonian equation as defined in (5).\n\n\n> L265: if memory budget permits replay buffer size 800 for K=24, for a fair comparison it would make sense to set the buffer size to 800 for K=8 and K=16 too.\n\nThanks for the suggestion, and the authors will provide an experiment using the buffer size 800 for all K values in the Appendix G.2 of the revised version.\n\n> $R(\\tau)$ should be defined after (2).\n\nYes, it should be given right after (2). The cumulative reward along a trajectory $\\tau$.", " Thank you for your feedback. We would like to address your concerns and your questions in the following.\n\n> In both theory and practice, practitioners typically set $\\gamma < 1$ in infinite horizon settings, in which case the Bellman optimality operator has a unique fixed point due to the monotonicity and contraction properties, and the Banach fixed-point theorem. That being said, there exist some exceptional cases such as those discussed in Ch. 3 of the Bertsekas ADP textbook, where discounting with $\\gamma < 1$ may fail to find the optimum policy and additional restrictions on the space of value functions is necessary (Bertsekas, 2019). However, I think saying that the Bellman optimality operator has multiple fixed points (in bold and italics, multiple times) without making it very clear early on that the setting involves $\\gamma \\in [0, 1]$ rather than $\\gamma \\in [0, 1)$ is misleading. \n\nThe reviewer’s objection seems to be highly relying on his/her misreading that our results ONLY hold for the undiscounted case $\\gamma=1$. The authors believe there are several factual errors regarding “motivation”, “soundness” and “practical usefulness”, resulting in highly biased comments on this paper.\n\nFirst, the authors discussed the case of $\\gamma=1$ and the case of $\\gamma \\in (0,1)$ separately. \n* In the Introduction (from line 21 to line 41), for easy understanding, the authors described the Bellman equation’s issue of multiple fixed points with three motivating examples of $\\gamma=1$.\n* The authors deferred the more complex case of $\\gamma \\in (0,1)$ and pointed out that “more examples are given in Fig. 5 and Appx. A”. For some unknown reason, the reviewer ignored the continued discussion. In the revised version, the authors change the wording from “more examples” to “examples with $\\gamma < 1$”.\n\n> Furthermore, there are lots of other valid sources of variance in deep RL including but not limited to optimization of non-convex/non-stationary objectives, stochastic gradients, reward sparsity, initial conditions, complexity of learner function class, etc. that the wording in this paper neglects. I don't suppose existence of multiple fixed points is a primary concern since practitioners use $\\gamma < 1$ in infinite-horizon (and long-horizon) settings. So, I fail to see the motivation behind the proposed regularization approach and remain deeply skeptical of the soundness of this paper.\n\nSecond, the existence of multiple policies is also common in practical tasks with $\\gamma < 1$, as mentioned in recent studies (benchmarks) [1, 2, 3], which contradicts the reviewer’s comment “I don't suppose existence of multiple fixed points is a primary concern since practitioners use $\\gamma < 1$ in infinite-horizon (and long-horizon) settings”. The observational experiments on MuJoCo tasks in Section 2.2 further verify the issue, and the experiments in Section 5 demonstrate the practical usefulness of the H term, where MuJoCo tasks are standard benchmarking tasks for continuous control. \n\nThird, the authors believe that this paper targets the issue of multiple fixed points, while other sources like “optimization of non-convex/non-stationary objectives, stochastic gradients, reward sparsity, initial conditions, complexity of learner function class” are out of the scope. For completeness, we summarize the sources in the Introduction of the revised version. \n*  [1] Duan, Yan, et al. \"Benchmarking deep reinforcement learning for continuous control.\" International Conference on Machine Learning. PMLR, 2016.\n*  [2] Eysenbach, Benjamin, et al. \"Diversity is all you need: Learning skills without a reward function.\" International Conference on Learning Representations. 2018.\n*  [3] Recht, Benjamin. \"A tour of reinforcement learning: The view from continuous control.\" Annual Review of Control, Robotics, and Autonomous Systems 2 (2019): 253-279.", " > There is a lack of discussion about related work in deep RL to reduce variance, e.g., https://openreview.net/pdf?id=9xhgmsNVHu\n\nThe authors acknowledge missing this closely related work, as that paper was presented two weeks before the NeurIPS’ submission deadline. After a careful review, the authors will add more relevant works, including the above one suggested by the reviewer:\n* [2] Bjorck, Johan, Carla P. Gomes, and Kilian Q. Weinberger. \"Is High Variance Unavoidable in RL? A Case Study in Continuous Control.\" International Conference on Learning Representations. 2021.\n* [3] Islam, Riashat, et al. \"Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control.\" RML workshop, ICML,  2017.\n* [4] Nikishin, Evgenii, et al. \"Improving stability in deep reinforcement learning with weight averaging.\" Uncertainty in artificial intelligence workshop on uncertainty in Deep learning. 2018.\n\n> My personal opinion is that the analogy is confusing to people without quantum physics background. And in the end, the algorithm used the classic policy gradient algorithm to estimate the Nabla of Hamiltonian. I feel that Algorithm 1 itself is indeed intuitive (heuristic) without introducing quantum physics. My suggestion is to defer some details of the analogy to the appendix (then the authors can have more space to clearly explain it), and complement the main texts with contents such as related work and ablation studies.\n\nThe authors foresee the reading difficulties for scholars without a quantum physics background, thus providing Table 1 and lines 111~119 for a detailed presentation. The authors like to point out that the H term is a physically inspired algorithm and therefore believe it is necessary to provide the novel analogy in the main body. Given the universality of the quantum K-spin Ising model, it is reasonable that the derived formula is intuitive. \n\nThe authors have to say that ablation studies are naturally included.  1). H-term is an add-on term, so we compare algorithms with and without it; 2). For DDPG, we also compare with DDPG+PER for fairness; and 3). For the key parameter K (steps), we show the results for K=8, K=16, and K=24 in Table 2.\n\n> Line 47: I suggest to add \"often\" or \"sometimes\" because there are cases we want some diversity, e.g., option-critic and DIAYN.\n\n> Line 87: polices --> policies\n\n> Fig.4 is not centered.\n\nThe authors thank the reviewer for the careful reading, and typos are fixed in the revised version.", " > The main motivation is based on that the Bellman’s optimality equation, which is the base of Q-learning-like algorithms such as DDPG, has multiple fixed points. But the algorithm also work well on PPO, which even does not use Bellman equation to learn the value function. Can the authors provide any explanation about why it PPO+H works so well?\n\nThe authors believe the reviewer made a factual error comment that “PPO does not use Bellman equation to learn the value function”. PPO is a policy gradient algorithm with advantage function estimation. \n\nIn the following reference [1], it is theoretically clear how a critic is plugged into the policy gradient theorem in equations (8) and (9). Thus, all actor-ciitic DRL algorithms use the Bellman equation to learn the value function.\n\nAs mentioned in 216~218, the authors use GAE [28] for advantage estimation, where a value function is approximated as a baseline and optimized via the Bellman equation. The authors follow several benchmark implementations as in Stable Baseline3, RLlib, Tianshou, etc, which update the value function by minimizing the TD-residual. Therefore, it is reasonable that the H term also works with PPO.\n\n* [1] Wen, Junfeng, et al. \"Characterizing the gap between actor-critic and policy gradient.\" International Conference on Machine Learning. PMLR, 2021.", " Thank you for your thoughtful comments. We would like to address your concerns and your questions in the following.\n\n>Line 48: \"we make a novel analogy between an MDP and a quantum K-spin Ising model\". However, ref [21] proposed to model MDP with K-spin Hamiltonian. What is the difference?\n\nRef [21] modeled MDP with K-spin Hamiltonian (in Section III), and made an analogy between an MDP and classic field theory in Table I.\nThere are several major differences:\n1. The objective function (10) in [21] has a penalty term (for their quantum optimization approach), while in our deep reinforcement learning (DRL) approach, it is automatically satisfied by employing a softmax function. Moreover, (10) in [21] is the objective function of a quantum optimization task, while we used the Hamiltonian equation as an add-on term (a regularizer) for existing actor-critic DRL algorithms.\n2. Their quantum optimization approach relies on the variational optimality condition (analogy to the Bellman optimality in DRL) and is amenable to quantum simulated annealing algorithms. Here, we use the K-spin Hamiltonian equation to regularize the policy network. A new policy gradient is added in Alg. 1 (line 15).\n3. Their solution discussed the potential implementation on rear-term quantum hardware. Here, our major conclusion is that K-spin Hamiltonian can help reduce the high variance of DRL algorithms, which is brought by the Bellman equation’s issue of multiple fixed points.\n4. Actually, the analogy (in ref [21]) between an MDP and classic field theory in Table I is not physically right. One should replace the classic field by a transverse field (a quantum field). First, there are no corresponding concepts of classic field’s potential energy and kinetic energy in RL. Second, the most important “conservation law” of classic field theory does not have a counterpart in RL. In contrast, our analogy to a quantum K-spin Ising model is more accurate, since the K-spin Ising model matches the sequential decision-making process, and the Hamiltonian equation measures the energy of an Ising model (here our policy network).\n\n> The optimal policy function $\\pi^*$ and a general policy $\\pi$ are mixed-up. E.g., line 111, I understand the optimal policy $\\pi^* \\in$ {0, 1} can be mapped to spin operator, which is a common practice in quantum computation. However, how about $\\pi(\\mu_k)$, which is a continuous-value scalar? Also, in Table 1, the optimal policy is analogous to the spin operators, while the Hamiltonian the functional of non-optimal policy. I am a bit confused about this mixing-up.\n\nWe agree that there was notation reuse and we did not make it explicit on purpose. However, this is a quite standard routine in both algorithmic design and theoretical analysis. In physics, a spin orients at an angle $\\in [0, 2\\pi)$ and takes continuous value $\\in [-1, 1]$, while the optimal spin configuration takes discrete value $\\in$ {-1, 1}. Therefore, both optimal policy $\\pi^* \\in$ {-1, 1} and policy $\\pi \\in [0, 1]$ could be naturally mapped to a quantum field (a spin configuration), through the mapping in lines 111~114. Physicists study the simplified case with a spin $\\in$ {-1, 1} since it already delivers theoretical results of phase transitions; in physical experiments, a spin takes values $\\in [-1, 1]$.\n\nRevision: in Table 1, the authors add a new row to further clarify it. Note that the authors follow the routine in physics that the configuration can take either continuous values $\\in [-1, 1]$ or discrete values $\\in$ {-1, 1}, depending on the context.\nSome exemplar references, where the spin angle takes value $\\in [0, \\pi/2]$. \n* [1] Stoudenmire, Edwin, and David J. Schwab. \"Supervised learning with tensor networks.\" Advances in Neural Information Processing Systems 29 (2016).  \n* [2] Huggins, W., Patil, P., Mitchell, B., Whaley, K. B., & Stoudenmire, E. M. (2019). Towards quantum machine learning with tensor networks. Quantum Science and technology, 4(2), 024001.", " The paper first suggests that the Bellman's optimality equation has multiple fixed points. Then a analogy is made between MDP and qunatum K-spin Ising model, and a reformulation of expected return into quantum K-spin Hamiltonian equation is proposed. It is argued that by regularizing the policy to have a stationary Hamiltonian,  the model can 1). achieves a relative high reward independent of the initialization; and 2). is robust to interference/noise in the inference stage, and thus reduce performance variance among random seeds. This idea has been practically implemented by randomly sampling consecutive trajectories from a specific replay buffer and minimizing the Hamiltonian by policy gradient. The experiments on MuJoCo robotic control tasks have shown the effectiveness of the proposed methods using both DDPG and PPO as base algorithms, in terms of slightly higher mean performance and significantly lower variance. Furthermore, the agents converged to the stationary policy with a substantially higher ratio with the proposed method. [Strength]\n1. The paper touches a relatively important problem in deep RL, namely how to reduce variance of policies with different random seeds.\n2. The proposed method is simple and easy to implement.\n3. The experimental results are good, which show the effectiveness of the proposed methods by reducing variance by 65.2% ~ 85.6%\n4. 3 simple yet motivated examples to show that the Bellman’s optimality equation has multiple fixed points.\n\n[Weakness]\n1. Lack of discussion of related work.\n2. The analogy is hard for a reader without quantum physics background. \n3. Some of the paper's claims need to be further supported.\n4. The paper sometimes mixes-up the optimal policy function $\\pi^*$ and non-optimal policy $\\pi$, making the analogy a bit confusing.\n\nSee below for the details of my concerns.\n\n---------------------- post-rebuttal -----------------\nThe author has resolved most of my conerns and agreed to update the manuscript to address the issues. Correspondingly, I update my score toward acceptance, mainly because the results (Table 2) are appealing with a relatively simple add-on (H term). Nonetheless, I expect the authors in the future to more comprehensively investiagte the motivation using empirical and theoretical analysis to support their claims.\n\n [Major]\n- Line 48: \"we make a novel analogy between an MDP and a quantum K-spin Ising model\". However, ref [21] proposed to model MDP with K-spin Hamiltonian. What is the difference?\n- The optimal policy function $\\pi^*$ and a general policy $\\pi$ are mixed-up. E.g., line 111,  I understand the optimal policy $\\pi^*(\\mu_k) \\in  $ {0 ,1}   can be mapped to spin operator, which is a common practice in quantum computation. However, how about $\\pi(\\mu_k)$, which is a continuous-value scalar? Also, in Table 1, the optimal policy is analogous to the spin operators, while the Hamiltonian the functional of non-optimal policy. I am a bit confused about this mixing-up.\n- The main motivation is based on that the Bellman’s optimality equation, which is the base of Q-learning-like algorithms such as DDPG, has multiple fixed points. But the algorithm also work well on PPO, which even does not use Bellman equation to learn the value function. Can the authors provide any explanation about why it PPO+H works so well?\n- There is a lack of discussion about related work in deep RL to reduce variance, e.g., https://openreview.net/pdf?id=9xhgmsNVHu\n- My personal opinion is that the analogy is confusing to people without quantum physics background. And in the end, the algorithm used the classic policy gradient algorithm to estimate the Nabla of Hamiltonian. I feel that Algorithm 1 itself is indeed intuitive (heuristic) without introducing quantum physics. My suggestion is to defer some details of the analogy to the appendix (then the authors can have more space  to clearly explain it), and complement the main texts with contents such as related work and ablation studies.\n\n\n[Minor]\n- Line 47: I suggest to add \"often\" or \"sometimes\" because there are cases we want some diversity, e.g., option-critic and DIAYN.\n- Line 87: polices --> policies\n- Fig.4 is not centered.\n N/A", " The paper posits that the Bellman optimality operator has multiple fixed-points. It becomes apparent that in defining a discounted MDP, the paper allows discount factors $\\gamma = 1$ in the infinite-horizon setting contrary to conventional wisdom in RL. Arguing that the existence of multiple fixed-points is a key source of high variance in RL, the work draws inspiration from statistical mechanics to regularize actor-critic and policy gradient algorithms, and presents results over PPO and DDPG with reduced variance across seeds and improved average performance.  Weaknesses:\n\n- In both theory and practice, practitioners typically set $\\gamma < 1$ in infinite horizon settings, in which case the Bellman optimality operator has a unique fixed point due to the monotonicity and contraction properties, and the Banach fixed-point theorem. That being said, there exist some exceptional cases such as those discussed in Ch. 3 of the Bertsekas ADP textbook, where discounting with $\\gamma < 1$ may fail to find the optimum policy and additional restrictions on the space of value functions is necessary (Bertsekas, 2019). However, I think saying that the Bellman optimality operator has multiple fixed points (in bold and italics, multiple times) without making it very clear early on that the setting involves $\\gamma \\in [0, 1]$ rather than $\\gamma \\in [0, 1)$ is misleading. Furthermore, there are lots of other valid sources of variance in deep RL including but not limited to optimization of non-convex/non-stationary objectives, stochastic gradients, reward sparsity, initial conditions, complexity of learner function class, etc. that the wording in this paper neglects. I don't suppose existence of multiple fixed points is a primary concern since practitioners use $\\gamma < 1$ in infinite-horizon (and long-horizon) settings. So, I fail to see the motivation behind the proposed regularization approach and remain deeply skeptical of the soundness of this paper.\n- I found the paper to be rather difficult to read. The paper could use copy editing.\n- The paper claims to fix the multiple fixed-point issue with the Hamiltonian regularization scheme, but only shows the effect of its usage for a few pedagogical examples. But I would expect to see a theorem attached to this claim with a mathematical proof of how the issue is fixed under the proposed framework.\n\nStrengths:\n- Despite the awkwardness of the motivation, story and positioning of the paper, I can see some merit in regularizing the policy in the way that this paper proposes. Indeed, temporal regularization has been shown to serve as a good variance reduction technique when applied to the value function [33]. While [8] proposes a form of temporal regularization on the policy with a control prior, their approach requires having access to some known dynamics, while this paper does not. So, I think a major revision of the paper with better motivation, presentation and discussion of related work has potential. - In L73, the discount factor is defined as $\\gamma \\in (0, 1]$. In L127, $\\gamma \\in (0, 1)$. Why the difference?\n- L114: there is no summation in (6), so why is this called a _cumulative_ reward?\n- In (7), what does a summation from $\\mu_k$ to $\\mathcal{S} \\times \\mathcal{A}$ mean?\n- L265: if memory budget permits replay buffer size 800 for K=24, for a fair comparison it would make sense to set the buffer size to 800 for K=8 and K=16 too.\n- $R(\\tau)$ should be defined after (2). As far as I can tell, there is no discussion of limitations, except maybe the complexity trade-off due to the truncation parameter K. I'm curious as to whether there exist MDPs such that Hamiltonian regularization negatively impacts performance. ", " This work proposes to help a policy network stably find a stationary policy by making an analogy between an MDP and a quantum K-spin Ising model. To demonstrate the existence of multiple fixed points of the Bellman Optimality equation, the authors used three examples from dynamic programming. The paper empirically evaluated the performance of the newly proposed method on 6 MuJoCo tasks. Strengths:\nInteresting problem and approach. The instability of DRL algorithms is definitely a major concern.\nDerivation seems to be sound.\n\nWeaknesses:\nThe experiment is quite limited (6 MuJoCo tasks)\nNumber of compared baseline is too few & did not compare the performance with any quantum RL/Hamiltonian mechanics method\nThe computational cost may prohibit its ability to scale to more complex problems 1. Rather than an analogy between optimal policy and quantum field, should it be just policy with the quantum field? \n2. How much more computational cost is needed for the additional H term as compared to the baseline methods?\n3. if the main point is to make an analogy with physical systems and MDP, why the quantum k-spin Ising model is specifically chosen?\n4. In the Broader Impace Statement the authors state that 'bring together the strengths of both approaches and yield new insights in both fields'. However, I'm not sure what this can bring for the quantum community? 1. Experiment is limited to the 6 MuJoCo tasks. \n2. The analogy to 'lowest energy' makes me worry that this method only works for physical tasks (humanoid, hopper, etc.).\n3. Lacking theoretical analysis on the resulting new algo.\n4. Maybe can include a comparison of the computational cost of this method versus trainiing ten times (depends on how many would be required to reach the same level of stability) of the original DRL agent.", " This paper aims to resolve the challenge that Bellman's optimality equation has multiple fixed points, which leads to instability in deriving its solution. To this end, the authors first observe that the evaluation of cumulative reward function can be reformulated into a K-spin Ising model. The authors then propose to use the energy of such an Ising model as a regularizer in the actor-critic algorithm. The authors further conduct multiple experiments on the MuJoCo environment. $\\textbf{Strength.}$\n\nThe quantum K-spin Ising model view of RL is interesting and novel to me. \n\n$\\textbf{Weakness.}$\n\nThe challenge raised by the authors does not seem to be fully addressed by the authors. See Question 2 for the details.\n $\\textbf{Question 1.}$\nIs the regularizer term in (7) equivalent to (2), namely, the corresponding cumulative reward function? Why does adding the cumulative reward function as a regularizer improves the stability of the actor-critic? In fact, by rewriting the log of products of policies into the sum of log policies and reorganizing the sum, I find the gradient equivalent with REINFORCE (with a truncation of trajectory to the $k$-th step). Can I understand the gradient of Hamiltonian as a variant of policy gradient estimation?\n\n$\\textbf{Question 2.}$\nHow does utilizing the Hamiltonian regularizer resolves the raised challenge that Bellman's optimality equation has multiple fixed points? Conjecturing that minimizing energy improves the stability is not sufficiently convincing as the formulation of Hamiltonian in equation (7) seems to be the same as the cumulative reward function. That said, minimizing the energy appears to be the same as maximizing the cumulative reward, which is also the objective of various existing policy gradient approaches. More importantly, the adopted AC-style approach is a policy gradient method, so it does not attempt to solve Bellman's optimality equation directly. In contrast, it improves the cumulative reward function by updating the policy directly. N.A."], "review_score_variance": 2.1875, "summary": "The paper proposes to add a regularisation term H to RL algorithms in order to work around issues caused by the multiple fixed points of the Bellman’s optimality equation. The added H term is inspired by quantum field theory, specifically the K-spin Ising model.\nAll reviewers thought this was an interesting idea, but by the end of the review period, there remained some problems with this paper. Indeed, this paper is not a theory paper, and there is no mathematical proof that the added H term does accomplish the stated goal of variance reduction. This leaves us with empirical evidence. Unfortunately, as was pointed out by reviewers, \"Experiment is limited to the 6 MuJoCo tasks\", which is not enough to convince that the algorithm should generally work. Finally, many reviewers were confused by the claim that PPO solves the Bellman Optimality Equation. By the end of the review, not all reviewers were convinced this problem had been resolved. This point should be clarified, and it would be better for the paper to go through a new round of reviews before being accepted for publication.", "paper_id": "nips_2022_DGwX7wSoC-", "label": "train", "paper_acceptance": "Reject", "anchored_texts": "The paper proposes to add a regularisation term H to RL algorithms in order to work around issues caused by the multiple fixed points of the Bellman’s optimality equation. The added H term is inspired by quantum field theory, specifically the K-spin Ising model.\nAll reviewers thought this was an interesting idea, but by the end of the review period, there remained some problems with this paper. there is no mathematical proof that the added H term does accomplish the stated goal of variance reduction. Unfortunately, as was pointed out by reviewers, \"Experiment is limited to the 6 MuJoCo tasks\", which is not enough to convince that the algorithm should generally work. Finally, many reviewers were confused by the claim that PPO solves the Bellman Optimality Equation. By the end of the review, not all reviewers were convinced this problem had been resolved."}
{"source_documents": ["In visual planning (VP), an agent learns to plan goal-directed behavior from observations of a dynamical system obtained offline, e.g., images obtained from self-supervised robot interaction. VP algorithms essentially combine data-driven perception and planning, and are important for robotic manipulation and navigation domains, among others. A recent and promising approach to VP is the semi-parametric topological memory (SPTM) method, where image samples are treated as nodes in a graph, and the connectivity in the graph is learned using deep image classification. Thus, the learned graph represents the topological connectivity of the data, and planning can be performed using conventional graph search methods. However, training SPTM necessitates a suitable loss function for the connectivity classifier, which requires non-trivial manual tuning. More importantly, SPTM is constricted in its ability to generalize to changes in the domain, as its graph is constructed from direct observations and thus requires collecting new samples for planning. In this paper, we propose Hallucinative Topological Memory (HTM), which overcomes these shortcomings. In HTM, instead of training a discriminative classifier we train an energy function using contrastive predictive coding. In addition, we learn a conditional VAE model that generates samples given a context image of the domain, and use these hallucinated samples for building the connectivity graph, allowing for zero-shot generalization to domain changes. In simulated domains, HTM outperforms conventional SPTM and visual foresight methods in terms of both plan quality and success in long-horizon planning. ", "Context image: The context can in principle be a scene image, camera angles, lightning variables, or any other observation that contains information about the configuration space in the domain. While our experiment are very simple, we found that even this setting of giving the context as the full map is *very challenging* for visual planning methods, so in this work we did not experiment with more complex context variables.\n\nWe point the reviewer to our new experiments, where the context there contains the agent shape and not the obstacle configuration, further demonstrating the generality of our approach.\n\nLong term planning: Moving around an obstacle definitely requires longer horizon planning than pushing an object without obstacles, as in [1,4,6]. That said, investigating visual planning for even longer horizon plans is an important future direction, which our work here gives even better motivation to study. \n\nDetermining that goal has been reached: That’s a good point. In our simulated experiments, we know the groundtruth distance, and use that to stop the policy. In a real-world application, we would use other measures, such as pixel-distance or an image classifier trained to predict task success. \n\nDisconnected hallucinated images: Our method indeed builds a *fully connected* graph whose weights are the inverse of the normalized score function. High weights in the graph can effectively act as disconnection between nodes. Our experiments show that, at least in the domains investigated, our hallucination method is expressive enough to imagine enough diverse images to always find a smooth connected path.\n\nOur method explicitly prevents disconnected hallucinated images. Instead of computing edge-weights as binary value if the classifier score is above a certain threshold (as in SPTM), our method creates a *fully-connected graph*, so this is never an issue. \n\nHuman evaluation: We found that the variance is quite small among the 5 testers. However, we are happy to add more human subjects to the evaluation if the reviewer finds it important. We also attached a link to the planning comparison examples sent to all participants for evaluation, which shows significant distinction in HTM planning results: https://tinyurl.com/htm-visualplan.\n\nAs a final note, please observe that Figure 3 in our submission, which displays the visual plans of our algorithm against a baseline, was incorrect, and we have since posted the correct figure as a response several weeks ago (10/18). We also updated the PDF submission to reflect this.\n\nWe thank you again for the questions to improve our paper. We take each of them seriously and will update the paper accordingly. We hope that our response also help clarifying the paper contributions. \nIn the context of visual planning algorithms, our paper (1) elucidates the difficulty of the problem in simple and easy to reproduce domains, (2) proposes a novel visual planning method, and (3) clearly demonstrates the benefits of the new method compared to previous state of the art. We kindly ask to re-evaluate our work in this context.\n\nAdditional References:\n[1a] Pinto, L. and Gupta, A., Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. In ICRA 2016.\n[2a] Nair, A., Chen, D., Agrawal, P., Isola, P., Abbeel, P., Malik, J. and Levine, S., Combining self-supervised learning and imitation for vision-based rope manipulation. In ICRA 2017.\n[3a] Ebert, F., Finn, C., Lee, A.X. and Levine, S., Self-supervised visual planning with temporal skip connections. arXiv preprint arXiv:1710.05268, 2017.", "Dear Reviewer #1,\n\nWe appreciate your effort reading, reviewing, and giving us valuable feedback. \n\nOur paper tackles the visual planning problem: given images from a dynamical system, learn to predict goal-conditioned image trajectories. This problem has been studied recently in several settings, e.g., SPTM for 3D navigation [22], CIGAN and Visual-MPC for robotic manipulation [1,4,6,13,29].\n\nHere we extend SPTM, but *we do not consider 3D navigation*. Our observations are simply the overhead view images as plotted in the paper. These simple domains allow us to easily assess the capabilities of VP algorithms.\nWhile a navigation policy can indeed be manually coded using the pixel values, *this requires prior knowledge about the task!* - i.e., knowing that this is navigation between obstacles, knowing how obstacles look like, and knowing how to map the image to a relevant state space, action space, and configuration space. Our algorithm, and any other algorithm in the visual planning setting, *does not require any such knowledge in advance!*\n\nWhile these problems are indeed simple, and the planning horizons are relatively short, *state of the art VP methods cannot solve them*! This clearly demonstrates the (in-) capabilities of current VP methods, and demonstrates how difficult the visual planning problem really is.\n\nWe kindly ask the reviewer to re-evaluate our paper *in the context of its contribution to the study of visual planning algorithms*, and not for general navigation problems.\n\nTo further motivate our claims, we have added an additional experiment to show the generality of our approach. In this experiment, an object can be translated and rotated (in SE(2)) slightly per timestep. Different from the experiments in the paper, here we change the object shape (and give the object shape as a context image). The goal is to plan a manipulation of an unseen object through a narrow gap between obstacles in zero-shot. Thus, our algorithm has to learn how object geometry is related to movement between obstacles, and has to generalize this knowledge for planning. Of course, all this has to be learned only from raw images - we use exactly the same algorithm, regardless of the fact that now the state space, action space, and configuration space are different. Please see our general comment for more details and additional results.\n\nPaper writing: We acknowledge that our writing should be improved, and we thank the reviewer for pointing out many unclear points in the paper. We promise to write the paper in a much more accessible format, and clearly relate the math to the application. We emphasize that this can easily be done, and in the following we address the specific unclear points.\n\nSelf-supervised data collection: Following work on self-supervised learning in robotics [1a, 2a, 3a], we mean that data is collected by letting the agent randomly explore the world without any specific supervision/guidance. Thus, we *do not* assume walkthroughs or demonstrations of any task, and the data only contains image sequences without any prior knowledge about the agent or the task. This is similar in spirit to the setting of off-policy batch RL, where the data collection policy is different from the learned behavior policy. \n\nPositive/negative data samples: Given the image sequence data, we use 1-step transitions from the data as positive samples, and random pairs of images from the data (collected under the same context image) as negative samples.\n\nCPC objective in sections 2, 3.2: We will clarify (including math) exactly how we use CPC on our data. As described above, by training on positive image pairs from the data and negative random pairs, the CPC loss learns to assign low energy to feasible transitions and high energy to infeasible transitions. This lets us use it as a proxy for connectivity in the graph.\n\nML trajectory: The main point here is that the CPC output of transition probability (i.e., the energy of the transition) is not normalized. We propose a simple way to normalize it, and this makes interpreting the shortest path as an ML trajectory correct. Thus, the ML interpretation is not novel, but relating it to CPC is. \n\nConnection between planner and policy: The policy is a simple inverse model. It is trained by supervised learning to predict the action needed to bring the current state to the next state on the trajectory dataset. We use this inverse model to track the hallucinated sequence of images outputted by HTM. The same method was used in [29] using a different visual planning algorithm.\n", "Following the reviewers' feedback, we have added an additional experiment to show the generality of our approach. In this experiment, an object can be  translated and rotated (SE(2)) slightly per timestep. Different from the experiments in the paper, here we change the object shape (and give the object shape as a context image). The goal is to plan a manipulation of an unseen object through a narrow gap between obstacles in zero-shot. Thus, our algorithm has to learn how object geometry is related to movement between obstacles, and has to generalize this knowledge for planning. Of course, all this has to be learned only from raw images.\n\nDue to time constraints of the rebuttal period, we were not able to perform a full evaluation, but we present encouraging preliminary results which are reflected in the resubmitted PDF in the Appendix. We trained a CPC energy model on the domain using the object shape as a context image, as shown in Figure 6 of the Appendix. We subsequently ran visual planning on samples collected in the test environment (similar to the SPTM setting) for shapes that were not seen during CPC training. We compare the CPC model with an SPTM-style classifier as a baseline. Note that the CPC energy model is able to generate smooth plans that mimic the proper rotational and movement constraints of unseen objects (see Figure 7). On the other hand, the plans produced by SPTM fail to assume such properties of the new object and often jump around (see Figure 8). We also present preliminary results for HTM, with a CVAE trained conditioned on the shape. The results of the CVAE hallucinated plans can be seen in Figure 9. Although the CVAE was unable to finish training, the results clearly show that HTM in a zero-shot generalization setting is able to generate a successful plan that rotates the object correctly in order to pass through a narrow opening. \n", "Dear Reviewer #3,\n\nThank you very much for your constructive feedback. \n\nAs mentioned in your review, we recognize that our paper has only covered a small subset of possible experiments possible when testing visual planning (VP) problems. \n\nMore difficult experiments: Actually, one contribution of our work is investigating what ‘difficult’ exactly means in the context of visual planning. As we show, methods such as SPTM and visual MPC, which were demonstrated on seemingly more complex tasks that involve first-person navigation or real images, fail on the simple tasks we investigate. This, at the very least, requires us to better think about the different difficulty axes in visual planning. Concretely, along with tackling different view-points and visual clutter, there is the difficulty of understanding the planning problem from an image, and solving it; our work addresses the latter.\n\nThat said, following some of the reviewer’s suggestions, we have added an additional experiment to show the generality of our approach. In this experiment, an object can be  translated and rotated (SE(2)) slightly per timestep. Different from the experiments in the paper, here we change the object shape (and give the object shape as a context image). The goal is to plan a manipulation of an unseen object through a narrow gap between obstacles in zero-shot. Thus, our algorithm has to learn how object geometry is related to movement between obstacles, and has to generalize this knowledge for planning. Of course, all this has to be learned only from raw images. Please see our general comment for more details and additional results.\n\nAsymmetric transitions: Our framework does not assume symmetric transitions. We are using a directed graph, and the bilinear weight matrix in our energy cost function is not symmetric.\n\nSPTM on already explored space: If we were to test SPTM vs. HTM when the space has been explored already, then the only difference between the two models would be (1) the classifier and (2) the method of edge weighting during planning. In Figure 3 (Right), we show that SPTM (vanilla classifier + binary edge weighting) quantitatively averages to about 1.8 L2 distance from the goal, where as our method (CPC energy model + inverse of norm edge weighting) averages to about 0.4 L2 distance. \n\nNote in our experiment we actually test the SPTM and HTM on the same samples from the CVAE. We find that HTM chooses higher fidelity images and the plans are significantly more feasible. This results in a higher execution success rate. \n\nSolution path cost: Measuring the cost of visual plans is indeed an interesting question. One could measure the number of subgoals in the plans. However, an algorithm can output no subgoals at all and claim the shortest plan, or output many small steps to make sure that the low-level policy can follow. This is a tricky problem, and we defer it to human evaluation in this work. We find that the cost seems to be a function of feasibility, fidelity, and completeness given a low-level policy.\n\nDefinitions for fidelity,etc: As requested, we will provide definitions of fidelity, feasibility, and completeness in Table 1 along with the source of the data. \n", "Dear Reviewer #2,\n\nWe are happy to hear that you enjoy our paper and appreciate our contributions. Thank you for your effort in reviewing the paper. \n\n(1) This is a great question, and you are exactly correct. Using the same pool of generated samples, we have found CPC to be more robust against poor generated images, e.g., irregular-shaped block, or double blocks, which give poor scores. On the other hand, a regular SPTM classifier tends to exploit these. We believe that this is due to the fact that the CPC estimates the mutual information which should be lower when the data are poorer. The CPC loss contrasts each positive pair against many negative pairs which make the score function more robust. \n\n(2)-(3) To simplify the learning problem, we modified the decoder architecture for domain 2 such that it learns a mask for combining the context and the CVAE output at the last layer. This helps reduce the number of parameters, speed up the training, and improve our sample quality. With better quality of image samples in the second domain, our algorithm was able to select from a larger pool of more realistic transitions, which elucidates the improvement in feasibility and fidelity scores. In conclusion, due to architecture differences, comparison between domains 1 and 2 is not fair, but comparison between different methods on the same domain is fair.\n\n(4) Yes. We plan to open-source the code for our algorithm, visual planning baselines, and the domains.\n", "The paper presents a method for learning agents to solve visual planning, in particular to navigate to a desired goal position in a maze, with a learned topological map, i.e. a graph, where nodes correspond to positions in the maze and edges correspond accessibility (reachability in a certain number of steps). The work extends previous work (semi parametric topological memory, ref. [22]) in several ways. It claims to address a shortcoming of [22], namely the fact that the graph is calculated offline from random rollouts, by using a conditional variational auto-encoder to predict a set of observed images which could lie between the current position and the goal position, and, most importantly from a context image which describes the layout of the environment. These predicted images are then arranged in a graph through a connectivity predictor, which is trained from rollouts through a contrastive loss. Training is performed on multiple environments, and the context vector provides enough information for this connectivity network to generalize to unseen environments.  At test time, the agent navigates using a planner and a policy. The planner calculates the shortest path on a graph where edges are connectivity probabilities, and the policy is an inverse model trained on the output of the planner.\n\nWhile the idea of a topological memory with dynamic graph creation is certainly interesting, the work is unfortunately not well enough executed and the paper structured written in a way which makes it up to impossible to grasp what has been really done, as much information is missing which would be required for understanding. \n\nAs a first example, we are never really told what the observations are, which the agent sees. The different figures of the paper show very small images with a 2D maze from a bird’s eye view consisting of a walls arranged in a single connected component (mostly 1 to 3 strokes) in red color and an agent shown as a position indicated as a green dot. Are these the observed images? In absence of any other information, this is what we need to assume, and then this problem is fully observable and does not seem to be very challenging. Given the figures, even a handcrafted algorithm should be able to calculate the optimal solution with Dijkstra’s algorithm on a graph calculated from the pixel grid.\n\nThis important missing information alone makes it difficult to assess the paper, but the rest of the writing is similarly confusing. The authors focus on very short and dense descriptions of mathematical properties, but seem to have forgotten to ground the different symbols and to connect them to physical entities of the problem. The technical writing is in large parts disconnected from the problem which is addressed by it.\n\nFurther examples are:\n\n-\t“the data (…) is collected in a self-supervised manner”: what does this mean? Self-supervision is way of creating loss from data without labels, but I am not sure what is meant by collecting data this way.\n-\tThe paragraph on CPC in section 2 can only be understood if the contrastive loss is known. To make the paper self-consistent, this should be properly explained, and tied to a training procedure which details how exactly the positive and negative samples are defined … and collected.\n-\tThe CPC objective in section 2 is only loosely connected to its usage in section 3.2. Barely writing “we optimize a CPC” objective is not sufficient for understanding how this objective is really tied to the different entities of the problem. This paper contains maths (which is always a pleasure to read), but it is not a purely and abstract mathematical problem - a real task is addressed, so it needs to be connected to it. This connection has certainly been done by the authors while they were working on the problem, but they should also communicate it to the reader.\n-\tThe section on ML trajectory is too dense and should be rewritten. I don’t understand what the authors want to tell us here. Basically, a (generalized) Dijkstra is run on a graph, where edge weights are the density or density ratios learned by the CPC objective, and if the edge weights are probabilities, that the shortest path corresponds to a trajectory likelihood. This is known, and this information is buried in a dense set of equations which are difficult to decipher and do not add any further value to the paper.\n-\tThe connection between the planner (generalized Dijkstra) and the policy is never explained. We don’t know how the policy is trained and how it works.\n\nOne of the downsides of the method is that it requires a context image. This image is responsible for the generalization to unseen environments, but it is a major drawback, as the image must be created beforehand. The authors claim that the context image must only contain the layout in any format which makes it possible to extract information about navigational space from it, but in the experiments the context image corresponds to the full map – and it is probably equivalent to the observed images, but we can’t be sure as we haven’t been told. In any case, it is far from sure how this could generalize to more complex environments, let alone 3D navigation as is currently addressed in standard simulators like VizDoom, GIBSON, Matterport, Deepmind Lab, Habitat AI etc. \n\nThe authors’ claim that the proposed environment requires long-term planning, but looking at the images this does not seem to be the case. \n\nThe paper claims to perform zero-shot generalization and to adapt to changes in the environment, like the slight changes in camera motion, variations in lightning, but it unclear how the solution solves this claim.\n\nHow does the agent determine that a goal has been reached, without ground truth information? \n\nWhat happens, if the hallucinated images are disconnected (form several connected components) or are disconnected from the current position and/or from the goal position?\n\nAs mentioned, the method is evaluated on an environment, which is too simple. The experiments are difficult to assess, as we don’t really know what the agent observes. An information asymmetry is mentioned (visual foresight having the object’s (=agent’s) position and the others not) … but if the proposed method observes the bird’s eye view, it can infer the agent’s position (as the position of the green dot).\n\nSubjective evaluation by humans on this kind of simple data does not seem to be meaningful, in particular with a very low number of observers (5 people).\n", "The paper propose a novel visual planning approach which constructs explicit plans from \"hallucinated\" states of the environment. To hallucinate states, it uses a Conditional Variational Autoencoder (which is conditioned on a context image of the domain). To plan, it trains a Contrastive Predictive Coding (CPC) model for judging similarities between states, then applies this model to hallucinated states + start/end states, then runs Dijkstra on the edges weighted by similarities.\n\nI vote for accepting this paper as it tackles two important problems: where to get subgoals for visual planning and what similarity function to use for zero-shot planning. Furthermore, the paper is clearly written, the experiments are well-conducted and analyzed.\n\nDetailed arguments:\n1. Where to get subgoals for visual planning is an important question persistently arising in control tasks. SPTM-style solution is indeed limited because it relies on an exploration sequence as a source of subgoals. Every time the environment changes, data would need to be re-collected. Getting subgoals from a conditional generative model is a neat solution.\n2. Benchmarking similarity functions is crucial. One productive way to approach zero-shot problems is to employ similarity functions, but the question arises: what algorithm to use for training them? The paper compares two popular choices: CPC and Temporal Distance Classification (in particular, R-network). It thus provides guidance that CPC might be a better algorithm for training similarity functions.\n3. The paper is well-positioned in the related work and points to the correct deficiencies of the existing methods. It also features nice experimental design with controlled complexity of the tasks, ablation studies and two relevant baselines.\n\nI would encourage the authors to discuss the following questions:\n1) Fidelity in Table 3 - why is it lower for SPTM compared to HTM if both methods rely on the same generated samples? Is it because HTM selects betters samples than SPTM for its plans?\n2) Why is fidelity larger for SPTM in a more complex task 2?\n3) Same question about fidelity/feasibility for HTM1/2?\n4) Are there any plans to open-source the code?", "The paper presents HTM, an extension of the semiparametric topological memory method that augments the approach with hallucinated nodes and an energy cost function. The hallucination is enabled by a CVAE, conditioned on an image of the environment, and allows the method to generalize to unseen environments. The energy cost function is trained as a contrastive loss and acts as a robustness score for connecting the two samples. The underlying graph is then used to plan for several top view planning problems.\n\nThe paper is well written and clear. I believe such latent representations are an interesting approach to solving visual navigation and general planning. HTM provides an interesting and useful extension to SPTM, allowing both generalization to unseen environments and a more robust loss function. The \n\nMy primary concern is the lack of rigorous experimentation to validate the concept and push it’s limits. The results in Table 1 show HTM outperforms baselines clearly on the given problems, but how it performs on more complex problems is unclear. These problems are dynamically simple and the obstacles are easily identified. Some more difficult problems may be:\n- The mazes in SPTM or environments from https://arxiv.org/pdf/1612.03801.pdf.\n- The original SPTM paper focuses on visual navigation from first person views. How does this method apply to such situations? How does the context translate to this scenario?\n- Planning in real environments with real images, as done in [6].\n\nOther comparisons and notes:\n- Can the method be applied to higher dimensional problems (dimensionality of the underlying space) where planning may be more difficult? E.g. SE(2), robot arms or other agents from UPN [26]. Application with actual 3D workspace problems too would be interesting as the image context may underspecify the environment.\n- The energy cost function acts as a proxy for connection probability when traversing an edge. This may also be useful for dynamical systems (e.g., the mujoco ant navigating a maze). Are there limitations for the method on such problems, e.g., edges may no longer be symmetric?\n- How does SPTM compare when the space has been explored already?\n- Can more quantitative results been shown such as solution path cost?\n- Provide definitions for Fidelity, feasibility, and completeness and the source of data (polling human’s) in the Table 1 caption.\n- “As shown in Table 5.2, “, should be renamed to Table 1.\n"], "review_score_variance": 8.666666666666666, "summary": "The submission presents an approach to visual planning. The work builds on semi-parametric topological memory (SPTM) and introduces ideas that facilitate zero-shot generalization to new environments. The reviews are split. While the ideas are generally perceived as interesting, there are significant concerns about presentation and experimental evaluation. In particular, the work is evaluated in extremely simple environments and scenarios that do not match the experimental settings of other comparable works in this area. The paper was discussed and all reviewers expressed their views following the authors' responses and revision. In particular, R1 posted a detailed justification of their recommendation to reject the paper. The AC agrees that the paper is not ready for publication in a first-tier venue. The AC recommends that the authors seriously consider R1's recommendations.", "paper_id": "iclr_2020_BkgF4kSFPB", "label": "train", "paper_acceptance": "reject", "anchored_texts": "The submission presents an approach to visual planning. The work builds on semi-parametric topological memory (SPTM) and introduces ideas that facilitate zero-shot generalization to new environments. The reviews are split. While the ideas are generally perceived as interesting, there are significant concerns about presentation and experimental evaluation. In particular, the work is evaluated in extremely simple environments and scenarios that do not match the experimental settings of other comparable works in this area. The paper was discussed and all reviewers expressed their views following the authors' responses and revision. In particular, R1 posted a detailed justification of their recommendation to reject the paper. "}
{"source_documents": ["We introduce ES-MAML, a new framework for solving the model agnostic meta learning (MAML) problem based on Evolution Strategies (ES). Existing algorithms for MAML are based on policy gradients, and incur significant difficulties when attempting to estimate second derivatives using backpropagation on stochastic policies. We show how ES can be applied to MAML to obtain an algorithm which avoids the problem of estimating second derivatives, and is also conceptually simple and easy to implement. Moreover, ES-MAML can handle new types of nonsmooth adaptation operators, and other techniques for improving performance and estimation of ES methods become applicable. We show empirically that ES-MAML is competitive with existing methods and often yields better adaptation with fewer queries.", "Note: I was asked to write a last-minute review for this paper since the overall rating of the other reviews are not consistent. Therefore, the review is rather brief and I will comment also on concerns raised by the other reviewers.\n\nThe paper introduces a new MAML algorithm based on evolutionary strategies (ES) for reinforcement learning tasks. Compared to prior MAML algorithms requiring an estimation of the Hessian, ES-MAML demonstrated to be more stable and efficient. Overall, the paper is well motivated, well written and uses a sound mathematical formulation of the solution approach. Furthermore, the results are convincing and show quite some promise.\n\nConcerning the remarks from Reviewer #3, I believe that it is totally fair to use here a simple ES algorithm that still shows reasonable performance. Of course, we would expect that other ES algorithms might perform better, but this is clearly not the point of the paper. Furthermore, also other papers [1,2] showed that very simple ES algorithm can perform very well on weight optimization of policies. \n(Remark: since there is no page limit for refs, I would recommend to cite [1,2] in the paper)\n\nI share some concerns from Reviewer #4 regarding the hyperparameters. By now, it is well known that hyperparameter tuning can improve the performance of RL algorithm quite a bit and is sometimes even the main factor for superior performance. The authors wrote in their reply to Reviewer #4: “In fact, we did not perform much tuning,”. I would like to reply: In fact, this is not a very useful answer.  If there was hyperparameter tuning involved, the amount has to be quantified (in the appendix) and the same amount should be applied to all approaches being compared in the paper.\n\nFurthermore, I missed a discussion about the limitations of the approach. For example, I would expect that the approach will fail if the networks get too large (and thus  the parameter space is too large (>1Mio Parameters?)) and the task is fairly complicated such that the parameter space is not too redundant. I think there is a reason why people tried to use ES for optimizing DNNs for decades, but failed, and now nearly everyone uses GD variants. So, the authors should be more explicit about potential failure cases and limitations.\n\nSmall remark: I haven’t found a description of the architectures used in Section 4.4. Since the paper should be self-contained, I would recommend to briefly make this explicit in the appendix.\n\n[1] Patryk Chrabaszcz, Ilya Loshchilov, Frank Hutter: Back to Basics: Benchmarking Canonical Evolution Strategies for Playing Atari. IJCAI 2018: 1419-1426\n[2] Lior Fuks, Noor Awad, Frank Hutter, Marius Lindauer:\nAn Evolution Strategy with Progressive Episode Lengths for Playing Games. IJCAI 2019: 1234-1240", "UPDATE(11/12): We posted a new version with some few-shot supervised learning experiments (on sine regression) in Appendix A6 and are still looking at bigger image tasks.\n\nThank you very much for the encouraging comments!\n\nWe provide answers to specific questions below.\n\n>> \"PG-MAML is known to be very sensitive w.r.t. hyperparameters, is this also the case for ES-MAML? How were good hyperparameters found for ES-MAML?\"\n\nOur implementation of ES-MAML uses two of the techniques from Augmented Random Search [1], an enhancement of the basic Evolution Strategy. Specifically, we use\n- state normalization: record the running mean and standard deviation of the observations in the state space, and normalize the state vectors\n- reward normalization: normalize the reward values used for each estimation of the ES gradient\n\nThese enhancements make ES relatively insensitive to the choice of hyperparameters (*), by adjusting the magnitude of the gradients. In fact, we did not perform much tuning, and we used the exact same hyperparameters for different environments and tasks. The inner step size \\alpha = 0.05 is inherited from the original PG-MAML paper [2, Appendix A.2]; this was tuned for their PG-MAML experiments but evidently also works for ES-MAML. The other hyperparameters (outer loop step-size, precision parameters) were also fixed across environments and tasks, and were inherited from our defaults on the OpenAI gym benchmarks.\n\n(*) Of course, robustness and reliability remain a challenge in RL [3]. As a comment on the situation, we quote from the paper [1],\n\"To put it another way, ARS is not highly sensitive to the choice of hyperparameters because its success rate when varying hyperparameters is similar to its success rate when performing independent trials with a “good” choice of hyperparameters.\"\n\n>> \"While this work focuses on RL, it would be interesting to see if ES-MAML is also advantages over vanilla MAML for common few-shot learning image classification problems.\"\n\nYes, we hope to post results in the final version. This requires some additional infrastructure work (in particular Tensorflow) into our distributed ES-MAML code. For supervised learning problems, (subsample) gradients of the loss are available: they can be exactly computed through backpropagation. Thus, for SL, the sensible method is to use backprop for the inner adaptation operator, and ES for the outer loop, leading to a mixed algorithm.\n\nWe note an important distinction arises in the supervised setting (SL):\n\nIn normal MAML, both SL and RL produce weight parameter updates through a stored buffer of data. For SL, this buffer contains K images from the task dataset, while for RL, this buffer contains K rollout trajectories from the environment.\n \nIn the ES case, the total reward is calculated from a trajectory, but since there is no RL replay buffer, the state-action data is thrown away after a trajectory is performed and hence the ES agent is restricted to K queries of the total-reward function for adaptation, in a blackbox fashion.\n\nHowever, in the SL case, applying this same blackbox-type logic would imply that each adaptation query consists of evaluating the cross-entropy loss on a single new image (which is thrown away afterwards) and there would also be a maximum of K queries allowed. However, the cross-entropy loss on a single image is simply too inaccurate to represent the full dataset’s population loss and this approach would not be sensible. Thus, we opt to instead use a hybrid method, where the inner loop still retains the K images in a buffer and uses a Tensorflow classifier with normal gradient descent initialized from the MAML-point, while the outer loop performs ES optimization.\n", ">> \"Could you clarify which version of PG-MAML was used as the baseline in your experiments?\"\n\nThe PG-MAML we compared with is the vanilla version from Eq. (2) without additional variance reduction. To remain fair in the comparison, we also did not use any variance reduction techniques for vanilla ES-MAML (e.g. we used ForwardFD which has a higher variance than Antithetic). We wanted a comparison between the simplest variants of the two algorithms to understand their fundamental differences.\n\nAlthough applying additional variance reduction techniques as mentioned by the reviewer might improve the performance of PG-MAML, we believe that it will not affect the main conclusions of the paper: fundamentally, PG-MAML still relies on stochastic policy for adaptation, which could lead to catastrophic outcomes in environments for low K and the low relatively low performance for particularly unstable environments shown in Section 4.3, whereas ES-MAML without any particularly complicated variance reduction techniques is able to perform well in these scenarios due to its deterministic policy use and robustness.\n\n\n>> \"In section 4.2, with reference to the text “one of the main benefits of ES is due to its ability to train compact linear policies, which can outperform hidden-layer policies”, could you clarify what did this text mean?\"\n\nWe have clarified this point in the paper, thanks.\n\nFor vanilla ES, linear policies can outperform hidden layer policies [1] (while policy gradient cannot normally train linear policies without modifications such as [2]), and we find that the same setting exists here, where vanilla ES-MAML allows linear policies to outperform hidden layer policies. \n\nWe make no claims about whether (a necessarily modified variant of) PG-MAML can train with linear policies and whether linear policies under this modified PG-MAML can outperform hidden layer policies under normal PG-MAML. At least a relevant work on this topic is [3], which shows an opposite trend that stacking layers (starting from at least 1 non-linear layer) improves PG-MAML performance, however. \n\n>> \"1. Page 2, R has not been introduced. \n2. Page 3, Section 3.1: does F mean f?\"\n\nThanks, we have fixed these typos.\n\n[1] Horia Mania Aurelia Guy Benjamin Recht: Simple random search provides a competitive approach to reinforcement learning. NIPS 2018\n\n[2] Aravind Rajeswaran, Kendall Lowrey, Emanuel Todorov, Sham Kakade: Towards Generalization and Simplicity in Continuous Control. arXiv:1703.02660, 2018.\n\n[3] Chelsea Finn, Sergey Levine. Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm. arXiv:1710.11622, 2017.\n\n[4] Krzysztof Choromanski, Mark Rowland, Vikas Sindhwani, Richard E. Turner, and Adrian Weller. Structured evolution with compact architectures for scalable policy optimization. ICML 2018\n\n[5] Yunhao Tang, Krzysztof Choromanski, Alp Kucukelbir. Variance Reduction for Evolution Strategies via Structured Control Variates. arXiv:1906.08868, 2019.\n", ">> \"[references]\"\n\nThanks for the references. We have revisited them and added several citations, in particular those of NES, and we have added some discussion of CMA-ES as an alternative to hill climbing as an adaptation operator. However, we would still hold that works focusing *purely* on CMA-ES are not central to ES/ARS for reinforcement learning to the same degree that e.g [2,4] are.\n\n>> \"Why should in (7) the same sigma be used as in (6)? Sigma, alpha etc should be learnable parameters learned by the outer ES.\"\n\nIt is not required that the inner ES-gradient \\sigma must be the same as the outside \\sigma. However, making it a learned parameter introduces the same conceptual issues as varying it over time (see above) and is not trivial.\n\n>> \"3.3.2: you are writing below (1) that rollouts come from a distribution, i.e. are stochastic. How would you implement a hill-climber in the stochastic setting? e.g. consider the case when the rewards are heavy-tailed.\"\n\nFor stochastic rewards, some of the queries can be used to obtain better estimates of the reward function at the same point. Yes, this can be difficult to estimate accurately when the distribution has heavy tails. Empirically, we found that hill climbing was effective on the MAML benchmark problems.\n\nOur motivation for experimenting with hill climbing was not to argue that hill climbing is always a great adaptation operator. Rather, we meant to demonstrate that ES-MAML can use non-smooth adaptation operators in a theoretically principled way (in this case, the local search operator), which is an advantage it has over policy gradient MAML.\n\n>> \"using a hill-climber goes completely against the SOTA in ES which showed repeatedly over the last 20 years that hill-climbing is inferior, especially in larger dimension search-spaces (>100).\"\n\nNote that we are using hill climbing as an inner adaptation operator, not for optimizing the outer ES loop. Our motivation is not to argue that hill climbing is always a great adaptation operator, rather it is an example of a nonsmooth adaptation operator. Since ES-MAML can handle non-smooth operators, we could also use more refined versions of CMA-ES in the inner operator, but this is tangential to the main goals of the paper.\n\n>> \"The experiments use the same hyper parameters for all variants. However, i am not sure this is a fair comparison. E.g. HC has way more spread over the search-space than the other two methods for a given sigma, with following sample steps allowing for fixing the \"too large\" or \"too small\" spread.\nSince the graph of the objective function is flat in a large area of the search space, the additional exploration through stochasticity alone might explain the results of Figure 1. In this case, the result would be pretty artificial, because real ES would adapt their step-size.\"\n\nThe primary comparison we wish to highlight is between ES-MAML and PG-MAML, not between variants of ES-MAML. There are indeed techniques for improving the performance of ES-MAML with ES-gradient adaptation (Algorithm 3), but whether we can make Algorithm 3 better than Algorithm 2 + HC is again tangential.\n\nSince in ES/ARS the estimator provides a stochastic gradient, Algorithm 3 is taking a SGD step with the fixed step-size \\alpha. This is completely standard for MAML (the PG-MAML takes a policy gradient step of fixed step-size). Furthermore, we found that this was the optimal \\alpha parameter - higher or lower \\alpha gave worse performance.\nThe question of whether this is \"real ES\" is only meaningful if one is interpreting ES/ARS as a type of CMA-ES, but this is derived from different principles.\n\n>> \"- In Figure 3, middle image, why does the green curve appear to have decreasing performance after iteration 200?\n- Figure 3/ 4.2 why do the three settings have different values for number of iterations and K? Why does L-DPP only appear in the third task?\n-Section 4.3 and Figure 4: why is there no L-PG and HH-ES? the only curve which is is available for both algorithms has the same performance.\"\n\nIn Fig 3, the performance of L-HC has plateaued at a near-optimality. There are fluctuations in the reward because the algorithm is stochastic, but we did not observe it to be systematically decreasing.\n\nIn Fig 3 and 4, the settings are different because the environments and tasks are different. This is also standard for MAML (e.g., one typically uses a larger K for the Ant agent which is more difficult to learn [1]). The key is that within each environment, the algorithms are given the same K.\n\nIn Figure 4, the reason for omitting L-PG is that PG-MAML does not train to a reasonable performance on linear policies. We found this in our experiments, and it has also been observed in previous MAML work [9]. Note that on the Forward-Backward Walker2d, ES-MAML with both architectures had better performance than PG-MAML; with the Swimmer agent, the hidden layer policies were indeed similar, but the linear policy was clearly above the others.\n", "Thank you for your detailed review and feedback. We provide detailed answers to your questions below.\n\n>> \"Thus, I feel that the paper does not provide significantly new results on these dimensions. For example, the substitution of policy gradient for evolution strategies in Eq. (6) is straightforward, and there is no theoretical justification for the choice of algorithmic designs made in the paper.\"\n\nIndeed, the formulation of MAML using ES via eq. (6) is very straightforward! We see this as an advantage, since it clearly shows that ES can be used to attack MAML, and the resulting algorithm is conceptually very simple but has nice theoretical properties (notably, eliminating the explicit derivative calculations). The ES-MAML algorithm then does not have many 'knobs' to adjust. Could you kindly clarify which parts of the algorithm design you feel have not been justified adequately?\n\nThere are some other theoretical ideas in our paper that we believe are novel, and deserve greater attention within meta-learning. To highlight these:\n\n- Exploration in sparse environments\nA fundamental aspect of PG-MAML in meta-learning is that exploration is off-loaded to the meta-policy. The meta-policy must use its K trajectories to generate (s,a,s',r) samples for its replay buffer, and in the sparse-reward case, the hope is that some of these samples achieved nonzero reward. For a single policy, effective exploration requires it to have high entropy.\nIn contrast, exploration in the parameter space with ES-MAML allows the meta-policy to use different 'styles' of exploration arising from different perturbations. This yields a richer set of behaviors and makes the meta-policy itself more stable. We discuss this further towards the end of the introduction, and in 4.1.\n\n- Estimation of meta-learning gradients\nThere are a lot of challenges in correctly estimating the gradient of the composite MAML loss function. One which has been studied extensively is the problem of estimating the Hessian in PG-MAML (hence the DiCE estimator, LVC, T-MAML, etc.). Another which has received much less attention is that we still need to evaluate rewards under the *adapted trajectories*, i.e., under the distribution of trajectories after task adaptation. However, since we only have *estimates* of the policy after adaptation, the resulting estimates of any quantities based on the adapted trajectory are quite likely to be biased. This issue affects both PG-MAML and ES-MAML, and related to the general problem of unbiased estimation of functions-of-expectations, which is difficult. We discuss this topic in Appendix A.2.\n\nWe also discuss the estimation of Hessians using ES in Appendix A.1. We found empirically that the zero-order ES-MAML is more effective than a 'first-order' ES-MAML using an ES-Hessian, so it is of marginal gain for the meta-learning problem under consideration. However our derivation of the ES Hessian estimator is, to the best of our knowledge, novel.\n\n>> \"I agree that the use of ES gradients avoids the need of second-order derivative estimation; however I am not very sure if we could say that ES-MAML here can address the high variance issue of PG given that ES can also suffer from high variance and that there is a rich literature in reducing variance of PG. \"\n\nYes, it is true that there is a rich literature for reducing the variance of PG. In the case of PG-MAML, there is also extensive literature on reducing variance, specifically of the Hessian component. Our belief is that avoiding explicit dependence on the estimated Hessian is still important, because the concentration properties of stochastic Hessians are generally much weaker than stochastic gradients. So while one may use LVC or control variates, it is an important alternative to sidestep the Hessian estimation altogether.\n\nWe also agree that the variance of ES methods can be problematic. There is also literature on reducing variance of ES gradients. The most simple method for instance is to use the antithetic estimator, but there are more sophisticated modifications such as orthogonal sampling and control variates [4,5].\n", ">> \"What’s the efficiency of ES-MAML compared to PG-MAML in terms of wall-clock time?\"\n\nThis is slightly hard to compare accurately since PG-MAML and First-Order ES-MAML (Appendix A.1) had similarly highly-parallel implementations, whereas Zero-Order ES-MAML was run on fewer cores. The short answer is that PG-MAML took on average 40 seconds for an outer step, and First-Order ES-MAML took about 20 seconds for an outer step. From Figure A.2, Zero- and First-Order ES-MAML are similar in terms of reward/outer iteration, and from Figure 5, PG-MAML and ES-MAML are similar in terms of reward/total rollout, so transitively, similar speeds hold for comparing PG-MAML and Zero-Order ES-MAML.\n\nThe longer answer is really that \"it depends highly on the implementation\". Key aspects are:\n1. Degree of parallelism - ES can be made highly parallel if desired, see below for details.\n2. Distributed computing - we used RPC to parallelize rollouts for ES and the communication time between machines was extremely variable on our network.\n3. Policy implementation - PG-MAML used Tensorflow, whereas for ES-MAML we used a lightweight numpy implementation of forward propagation. Empirically, Tensorflow seems to add overhead.\n4. Hardware - the machines are heterogeneous, and PG-MAML is more dependent on GPUs.\n\nThe main cost is the time to evaluate rollouts. Let T denote the time required to execute one rollout, and let K be the number of queries. To evaluate one sample for the outer loop in ES-MAML, it takes T*(K+1) = O(T*K) time if the rollouts are done sequentially, and T + 1 = O(T) if the K queries are done in parallel. Multiple samples (i.e perturbations) can also be parallelized, so if P perturbations are used, then the maximum time if all rollouts is sequential is O(P*T*K), but this is reduced to O(T) if the rollouts for each perturbation are also done in parallel. For PG-MAML, if the meta-policy is running on K parallelized environments, then this also requires O(T) time, and thus is asymptotically similar to ES-MAML.\n\n\n>> \"multiple times in the paper, \\citep{} and \\citept{} are used incorrectly.\"\nThanks! We fixed these issues.\n\n[1] Horia Mania, Aurelia Guy, Benjamin Recht: Simple random search provides a competitive approach to reinforcement learning. NIPS 2018\n\n[2] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. ICML 2017\n\n[3]  Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep Reinforcement Learning that Matters. AAAI 2018\n", "Our response is long, so we have included a summary and then a detailed response.\n\nImportant point on terminology:\nWe believe the review is inappropriately conflating the \"ES\" algorithms in our paper with other methods called \"evolution\", particularly CMA-ES. While these are related, neither is 'extended' from the other, so e.g. it is not correct to imply that any modifications developed to speed up CMA-ES can also be applied to our ES.\nTo clearly distinguish these, we will refer to our algorithm as \"ES/ARS\" (since it has been called Random Search in some papers), and since the review mainly discusses CMA-ES, we will use \"CMA-ES\" as an umbrella term for their referenced methods.\n\nSUMMARY:\nThe main criticism seems to be that the ES/ARS algorithm used in our paper is not SOTA. We do not believe this to be valid for these reasons:\n1. There are recent papers in RL using ES/ARS which achieve SOTA results [2,3,5,6], so we strongly disagree with the blanket statement that \"the ES used is inferior\".\n2. Moreover, the review does not address MAML or meta-learning, the specific problem class we are solving.\n3. Though ES/ARS and CMA-ES have similarities, they are different methods and correspond to different formulations of the loss function. Techniques such as dynamic sampling variance for CMA-ES are not readily justified in the theoretical framework for ES/ARS. So it is not the case that our ES/ARS is somehow an 'obsolete' version of CMA-ES.\n4. Our experiments show that a simple ES/ARS algorithm (with constant step-size) is *already* competitive with the existing policy gradient methods on MAML. Thus, if it were true that ES/ARS can be improved even more by using a step-size schedule and other heuristics, then this should evidence *in favor* of ES/ARS for meta-learning, not a point against it. That is to say, we are arguing that our ES-MAML has advantages over the current PG-MAML, not that we have designed an \"optimal\" ES.\n\n\nDETAILED COMMENTS:\n\n>> \"Almost all proper ES literature with real working ES algorithms are missing and ESGrad is more than 20 years behind SOTA in the field. Since ES are central to the paper, an algorithm that would not even be considered a baseline at any conference in that field is difficult to accept.\"\n\nThe claim that \"ES Grad is more than 20 years behind SOTA\" and \"would not even be considered a baseline at any conference\" is clearly contradicted by the recently published papers [2,3,5,6].\nWe object to the assertion that ES/ARS is not \"real working ES\"  and to the reviewer's complete dismissal of related work as not being \"proper ES literature\".\n\n>> \"The reason for this is that nowadays all ES use dynamic sample-variances based on progress measures, e.g. Cumulative step-size adaptation and Two-Point-Adaptation as the SOTA. Without this, it can be very difficult to find reasonable solutions.\"\n\nThough there are similarities, ES/ARS is not the same as CMA-ES. ES/ARS algorithms are based on a particular Gaussian smoothing of the loss function, which rigorously establishes that the estimator does yield a stochastic gradient of the smoothed loss. Other elements such as dynamic sample variance can (for specific cases) be viewed as covariance adaptation or as arising from the natural gradient [10], but are not theoretically justified given the objective for ES/ARS. Moreover, many techniques such as the two-point-adaptation are heuristics and currently lack a theoretical basis, and it is not our goal to use every heuristic in our algorithm.\n\nAs to whether such elements are necessary for good performance, [7,8] (using population search) report SOTA results on RL using a simple evolutionary algorithm which does not employ covariance adaptation. So it is not a given that such elements are essential for good performance.\n\nWe also wish to point out again that our experiments show that a simple version of ES/ARS is already competitive with policy gradients on MAML. If additional heuristics can make ES/ARS even better, then that further supports the importance of investigating ES/ARS for meta-learning. Our experiments indicate that we can already obtain reasonable solutions using ES/ARS without those modifications.\n\nRegarding \"ES Gradient\", we want to clarify that the experiments are not using equation (4), which is the algorithm in Box 1, to estimate the ES-gradient; this equation is provided to explain the theory of the smoothing. As we discuss in the paragraph immediately following, in practice the Forward FD or antithetic version of the estimator is used. The antithetic formula has some resemblance to \"Two-Point-Adaptation\" (though, note in the ES/ARS context, it is a method for reducing the estimator variance, not for modifying the step-size). Our experimental results use the Forward FD estimator (as described in the hyperparameters table), not eq (4).\n", "References:\n[1] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. ICML 2017\n\n[2] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv:1703.03864, 2017.\n\n[3] Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search provides a competitive approach to reinforcement learning. NIPS 2018\n\n[4] Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions. Foundations of Computational Mathematics, 17(2):527–566, 2017.\n\n[5] Krzysztof Choromanski, Mark Rowland, Vikas Sindhwani, Richard E. Turner, and Adrian Weller. Structured evolution with compact architectures for scalable policy optimization. ICML 2018\n\n[6] Krzysztof Choromanski, Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Deepali Jain, Yuxiang Yang, Atil Iscen, Jasmine Hsu, and Vikas Sindhwani. Provably robust blackbox optimization for reinforcement learning. CoRL 2019\n\n[7] Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth Stanley, and Jeff Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning. arXiv:1712.06567, 2017.\n\n[8] Sebastian Risi and Kenneth Stanley. Deep neuroevolution of recurrent and discrete world models. arXiv:1906.08857, 2019.\n\n[9] Chelsea Finn, Sergey Levine. Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm. arXiv:1710.11622, 2017.\n\n[10] Daan Wierstra, Tom Schaul, Jan Peters, and Juergen Schmidhuber. Natural Evolution Strategies. 2008 IEEE Congress on Evolutionary Computation", "The authors propose a new method for model agnostic meta learning (MAML) based on evolution strategies (ES) rather than policy gradients (PG). The proposed method has clear advantages over prior work: it is conceptually much simpler, simpler to implement and is a zero-order method (while PG-MAML requires 2nd order derivatives and differentiation through the update steps).  Also, the method natively allows to incorporate methods from evolution strategies, e.g., to improve exploration. Empirical results are convincing: ES-MAML consistently outperforms PG-MAML (or is at least not worse) on various tasks. Also, ES-MAML seems to be much more robust compared to PG-MAML, which is known to be brittle. The paper is well motivated and well written. The mathematical formalism is precise.\n\n\nComment/questions:\n\n- PG-MAML is known to be very sensitive w.r.t. hyperparameters, is this also the case for ES-MAML? How were good hyperparameters found for ES-MAML?\n- While this work focuses on RL, it would be interesint to see if ES-MAML is also advantages over vanilla MAML for common few-shot learning image classification problems.\n- What’s the efficiency of ES-MAML compared to PG-MAML in terms of wall-clock time?\n- (minor:) multiple times in the paper, \\citep{} and \\citept{} are used incorrectly.\n\n", "The paper proposes ES for the task of Model agnostic meta learning. Instead of the gradient-approximation which requires computing a hessian matrix, MC samples from a search distribution are used to estimate a search direction. The approach is validated on a number of experiments.\n\nUnfortunatly, I am unable to accept this paper for a number of reasons. Mainly that the ES used is inferior and the constant step-size used can have a major effect on the experimental outcome. \n\nAlmost all proper ES literature with real working ES algorithms are missing and ESGrad is more than 20 years behind SOTA in the field. Since ES are central to the paper, an algorithm that would not even be considered a baseline at any conference in that field is difficult to accept.\nThe reason for this is that nowadays all ES use dynamic sample-variances based on progress measures, e.g. Cumulative step-size adaptation and Two-Point-Adaptation as the SOTA. Without this, it can be very difficult to find reasonable solutions.\n\t\nMost important missing references from the ES-field in this context:\n\t\n1. and most importantly The original ES-based RL paper:\nHeidrich-Meisner, Verena, and Christian Igel. \"Neuroevolution strategies for episodic reinforcement learning.\" Journal of Algorithms 64.4 (2009): 152-168.\n\t\n2. CMA-ES and NES\nHansen, N., Müller, S. D., & Koumoutsakos, P. (2003). Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES). Evolutionary computation, 11(1), 1-18.\nKrause, O., Arbonès, D. R., & Igel, C. (2016). CMA-ES with optimal covariance update and storage complexity. In Advances in Neural Information Processing Systems (pp. 370-378).\nWierstra, D., Schaul, T., Peters, J., & Schmidhuber, J. (2008, June). Natural evolution strategies. In 2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence) (pp. 3381-3387). IEEE.\n\n3. Review of SOTA in large-scale ES:\nVarelas, K., Auger, A., Brockhoff, D., Hansen, N., ElHara, O. A., Semet, Y., ... & Barbaresco, F. (2018, September). A comparative study of large-scale variants of CMA-ES. In International Conference on Parallel Problem Solving from Nature (pp. 3-15). Springer, Cham.\n\n4. Recent developments for noisy functions (also references other relevant algorithms with noise-handling)\nKrause, O. (2019, July). Large-scale noise-resilient evolution-strategies. In Proceedings of the Genetic and Evolutionary Computation Conference (pp. 682-690). ACM.\n\nSection 3.2\n-Why should in (7) the same sigma be used as in (6)? Sigma, alpha etc should be learnable parameters learned by the outer ES.\n-3.3.2: you are writing below (1) that rollouts come from a distribution, i.e. are stochastic. How would you implement a hill-climber in the stochastic setting? e.g. consider the case when the rewards are heavy-tailed.\n- using a hill-climber goes completely against the SOTA in ES which showed repeatedly over the last 20 years that hill-climbing is inferior, especially in larger dimension search-spaces (>100).\n\nExperiments:\n- I am not an expert of MAML, but i would not consider this as different tasks, just as different environments for the same task. i.e. a circular running strategy should be optimal for all environments. but when considering different tasks, we would consider different policies to be optimal.\n- The experiments use the same hyper parameters for all variants. However, i am not sure this is a fair comparison. E.g. HC has way more spread over the search-space than the other two methods for a given sigma, with following sample steps allowing for fixing the \"too large\" or \"too small\" spread.\nSince the graph of the objective function is flat in a large area of the search space, the additional exploration through stocasticity alone might explain the results of Figure 1. In this case, the result would be pretty artificial, because real ES would adapt their step-size.\n- Similar holds for the number of samples used by the outer ES (n, but named differently in th appendix?). The gradient-based approaches might require a lot more initial points with a smaller K , especially on the flat surfaces of the objectives.\n- In Figure 3, middle image, why does the green curve appear to have decreasing performance after iteration 200?\n- Figure 3/ 4.2 why do the three settings have different values for number of iterations and K? Why does L-DPP only appear in the third task?\n-Section 4.3 and Figure 4: why is there no L-PG and HH-ES? the only curve which is is available for both algorithms has the same performance.", "This paper proposes a method, ES-MAML, for optimizing the Model Agnostic Meta Learning (MAML) objective by using Evolution Strategies (ES) gradients instead of policy gradients (PG) as in the previous approaches in the literature. As a result, the use of ES avoids the need of second-order derivative estimation resulted from PG in computing the gradients of the MAML objective; second-order derivatives in MAML are known to be tricky for proper estimation. They also explore ES-MAML with different advanced adaptation operators to improve the ES gradient estimator. They perform empirical study to demonstrate the benefits of ES-MAML as compared with PG-MAML. In particular, they evaluate the comparable algorithms (ES-MAML and variants vs PG-MAML) in terms of exploratory behaviors in sparse-reward environments, adaptation ability, the stability of deterministic policies in unstable environments, and low-K benchmarks. The experimental results are rigorous and promising. They also discuss several potential extensions to ES-MAML in the appendix. \n\nRegarding the theoretical and algorithmic contributions, this paper combines existing techniques from ES and gradient estimators to make ES gradients work for MAML. Thus, I feel that the paper does not provide significantly new results on these dimensions. For example, the substitution of policy gradient for evolution strategies in Eq. (6) is straightforward, and there is no theoretical justification for the choice of algorithmic designs made in the paper.  However, given that the paper attempts to address an important problem (stably optimizing the MAML objective) with interesting perspective (using ES), that the proposed methods are well developed and extended, and that rigorous experiments to evaluate the proposed methods are provided, this paper could be an interesting contribution to the conference where it can encourage different perspective beyond the gradient policy view for MAML problems. \n\nQuestions and comments. \n\n1. On page 3, with reference to the text “These issues: the difficulty of estimating the Hessian term (3), the typically high variance of ∇θJ(θ) for policy gradient algorithms in general, and the unsuitability of stochastic policies in some domains, lead us to the proposed method ES-MAML in Section 3.” I agree that the use of ES gradients avoids the need of second-order derivative estimation; however I am not very sure if we could say that ES-MAML here can address the high variance issue of PG given that ES can also suffer from high variance and that there is a rich literature in reducing variance of PG. \n\n2. Could you clarify which version of PG-MAML was used as the baseline in your experiments? Is this the “vanilla” version from Eq. (2) without any variance reduction techniques (e.g., Rothfuss et al. (2019), Liu et al. (2019)) or did you include one of the variance reduction techniques to the baseline PG-MAML? \n\n3. In section 4.2, with reference to the text “one of the main benefits of ES is due to its ability to train compact linear policies, which can outperform hidden-layer policies”, could you clarify what did this text mean? Did you mean that compact linear policies are better than hidden-layer policies for MAML, or does it mean that ES is not good at training hidden-layer policies, so it can train linear policies better than hidden-layer policies? \n\nMinor comments.  \n1. Page 2, R has not been introduced. \n2. Page 3, Section 3.1: does F mean f?"], "review_score_variance": 8.1875, "summary": "This paper introduces an evolution strategy for solving the MAML problem. Following up on some other evolutionary methods as alternatives for RL algorithms, this ES-MAML algorithm appears to be quite stable and efficient. The idea makes sense, and the experiments appear strong.\n\nThe scores of the reviews showed a lot of variance: 1,6,8. Therefore, I asked a 4th reviewer for a tie-breaking review, and he/she gave another 8. The rejecting reviewer mostly took objection to the fact that learning rates / step sizes were not tuned consistently, which can easily change the relative ranking of different ES algorithms. Here, I agree with the authors' rebuttal: the fact that even a simple ES algorithm performs well is very promising, and further tuning would only strengthen that result. Nevertheless, it would be useful to assess the algorithm's sensitivity w.r.t. its learning rate / step size.\n\nIn summary, I agree with the tie breaking review and recommend acceptance as a poster.", "paper_id": "iclr_2020_S1exA2NtDB", "label": "train", "paper_acceptance": "accept-poster", "anchored_texts": "This paper introduces an evolution strategy for solving the MAML problem. Following up on some other evolutionary methods as alternatives for RL algorithms, this ES-MAML algorithm appears to be quite stable and efficient. The idea makes sense, and the experiments appear strong.\n\nThe scores of the reviews showed a lot of variance: 1,6,8. he/she gave another 8. The rejecting reviewer mostly took objection to the fact that learning rates / step sizes were not tuned consistently, which can easily change the relative ranking of different ES algorithms. Here, I agree with the authors' rebuttal: the fact that even a simple ES algorithm performs well is very promising, and further tuning would only strengthen that result. Nevertheless, it would be useful to assess the algorithm's sensitivity w.r.t. its learning rate / step size.\n\nIn summary, I agree with the tie breaking review and recommend acceptance as a poster."}
{"source_documents": ["Graph Neural Networks (GNNs) have received tremendous attention recently due to their power in handling graph data for different downstream tasks across different application domains. The key of GNN is its graph convolutional filters, and recently various kinds of filters are designed. However, there still lacks in-depth analysis on (1) Whether there exists a best filter that can perform best on all graph data; (2) Which graph properties will influence the optimal choice of graph filter; (3) How to design appropriate filter adaptive to the graph data. In this paper, we focus on addressing the above three questions. We first propose a novel assessment tool to evaluate the effectiveness of graph convolutional filters for a given graph. Using the assessment tool, we find out that there is no single filter as a `silver bullet' that perform the best on all possible graphs. In addition, different graph structure properties will influence the optimal graph convolutional filter's design choice. Based on these findings, we develop Adaptive Filter Graph Neural Network (AFGNN), a simple but powerful model that can adaptively learn task-specific filter. For a given graph, it leverages graph filter assessment as regularization and learns to combine from a set of base filters. Experiments on both synthetic and real-world benchmark datasets demonstrate that our proposed model can indeed learn an appropriate filter and perform well on graph tasks.", "Thank you so much for the positive feedback! We really appreciate your support for our paper as well as your constructive suggestions. We have improved our paper based on your advice (we marked the modifications related to your suggestions with blue text, and highlighted the previous version with strikethrough): \nhttps://drive.google.com/file/d/1wJYwz1oPDK1-NbpesHUR6ZMCSBRVxdh3/view?usp=sharing\n\nFor Eq. (3): to get the GFD score for a multi-class setting, we first calculate the Fisher Difference for each possible pair of classes, then normalize them based on class size, and finally sum them together to get GFD score. Based on the normalization, the class imbalance would not be a problem. We have improved our writing to make this part more clear.\nFor the writing errors, thank you for pointing them out. We have corrected all the errors pointed out by you and also have carefully gone through the paper to improve the writing.\n", "Thank you for your valuable feedback! We improved our paper based on your advice (we marked the modifications related to your suggestions with orange text, and highlighted the previous version with strikethrough)： \nhttps://drive.google.com/file/d/1qAe72_w9Zn_mXg7rwu25o54kcQ6QhsME/view?usp=sharing\n\nThe reviewer is majorly concerned about whether our pointed problem indeed exists on real-world datasets, and whether our proposed method can solve it. To alleviate this concern, we added a new real-world dataset and show consistent results with our analysis, which is discussed in Q1. \n\nNow we address your comments and concerns in detail:\n\nQ1: Synthetic datasets appear to be extreme and unrealistic and look carefully selected in favor of the proposed method.\nA1: The problem we found out is not unrealistic and carefully selected, but indeed appear in real-world datasets. Each synthetic dataset we choose corresponds to a specific graph data property we analyze in Section 3.2, including \"small density gap\" (i.e., the graph structure is not highly correlated with labels) and \"large label ratio gap\" (i.e., classes are imbalanced). These properties widely occur in real-world datasets. To further justify this, we choose the \"large label ratio gap\" as an example and try to find a graph dataset that has this problem. We download a large scale academic graph called Open Academic Graph (OAG) [1][2][3] and choose two fields that have a large disparity in the number of papers: (1) “History of ideas”, which consists of 1041 papers; and (2) “Public history”, which consists of 150 papers. Obviously, these two classes are imbalanced so that the graph data has the \"large label ratio gap” problem. We then compare our method with baselines on this OAG graph data (We open-source this dataset in the github. Detailed experiment settings and results are in Appendix A.10). According to our results (Table 8), our proposed AFGNN_inf achieves 88.22 macro F1-score, which outperforms all the other baselines (our macro F1 is at least 3% higher than all the baselines). Such a result on the real-world dataset is consistent with what we achieve on the same synthetic dataset, which indicates that the problem we reveal is not “unrealistic”, but exists in real-world datasets. The widely adopted benchmark graph datasets (cora, citeseer, pubmed), however, do not have these potential problems. Thus our analysis can also benefit the GNN research community to find other representative benchmark datasets for the node classification task.\n\nQ2: Model is not novel\nA2: We’d like to emphasize that the main contribution of our work is the proposed graph filter assessment tool (GFD score) and the insights we found with the tool, which provides a unique perspective in understanding why GNN will work and how we should choose graph filters for graph data with different properties. The AFGNN model is our first attempt to learn a flexible filter that is adaptive to the graph data by leveraging the GFD score, which has successfully demonstrated the power of our assessment tool. The basic idea of the AFGNN is simple, but it works well with much less memory and time consumption than the sophisticated model as GAT. According to our results (Table 6 and 7), on Cora and Citeseer, GAT's time cost is at least three times of AFGNN's time cost, and GAT’s memory cost is two times of AFGNN's memory cost.  \n\nQ3: \"Regularization term\" is inadequate.\nA3:  Thanks for pointing it out. The standard definition of regularization is: regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting. Our GFD is added to the loss function to guide the learning process of the filter and to avoid overfitting, therefore, previously we call this GFD term a regularization. To avoid confusion, we have changed it into “GFD loss”.\n\nQ4: $AFGNN_{infinity}$ is not equivalent to applying infinite lambda\nA4: Thanks for pointing it out, we agree that our previous writing in this part is not accurate enough. We use the previous writing to emphasize we optimize $\\alpha$ and $W$ iteratively by minimizing CrossEntropy loss and GFD loss respectively. We have improved our writing now. \n\nQ5: For $AFGNN_1$, is it also iteratively optimized as $AFGNN_{infinity}$? \nA5: $AFGNN_1$ and $AFGNN_{infinity}$ are different. For $AFGNN_1$, we learn $\\alpha$ and W simultaneously by directly minimizing the overall objective function (=CrossEntropy loss + GFD loss), but for AFGNN_inf, we learn $\\alpha$ and $W$ separately. We have improved our writing for this part to make it more clear. \n", "Thank you for your constructive comments! We improved our paper based on your advice (we marked the modifications related to your suggestions with red text, and highlighted the previous version with strikethrough): \nhttps://drive.google.com/file/d/1adtmKH61RLyBKzn46DWdEj5JQvu5M5WO/view?usp=sharing\n\nFirst, we want to emphasize this paper’s contribution. Our work provides a theoretical understanding to  GNNs in a novel way, and we are the first to analyze GNNs for the node classification task from a data perspective. Since rich literature demonstrated that the key of GNNs lies in their graph convolutional filters, we propose a new assessment tool (GFD) to evaluate the effectiveness of filters given a specific graph data Further, this tool is applied to analyze existing filters and found some meaningful insights. Finally, we propose the AFGNN model to automatically learn the best filter from the given family (i.e., learn the best coefficients for the linear combination of a set of filters) for the given graph data.\n\n\nNow we address your comments and concerns in detail:\n\nQ1: Is it reasonable to use the Fisher score” to support the second term in equation (3), where the Fisher score is used to evaluate before the filters applied?\nA1: We’d like to clarify our GFD is an assessment tool to see whether a filter is good for a particular graph data.\nFisher Score is used to evaluate the separability of data. GFD defined in  Eq. (3), which is the Difference of Fisher Score before and after the filter, is to evaluate whether a filter can increase the data separability. \nAs we show that a good graph convolutional filter can help the non-linear separable data to be linearly separable, we can expect the GFD score is higher for these filters. Note that not every graph filter has this property for every dataset, and our final model is to learn the best filter that could enhance this property for a given dataset. Experimental results in Figure 5 and Table 2 also support this claim.\n\nQ2: The presentation of the last paragraph of “graph filter discriminant score” in page 4 can be improved. The Figure references seem incorrect and confusing.\nA2: As mentioned in Q1, we use Fisher score to evaluate the separability of two classes, we use Fisher Score Difference to evaluate the power of a filter on two classes, and we use GFD, which is a weighted sum of Fisher Score Difference of each pair of classes, to evaluate the power of a filter on the given graph. We have revised our writing and corrected our Figure reference to make this more clear. \n\nQ3: The analysis of the influence of label ratio seems not accurate enough.\nA3: Suppose the density and density gap are fixed, when label ratio drops, which means the two classes become more imbalanced, and nodes in a larger class tend to have more neighbors. Then, with the column normalization strategy that does not have any constraint on the range of representation, those nodes with a larger degree tend to aggregate more information and thus have larger new representations. This would be helpful to differentiate the two classes. Take Figure 6 (g) as an example, nodes in the large-size class (green nodes) are gathered in the upper right part while nodes in the small-size class (purple nodes) are gathered in the lower left part, so the two classes become more separable after applying a column-normalized filter. We revised our writing to make this more clear.\n\n\nQ4: For the GFD score comparison in Figure 4, why choose order 1,3,7 for density and different order 2,3,6 for the density gap?\nA4: Thanks for pointing out. Previously we just pick the orders that can show our findings most clearly, but we agree it is important to use consistent choice in two subfigures. We now choose the same set of orders in these two figures. The result remains the same. \n\nQ5: What is the meaning of the symbol psi(l)?\nA5: It is a learnable intermediate weight (before normalization) for each base filter. We then apply softmax normalization to it to get alpha(l). We revised our writing to make this more clear.\n\n", "Q6: The results on three real datasets do not show significant gains.\nA6: First, our method performs the best among all the baselines that also adopt the same base filter family, and only perform worse than GAT, which has a much wider filter family space than us. However, GAT actually requires much more computation resources than us. Compared with GAT, our model needs less time and memory costs (According to our results in Table 6 and 7, GAT's time cost is at least three times of AFGNN's, and GAT’s memory cost is two times of AFGNN's.). Also, our model can deal with class imbalance issues much better than GAT (the performance of GAT on SmallRatio and Imbalanced OAG are not good, our $AFGNN_{infinity}$ achieves 93.8 and 96.3 on these two datasets, while GAT  only achieves 82.1 and 95.1). Second, currently we only use a family of simple base filters, and the performance of AFGNN is expected to be further improved by enlarging the filter family. We leave the design of the filter family as future work. Finally, we want to emphasize again that this paper’s biggest contribution is to understand and evaluate GNNs’ filter rather than to propose another GNN model. We find some hard cases that existing GNNs with fixed filter can not handle well, and propose AFGNN to enhance the performance under these hard cases. Existing benchmark real-world datasets are not sensitive enough to differentiate existing baseline models, so we also analyze how to find a more powerful dataset that can differentiate different models and generate synthetic datasets based on our analytics. \n\nQ7: Inductive learning (e.g., PPI) is not tested.\nA7: Thanks for pointing out. Our current proposed model is designed mainly for transductive semi-supervised node classification, and may have some limitations for the inductive setting in some cases. But our analysis result can help design the inductive filter learning model.\n\nOur analysis assumes that for a set of feature information (X), structure information (A), and their dependency relationship with labels (A|Y and X|Y), there should exist an optimal filter, and our algorithm is designed to learn a good filter for a single graph, which can approximate such optimal filter. For a transductive setting (for example, Cora, Citeseer, Pubmed), where we only need to deal with a single graph, our algorithm has shown to be effective to learn a good filter for it. For an inductive setting (for example, PPI dataset), where we are dealing with multiple graphs with different graphs, the structure information (A) is different for each graph, and thus the optimal filter for each graph can be different. Since our current algorithm can only learn a single filter for all the graphs, if the optimal filters for testing graphs are different from what the ones for training graphs, our current algorithm cannot improve the performance too much. For cases where the graph structure property doesn’t change too much, we can assume that there still exists a single optimal filter and our current algorithm can generalize well.\n\nWe evaluate our model on PPI and found our AFGNN obtains better performance than all the other baselines but is worse than GAT. This is mainly because PPI have different chemical graphs that have totally different structures, and thus fall into the first case where our algorithm that only learns a single filter cannot improve the performance too much.\n\nIn spite of the limitation of our current model, we’d like to point out that it is feasible to adopt our analysis result for designing an inductive filter learning algorithm. Since we’ve already found that we can infer the optimal filter based on the graph data’s properties (e.g., label imbalance will benefit column normalization, etc), we can design an model (f) that takes these graph properties as input and infer the optimal alpha, instead of learning alpha from scratch for a new graph. If we can train the f with graphs with various properties, ideally it should learn to get optimal filter for any kind of graph data. In this way, such a model can be well suitable for inductive node classification. Therefore, our analysis can still be useful to deal with inductive node classification and even other graph-related tasks. Since such model improvement is out of range to what we want to focus on in this paper, we leave it as future work.\n\n\n\n[1] https://www.openacademic.ai/oag/\n[2] Fanjin et al. OAG: Toward Linking Large-scale Heterogeneous Entity Graphs. In Proc. of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD'19)\n[3] Arnab Sinha, et al. An Overview of Microsoft Academic Service (MAS) and Applications. In Proc. of the 24th International Conference on World Wide Web (WWW ’15)\n\n\n\n\n\n\n", "We appreciate your valuable comments and we are now actively working on the supplementary experiments on real-world datasets!", "Q6: It would be better if the explanation of the training loss section is more detailed and clear.\nA6: Generally, our overall loss is a weighted sum of cross-entropy loss in terms of node classification and GFD loss in terms of the filter’s capability in enhancing linear separability. By changing the value of the weight, we have $AFGNN_0$, $AFGNN_1$, and $AFGNN_{infinity}$. $AFGNN_0$ and $AFGNN_1$ correspond to the case that the weight of GFD loss is 0 and 1 respectively, and parameter $\\alpha$ and W are optimized simultaneously with overall loss. $AFGNN_{infinity}$ is different, and it is not exactly the case where the weight equals infinity. To train $AFGNN_{infinity}$, we iteratively optimize $\\alpha$ and W with GFD loss and classification loss respectively. Thanks for pointing out this unclear part, we have revised our writing to make it more clear.\n\nQ7: What is “AFGNN_P” in the experiment analysis?\nA7: It should be $AFGNN_{infinity}$, thanks for pointing out this typo.\n\nQ8: It could be interesting to see the time comparison between the proposed method and the GAT.\nA8: Thanks for the valuable suggestion! We have added the time, memory comparison table in Appendix A.9, our experiment results show our AFGNN models need less time and memory consumption. According to our results (Table 6, 7), on Cora and Citeseer, GAT's time cost is at least three times of AFGNN's time cost, and GAT’s memory cost is two times of AFGNN's memory cost. GAT does not have recorded time and memory cost for Pubmed dataset because it requires too much memory cost and is not able to run on GPU. Therefore, we claim that AFGNN needs less time and memory cost than GAT.\n\nQ9: For the graph filter discriminant analysis, is it fair to compare the learned layer with the other base filter using the GFD score? Since the learned layer is picked with the highest GFD score. Maybe one or two sentences on this will be helpful.\nA9: First, we’d like to clarify that we do not pick the best filter from the filter family. Instead, the combination weights (alpha) are learned in an end-to-end fashion on the training dataset, while the evaluated GFD scores are calculated on the test dataset. Therefore, it’s not guaranteed that a learned filter will definitely generalize better than all the base filters. Second, this experiment is to verify whether we can learn an optimal filter adaptively instead of using a fixed filter. For different datasets, there exist different optimal base filters (for example, column normalization is the best for SmallRatio and row normalize is the best for benchmark citation network), and our algorithm can indeed learn a good combination of them that generalizes well, as we expected.\n\nQ10: The writing of the paper must be improved. Too many typos and grammar problems will impair the presentation and the reader can be distracted.\nA10: Thank you for pointing them out.  We have carefully proofread our paper again and polished the paper to alleviate typos and grammar errors.\n\n", "This is a very interesting study about GNN. Authors proposed to extend LDA as a discrimination evaluator for graph filleters. Also authors proposed Adaptive Filter Graph Neural Network to find the optimal filter within a limited family of graph convolutional filters. The whole study is novel, and beneficial for the community of graph neural network study. It provides a new way to understand and evaluate GNN.\n\nThere are some questions authors should clarify. And some writing errors to correct.\nEq (3) defines GFD for a pair of classes i and j. For a graph with more than two classes, the GFD will be the average of all pairs? Will class imbalance will have any impact on this GFD measure?  \nErrors:\n•\twe studies the roles of this two components \n•\tthere exist a best choice \n•\twe only consider to to find\n", "In this paper, the authors raise and address three questions about graph neural network: (1) Whether there is a best filter that works for all graphs. (2) Which properties of the graph will influence the performance of the graph filter. (3) How to design a method to adaptively find the optimal filter for a given graph.\nThe paper proposes an assessment method called the Graph Filter Discriminant Score based on the Fisher Score. It measures how well the graph convolutional filter discriminate node representations of different classes in the graph by comparing the Fisher score before and after the filter.\nBased on the GFD scores of different normalization strategy and different order of the graph filter in the experiments on synthetic data, the authors answer the first two questions: (1) There is no optimal normalization for all graphs. (2) row normalization performs better with lower power-law coefficient, but works worse with imbalanced label classes and large density gap.\nFor the third question, the authors propose a learnable linear combination of a limited family of graph convolutional filters as the layer of model AFGNN, which can learn the optimal arguments of the combination based on the FGD score.\nThe paper focuses on a significant topic and proposes an assessment tool for the graph filters. Based on that, it also introduces a model to choose filters from a family of filters for any specific graph.\nThe description of preliminaries is clear.\nThe observations of the impact of the graph properties on the filter choice are interesting and explanations are provided.\nThe results of the test accuracy on both bench mark and synthetic datasets demonstrate the good performance of the proposed model.\nIt is good that the paper provides proof for the claim that the graph convolutional can help the non-linear separable data to be linearly separable, so it is reasonable to use Fisher score. However, does this claim support the second term in equation (3), where the Fisher score is used to evaluate before the filters applied?\nThe presentation of the last paragraph of “graph filter discriminant score” in page 4 can be improved. The Figure references seem incorrect and confusing.\nThe analysis of the influence of label ratio seems not accurate enough.\nFor the GFD score comparison in Figure 4, why choose order 1,3,7 for density and different order 2,3,6 for density gap?\nWhat is the meaning of the symbol psi(l)?\nIt would be better if the explanation of the raining loss section is more detailed and clear.\nWhat is “AFGNN_P” in the experiment analysis?\nIt could be interesting to see the comparison of time between the proposed method and the GAT.\nFor the graph filter discriminate analysis, is it fair to compare the learned layer with the other base filter using the GFD score? Since the learned layer is picked with highest GFD score. Maybe one or two sentences on this will be helpful.\nThe writing of the paper must be improved. Too many typos and grammar problems will impair the presentation and the reader can be distracted.\nMinor comments:\nThe layout of the sub caption of Figure 1 can be improved.\nThe usage of capital letter in the phrase “density gap” is inconsistent.\n“As shown in figure” instead of “As is shown in figure”.\nMany sentences miss article.\nThere are many typos in the writing.\nFor example, “Note that for given (feature)”, “…make the representation of nodes in different (class) more separable.”, “Noted that there are some other (variant) of GNN filters that (does) not fall into…” in page 4.\n“Here we give (a) empirical explanation to this phenomenon”, “this normalization strategy (take) into account…”, “Thus even in the case that the other two (doesn’t) perform well…” in page 5.\n“…a very important factor that (influence) the choice of normalization strategy”, “when power-law coefficient (decrease)”, “when the (sizes) of each class (become) more imbalanced”,  “This is because column normalization better (leverage) …” , “in a similar manner (with) label ratio”, “when the density or density gap (increase)”, “high-order filters can help gather… and thus (makes) the representations…”, “when the density gap (increase)” in page 6.\nThese can be continued but it is obvious that this paper needs proofreading.\n", "This paper introduces an assessment framework for an in-depth analysis of the effect of graph convolutional filters and proposes a novel graph neural network with adaptable filters based on the analysis. The assessment framework builts on the Fisher discriminant score of features and can also be used as an additional (regularization) term for choosing optimal filters in training. The assessment result shows that there is no single graph filter for all types of graph structures. Experiments on both synthetic and real-world benchmark datasets demonstrate that the proposed adaptive GNN can learn appropriate filters for different graph tasks. \n\nThe proposed analysis using the Fisher score is reasonable and interesting, giving us an insight into the role of graph filters. Even though the analysis is limited (using simple graph models and filter family) and the result is not surprising (given no free lunch theorem, there is very likely to be no single silver bullet fo graph filters), I appreciate the analysis and the result. But, I have some concerns as follows. \n\n1) The proposed GNN and the optimization process\nThe proposed method is to extend CNN to a simple linear combination of different filter bases with learnable weights, which I don't think is very novel. Adding the GFD score as an additional constraint term is interesting, but the way of optimizing the whole objective function is unclear. (In addition, I think calling it the \"regularization term\" is inadequate since the term actually involves data observation, rather than a prior on parameters only.) \nIn the case of AFGNN_inf, I don't think it is equivalent to applying infinite lamda. If lamda is infinite, L_CE needs to be completely ignored. This needs to be clarified. \nIn the case of AFGNN1, I don't clearly understand how the whole objective function is properly optimized with fixed data representation. Is it also iteratively optimized? I hope this is also clarified in more detail. \n\n2) Unconvincing experiments\nThe results on three real datasets do not show significant gains, and two of them are even worse than those of GAT. Furthermore, inductive learning (e.g., protein-protein interaction (PPI) dataset used in GAT) is not tested, which I think needs to be also evaluated. While two synthetic datasets (SmallGap and SmallRatio) created by the authors show significant improvement, these datasets appear to be extreme and unrealistic and look carefully selected in favor of the proposed method. I recommend the authors use for evaluation more realistic datasets that can be found in related research.  \n", "Thank you so much for the comment!\n\nThe key point of our work is to analyze the GNNs for node classification from a data perspective. We pointed out that there’s no best GNN filter for all datasets, and we proposed a GFD score that can assess the power of filter and help to find the optimal filter for a given dataset as well. \n\nFor question 1: We proposed the AFGNN to verify our analysis, according to the experiment result, for whichever dataset, the performance of our proposed AFGNN is always among the best, which shows AFGNN is robust and effective and indicates GFD can help to select the best filter for a given dataset. Also, the current benchmark datasets cannot clearly differentiate different graph neural networks (as shown in Table 2, while the order is the same, scores of different filters are close to each other). So in our work, we identify some challenging cases for existing GNNs, then create corresponding synthetic benchmark datasets to test all GNN models. It will also guide us to look for real-world graph data to serve as the new benchmark datasets.\n\nFor question 2: For the benchmark datasets split, we used 20 samples each class for training, 500 samples in total for validation, and 1000 samples in total for test. This is a standard split strategy, and most existing works, including all of our baselines (GCN, GAT, GFNN, SGC), follow this strategy. It’s true that using the mean results of multi-splitting methods may help to reduce the impact of dataset partitioning on experimental results. But in order to have fair comparisons between our model and baselines, we follow the split convention. \n", "Thank you for the nice work. I really appreciate the idea of GFD score and the detailed analysis. Here are some aspects I care about.\n\n1. The improvement of AFGNN seems marginal on the real dataset.\nAlthough I like the idea of AFGNN, which combines different filters and adaptively learn a graph-specific one, the performance improvement of it on the real dataset (CORA/CiteSeer/Pubmed) seems marginal according to Table1. (BTW, is this result statistically significant?)  While in the two manual dataset, the results are excellent. So what is the reason for the performance gap? And can AFGNN be extended to be more suitable for the actual data?\n\n2. About the dataset split.\nFor the three benchmark datasets, you adopt the same setting of (Kipf & Welling, 2017). I guess it should be 20 samples each class for training and 30 samples each class for developing?  Is your dataset split fixed in all the experiment? Existing work [1] has proven that the split of dataset has a significant influence on the classification result. So I think using the mean results of multi-splitting methods may be a better idea for the node classification task likes [2][3].\n\n[1]Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, Stephan Günneman: Pitfalls of Graph Neural Network Evaluation\n[2]Sun, K.; Koniusz, P.; and Wang, J Fisher-Bures Adversary Graph Convolutional Networks.\n[3]Deli Chen, Yankai Lin, Wei Li, Peng Li, JieZhou, Xu Sun: Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View", "We've found a typo when we define training loss in formula 5. The GFD score should be the cumulative negation GFD score of the filter in each layer with respect to its previous layer's output. Previously we missed the GFD. The corrected version is in https://github.com/conferencesub/ICLR_2020/blob/master/DissectingGNN_ICLR%20(3)%20(1).pdf\n\nSorry for the mistake.", "This paper tackles the problem of GNN property and explainability, by studying how graph convolution kernels discriminate nodes from different classes. \n\nI think the primary novelty in this paper is that it provides easy and straightforward interpretation on GNN convolution kernels, which has been previously thought of as hard to depict. In addition, the techniques and intuitions are extremely straightforward and elegant, which is really a surprise. ", "Thank you for your comment and your interest in our work!"], "review_score_variance": 8.666666666666666, "summary": "The paper investigates graph convolutional filters, and proposes an adaptation of the Fisher score to assess the quality of a convolutional filter. Formally, the defined Graph Filter Discriminant Score assesses how the filter improves the Fisher score attached to a pair of classes (considering the nodes in each class, and their embedding through the filter and the graph structure, as propositional samples), taking into account the class imbalance.\n\nAn analysis is conducted on synthetic graphs to assess how the hyper-parameters (order, normalization strategy) of the filter rule the GFD score depending on the graph and class features. As could have been expected there no single killer filter.\n\nA finite set of filters, called base filters, being defined by varying the above hyper-parameters, the search space is that of a linear combination of the base filters in each layer. Three losses are considered: with and without graph filter discriminant score, and alternatively optimizing the cross-entropy loss and the GFD; this last option is the best one in the experiments.\n\nAs noted by the reviewers and other public comments, the idea of incorporating LDA ideas into GNN is nice and elegant. The reservations of the reviewers are mostly related to the experimental validation: of course getting the best score on each dataset is not expected; but the set of considered problems is too limited and their diversity is limited too (as demonstrated by the very nice Fig. 5).\n\nThe area chair thus encourages the authors to pursue this very promising line of research and hopes to see a revised version backed up with more experimental evidence. ", "paper_id": "iclr_2020_r1erNxBtwr", "label": "train", "paper_acceptance": "reject", "anchored_texts": "The paper investigates graph convolutional filters, and proposes an adaptation of the Fisher score to assess the quality of a convolutional filter. Formally, the defined Graph Filter Discriminant Score assesses how the filter improves the Fisher score attached to a pair of classes taking into account the class imbalance.\n\nAn analysis is conducted on synthetic graphs to assess how the hyper-parameters (order, normalization strategy) of the filter rule the GFD score depending on the graph and class features. As could have been expected there no single killer filter.\n\nA finite set of filters, called base filters, being defined by varying the above hyper-parameters, the search space is that of a linear combination of the base filters in each layer. Three losses are considered: with and without graph filter discriminant score, and alternatively optimizing the cross-entropy loss and the GFD; nAs noted by the reviewers and other public comments, the idea of incorporating LDA ideas into GNN is nice and elegant. The reservations of the reviewers are mostly related to the experimental validation: "}
{"source_documents": ["Although deep neural networks (DNNs) achieve state-of-the-art accuracy on large-scale and fine-grained prediction tasks, they are high capacity models and often cannot be deployed on edge devices. As such, two distinct paradigms have emerged in parallel: 1) edge device inference for low-level tasks, 2) cloud-based inference for large-scale tasks. We propose a novel hybrid option, which marries these extremes and seeks to bring the latency and computational cost benefits of edge device inference to tasks currently deployed in the cloud. Our proposed method is an end-to-end approach, and involves architecting and training two networks in tandem. The first network is a low-capacity network that can be deployed on an edge device, whereas the second is a high-capacity network deployed in the cloud. When the edge device encounters challenging inputs, these inputs are transmitted and processed on the cloud. Empirically, on the ImageNet classification dataset, our proposed method leads to substantial decrease in the number of floating point operations (FLOPs) used compared to a well-designed high-capacity network, while suffering no excess classification loss. A novel aspect of our method is that, by allowing abstentions on a small fraction of examples ($<20\\%$), we can increase accuracy without increasing the edge device memory and FLOPs substantially (up to $7$\\% higher accuracy and $3$X fewer FLOPs on ImageNet with $80$\\% coverage), relative to MobileNetV3 architectures.\n", "The paper proposes to combine the inference on edge and cloud together, taking advantage of the communication-free inference on edge and the high-accuracy model on cloud. It utilizes the OFA to obtain models with different resource requirement at low cost. It achieves higher accuracy on the ImageNet dataset compared to the edge-only inference. \n**Strengths**\n1. It is interesting to have models with different MACs on cloud and edge to improve the inference accuracy with minimal latency increase.\n2. The idea of introducing a routing model to route more challenging inputs to the cloud is quite novel.\n3. The paper proposes to provide a better tradeoff between the latency and accuracy, by leaving the more challenging inputs to the clouds with higher latency. \n\n**Weakness**\n1. The goal is to improve the inference accuracy by predicting the more challenging inputs on the high latency cloud. It is weird to only focus on the MACs of the models while evaluating the system. How the system affects the latency of edge inference should be shown in the paper.\n2. The router model seems only to be updated with soft constraints, which may make the whole system unreliable in practice, since it does not guarantee the average latency (how many inputs are sent to the cloud). \n3. Table 7 is the only experiment that indicates the latency of the hybrid inference system proposed, but no detailed experiment setup is given, including the network bandwidth and the computation resources on the edge and cloud.\n4. In the technical contribution and experiment section, the author mentioned the 70% communication latency reduction (3 out of 10 examples to the cloud), but it is <20% in the abstraction. \n5. It would be better to have a detailed comparison of the edge, cloud, and hybrid inference in terms of latency taking the computation latency of the edge, cloud, and router model into consideration.\n\n\n**Minor Issues**\n1. The MACs and FLOPs are different terms. FLOPs are typically 2x MACs. It would be better to use the same term in the paper.\n2. There are several missing related works that combine edge and cloud inference together.\n   [1] Huang, Yangsibo, et al. \"Instahide: Instance-hiding schemes for private distributed learning.\" ICML, 2020.\n   [2] Liu, Zhijian, et al. \"DataMix: Efficient Privacy-Preserving Edge-Cloud Inference.\" ECCV, 2020.\n\n The paper proposes an interesting idea to combine the edge and cloud inference together, but as mentioned above, it is not practical to use MACs as a metric when the main concern of the paper is to maintain the low latency of edge inference while improving the accuracy. The experiments should be redesigned to make the proposed method more convincing.\n", " I appreciate the detailed response from the authors. The latency table looks pretty good to me, and I would like to increase the score to 5. The authors proposed an interesting idea, but the paper needs to be in a better form.\n1. The whole paper spends a lot of space comparing the proposed method with the OFA. It would be better to talk more about the motivation and the ablation of the proposed routing method.\n2. The tables and the figures should be redesigned to illustrate contributions more clearly.\n", " Improvement in latency is indeed encouraging. You should definitely discuss in the final version in some detail the merits and demerits of relatively more expensive rbg training vs much simpler and cheaper r training. ", " I have decided to keep my score for the following reasons:\n\n1. Motivation and algorithm novelty is the most important part (for me) in a research paper. The proposed method show great improvement and practical feasibility on real scene, as shown in \"Addressing main reviewer concerns (2/2)\". But the core part (OFA+routing) behind the solution is not novel enough.\n\n2. To prove that OFA is mere an option for simplicity and does not harm the contribution, try to do the two things: 1) Try other search space and the proposed method still works. 2)  Find out any inherent property between OFA and proposed method.\n\nBest regards.", " \nSome more anecdotal examples where RBG achieves non-trivial improvements over R:\n\n - **Accuracy improvements**: In Figure 3, for MobileNetV3, all the RBG models between $70-180$M MACs achieve $\\geq 0.5$% improvements in accuracy over R. Similarly, for OFA, most of the RBG models between $100-200$M MACs achieve $\\geq 0.7$% improvements over R.\n\n- **Latency improvements**: In Figure 3, for MobileNetV3, to achieve $75.4$% accuracy, RBG uses $60$% coverage while R utilizes $50$% coverage. It shows that RBG gains $10$% improvements in the latency metric. \n\nWe will include these metrics along with additional details in the next iteration.", "This paper introduces a hybrid Cloud-Edge design for Deep Neural Network (DNN) inference in which only a small portion of samples (more complicated ones) are sent to cloud for processing. Inference for most of the samples happens on the edge devices itself which reduces the task’s latency. \nThe paper provides an end-to-end training methodology for system’s components, a Neural Architecture Search (NAS) approach for the design of models deployed on the edge and the cloud. It also does an empirical evaluation of its design. Strengths:\n\nThe problem they consider is highly significant for practical application of DNNs on edge devices.\n\nWeaknesses:\n\nFrom the introduction: “The fundamental drawback of cloud ML, however, is the increased latency and energy consumption arising from communication, which can be prohibitive for many applications”, “we propose a best-of-both hybrid solution, which allows for deploying cloud-based AI tasks on edge devices like MCUs, while lowering the average total latency (the sum of communication and computational latency)”, but it is not clear if the proposed method reduces energy consumption. It would be nice to give some comparison of the energy cost of classifying an example on device vs. the energy cost of transmitting a sample to a cloud service and then retrieving the result.\n\n\nFrom the introduction: “the best available model which can be deployed on an STM32H743 MCU achieves 62.2% accuracy with 12.8M FLOPs (Lin et al., 2020)”, but Table 4 in (Lin et al., 2020) states that MCUNets achieve 70.7% accuracy on the exact same device. Even their baselines achieve 68% accuracy which is more than 62.2%.\n\nFrom the introduction: “The base model is compact and designed for devices with low-resource hardware constraints like latency and memory usage”. Latency is not a device constraint, it is an application constraint. I suggest using clock frequency instead of latency.\n\nIt is not clear how the Hybrid model was trained and what the baseline is in Fig2. There should be a clear explanation for this figure in the main text..\n\nIt is not clear to me what ‘base limited setting’ and ‘global limited setting’ mean in the last sentence of the introduction. A clear explanation is needed.\n\nA parameter “t” is introduced which “allows a routing model to trade-off accuracy and resource usage in order to avoid separately training for each desired level”. How much does it degrade the performance compared with end-to-end training of components for each desired level? An ablation study is needed.\n\nIn Section 2, “Architectures and Costs”: “we investigate communication limited settings in §3.3.” Where is this investigated? I cannot find the associated experiment.\n\nThe experiments section starts with analyzing results of the experiments and then goes into the description of the experiments, which is a poor writing choice.\n\n“Neurosurgeon: Collaborative Intelligence Between the Cloud and Mobile Edge” tries to tackle the same problem by dividing the inference dynamically among the edge and the cloud. How do you compare your method with it?\n\n“AppealNet: An Efficient and Highly-Accurate Edge/Cloud Collaborative Architecture for DNN Inference” Tries to tackle the same problem with a very similar solution. How is your method different from theirs?\n\nComments:\n\nI think it makes the last sentence of the abstract more clear if “substantially” comes after the word “accuracy” instead of where it is right now.\n\nI think it is good to give some numbers for on-device computation latency, cloud computation latency, and also communication latency so that the reader develops a sense of them.\n\nFig1 should change to Fig2 in Empirical validation (last subsection of intro).\n\nI’m curious about the task of learning a router model. Is it a well-defined task? It is interesting to show some validation results in that specific task to see if the router model learns something generalizable and reasonable.\n\nIn the “overall formulation”, the constraint is on R(r, b, g), which is the overall cost per inference of models deployed on both the edge device and the cloud. But based on the overall development, the edge device is the only compute limited part and the cloud is an over-provisioned server. I think having a constraint on R(r, b, g) instead of R_r + R(\\alpha_b) creates some confusion.\n I think the authors are thinking about a very interesting problem, but the work is at an early stage and not ready for an ICLR publication. The main reason is the lack of comparison with similar work. For more details, see the cons above.", " \"We also agree with the reviewer that marginal gains between  and other components need some investigation.\"\n\nThanks.  Will look forward to this.", " - **Figure 2 details.** \\\nFigure~2 collates the performance of the hybrid and baseline models from the Experiments section (see Tables 1 and 3, $70\\%$ coverage column). Baseline corresponds to the best baseline models at various MACs. Hybrid numbers correspond to the hybrid model where base operates at $70\\%$ coverage level. We list the baseline and hybrid performance metrics in Table 10 (see Appendix B.5) for completeness. \n- **Ablation study for the parameter $t$. How much does it degrade the performance compared with end-to-end training of components for each desired level?** \\\nDue to a lack of time and compute resources, we have not included the end-to-end training of components at each desired level. We point out that training a hybrid model for a coverage level would yield marginally better performance for the specific coverage but would be a computationally expensive exercise.\n- **Is learning a router well-defined task. Does the router generalizes.** \\\nLearning a routing network is a hard-task. We tackle this problem with a routing oracle that reduces this learning problem into a binary classification task. Router learnt using this procedure generalizes well. For instance, while training a hybrid model with pre-trained MBV3-small and MBV3-large models, on the oracle labels, the router achieves a training accuracy of $\\approx 87\\%$ and this translates into a validation accuracy of $\\approx 84\\%$. In contrast, entropy thresholding on the validation dataset achieves $\\approx 77\\%$ accuracy on the oracle labels.\n\n- **Confusion on the including global in the resource constraint in \"Overall Formulation\" Eq.3** \\\nWe point out that Eq.3 shows a generic formulation where the resource constraint depends on $g$. One scenario where this formulation is helpful is in the situation where communication cost is of the order of the global inference cost. In such a case, resources consumed at the global model needs to be accounted in the optimization.\n", " \nWe thank the reviewer for their constructive comments. Below we address the reviewer's main concerns.\n\n- **Communication Limited Settings, Latency and Energy experiments.** \\\n  In the main rebuttal thread, we have shown the relation between coverage and latency. We explicitly define latency and provide detailed experiments. In addition, we have added these experiments in Section.3.4  with setup details in Appendix B.6. \n \n- **Comparison with Neurosurgeon** \n\t- Given a fixed high-cost model, Neurosurgeon splits the computation between the device and cloud model by processing the initial part of the network on the device and processing the remaining on the cloud. \n\t- It is a very computationally expensive approach compared to our proposal, as every model incurs communication overhead. In their case, the communication cost is not just the input data (image) but a set of feature maps that will be much larger and have more redundancy. For instance, a typical image would consist of $224 \\times 224 \\times 3$ floats, while a feature map in their VGG baseline can go up to intermediate stage $224 \\times 224 \\times 64$.\n\t- In our case, there will be many instances where we do not route the example to the global model, say at a coverage level of $80\\%$, our routing scheme on average will only route $20\\%$ examples to the global model. In contrast, Neurosurgeon aims to split the computation of a given model between the edge device and cloud, i.e. to achieve accuracy of a global model, the total computation for this high-cost model will be used irrespective of the input example.  \n- **Comparison with AppealNet** \n\t- AppealNet formulation does not explicitly model any coverage constraint that enables the base model to operate at a tunable coverage level. In contrast, we explicitly model a coverage penalty.\n\t- Jointly learning the routing without any supervision is a hard problem. Instead, we relax this formulation by introducing the routing oracle that specializes in a routing network for a given base and global pair. With this oracle, the task of learning routing reduces to a binary classification problem with the routing labels obtained from the oracle. This also decouples the routing task from the base and global entanglement.\n\t- In addition, we propose a neural architecture search that finds a pair of base and global architectures that optimise the hybrid accuracy at any given combined resource usage. \n\t- Empirically, AppealNet does not have any evaluations for the Imagenet scale dataset. The closest comparison we can find is with the Tiny-Imagenet dataset (one-tenth of the size of the Imagenet).  While we cannot compare the two directly, since we solve a much harder problem than Tiny-Imagenet, we can make the following observations. At $70\\%$ coverage level,  for AppealNet, the minimum performance difference between the hybrid model and the global model is $\\approx 1.2\\%$ (see AppealNet, Fig.5(d)), while our closest to the global in case of the MobileNet baseline is $0.3\\%$ (see our paper Table 1, row 3). Note that  AppealNet performance will go down on Imagenet in comparison to Tiny-Imagenet due to the hardness of the problem.\n- **Incorrect numbers for MCUNet baseline in the introduction.**  \\\n\t We thank the reviewer for pointing out the error in our citation. We borrowed the pre-trained models from the MCUNet github repository (https://github.com/mit-han-lab/tinyml/tree/master/mcunet). We seem to have mixed the controllers STM32F746 (320kB SRAM, 1MB Flash) and STM32H743  (512kB SRAM, 2MB Flash).  \n\t- On STM32H743, MCUNet model (467M FLOPs, 3.3M Params) achieves $70.7\\%$ Top-1 accuracy. \n\t- On STM32F746, MCUNet model (12.8M FLOPs, 0.6M Params) has 5FPS latency and achieves $51.5\\%$ Top-1 accuracy.  We have access to STM32F746 controller and we have used this 5FPS latency model in our experiments. We have clarified this statement in the paper. \n\t- MCUNet authors did not release the deployment implementations, instead we rely on their released tf-lite binaries for execution using the tf-lite library. The 5FPS model is the only one that does not lead to OOM error (see \"MCUNet under different latency constraints\" on their github ). \n\t- Once again, we re-iterate the point that our hybrid model only improves by using an improved base model. In this case, if a better base model existed than 12.8M FLOPs achieving 51.5\\% Top-1, our hybrid performance will only increase. \n- **Latency is not a device constraint, it is an application constraint. Suggests using clock frequency instead of latency in introduction.** \\\n We believe there has been a slight misunderstanding of the way we use the term device constraint. We do not mean a constraint imposed on the device. Instead, we mean a constraint imposed on the model by the device. Our use of the term device constraint is in line with the popular use of that term in the literature (OFA, MCUNet, FBNet, etc.).\n\n", " \nWe thank the reviewer for their constructive comments. Below we address the reviewer's main concerns.\n\n\n- **Predictive tasks that are inherently challenging for low capacity models are unlikely to benefit from the hybrid approach** \\\nAs discussed in related works, learning with abstention literature shows that the model performance can be increased substantially by abstaining on some examples. In this work, we extend such a framework to route the abstained examples to a high capacity model. Similar routing spirit can be found in dynamic neural networks. \n\n  In addition, our framework provides a tunable threshold $t$ to control the coverage constraint, i.e. how many examples are routed to the cloud. Currently, to achieve near cloud performance, the device has to route all the inputs to the cloud (inference cost becomes too high) or use a baseline that utilizes entropy thresholding (our hybrid scheme outperforms this baseline). In contrast, hybrid design enables trading off coverage for near global performance.\n\n\n - **rb and rbg are marginally better than r.** \\\n While the plots do not show a significant gap between the three components. A closer look at the hybrid model with MBV3-small and MBV3-large architectures reveals that at $\\approx 180$M hybrid MACs,  we see that the 'r' achieves $75.63\\%$ accuracy while 'rb' and 'rbg' achieve $75.9\\%$ and $76.1\\%$ Top-1 accuracy. In this case, the gap between 'r' and 'rbg' is nearly $0.5\\%$.  On the Imagenet dataset, this is a non-trivial improvement. We also agree with the reviewer that marginal gains between $r$ and other components need some investigation. We conjecture that the proposed approach is hitting an upper bound in terms of performance. ", " \nWe thank the reviewer for their constructive comments. Below we address the reviewer's main concerns.\n\n - **Novelty of the proposed solution.** \\\n   We refer the reviewer to the main rebuttal thread where we discuss our problem and technical novelty. \n\n- **Method section simplification and Algorithmic procedure** \\\n Due to lack of space, we provide the algorithmic procedures in the Appendix (Algorithms 1-4). We also modified the method section to simplify the language. \n\n- **Confusion between OFA and Joint Hybrid Architecture Search** \\\nWe point out that our architecture search problem is a joint search for the base and global architectures to maximize hybrid performance. To the best of our knowledge, this architecture search problem is unique to our hybrid setup. Our evolutionary search uses the same routing oracle as the one used in training the hybrid model. We rely on the OFA framework since our search is restricted to the architectures available in the OFA space. \n\n- **Comparison between our method and dynamic neural networks** \\\nAs discussed in our related work (see Adaptive Neural Networks), dynamic neural networks mainly use entropy thresholding as the confidence operator. For a fair comparison, we have added this as a baseline in our experiments (see Fig. 3 and Table 2). We also point out that dynamic neural networks can be used as a base or a global model for additional savings. \n\n- **Our technical contributions and router baselines** \\\nWe beg to differ from the reviewer on the novelty of the architecture search and refer the reader to the main rebuttal thread where we explicitly state our technical novelty.\nWithout the architecture search component, our proposal boils down to learning the routing, base, and global models to maximize hybrid accuracy at a given resource constraint. Note that in this case, we fix the base and the global architectures apriori. In addition, in our architecture search section,  we perform an evolutionary search algorithm to find the base and global model pairs that maximize the hybrid performance at a given resource constraint. We use entropy thresholding as a baseline to compare against the proposed routing network.\n", " We thank the reviewer for their constructive comments. Below we address the reviewer's concerns.\n\n - **Latency experiments.** \\\n In the main rebuttal thread, we have shown the relation between coverage and latency. We explicitly define latency and provide detailed experiments. In addition, we have added these experiments in Section.3.4 and with setup details in Appendix B.6. \n \n- **No guarantee on the average latency (how many inputs are sent to the cloud)** \\\nWe model the coverage constraint ( i.e., how many inputs are sent to the cloud) explicitly in our routing loss function. Our framework allows tuning the threshold $t$ associated with the routing network. In practice, using a validation set, the routing network can be adjusted to achieve a  desired target coverage level. \n\n- **Table 7 details** (table number got changed to $8$ in the revised version)\\\nAs mentioned in the Section.3.3 and Appendix B.4, in this experiment we create a hybrid model with a severely resource constrained edge device. We borrow the base model ($12.8$M MACs, $51.5\\%$Top-1 accuracy) from the MCUNet repository. The latency in the Table only compares the overhead of operating the routing network in addition to the base model. In this experiment, we still use the coverage metric as the proxy for communication. Table 3 shows the hybrid performance at different coverage levels. In this rebuttal, we have included the latency experiment that includes the communication cost for this base and global configuration (see Section 3.4 and Appendix B.6).\n\n- **Mismatch between coverage numbers in abstract and introduction**\\\nIn our empirical evaluation, we use three different coverage levels $90\\%, 80\\%, 70\\%$ (see Table 1, 3, and Figure 4). In Figure 2, we point out that the accuracy improvements are corresponding to $70\\%$ coverage levels. While in the abstract, we mention correct improvements for the $80\\%$ coverage level, i.e. $20\\%$ abstentions. For consistency between introduction and the abstract, we will update the abstract with the improvements corresponding to $70\\%$ coverage level, i.e., $30\\%$ abstentions. \n\n- **Related works: Instahide and DataMix**\\\nWe thank the reviewer for pointing out these works. Although the setup in both the works involve an edge and a cloud device, these works do not aim to maximize hybrid performance at a given resource constraint. Their main contributions in this hybrid setup is to preserve privacy when relying on the cloud inference. \n\n", " \n|   Base+Global  |    Method    | Params | Top-1 |  MACs | Latency | Energy |\n|:--------------:|:------------:|:------:|:-----:|:-----:|:-------:|:------:|\n|       MCU      |  Cloud-Only  |  9.1M  | 79.93 |  595M |  1200ms |  300mJ |\n|    STM32F746   |   On-Device  |  0.6M  |  51.5 | 12.8M |  197ms  |  49mJ  |\n| + LoRAWANN     | On-Device    | 0.74M  | 62.6  | 82M   | 1075ms  | 269mJ  |\n| GPU Tesla V100 | Hybrid@70Cov |        | 66.69 | 191M  | 557ms   | 139mJ  |\n|                | Hybrid@60Cov |        | 70.77 | 250M  | 677ms   | 169mJ  |\n||||||||\n|     Mobile     |  Cloud-Only  |  9.1M  | 79.93 |  595M |  205ms  |        |\n|  Samsung Note8 |   On-Device  |  5.3M  |  75.7 |  215M |   65ms  |        |\n|      +LTE      | Hybrid@70Cov |        | 79.59 |  393M |  119ms  |        |\n| GPU Tesla V100 |              |        |       |       |         |        |\n||||||||\n\n### Missing References.\n\n 1. *Neurosurgeon* --- This approach splits a large neural net between a device and a cloud model, so that computation begins locally, and then continues on the cloud. In particular **every instance is communicated to the cloud**. This means that **every instance suffers the communication overhead**. In complete contrast, **our work reduces the communication overhead by only using it for difficult instances**. Further, neurosurgeon does not consider edge-device constraints like limited memory.\n 2. *AppealNet (AN)* --- This (unpublished) preprint bears strong similarities to the reference \\cite{nan2017adaptive}. While the design is broadly similar, crucial design considerations that we study are missing from AN :   \n    - AN has no coverage penalty in the training loss. \n    - The AN router training is entangled with the base and global networks, while we decouple its training using the routing oracle.\n    - The AN model is not evaluated on ImageNet, which is a challenging dataset from both the statistical and computational perspective.\n    The differences between AN and our approach are discussed in greater detail below, but the salient points above serve to distinguish our work from it.\n 4. *InstaHide and Datamix* --- **These focus on privacy, rather than efficiency of inference**. This is orthogonal to our investigation.\n\n\n- nan2017adaptive --- Adaptive classification for prediction under a budget \n- Sandler_2018_CVPR_MobilenetV2 --- MobileNetV2: Inverted Residuals and Linear Bottlenecks", " \nWe thank the reviewers for their constructive comments. We have updated the paper with appropriate modifications. Below we answer main reviewer concerns and address specific comments in reviewer threads. To summarize our discussion below, we point out that our depicted scenario is novel (not present in other works), is technically sound and new, and does address the cases pointed out by the reviewers.\n\n### Problem Novelty\n - **Our focus is on meeting constraints to deploy models on low-resource edge devices**. This contrasts with prior work including the ones pointed out by reviewers. These works are only concerned with budgeting more resources for complex examples without worrying about the more important device-specific hard constraints (FLOPs, communication latency) imposed by edge devices. We highlight these aspects in the related works while comparing adaptive neural networks.\n - **We propose and study end-to-end system-wide objectives**. Our focus on edge devices necessitates posing a system-wide hybrid objective, and we systematically integrate and optimize all degrees of freedom (architectures, routing \\& coverage, base, and global networks). In contrast, prior works optimize these aspects in a decoupled and isolated manner. \n\n### Technical Novelty\nWhile earlier works have tackled similar problems, our technical novelty lies in the following aspects.\n\n - **Improved training of router.**  Learning a good router is difficult since the optimal routing affects the base and global performance. \n\t - *Prior approaches*.  Earlier works either use simplistic post-hoc methods like entropy thresholding or relax the router $r$ in Eq.5. These yield sub-par performance.  \n\t - *Our approach*. Instead, we transform the routing into a binary classification task by using the routing oracle (see $2.1). It enables us to use standard ML tools for learning good binary functions, thus gaining over approaches that directly try to relax the objective of Eq.5 . Further, our alternating minimisation (see Appendix Algorithm~3) allows training all or a subset of the three components (base, global, and router).\n- **Novel Architectural Search Problem.**\n\t- Our hybrid design raises the novel problem of jointly searching for the base and global architectures to maximise performance under resource constraints.\n\t- We design an evolutionary search algorithm for this using the routing oracle to provide fast proxies for the fitness of joint architectures.\n\t- Note that the method is general, and our empirical search is restricted to the OFA architecture space only for simplicity.\n\t- OFA should be viewed as a search space for different architectures and the search is an integral component within our end-to-end optimization framework. Therefore, it is incorrect to view our contribution as limited to routing (as commented by the reviewer EcwK). \n\n### Latency Experiments. \n\n- **Coverage is a proxy for communication latency, and is well studied in the paper**\n\t- Communication latency is the major contributor to inference cost in a hybrid system. For instance, on an MCU, the cost of transferring an image to the cloud is $6\\times$ the cost of on-device inference. \n\t- Coverage represents the fraction of points at which no communication is used, and is thus a direct proxy for the savings in communication latency. We have demonstrated advantages in this respect in a variety of settings (Tables 1,2,3; Figure 4). \n\t- In contrast to the cloud-only solution, our hybrid design gains $3.3\\times$ in latency with little drop in accuracy (see Table~1, $70\\%$ coverage column). \n- **More detailed and explicit modeling.** We augment the rebuttal with the following:\n  - We explicitly define communication devices and measure three latency components in the hybrid system\n\t  A) Inference latency on-device.\n\t  B) Communication latency for examples sent to the cloud.\n\t  C) Inference latency on the cloud.\n  - Tables below benchmark the latency (A + B + C) of the hybrid approach against baselines.\n  - In model IoT type settings - computing on a micro-controller with LoRAWAN communication - our hybrid method \n\t  - Operates at about **half the latency and power consumption versus a cloud-only** solution.\n\t  - Gives **$8\\%$ improvement in accuracy over** the best available **on-device** model. Note that even $1\\%$ gain in accuracy is considered non-trivial on Imagenet dataset (see \\cite{Sandler_2018_CVPR_MobilenetV2}).\n\t  - Detailed results below.\n  - In model mobile phone settings with an LTE connection the hybrid model \n\t  - Operates at **half the latency of a cloud-only solution**.\n\t  - Achieves near-cloud only performance, that is, $4\\%$ improvement in accuracy versus the best available **on-device model**.\n\t  - Detailed results below.\n\n", "As edge devices are severely resource constrained, models (with low capacity) deployed on them don't achieve the same level of accuracy on many prediction tasks as their high capacity cloud counterparts. This work attempts to improve the accuracy of prediction tasks on edge devices by employing a hybrid approach where a low capacity model is deployed on the edge device and a counterpart high capacity model is deployed on the cloud. A query routing model running on the edge device decides which query needs to be handled by the cloud model and thereby achieving a trade-off between classification accuracy and cloud computation/communication costs. All the three models are learnt end-to-end from training data for the desired trade-off. \n As edge devices are severely resource constrained, models (with low capacity) deployed on them don't achieve the same level of accuracy on many prediction tasks as their high capacity cloud counterparts. This work attempts to improve the accuracy of prediction tasks on edge devices by employing a hybrid approach where a low capacity model is deployed on the edge device and a counterpart high capacity model is deployed on the cloud. A query routing model running on the edge device decides which query needs to be handled by the cloud model and thereby achieving a trade-off between classification accuracy and cloud computation/communication costs. All the three models are learnt end-to-end from training data for the desired trade-off. \n\n\nPositives:\nA principled solution to the problem by modelling it as a coupled maximization problem. This involves  a) solving the architecture search problem using evolutionary search and proxies for accuracy of the architecture and b) learning the hybrid model using alternating optimisation.\nPrincipled solution to the routing problem through supervised learning of the routing oracle.\nEmpirical results that demonstrate that the proposed approach can give competitive results compared to cloud models while reducing the computation required substantially. \n\nNegatives:\nOnly one dataset is used in the empirical study.\nFrom the graphs, it appears that rbg and rg are very marginally better than r. This means that end-to-end training of all the three models is not really helping much. \n\nThe effectiveness of the proposed approach is not only contingent on the predictive accuracy of the routing model (it needs to do a good job of identifying challenging queries (on which the low capacity model is likely to fail) but also on most queries being relatively less challenging (so that the low capacity model can make accurate predictions on them). This implies that predictive tasks that are inherently challenging for low capacity models are unlikely to benefit from the hybrid approach as most queries would need to be routed to the cloud model thereby making it cloud-heavy. \n The work is interesting as it addresses an important practical problem and proposes a principled solution to it. Experimental study can be made stronger by considering additional tasks and datasets. Also, practical usefulness of the proposed approach needs to be carefully reasoned as it appears to be critically dependent on most queries being easy for the edge model whereas in practice this might not be the case. ", "This paper proposed a hybird framework to process inference efficiently: the framework contains a global network to deal with hard query, a base model for easy query and a router protocol to deliver queries. It designed a proxy supervision algorithm to train the router and proposed to use Neural Architecture Search (NAS) to search for global/base models. 1. The idea of hybird framework to process queries differently based on their difficulty is not a new idea. The basic framework proposed in the submission (big + small model for hard/easy queries and router model for determination) is straight forward and not a very novel idea. Even though the end-to-end training diagram is easy to come up with.\n2. What contributes to this framework should be: 1) How to train the router, 2) How to determine the big/small model architectures. Authors address the first question in Section 2.1 and the second in Section  2.2. But in my opinion, explanation in Section 2.1 is a little bit sophisticated: the algorithm should be straight forward but the writting seems to complicate it. I am still confused about the details of how to train the router. Could author formulate the whole process in an Algorithm format?\n3. For the determination of model architecture, author proposed to use OFA, which is a seperated work.  But author emphasize in techinical contributions that \"propose a NAS method ...\". I don't think using OFA can be described as \"propose\" in the writting. Instead, NAS and the proposed hybird framework is decoupled: NAS does not take advantage of any properties in the hybird framework, vice versa.\n4. I may miss some important points in the submission. Please remind me if necessary.\n\nQuestions:\n1. As author mentions in the related work that this work is similar to dynamic network for they share a similar idea that \"different queries should be processed by different network\". Though these two kinds of methods differ a lot, especially the proposed method can be deployed much easier, I am still interested in the comparison experiments on dynamic networks. Noted that this is not compulsory.\n2. Since the determination of models is handled by another work (OFA), can I regard the submission as the hybird framework as big/small model (given) + router model training. If so, did author try another router model training methods? Different kinds of router models (if applicable) should be compared. The overall contribution is not significant as the hybird framework is straight forward and the model determination is decoupled and directly from published work."], "review_score_variance": 0.1875, "summary": "This paper aims to improve performance on edge devices by utilizing a large-capacity network in the cloud. To this end, the authors suggest using the routing network that decides whether to use the base model (on the edge device) or the global model (on the cloud). They also propose an overall training scheme for learning not only model parameters, but also network architectures. After the discussion period, 3 reviewers are on the negative side, and 1 reviewer is positive. AC thinks that the authors’ response was not enough to convince the negative reviewers. In particular, AC agrees with the negative comments of reviewers on limited novelty, unclear motivation for the proposed method, and unclear presentations. Overall, AC recommends rejection.", "paper_id": "iclr_2022_2DJwuD-elOt", "label": "train", "paper_acceptance": "Reject", "anchored_texts": "This paper aims to improve performance on edge devices by utilizing a large-capacity network in the cloud. To this end, the authors suggest using the routing network that decides whether to use the base model (on the edge device) or the global model (on the cloud). They also propose an overall training scheme for learning not only model parameters, but also network architectures. After the discussion period, 3 reviewers are on the negative side, and 1 reviewer is positive. on limited novelty, unclear motivation for the proposed method, and unclear presentations."}
{"source_documents": ["The goal of generative models is to model the underlying data distribution of a\n      sample based dataset. Our intuition is that an accurate model should in principle\n      also include the sample based dataset as part of its induced probability distribution.\n      To investigate this, we look at fully trained generative models using the Generative\n      Adversarial Networks (GAN) framework and analyze the resulting generator\n      on its ability to memorize the dataset. Further, we show that the size of the initial\n      latent space is paramount to allow for an accurate reconstruction of the training\n      data. This gives us a link to compression theory, where Autoencoders (AE) are\n      used to lower bound the reconstruction capabilities of our generative model. Here,\n      we observe similar results to the perception-distortion tradeoff (Blau & Michaeli\n      (2018)). Given a small latent space, the AE produces low quality and the GAN\n      produces high quality outputs from a perceptual viewpoint. In contrast, the distortion\n      error is smaller for the AE. By increasing the dimensionality of the latent\n      space the distortion decreases for both models, but the perceptual quality only\n      increases for the AE.", "Impact of the latent space on the ability of GANs to fit the distribution\n\nThis paper purports to study the behavior of latent variable generative models by examining how the dimensionality of the latent space affects the ability of said models to reconstruct samples from the dataset. The authors perform experiments with deterministic autoencoders and WGAN-GP on CIFAR, CelebA, and random noise, and measure MSE and FID as a function of the dimensionality of z.\n\nMy take: This paper does not offer any especially intriguing insights, and many of the conclusions the authors draw are, in my opinion, not supported by their experiments. The paper is confusingly written and hard to follow—throughout my read I struggled to determine what the authors meant, and it was not clear to me what this paper is supposed to contribute. The potential impact of this paper is very low, and I argue strongly in favor of rejection.\n\nNotes:\n\nMy most critical complaint is the central experiment set of the paper: measuring MSE and FID as a function of Dim-Z for two models. First of all, the authors assume that the reduction in MSE as a function of dim Z is indicative of increased memorization in the GAN models. I disagree that this is the case; since the GAN-based reconstruction is done via optimization it is unsurprising that increasing dim Z increases the reconstruction quality, as you are literally giving the model more degrees of freedom with which to fit the individual samples. This is glaringly evident in Figure 8, where increasing dim Z renders the model better able to reconstruct a sample of pure noise, which is almost certainly not in its normal output distribution, (or if it is, is in there with staggeringly low probability). The fact that the higher dim-z models are better able to reconstruct the noise supports the notion that it is merely the number of degrees of freedom that matter in these experiments, rather than what the model actually learns.\n\nSecond, it is important to note that FID can be easily gamed by memorization, and for an autoencoder (which has direct access to samples) with an increasingly large bottleneck it is unsurprising that increasing dim-Z tends to decrease the FID, and equally unsurprising that increasing the dim-Z for the GAN does not tend to improve results, since this does not really allow the model increased memorization capacity (not to mention the relationship between performance and dim-Z has been explored before in GAN papers).\n\nThird, the organization of the experimental section makes it very difficult to infer what the authors are trying to conclude from these experiments. The noise experiment is presented, but no insights or conclusions are drawn, other than (a) noting that the model has a harder time reconstructing the noise than training samples and (b) that lower dim models have a harder time reconstructing the noise, both of which are just restatements of the information presented in the figure rather than an actual insight or conclusion.\n\n-I’m not really sure what the experiment in section 3 is supposed to show. This experiment is poorly described and lacking details. First of all, what is the loss function used there? Is this the output of the discriminator or the MSE between the output and a target sample? How is z* found and what does it represent—is it just a randomly sampled latent, or is it the latent that corresponds to the z-value which minimizes some MSE loss for a target sample? If it’s the latter, why is this notation not introduced until section 4? If it’s a latent, why are you calling it a data point? Why are there no axes and no scales on these plots? How is it clear that there is an optimization path from z0 to z*; is that supposed to be inferred from z0 having a higher value than z* or appearing to be directly uphill from z*, because it’s not clear to me that that is the case in Figure 2a. In general I did not find this experiment to support the conclusions the authors draw.  \n\n-Figure 4: It is important to note that FID can be trivially gamed by memorizing the dataset, and an autoencoder is much more well-suited to memorizing the dataset as it has direct access to samples (whereas a GAN must get them through the filter of the discriminator). Authors should test interpolation or hold-out likelihood for the  autoencoder, these models are not directly comparable in this manner.\n\n-The presentation of this paper is, in general, all over the place. The authors should focus on writing such that each point follows the next, building progressively towards their results and insights, and making it easy for a reader to follow their train of thought.\n\n“In this work, we show that by reducing the problem to a compression task, we can give a lower bound on the required capacity and latent space dimensionality of the generator network for the distribution estimation task.” At what point is this lower bound (either in terms of model capacity or latent space dimensionality) specified in the paper? Is Figure 3 supposed to be this lower bound, because to me it only indicates that the autoencoder tends to have a lower MSE, not that it conclusively lower bounds the memorization capacity of the GAN. Wouldn't a method like GLO which directly optimizes for memorization be a better lower bound for this, anyhow?\n\n“We rely on the assumption, that less capacity is needed to reconstruct the training set, than to reconstruct the entire distribution” What does this phrase mean? Are the authors referring to the entire distribution of natural images, of which the training set is assumed to be a subset? Or do they mean the output distribution of the generator? This was not clear to me.\n\n\nMinor:\n\n-“ style transfer by Karras et al. (2018),”, and “anomaly detection (Shocher 2018).” StyleGAN is not a style transfer paper, and InGAN  is not about anomaly detection. Please do not incorrectly summarize papers.\n\n-“Trained GAN newtworks” While amusing, this is a typo. Please thoroughly read your paper and correct all typos and grammatical mistakes, like “combiniation.”\n\n-“…that an accurate reconstruction of the generator manifold is possible works using first order methods”  The word “works” seems to be out of place here. Again, please thoroughly proofread your paper.\n\n-The legend in Figure 2 has a white background, making the white x corresponding to z0 invisible. Please fix this, and add appropriate axes to this plot.\n\n-Figure 7 and 8 may in fact have error bars, but they are not described (are they 1 std or another interval?) or referenced, and in Figure 8 (if these are error bars) they are nearly invisible. \n\n", "The work performs a systematic empirical study of how the latent space design in a GAN impacts the generated distribution. The paper is well-written and was easy to read. Also, I find this to be an interesting and promising direction of research.\n\nThe convergence proof in Goodfellow (2014) assumes that the data distribution has a density, and essentially states that the JS-divergence is zero if and only if the two distributions are the same. In practice, the data distribution is discrete, while the latent distribution has a probability density function. It is not possible to transform a density into a discrete distribution by a continuous map and neural networks are always continuous by construction. In theory, as training progresses, more and more latent mass will be pushed on the discrete samples and no minimizer exists (unless the function space of generator is constrained or the real distribution is smoothed out a bit). \n\nSince it is not possible to assess whether the GAN training has converged due the nonconvexity of the energy and non-existence of a global optimizer, the empirically observed results might be very specific to the chosen optimization procedure, stopping criterion, dataset, hyper-parameters, initialization, network architectures, etc etc.  It is a challenge to study the choice of latent space in a somewhat \"isolated\" way. These issues should be discussed in the paper and the reader should be made aware of such problems.\n\nAnother point, could it be, that by increasing the dimension of the latent space, one makes it easier for the nonconvex optimization in (5) to converge to \"unlikely but realistic looking samples\"? I think this is not too far-fetched, as increasing the dimension of an optimization problem often makes local optimization less likely to get stuck at local optima. Also it might not be the best idea to optimize (5) with Adam since it is not a stochastic optimization problem and there are provably convergent solvers out there for this problem class. \n\nSince it is possible to evaluate the likelihood of the optimized reconstructions that are nearby the data points, one could check whether this is indeed the case. While constrained not to be too unlikely, I wonder whether the likelihood increases or decreases with the dimensionality of the latent space and this would make an interesting plot. \n\nUnfortunately, I did not understand the connections to auto-encoders, as they might optimize a fundamentally different criterion than GANs. In particular \"In principle, getting from an AE to a GAN is just a rearrangement of the NNs. \" is unclear to me. \n\nAlso, what is meant by lower-bound? Is the claim that the reconstruction error in an auto encoder will be lower, than if one optimizes the latent code in a GAN to reconstruct the input? Figure 3 seems to support this hypothesis, but I don't have an intuition why this should be true and have some doubts. A mathematical proof seems out of reach. \n\nI have trouble to understand the \"intuition that the AE complexity lower-bounds the GAN complexity.\" Before reading this paper, my intuition was the opposite: If the generator distribution covers the real distribution, the reconstruction error for GAN is zero. Intuitively, it seems a much easier task to somehow cover a distribution than to minimize an average reconstruction error. \n\nThe connection of WGANs to the L2 reconstruction loss in the auto-encoder is very hand-wavy. It is still an open question whether WGANs actually have anything to do with the Wasserstein distance. People working in optimal transport doubt this, due to huge amount of approximations going on. \n\nAt this point I'm reluctant to recommend acceptance, as the paper tries to connect things, which for me are quite disconnected and the evaluations of reconstruction error, etc. might depend in intricate ways on the nonconvex optimization procedures.\n\nMinor suggestions, typos, etc (no influence on my rating):\n\n- What is the \"generated manifold\" that is talked about in the introduction, contributions and throughout the paper? To me, it is not directly clear that the support of the transformed distribution will be a manifold (especially if G is non-injective). Anyway, the manifold structure is nowhere exploited in the paper, so I suggest to call it \"transformed latent distribution\".\n\n- Had to pause a little bit to understand Eq. 2 (simple polynomial interpolation). It is unnecessary to show the explicit form, as I'm sure no one doubts the existence of a smooth curve interpolating a finite set of points in R^d.  \n\n- Equations should always include punctuation marks. \n\n- Eq. 5: dim --> \\text{dim} and s.t. --> \\text{s. t.}\n\n- Fig 3b: the red curve is missing or hidden behind another curve.\n", "Summary: The paper explores the influence of the dimensionality of the latent space to the quality of the learned distributions for autoencoders (AE) and GANs (more precisely Wasserstein GANs). In particular, the paper looks at the ability of the learned AE or GAN to reconstruct the training images, the visual quality of the images as the dimension of the latent space increases, and the ability to reconstruct images not in the training set (structured in some ways). \n\nEvaluation: While the general flavor of question the paper studies is undoubtedly interesting, I found the paper severely lacking both in terms of the quality of writing (in particular, I was at confused about the goal of various sections/experiments), as well as the significance of the results the authors observe (and how they are reported -- I found them to be oversold). \n\nRegarding the quality of results: \n\n* The paper primarily talks about the ability of AEs and GANs to *reconstruct* images, either in the training set, or in the test set, or in some different dataset altogether (e.g. shifted images, different image dataset). This is a problematic thing on multiple levels: first, the goal of a GAN or AE is to fit a distribution -- merely having a data point in its domain says nothing about the *probability* of that point; second, the way these \"spans\" are tested is via running a gradient descent search for a pre-image for the data point. The authors never comment or explore whether the problem may *not* be that these data points are not in the image of the GAN, but rather that the optimization procedure doesn't succeed. (And indeed, increasing the dimensionality of the latent space may act as \"overparametrization\" for this gradient descent procedure, making it more likely to succeed.) \n\nFinally, there are some fairly arbitrary choices in the entire experimental setup: why WGANs vs another architecture -- are the GAN results sensitive to architecture choices? why AEs and not VAEs (and with what variational posterior) -- how sensitive are the observations here to choosing the most vanilla variant of autoencoders? These are all questions that invariably linger after reading the paper. \n\nRegarding the quality of writing: \n\n* There are various sloppy sentences in crucial parts of the paper. I will only list a few:   \n-- \"Once a suitable AE for a dataset is found, the decoder part of is used as the generative model of choice.\" -- this seems to suggest a semi-synthetic setup where an AE is trained to use as a generator of a data set for which a GAN is fit. I never saw this setup in Section 5 -- although this would be a good way to test \"relative\" representational power of GANs and AEs. \n-- \"In principle, getting from an AE to a GAN is just a rearrangement of the NNs\" in Section 5.1 -- I wasn't sure what this is supposed to mean, and this is a critical part of that section. \n-- \"The AE network acts as a lower bound for the GAN algorithm, therefore validating our intuition that the AE complexity lower-bounds the GAN\" in Section 5.1 -- also very sloppy, and I'm not sure what it means -- I guess the authors mean the reconstruction performance of AE is a lower bound on the GAN reconstruction. Not sure what this has to do with \"complexity\". \n\n* Various sections are meandering, and I wasn't sure what the goal is. Just a few examples: section 3 spends a lot of time talking about known theoretical results wrt. to invertibility of random-like neural nets. It wasn't clear to me how this relates to the results in Section 5, especially since the authors never leverage/talk about these theory results again. (Instead, they study empirical invertibility via gradient-descent based procedures.) Similarly, interpolating by polynomials is talked about in (2), seemingly without any point. \n"], "review_score_variance": 0.0, "summary": "The reviewers have pointed out several major deficiencies of the paper, which the authors decided not to address.", "paper_id": "iclr_2020_Hygy01StvH", "label": "train", "paper_acceptance": "reject", "anchored_texts": "The reviewers have pointed out several major deficiencies of the paper, which the authors decided not to address."}
