{"source_documents": ["In this paper, we study a sequential decision-making problem, called Adaptive Sampling for Discovery (ASD). Starting with a large unlabeled dataset, algorithms for ASD adaptively label the points with the goal to maximize the sum of responses.\n\nThis problem has wide applications to real-world discovery problems, for example drug discovery with the help of machine learning models. ASD algorithms face the well-known exploration-exploitation dilemma. The algorithm needs to choose points that yield information to improve model estimates but it also needs to exploit the model. We rigorously formulate the problem and propose a general information-directed sampling (IDS) algorithm. We provide theoretical guarantees for the performance of IDS in linear, graph and low-rank models. The benefits of IDS are shown in both simulation experiments and real-data experiments for discovering chemical reaction conditions.", " Okay, thank you for clarifying.", " Thank you for the responses, especially the detailed discussion of the differences between IDS and ENS.", " > I am a bit fuzzy on the real-world application which could be better explained in the supplementary material if not the main text. Per my understanding, the dataset has been taken - PDNC or CNCCI (https://doyle.chem.ucla.edu/wp-content/uploads/2020/07/43-Predicting-Reaction-Performance-in-C-N-Cross-Coupling-Using-Machine-Learning.pdf), but the final result is not comparable to cited papers. For example, Shim etal. (https://pubs.rsc.org/en/content/articlelanding/2022/sc/d1sc06932b original ref [34]) - I would have expected to see something like Figure 3 (if comparing the IDS in the Active Learning setup). {\\em Else it should be clarified that a contrived subtask has been taken to evaluate the problem and show a possible future application.} I understand this is not the main focus of the paper so that clarification would be sufficient.\n\nAnswer: [34] is a related paper studying the problem in a transfer learning framework. It would be an promising direction to extend our results. We will include the Figure 3 for real data application in the appendix.\n\n> For clarity and completeness, please add a section in supplementary material explicitly describing the PDNC and CNCCI datasets (tied to the data available in this upload, namely, Informer.xlsx and doyle_matrix.xlsx), and, how this translates to the problem setting.\n\nAnswer: thanks for your suggestions. We will add a `README.md` in the code to describe the meaning and usage of each file and add a new section in the Supplementary to describe that as well.\n\n> The parameter $\\lambda$ controlling the ratio between instant regret and information gain is also used in the approximate algorithm (supplementary material H). However, its value is not clarified and the sensitivity of empirical results depending on its choice is unclear to me (or I missed it)?\n\nAnswer: we use $\\lambda = 2$ for linear and generalized linear models and $\\lambda = 3$ for low-rank matrix and graph models.\n\n> On the feedback graph model [https://arxiv.org/pdf/1106.2436.pdf], would it be correct to say that the setup in this paper can be modeled by considering at each point, the previously sampled point is removed from $G_t$ (the next feedback graph). If so, would the algorithms for sleeping expert setting be comparable? For example, the empirical evaluations in [9] are not directly comparable per my reading - if so, this can be mentioned explicitly in the text.\n\nAnswer: sleeping expert can be comparable. We will mention it explicitly in the text.", " > In the pseudocode for the algorithm (Algorithm 1), the computation of $\\Delta_t$ and $g_t$ is vaguely described as being done \"using posterior. It is very unclear to me how that would happen, even if we had complete knowledge of the posterior and could easily draw samples or compute any probability according to the posterior. I guess my issue or maybe my question is whether, in the definitions of \n$\\Delta_t$ and $g_t$, the expectations are taken also w.r.t. the posterior (and if so, why is that correct). I think that, if that is the case (or whatever is the case), this aspect should be better clarified (perhaps right now is mentioned on lines 83-84, but perhaps it should be repeated closer to the definitions above.\n\nAnswer: Yes, the instant regret and information gain are computed based on posterior distributions. We will make it more clear when the abstract algorithms are introduced. In practice, calculating them directly from posterior distribution can be hard. We discuss this issue in Section 5.1.\n\n", " > Along the same lines, there could be variant of IDS where the numerator of the acquisition function is the cumulative regret, as opposed to the instant regret. I think this would have the same cost as IDS, since you would have to pick out the top unlabeled points anyway. I would be interested to see how this variant performs.\n\nAnswer: we can implement that variant, but we don't understand the intuition of using cumulative regret. The future cumulative effects are given in the information gain term as we discussed in the comparison between IDS and ENS. The sum of all the $T-t$ future gain will over-estimate the effects of exploration as we discussed above.\n\n> What is the interpretation for the sharp turns in the first row of Figure 1 (in the middle of Figure 1c, for example)?\n\nAnswer: In Figure 1a, the phase change happens when the number of samples reaches the number of parameters, before which the estimates can be arbitrary. In Figure 1c, the calculate the regret with respect to the actual labels that are either $1$ or $0$. As a result, the first half of regrets are compared to all 1's and the second half is compared to all 0's.\n\n> UCB is missing from some plots: we only tested UCB for linear and generalized linear models.\n\nAnswer: It is not clear how to implement UCB for low-rank matrix model. There is no trivial solution to that. That is why we only compared IDS with TS and random.\n\n> The authors could consider adding information about whether results in Table 1 are statistically significant.\n\nAnswer: thanks for the suggestions. We will add that information to the table.\n\nAll the other typos are fixed in the revised version.\n", " We thank the reviewer for pointing out the important literature on Active Search (AS). Active Search considers the same formulation as ASD. However, we argue that our paper makes significant contributions to the literature.\n\n**Comparing IDS and ENS**\n\nWe first discuss the similarity between IDS and an important method in AS literature, ENS (efficient nonmyopic search) and argue that it is significant to introduce IDS to the literature. As we discussed in the paper, IDS tries to balance between exploitation and exploration by minimizing the ratio of instant regret and information gain on the top $T-t$ points at the step $t$. There is a variant of IDS that considers a weighted sum of the two, i.e. $\\Delta^T \\pi + \\gamma g^T \\pi$ for some constant $\\gamma$.\n\nENS targets at a similar exploration and exploitation as discussed in [3]. Using the notations in [3], ENS maximizes \n\n$Pr(y_t = 1 \\mid x_t, \\mathcal{D}_{t - 1}) + E\\_{y_t}[\\sum\\_{T-t}^{'} Pr(y = 1 \\mid x, \\mathcal{D}\\_{t})]$,\n\nSince $E[\\sum\\_{T-t}^{'} Pr(y = 1 \\mid x, \\mathcal{D}\\_{t-1})]$ is constant for different choices of $x_t$, maximizing the above term is  equivalent to minimize \n\n$1 - Pr(y_t = 1 \\mid x_t, \\mathcal{D}_{t - 1}) (term (1)) + E[\\sum\\_{T-t}^{'} Pr(y = 1 \\mid x, \\mathcal{D}\\_{t-1})] - E\\_{y_t}[\\sum\\_{T-t}^{'} Pr(y = 1 \\mid x, \\mathcal{D}\\_{t})] (term (2))$.\n\nThe term (1) $1 - Pr(y_t = 1 \\mid x_t, \\mathcal{D}_{t - 1})$ is the instant regret as in IDS. The term (2) represents the extra gain brought by labeling $x_t$ at the step $t$. One can see that term (2) depends on the change of posterior distribution of the top $T-t$ points altering observing $y_t$, which is a reflect of the KL-divergence of $\\phi(\\cdot \\mid \\mathcal{D}_t)$ and $\\phi(\\cdot \\mid \\mathcal{D}\\_{t-1})$. Lemma 2 shows that information gain can be used to upper bound term (2). Thus, we two methods are intrinsically connected.\n\nNow we discuss the main difference between IDS and ENS. We observed that ENS tends to over explore in the experiments on linear models. See our new Figure 9 in the appendix. We believe this is because ENS assumes that the labels of all remaining unlabeled points are conditionally independent. That is the extra gain by observing the new label $y_t$ is uniform across all the remaining $T-t$ points. This is over-estimating the gain, because in the later stage when the estimates on $Pr(y = 1 \\mid x)$ are more accurate, the extra gain from observing a single label is also much less. ENS, thus, weighs too much on the exploration side. We highly believe that this will lead to linear regret instead of $\\sqrt{T}$ regret that can be achieved by IDS. This is also reflected in Figure 9, where ENS performs worse than IDS when the noise level of the problem is low and we need more exploitation. In general, IDS provides a more flexible balance between exploration and exploitation.\n\n**Summary of contributions to Active Search**\n\nWe give a brief summary of contributions of our work to the Active Search literature.\n- We introduce IDS that is substantially different from previous methods in the literature. We show a generic regret bound for IDS and discussed the actual forms of the regret bound for three interesting cases that have not been theoretically studied in the literature. To my best of knowledge, the only work that applies bandit algorithm with a regret guarantee is [2], which considers a different Gaussian Process model.\n- We first apply the algorithm to reaction condition discovery model, which achieves significant improvement over baseline methods.\n\n[3] Jiang et al. Efficient Nonmyopic Active Search. ICML 2017.\n[2] Vanchinathan et al. Discovering valuable items from massive data. KDD 2015.", " The paper tackles an active learning/bandit problem in which utility is defined to be the sum of the observed labels.\nThe proposed policy minimizes the ratio between the expected instant regret and the expected gain in information about the labels of the top unlabeled points (the number of these top unlabeled points matches the remaining budget).\nIn some settings where exact computation is intractable, the policy falls back to MCMC posterior sampling.\nThe authors then present theoretical results about the regret bound for their policy under specific settings such as linear models, low-rank matrix, and graph problems, and discuss situations in which we can obtain a low regret.\nThe paper also includes extensive experiment results showing that their method is competitive against popular baselines like UCB and Thompson sampling. The paper is well written and clear.\nThe structure of the exposition is good, and both the problems and proposed policy are well-motivated.\nThe authors did a good job connecting this problem to multi-armed bandit, and I enjoyed reading about the special settings like low-rank matrix and graph problems.\nIt is clear from their experiment results that their policy consistently performs well across many tasks compared to other benchmarks in the bandit literature.\n\nIn terms of weaknesses, I believe the authors have missed a particularly relevant line of research on the \"active search\" problem [1, 2, 3].\nThere seem to be significant similarities between ASD and AS: utility being the sum of observed labels, large amount of unlabeled data, limited labeling budget, etc.\nAs of now, I am not sure whether this setting is any different from those studied in the references above.\nFurther, although the information-theoretic approach is a common heuristic, it is not clear to me whether it actually brings additional value, compared to a variant of the policy that aims to minimize the expected _cumulative_ regret (which should roughly have the same cost as the proposed policy).\nMore details are included in the Questions section.\n\nOverall, the submission addresses an important problem and the proposed solution seems to work well on the inspected settings.\nThe theoretical results are also interesting.\nHowever, this work would benefit from a closer look at the AS problem and highlighting why their approach of optimizing their acquisition function is novel and beneficial.\nThis would help situate the paper in the literature better and clarify its contributions.\n\n[1] Garnett et al. Bayesian Optimal Active Search and Surveying. ICML 2012.\n\n[2] Vanchinathan et al. Discovering valuable items from massive data. KDD 2015.\n\n[3] Jiang et al. Efficient Nonmyopic Active Search. ICML 2017. It would be great if the authors could elaborate on their contributions with respect to the AS literature.\nI would be interested to see a comparison between IDS and the policies in [2] and [3], which are specifically designed for AS and should serve as stronger baselines than UCB and Thompson sampling.\nIn particular, the policy in [3] maximizes the expected value of the sum of labels of the top unlabeled points, which should have the same computational cost as IDS but is more utility-centric.\nAlong the same lines, there could be variant of IDS where the numerator of the acquisition function is the _cumulative regret_, as opposed to the instant regret.\nI think this would have the same cost as IDS, since you would have to pick out the top unlabeled points anyway.\nI would be interested to see how this variant performs.\n\nI would also like to see a couple of things added to the presentation of the experiment results.\n- What is the interpretation for the sharp turns in the first row of Figure 1 (in the middle of Figure 1c, for example)?\n- UCB is missing from some of the plots.\n- The authors could consider adding information about whether results in Table 1 are statistically significant.\n\nMiscellaneous:\n- The authors could consider making the plot axis labels larger and use a larger, shared legend.\n- Line 86: missing dot before \"For categorical\"\n- Line 94: I believe the authors meant \"each arm can only be pulled once.\"\n- Line 146: Redundant \"A higher lambda\" Yes, the paper mentions its limitations in the Discussion section in the form of future directions.", " This paper introduces a sequential decision-making problem where the goal is to select a few samples and the reward is the sum of the responses from the samples. We want to maximize the reward (or minimize the regret), and we want to choose the samples adaptively, in an exploration-exploitation fashion. This is achieved by using information gain to determine the next element to sample. A number of heuristics/variants are proposed for when the information gain cannot be computed easily. An experimental evaluation, and a realistic case study, complete the paper, showing the performance of the proposed method. * The presentation is quite clear. The discussion of the relation with MAB and related problem is commendable in terms of clarity. The one issue in clarity is a single but crucial aspect about the use of the posterior. See the question below. \n* The proposed problem is quite interesting, and relevant to areas outside \"core\" machine learning.\n* The proposed algorithm, with its use of information gain to minimize Bayesian regret, makes sense and it is described well. The \"instatiations\" of the algorithm under different knowledge of the model are also interesting and make the paper a bit more concrete. \n* The evaluation is pretty convincing.\n* Additional discussion of the limitations of the proposed approach would have well completed the work. * In the pseudocode for the algorithm (Algorithm 1), the computation of $\\Delta_t(x)$ and $g_t(x)$ is vaguely described as being done \"using posterior $\\phi(\\cdot \\mid \\mathcal{F}_{t-1})$. It is very unclear to me how that would happen, even if we had complete knowledge of the posterior and could easily draw samples or compute any probability according to the posterior. I guess my issue or maybe my question is whether, in the definitions of $\\Delta_t(x)$ and $g_t(x)$, the expectations are taken also w.r.t. the posterior (and if so, why is that correct). I think that, if that is the case (or whatever is the case), this aspect should be better clarified (perhaps right now is mentioned on lines 83-84, but perhaps it should be repeated closer to the definitions above.\n\n* On line 94, I think that \"be be\" should be changed to \"not be\". I do miss a discussion of the limitations of the proposed approach and/or of the proposed problem. It is hard to predict negative societal impact, but one could assume that such an  algorithm could be used to target individuals on the bases of some reward that could be obtained by having them perform some action, which one has to wonder whether would negatively affect underserved populations and minorities.", " This paper presents an adaptive sampling strategy for labeling in the finite sample region (without replacement in contrast to MultiArmed Bandit (MAB) scenario) with the overall objective to maximize the sum of label values of sampled points. The proposed algorithm (IDS) uses  information gain (objective being ratio of regret and information gain at that time step) in order to sample the next point to label. The authors show leveraging model structure leads to improved regret bounds for low-rank matrix and feedback graph models. Finally, the authors present real-world application of IDS to the problem of reaction discovery. \n ### Strengths\n+ Good survey of related work and putting their results in context. \n+ Application of IDS to linear model, low-rank matrix model and graph model.\n+ Interesting real-world application (Reaction Condition Discovery)\n\n\n### Weaknesses\nThe writing could be made a bit clearer.\n\n- I am a bit fuzzy on the real-world application which could be better explained in the supplementary material if not the main text. Per my understanding, the dataset has been taken - PDNC or CNCCI (https://doyle.chem.ucla.edu/wp-content/uploads/2020/07/43-Predicting-Reaction-Performance-in-C-N-Cross-Coupling-Using-Machine-Learning.pdf), but the final result is not comparable to cited papers. For example, Shim etal. (https://pubs.rsc.org/en/content/articlelanding/2022/sc/d1sc06932b original ref [34]) - I would have expected to see something like Figure 3 (if comparing the IDS in the Active Learning setup). {\\em Else it should be clarified that a contrived subtask has been taken to evaluate the problem and show a possible future application.} I understand this is not the main focus of the paper so that clarification would be sufficient. \n\n- For clarity and completeness, please add a section in supplementary material explicitly describing the PDNC and CNCCI datasets (tied to the data available in this upload, namely, Informer.xlsx and doyle_matrix.xlsx),  and, how this translates to the problem setting. \n\n- The parameter $\\lambda$ controlling the ratio between instant regret and information gain is also used in the approximate algorithm (supplementary material H). However, its value is not clarified and the sensitivity of empirical results depending on its choice is unclear to me (or I missed it)? \n - On the feedback graph model [https://arxiv.org/pdf/1106.2436.pdf], would it be correct to say that the setup in this paper can be modeled by considering at each point, the previously sampled point is removed from $G_t$ (the next feedback graph). If so, would the algorithms for sleeping expert setting be comparable? For example, the empirical evaluations in [9] are not directly comparable per my reading - if so, this can be mentioned explicitly in the text. \n - Line 94. \"each arm can be be pulled twice\" -- per my understanding this should read \"each arm {\\em cannot} be pulled twice\" \n- Definition 1 Line 134. Request to define $H(X | Z)$ for completeness. \n- Line 146. \"A higher  $\\lambda$ weigh\" - double line. \n- Line 177. Define $e_i$ (basis vector) before using it. Its' defined later in Line 187.  \n- Line 201. \"given $x$ being selected\", I understood this as $X_t = x$ (at time t, node $x$ selected for labelling which gives its value and noisy values for all its labels). Can this be clarified in writing? \n"], "review_score_variance": 0.888888888888889, "summary": "This was a boredline paper.\nHowever, the reviewers like it, and it seems that the authors answered all the concerns of Reviewer LiPB and myself.\nPlease add the comparison of IDS and ENS to the final version.\n", "paper_id": "nips_2022_x7S1NsUdKZ", "label": "train", "paper_acceptance": "Accept"}
{"source_documents": ["One of the difficulties in modeling real-world data is their complex multi-manifold structure due to discrete features. In this paper, we propose quotient manifold modeling (QMM), a new data-modeling scheme that considers generic manifold structure independent of discrete features, thereby deriving efficiency in modeling and allowing generalization over untrained manifolds. QMM considers a deep encoder inducing an equivalence between manifolds; but we show it is sufficient to consider it only implicitly via a bias-regularizer we derive. This makes QMM easily applicable to existing models such as GANs and VAEs, and experiments show that these models not only present superior FID scores but also make good generalizations across different datasets. In particular, we demonstrate an MNIST model that synthesizes EMNIST alphabets.", "_Summary_:\nThe author extends generative models with multi-generators by restricting the generators to share weights and all bias to be regularized in order to enforce that the inverse maps of the generators can be represented by a single encoder. The regularizer proposed minimizes an upper bound the the sum of the bias variances. This extension is evaluated on a set of visual datasets (MNIST, 3DChair and UT-Zap50k) with respect to density estimation (evaluated with the FID score) and disentanglement (evaluated with their own disentanglement score).\n\n_Strengths_:\n- The method proposes a regularizer which is easy to understand and implement. This makes it easy to apply to existing generative models.\n- The evaluation showed that sample diversity w.r.t. FID score is comparable/superior to baseline generative models.\n\n_Weaknesses_:\nFor me, the main problems are the motivation of encoder compatibility, the clarity of the paper and limited evaluation:\n1) Motivation of encoder compatibility: I don't understand why the inverse of all generators needs to represented by one (!) single encoder? Why can't the generators be in general invertible and thus there exists a mapping for each observed variable to the latent space? What is the advantage of one encoder?\n2) The paper needs to improve its preciseness and clarity. I found it at parts confusing to understand because it introduces terms without really explaining or justifying them, e.g., \"encoder compatibility\", \"good generalizability\", \"equivalence relation\". Further, background is lacking to give details about the model that this paper extends on. This also hinders readability of the section where the authors introduced their own method. Further, assumption of continuity and generalizability have been made without defining these terms or reference.\n3) The evaluation is limited to generators w.r.t. generator architecture (fully-connected layers only). Further, FID score show high standard deviation and thus are not necessarily superior compared to baselines and the introduced disentangle metric are only reported for specific attributes of MNIST and 3D-Chair.\n\nMore detailed comments are below.\n\n_Overall assessment_: \nUnfortunately, I do not believe this paper is ready for acceptance. Therefore I am recommending a rejection. I do believe it is an interesting approach. And with improvement in terms of clarity and evaluation, I am happy to improve my score if the authors can make the necessary improvement.\n\n_Detailed comments and questions_:\n- \"since discrete features are both common and combinatorial\" (Introduction): Common to what?\n- \"they usually have the same generic structure since the underlying continuous features since the underlying continuous features remain the same\" (Introduction): Has this been shown in any work before (if so, please cite)? I can also imagine this being depending on the specific learning task.\n- \"it induces a generalizable equivalence relation between data, and the manifold structure of out-of-sample data can be derived by taking the quotient of this relation\" (Introduction): What is a generalizable equivalence relation between data? Is the quotient of this relation defined somewhere?\n- \"Since deep encoders usually exhibit good generalizability\" (Introduction): What is the generalizability considered with respect to? Can you elaborate?\n- Multi-generator scheme (Sections 2.2, 3): Can you elaborate on the actual model used for Quotient Manifold Modelling (QMM)? The paper mentions that a multi-generator scheme is used but there are no more details than that. It would be helpful to know what the learning objective is, how the (multiple) generator (and discriminator) are used.\n- \"Let H be a set of encoding maps $(X \\to Z)$ that can be represented by a deep encoder\" (Section 3.1): Why is being represented by one encoder important?\n- \"this binding is meaningful only when H has a certain property\": Which one? Is that the generalizability that is mentioned in the next paragraph? \n- \"its elements--deep encoders--have good generalizability\": Similar to the claim above, what does that mean to have generalizability? How can generalizability be quantified?\n- High standard deviation of FID scores of proposed methods: The proposed method have high standard deviation. Can you explain why this is? Is this an optimization problem?\n- Proposed disentanglement metric: There is a vast literature of evaluation for disentanglement, why did you choose to propose your own disentanglement metric? How does it compare to the existing disentanglement metrics (advantages, similarity, etc.)?\n- Table 1 (disentanglement): Can you also report overall disentanglement of the dataset? For disentanglement specifically you can also use existing benchmarks and datasets from Locatello et al. (2018).\n\n_Minor_:\n- Definition of generative maps ${f_G^{(i)}:Z->M [...]\\}_i^A$ (Section 3.1): I believe $A$ was not defined before, is that just the number of generators?\n\n_Post-Rebuttal_:\nUnfortunately, the authors neither did update their paper nor addresses my comments. Therefore, I'm keeping my recommendation of rejection.", "1. The \"orthogonal directions  to  the  equivalence-relation  contours\" in section 3.2. should be properly defined. I can understand that the authors want to say that the horizontal spaces along the sections are perpendicular to the vertical spaces along the fibers. But still need clarification for a general audience.\n\n2. The tangential components are ``parallel'' to U and hence in prop. 1, they need to be same. This is not obvious and need to be properly defined. Along this direction, the tangent and normal of a are with respect U. On the other hand, along the fibers there exists a group operation, which is translation for a linear layer and justify Prop. 1. So Prop. 1 is not surprising or worth proving in the paper. \n\n3. The same nonlinear activation functions (we useLeakyReLU) and batch-normalization layers constraint the explanability of the network. This implies one needs more linear layers. To me, this needs further investigation, and while reading the paper, I thought some interesting ideas will come up during developing non-linearity. Although the design using quotient structure is interesting, the choice of sharing same vertical component is a easy consequence of principal fiber bundle theory. It will be interesting to study ``better'' choice of non-linearity. Can authors comment on that?\n\n4. The experimental setup is a bit under-developed. The disentnglement scores have only been shown on MNIST images, what about for more complicated images like CelebA, e.g., \"Learning Disentangled Representations via Independent Subspaces\", CVPR 2019.\n\n5. Similar kind of simplistic setup has been used for showing generalizability on untrained datasets.\n\n6. These kind of simplistic experimental setup points to the simplicity of just changing liner layer and using same non-linearity. Can authors please verify whether the method ``works'' (in such a simplistic manner) on more complicated datasets, i.e., do we need a very deep network? If yes is the training still stabilized or does converge?", "The authors propose a method to train deep generative models on quotient manifolds and show improved performance on some simple standard test sets.\n\nThe article makes a strong and compelling case for the need of working with quotient manifolds, and shows good results over baselines in all the test cases investigated. The method is simple enough to implement and it’s theoretical motivation plausible (although far from conclusively proven).\n\nHowever, I feel the descriptions of the quotient manifolds investigated (e.g. the continuous-discrete differences) are somewhat lacking. Figure 1 strongly implies the quotient spaces are w.r.t. some continuous quantity, while the method is discrete. It does feel like the method is more aimed towards manifolds with multiple disjoint components than true quotient manifolds. The experiments are also lacking. The baselines are old, and for some of them I know for sure that there are better results available out there. I would be much happier if the ablations were one-to-one, e.g. using the exact same architecture except for this change.\n\nOverall I find this an OK contribution, but it would be strengthened by either giving more sharp theoretical results or by using more up to date experimental methods.\n\nSome smaller comments:\n* It’s not quite clear that the regulariser will force the parallel components to behave as expected. Wouldn’t another possible way for the network to solve this be to make the U’s very small?\n* The architecture seems to assume a standard feed-forward setup. This isn’t really state of the art (almost everyone does resnets). Would it be possible to extend the setup to this slightly more general setting?\n* The domain of “i” should be defined\n* The definition of the pseudo inverse uses only holds for full rank matrices. How do you make sure this is the case in practice?\n* GANs, VAEs, etc have reasonably strong theoretical foundations and simply adding a regulariser might break them in weird ways. Would it be possible to incorporate the proposed methods more rigorously?\n", "Natural images may lie on the disjoint multi manifolds rather than one globally connected manifold, and this can cause several difficulties for the training of Generative Adversarial Networks (GANs) and variational autoencoders (VAEs).\nThe paper proposes QMM, a new generative modeling scheme that inherits the multi-generator scheme but involves an essential regularizer enforcing the encoder compatibility. QMM considers generic manifold structure by the generalizable equivalence relation between data, thereby taking the quotient of this relation and driving the manifold structure of untrained data.\n\nClarity: The paper is well written and easy to follow.\n\nNovelty:  Inheriting multiple generators scheme for generate model training has been proposed in many previous works. There are some recent work for applying multiple generators to VAEs or GANs, e.g., Pan et al., Latent Dirichlet allocation based generative adversarial networks, Neural Networks, 2020,  etc.   A major difference in this paper is that the additional regularizer enforcing the encoder compatibility and the quotient of the plausible equivalence relation. \n"], "review_score_variance": 2.1875, "summary": "This paper presents an interesting method dubbed quotient manifold modeling to handle the \"multi-manifold\" structure of natural data and generalize to new manifolds that arise from novel discrete combinations. While some of the methods and ideas were appreciated by reviewers, there were a number of experimental and clarity concerns. The authors's did not submit a rebuttal, and the many unaddressed concerns (especially around experimental baselines) lead me to recommend rejecting this work.", "paper_id": "iclr_2021_n5ej38Vfuup", "label": "train", "paper_acceptance": "withdrawn-rejected-submissions"}
{"source_documents": ["We present a novel machine learning architecture that uses a single high-dimensional nonlinearity consisting of the exponential of a single input-dependent matrix. The mathematical simplicity of this architecture allows a detailed analysis of its behaviour, providing robustness guarantees via Lipschitz bounds. Despite its simplicity, a single matrix exponential layer already provides universal approximation properties and can learn and extrapolate fundamental functions of the input, such as periodic structure or geometric invariants. This architecture outperforms other general-purpose architectures on benchmark problems, including CIFAR-10, using fewer parameters.", "This paper explores matrix exponentiation as an alternative non-linearity in a neural network. The key idea is to compute an affine transform of the inputs, there by generating an nxn feature map, followed by applying a matrix exponential to this feature map, that is subsequently used for classification. The paper provides several scenarios where matrix exponential could be an interesting non-linearity to use. Experiments are provided on synthetic examples, and standard image recognition benchmarks in a limited setting and show some promise. \n\nPros:\n1. The idea of using matrix exponential as a non-linearity is an interesting idea that is perhaps not explored in the context of neural networks so far. \n2. The paper provides several contexts in which the matrix exponential can be a good non-linearity to adopt.\n3. Experiments on out-of-sample extrapolation shows promising results.\n\nCons:\n1. I think the key factor missing in the paper is perhaps a lack of a solid motivation for using the matrix exponential. What does such an operator capture intuitively in a neural network that is not possible with prior activations, for example, tanh, elu, etc.?\n\n2. Note that the matrix exponentiation operator has a complexity of O(n^1.5) for a feature matrix of nxn, and demands significant computational expense. For large n, which is what is typically used in deep networks, this operation could be impossible. It is unclear if this operation can be done on a GPU and if so to what approximation accuracy? \n\n3. There are other matrix operations that have been attempted before; for example, matrix log-maps, which can also offer non-linearities, and have been used in bilinear CNN models for fine-grained recognition tasks. The paper may include these topics in the survey and contrast the technical advantages of using the exponential instead. \n\n4. The experiments are very limited, and networks that use convolutional layers are shown to significantly outperform the proposed non-linearity. \n\n5. The paper also lacks details in analyzing the feature maps produced by the non-linearity, or provide any gradient derivations, but relies on standard autograd packages for such. It is also not clear why the paper decided to use a full image as input, and why not use convolutional layers within the pipeline. The paper title is also a bit ambiguous as it is unclear what is \"intelligent\" regarding matrix exponentiation? \n\nOverall, this paper explores an interesting non-linearity, however lacks theoretical or empirical rigour in showing that the approach is practically useful. \n\n---------------------------------\nPost-Rebuttal: Thanks to the authors for the detailed response. I think the idea has potential, however (resonating with the comments of other reviewers) I still think the paper should be significantly improved for better motivation, stronger theoretical results, and more elaborate and diverse experiments that can demonstrate the effectiveness of the method. ", "### Summary\n\nIn this work, the authors propose the M-Layer, a neural network \"layer\" that consists in affinely transforming the input to a matrix, applying its matrix exponential, and applying another affine transformation. The parameters describing the affine transformations are the learnable parameters. \nThe authors provide a \"universal approximation result\" that shows that when using a sufficiently large latent space, the matrix exponential is able to replicate arbitrary polynomial functions of the input data, as well as periodic functions. \nIn numerical experiments on synthetic data (determinant of a matrix, swiss roll), time series data, as well as image data (MNIST, CIFAR10, SVHN), a single M-Layer compares favorably to neural ODEs as well as a very simplistic RELU network, while being outperformed by augmented neural ODE's. \nThe authors furthermore provide robustness certificates for the M-layer, and point out its connections to Lie-theory and dynamical systems interpretations of deep learning.\n\n### Decision\n\nOn the positive I think that the idea of going beyond component-wise nonlinearities is interesting and the matrix exponential, with its rich mathematical structure, makes for an intriguing candidate. The authors analyse this proposal from many different view including robustness, approximation capability, computational implementation, and empirical results. \n\nOn the negative side, I find the comparison to conventional neural networks not entirely fair. In particular, the claims of improved representational power compared to traditional neural network layers seem exagerated and vague. \nThe connections drawn to Lie theory and dynamical systems seem superficial in that they mostly restate properties of the matrix exponential instead of providing interpretations of the M layer. \nFinally, the numerical experiments are comparing a single M-layer to a very shallow dense RELU network. While I appreciate the intention of the authors to keep the experimental setup simple, comparing the performance of individual layers is a very artificial situation that does not seem to give any insight if M-layers are useful as layers for deep learning. \nIf they are not, however, this casts doubt on the usefulness and relevance of M-layers in general.\n\nWhile I do think that the idea of an M-layer is interesting and in particular the relative stability bounds are promising, I feel that the message of the paper is diminished by the unclear comparison to existing deep learning methods. \nThis makes it difficult to assess the relevance of the work which is why, for now, I tend to recommend rejection. \n\n\n### Detailed comments \n \n#### Conceptual comparison to RELU:\n\n\"While highly successful in practice, this approach also has disadvantages. In a conventional DNN, any two activations only ever get combined through summation. This means that such a network requires an increasing number of parameters to approximate more complex functions even as simple as multiplication. This approach of composing simple functions does not generalize well outside the boundaries of the training data.\" \n\nI would argue that this is misleading, since the results in later layers of a deep neural network depend on the activations in a highly nonlinear way. Furthermore, I believe there exist universal approximation results even for shallow neural networks with arbitrary width, just like the universal approximation results provided for M-layers are true only in the limit of infinitely large matrix exponentials.  They also do not provide any evidence for their claim that the lack of generalization is linked to the composing of simple functions. The authors do provide some experiments to this end on swiss roll and meteorological datasets. However these examples seem somewhat taylor-made for the exponential map approximation. Furthermore, the RELU network used is not even able to fit the training data well, which suggests this issue might be more due to a lack of model capacity.\n\n#### Lie Algebras and dynamical systems\n\nMy understanding is that these sections mostly recapitulate how the matrix exponential appears in lie theory and the solution of linear ODE. However, I don't think they provide a compelling interpretation of the M-layer in terms of either of those points of view. In neural ODE for instance the input data to the layer can be thought of as a state of a dynamical system that is determined by the model weights. \nIn the M-layer, instead, the input becomes part of the system matrix, then the propagator of this system matrix (the exponential map) is computed, and finally this propagator is applied to initial conditions given by another set of weights by the M-layer. \nI cannot think of a meaningful interpretation and I don't think the authors have provided one, either.\n\n#### Numerical experiments\n\nReiterating what I wrote earlier, I really think that the performance of deep neural networks constructed with M-Layers needs to be compared to that of conventional deep neural networks to establish the relevance of the M-layer as a neural network layer. \nIn a nutshell: if the differences between M-layers and RELUs were to disappear when working with deeper networks, then why should one still use the M-layer. \n\n=====================================================================================\n\nAfter reading the rebuttal and the other reviews I still lean towards rejection, the main reason being that I am not convinced at this point that exponential nonlinearities are a direction worth pursuing.\nI furthermore find the passages on dynamical systems and lie groups to be still lacking, for the same reasons as detailed above.", "This work proposes a novel machine learning architecture (or M-layer) that is in the form matrix exponential. This architecture can effectively model the interaction of feature components and learn multivariate polynomial functions and periodic functions. The architecture of the M-layer is well described. Its universal approximation capability is explained and proved in the Appendix. Other properties related to periodicity, connection to Lie groups, the interpretation from the perspective of dynamical systems, and the robustness bounds are discussed. Experimental study is conducted on toy datasets and three image classification datasets to demonstrate the properties of the proposed M-layer. \n\nOverall, this is a nice piece of work. The proposed M-layer is novel, and the properties shown by this kind of architecture are interesting, especially the capability in modelling periodic functions and extrapolating data. Theoretical discussion is generally clear although some places are a bit hard to follow. Experimental study supports the claims. \n\nMeanwhile, this work could be further improved at the following points:\n\n1. The experimental study does not combine the M-layer with the convolution layers, and it only tests the M-layer on relatively simple image recognition datasets. It will be interesting to know whether the M-layer can be combined with convolutional layers and trained in an end-to-end manner. This is important to check the potential of the M-layer for the applications of visual, audio, and text data analysis, for which end-to-end training has proven to be more effective. This paper could test this setting on some larger-scale benchmark such as ImageNet if possible. \n\n2. In the experimental study, it will be interesting to see the comparison between the M-layer and Gaussian RBF kernel based SVM (RBF is mentioned in Related Work) on these toy datasets and image benchmarks. RBF-SVM can be regarded as a neural network with one hidden layer. \n\n3. This work indicates that the M-layer can better consider the cross-terms of feature components. This could be related to the recent work on second (or higher)-order visual representation, in which the correlations of the channels in a convolutional feature map are extracted and used as feature representation for the subsequent fully connected and softmax layers. This paper can discuss the potential connections with this line of research ([R1]-[R4])\n\n[R1] Tsung-Yu Lin; Aruni RoyChowdhury; Subhransu Maji, Bilinear CNN Models for Fine-Grained Visual Recognition, 2015 IEEE International Conference on Computer Vision (ICCV).\n[R2] Peihua Li; Jiangtao Xie; Qilong Wang; Wangmeng Zuo, Is Second-Order Information Helpful for Large-Scale Visual Recognition? 2017 IEEE International Conference on Computer Vision (ICCV)\n[R3] Melih Engin, Lei Wang, Luping Zhou, and Xinwang Liu, DeepKSPD: Learning Kernel-Matrix-Based SPD Representation For Fine-Grained Image Recognition, European Conference on Computer Vision ECCV 2018\n[R4] Piotr Koniusz; Hongguang Zhang; Fatih Porikli, A Deeper Look at Power Normalizations, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n\n4. The capability of \"extrapolate learning\" of this M-layer is very interesting. This work uses the results in Figures 2 and 3 to demonstrate it. In addition to the double spiral data and the periodic data, is this \"extrapolate learning\" capability applicable to other more general data or patterns? Please comment.  \n\n--- Thank the authors for the detailed response. After reading the response and the comments of peer reviewers, the rating is altered as follows. ", "We thank the reviewer for the valuable comments. We provide below answers to each point.\n\n1. Combining the M-layer with convolutional architectures is something we plan to explore in future work, as we mention in the newly added “Future Work” section. Due to space constraints, we chose not to include larger experiments involving convolutions and advanced regularization techniques in this paper, but to rather focus on a thorough theoretical explanation. The experiments we included involve tasks that seem relatively simple and might be considered toy problems, but are in fact very hard for DNNs, and clearly show the extrapolation abilities of the M-layer. On the other hand, we chose the experiments on generic benchmarks (image recognition datasets) to demonstrate the M-layer can also perform on par with other *non-specialized* architectures on genetic problems.\n\n\n2. Note that SVM with RBF kernel has already been studied on the same type of problems, by e.g. Suykens (2001) (see Fig. 5, which demonstrates excellent fitting but the lack of ability to generalize). We nevertheless decided to add RBF-SVM as an additional baseline for all the experiments in our paper - please see the updated figures and table. The results demonstrate that the SVM can learn and generalize very well on data from the same distribution, in multiple cases better than a DNN. However, it is unable to extrapolate beyond the training domain, as best shown in the spiral and the periodicity experiments. In contrast, the M-layer is able to extrapolate such functions.\n\n\n3. We thank the reviewer for pointing out these references. We have now added and discussed them in Section 2. Specifically, we have emphasized that \"the main differences between the M-layer and other architectures that can utilize feature-crosses are the M-layer's ability to also model non-polynomial dependencies (such as a cosine) and the built-in competition-for-total-complexity regularization\".\n\n\n4. Considering its extrapolation properties, the M-layer architecture is expected to be especially useful for regression problems. One specific example where the M-layer is promising is identifying geometric invariants in input data. The determinant is one example of such a geometric invariant. Another example would be the area of a parallelogram spanned by two vectors (and wide generalizations to *d*-dimensional objects embedded in *n*-dimensional space). Given that word embeddings allow us to meaningfully do vector arithmetic (along the lines of \"Berlin = Paris - France + Germany\"), using an M-layer after an embedding layer could provide more geometric structure to embedding spaces, such as detecting relevance of the volume spanned by three vectors obtained from different aspects of one example by the same trainable embedding. This is a topic that we are exploring in further work.", "1. There are multiple advantages of the matrix exponential operator, which we have underlined throughout the paper. First, this operator can capture functions and combinations of the input features in a more effective way than conventional DNNs, resulting in fewer required parameters for similar performance. Secondly, the matrix exponential allows extrapolation beyond the training domain because it can naturally express oscillations and exponential decay functions, whereas activation functions like ReLU, tanh, ELU are unable to do so. Finally, this paper originates from a series of unrelated research that extensively used backpropagation through matrix exponentials (to which we cannot refer without revealing our identity), where we found that this is numerically more efficient than anticipated, and so we decided to show that matrix exponentiation is a useful tool in ML as well.\n\n2. In our experience, the ability of the M-layer to fit complex data rapidly grows with increasing matrix size. An M-layer used as part of a larger (possibly DNN) architecture would not automatically require large matrix sizes. In a different line of research, we have used automatic differentiation on exponentiation of matrices of size up to 1000 x 1000, with acceptable performance. Finally, as reported in Section 4.4, training can be indeed done on the GPU, with training times comparable to those of a ReLU DNN.\n\n3. Thank you for pointing out this research. We now discuss it in Section 2. Specifically, we have now emphasized that \"the M-Layer differs from this approach in two ways: first, it uses matrix exponentiation to *produce* feature crosses, and not to improve trainability on top of existing ones; secondly, it computes a matrix operation of an arbitrary matrix (not necessarily symmetric positive semidefinite), which allows more expressibility, as explained in Section 3.3\".\n\n4. We strongly challenge the idea that the experiments are \"very limited\", in particular in the light of the page count constraint. We have performed four different types of experiments on a total of seven datasets. First, we did an experiment on \"Swiss roll\" synthetic data, showcasing the unique capability of the M-layer to extrapolate beyond the training domain. Secondly, we demonstrated the ability of the M-layer to extrapolate periodicity on a real-world dataset, which a ReLU DNN cannot do without additional feature engineering; moreover, a DNN with a similar number of parameters cannot even learn the training set. Then, we have shown the ability of the M-layer to outperform, with fewer parameters, ReLU DNNs on the problem of determinants and permanents, which can be useful for larger problems where higher-level features such as the volume are important (but where a more complex experiment would not clearly show what causes the performance improvement). Finally, we have compared the M-layer on generic benchmarks, where we do not expect it to shine, but rather to demonstrate its validity as a generic ML architecture comparable to other *non-specialised* architectures. Note that the architecture that outperforms the M-layer is already specialized through convolution.\n\nAs a general remark, the scientific method tries to explain relevant differences in the simplest possible setting where they arise, where all distracting elements have been eliminated. As a comparison from physics, the dynamics of the Lorenz attractor was found by trimming down a 12-parameter atmospheric physics ordinary differential equation to the bare minimum that shows the observed puzzling behavior; the resulting 3-parameter model admits an understandable interpretation of the phenomenon that would be hard to discuss in the “more realistic” model. We are doing the same for our architecture here - presenting the differences in the simplest possible setting where they arise. \n\n5. We have now addressed the question about gradient derivations in Section 2.2: \"In general, there is no simple analytical expression for the derivative of $exp(M)$, but one can instead compute derivatives for each step of the algorithm chosen to compute the exponential. For example, tf.linalg.expm uses the scaling-and-squaring algorithm with a Padè approximation, whose steps are all differentiable\". We have also added an appendix with further details. Regarding feature maps, these are usually used to understand deep CNNs, and were not of particular interest in this work, but can be explored in the future. We have explained above our reasoning about not including experiments with convolutions; we aimed to showcase the ability of the M-layer to extrapolate periodic and geometric functions with a small number of parameters, as opposed to presenting it as an architecture specialized on image classification. Finally, regarding the title, we clarify that we chose it as a concise way to refer to an architecture that *learns* (a common meaning of “intelligence” in ML and AI) using matrix exponentiation.", "We thank the reviewer for the valuable comments. However, we firmly challenge the idea that our paper lacks rigour. On the contrary, we have explained in detail the M-layer architecture, its mathematical properties and why it is capable of better extrapolation for periodic and geometric functions compared to conventional DNNs, we provided universal approximation and certified robustness proofs, and we demonstrated its performance on both generic ML datasets (image classification) and on datasets that highlight its advantages over conventional DNNs. \n\nIt is important to convey that the aim of this paper is to provide a thorough mathematical introduction to a new model, as opposed to delving into large and harder-to-interpret experiments without a solid theoretical foundation. We chose to use a significant part of the paper to explain the properties of the M-layer, and we performed experiments that were relatively simple but clear, showcasing the abilities of the model without leaving the possibility of confounding factors, as might have been the case with larger networks or datasets. This is why we chose not to present results combining the M-layer and convolutional layers for this paper, although we do aim to do this in future research. To clarify this, we have added a section on “Future Work”. Overall, especially considering that low interpretability is currently a problem in ML, we believe that this is the proper and mathematically rigorous way of introducing a novel architecture.\n\nA point-by-point response to all the questions raised by the reviewer will follow within the next few days.", "[Part 2 of 2]\n\nWe thank the reviewer for the valuable comments on the connection to Lie groups and the numerical experiments.\n\nThe second concern of the reviewer relates to the connection to Lie algebras, which is not considered strong enough. In fact, already thinking about the M-layer as a \"circuit breadboard for feature crosses with built-in regularization as different circuits compete for space\" is rooted in our research experience with nilpotent Lie algebras and provided the original intuition why such an architecture would be interesting to characterize. For the special case of the generator matrices forming a nilpotent Lie algebra, section 3.3 provides one such intuitive and accessible interpretation. The M-layer architecture can be seen as a generalization of this idea to other Lie algebras, and training can demonstrably make it fall back to effectively using a nilpotent algebra to solve some given task, such as computing a determinant. To further address the connection, we have added the following to the paper: \"The connection to Lie groups is important because it can provide techniques for the regularization of the M-layer. It is, for example, possible to impose a constraint on the maximal depth of feature crosses by taking a generator-matrix tensor $T_{imn}$ that itself is a tensor product with a nilpotent matrix algebra. There are also other Lie-group-inspired ways to think about regularization, which we will discuss in future work\". Considering the space restrictions for this paper and our aim to present here the fundamentals of the M-layer, we did not discuss in more depth regularization, which is tightly linked to Lie theory, but we plan to do this in future work.\n\nFinally, the third concern of the reviewer relates to the numerical experiments and comparisons between the M-layer and DNNs. Our paper repeatedly demonstrates that the M-layer is more parameter-efficient than DNNs, while having comparable training speed, which by itself is a good reason to use the former. Regarding experiments involving deeper networks, we had to make a difficult choice about what to include, given the limited space of the paper. As this is a new architecture, we aimed to provide a good explanation of its mathematical foundation, including universal approximation and robustness. We also included experiments showcasing the use of the M-layer both on general ML benchmarks and problems where it has a natural advantage over regular DNNs (domain extrapolation on periodic and geometric data). Rather than going directly to large models where it would be comparatively more difficult to make a proper comparison, we chose an approach with smaller but clearer steps. That being said, we intend to follow up this paper with research on larger models and advanced regularization techniques, including those based on insights from Lie theory. In order to make that more clear, we have added a “Future Work” section.\n", "[Part 1 of 2]\n\nWe thank the reviewer for the valuable comments on the comparison between the M-layer and ReLU networks.\n\nWe understand that this comparison came across as misleading, possibly because the space constraints limited some of the explanations. To clarify this comparison, we have now improved the Introduction with a point about parameter efficiency and added a paragraph on the advantage of the matrix exponential over widely-used activation functions like ReLU that approximate the target function locally. We do agree that the results in later layers of a deep neural network depend on the activations in a highly nonlinear way, and that there exist universal approximation results for shallow neural networks. Indeed, we know from the Kolmogorov-Arnold theorem that combining different features via adding non-linear activations is sufficient to represent any function to any given accuracy. However, this is not parameter-efficient: in a setting where the features only get combined via addition, something as simple as multiplying two features requires learning an approximate logarithm twice, and an approximate exponential, per every instance of where multiplication is useful. Indeed, throughout the paper, we provide evidence that the M-layer is more parameter-efficient than conventional ReLU networks.\n\nWe have also addressed the reviewer’s concern regarding the lack of capacity of the ReLU network in the second experiment by adding to Figure 3 the results for a larger ReLU network, which has appropriate capacity to learn the training set. As shown, this larger network is still not able to extrapolate periodicity. This is another important aspect we would like to emphasize: unlike the M-layer, a ReLU network with no additional feature engineering will be unable to extrapolate oscillatory behavior, or even simple exponential decay, beyond that it has seen in its training examples. The ReLU network can only learn oscillatory behaviour by locally approximating small pieces of the function, which it will not learn anything about outside the training set. In contrast, we show (see section 3.4) that the M-layer can naturally represent oscillations and exponential decay through the matrix exponential operator. In addition to mathematical applications, many natural events are periodic and can therefore benefit from the extrapolation properties of the M-layer. We have added a new paragraph to the Introduction further elaborating on this.\n\nWe would also like to add a clarification regarding the problems we employed for showcasing the extrapolation abilities of the M-layer. While these may be perceived as tailored to the M-layer, they can also be regarded as benchmarks for a model’s capacity of extrapolation on periodic and geometric datasets. We argue that the community might not be accustomed to this type of problems precisely because conventional DNNs do not perform well on them. With our experiments showing that an alternative architecture can have natural extrapolation abilities with relatively fewer parameters and comparable performance, we hope to persuade the community that such problems are also important as benchmarks.", "We thank the reviewer for the valuable comments and for the two suggestions on clarifying matrix squareness and the missing tensor indices, which we have now fixed. We were pleased that our paper was easy to follow, as behind the scenes we spent some effort to present clearly multiple concepts that may be unfamiliar to many readers, including Lie groups and the matrix exponential itself. We designed this paper to be an accessible yet thorough introduction to these topics, setting a good theoretical ground for further research on the M-layer.\n\nThe straightforward character of the universal approximation proof and certified robustness are, we consider, strengths of the M-layer in terms of interpretability - a key topic in current ML. In the universal approximation proof (which we believe should be mandatory to demonstrate for any new architecture, and not simply \"something better than nothing\"), the polynomial decomposition is an important insight into the representational capabilities of the model. For certified robustness, we have now expanded on options allowed by the M-layer for future work: \"An alternative way of studying robustness for a specific example involves the diagonalization of the matrix that corresponds to that example. In particular, we can use the Baker-Campbell-Hausdorff (BCH) formula to understand the neighborhood of the example. For example, if we regularize the tensor $T$ to punish non-commutativity of generator matrices, we expect the terms in the BCH formula to decrease in magnitude according to the number of commutators, so we can obtain an understanding of the non-linear effects in the neighborhood of each specific example. To our knowledge, no equivalent type of analysis exists for ReLU networks\".\n\nConcerning the motivation of using expm, as a side note, this paper originates from a line of unrelated research that extensively used backpropagation through matrix exponentials for sizes up to 1000 x 1000 (which we do not cite in order to preserve anonymity), where we found that this is numerically more efficient than anticipated. We have now added a paragraph to the Introduction, better clarifying the theoretical grounds for this choice: \"A unique advantage of the matrix exponential is its natural ability to represent oscillations and exponential decay, which becomes apparent if we decompose the matrix to be exponentiated into a symmetric and an antisymmetric component. [...] This gives the M-layer the possibility to extrapolate beyond its training domain. [...]\".\n\nRegarding the experimental evaluation of the M-layer, we had to make a difficult decision about what to explore within the limited space of 8 pages. First, we wanted to provide adequate mathematical background and intuition for readers who would be less familiar with the concepts we used. Then, since this is a new model, we considered it good practice to provide universal approximation and certified robustness proofs. Finally, to relate our model to existing models, we wanted to include its performance on a widely understood generic classification problem, which we deemed the image datasets to be. These experiments demonstrate the validity of the M-layer as a *non-specialized* architecture that can solve generic problems.\n\nIn the limited remaining space, we chose to showcase the key properties of the M-layer (domain extrapolation on periodic and geometric data) on relatively simple problems in order to paint a clear picture and avoid confounding factors that might affect larger datasets. We are aware that other techniques can be used to learn in particular periodicity, but, in contrast to these, the M-layer can solve such problems *without any feature engineering*. Importantly, we consider these experiments more relevant to the message of the paper compared to the image recognition experiments. These are not simply \"toy problems\", but can rather be considered benchmarks for domain extrapolation and parameter efficiency. Conventional DNNs do not perform well on extrapolating even on a simple \"Swiss roll\" problem, hence the community is not used to regarding these problems from this perspective. We hope this will change.\n\nIn summary, we think that our piece of work is valuable and rigorous in accordance to its goal of being a well-motivated mathematical introduction of a new ML architecture with powerful extrapolation properties. Its elegant mathematics should not be confused with conceptual simplicity. In particular, a strength of the M-layer that follows from its mathematical elegance is its potential for interpretability. We chose what to include in the paper with the aim of crafting a clear introduction to the M-layer, as opposed to delving into large experiments without a solid foundation. Nonetheless, we also intend to follow up with research involving more advanced architectures and regularization techniques, in particular some inspired by Lie theory, for larger experiments, as clarified in the “Future Work” subsection.", "Title: INTELLIGENT MATRIX EXPONENTIATION\n\nSummary of the paper:\n\nThe idea is to use the matrix exponential as a layer in a neural network. This basically requires transforming to a latent variable which has a rank two tensor shape (a matrix) then applying the e.g. \"expm\" function. A robustness result is given (section 3.7) and a universal approximation result (appendix A) and some experiments on mainly toy but also image classification problems, and a univariate regression on seasonal data.\n\nPros:\n\nThe paper is written in a nice easy to read way. I enjoyed reading it because it is very easy to follow, though to be fair the contribution is rather simple so this should be a given.\n\nThe robustness result is something, but it follows from a very basic result on matrix exponentials. Also, the universal approximation result is better than nothing, but also quite straightforward and constructive and simply comes from observing the matrix exponential as an arbitrary polynomial.\n\nSection 3 is nice throughout ; simple and easy to follow yet interesting.\n\nCons:\n\nI hate to be that reviewer, but the paper should go further to motivate the use of the expm function. Figure 2 is very cool indeed, but then the real world experiments are not highly motivating. The seasonality data is neat, but this can presumably be better handled by a Gaussian process with periodic covariance, anyway (which is not compared with). To motivate using expm would require far more extensive experiments, even if this requires some heavy computational resources. \n\nSuggestions:\n\nFigure 1 - mention the need for a matrix layout (you seem to want to suggest a 3 by 3 matrix with your diagram).\n\nEquation 1 - is it normal to omit j and k from the l.h.s. in tensor notation ?\n\nRecommendation:\n\nThis paper is crying out for a more thorough experimental evaluation ; I am near the fence on this paper but I think it could be much more impactful with such and as such lean toward rejection."], "review_score_variance": 0.1875, "summary": "This paper was reviewed by 4 reviewers who scored the paper below acceptance threshold even after the rebuttal. Reviewer 4 is concerned about motivation, Reviewer 2 rightly points out that there exist numerous works that use some form of spectral layers in a deep setting on challenging datasets - something lacking in this work. Reviewer 3 is concerned about limited discussion on lie groups and the overall benefit of expm(.). Reviewer 1 reverberates the same comments regarding insufficient experiments,  comparisons and limited motivation. We encourage authors to consider all pointers given by reviewers in any future re-submission.", "paper_id": "iclr_2021_GtiDFD1pxpz", "label": "train", "paper_acceptance": "withdrawn-rejected-submissions"}
{"source_documents": ["In a broad Degree-Corrected Mixed-Membership (DCMM) setting, we test whether a non-uniform hypergraph has only one community or has multiple communities. Since both the null and alternative hypotheses have many unknown parameters, the challenge is, given an alternative, how to identify the null that is hardest to separate from the alternative. We approach this by proposing a degree matching strategy where the main idea is leveraging the theory for tensor scaling to create a least favorable pair of hypotheses. We present a  result on standard  minimax lower bound theory and a result on Region of Impossibility (which is more informative than the minimax lower bound). We show that our lower bounds are tight by introducing a new test that attains the lower bound up to a logarithmic factor. We also discuss the case where the hypergraphs may have mixed-memberships.\n", " Thanks for your comments. We are glad that you think the test we use in simulations is an interesting contribution.\n\nIn our point-by-point response (Point 3), we gave a specific example of how these results are practically useful, which we do not repeat here. We wish to take this opportunity to re-iterate our point: Lower bound studies are practically relevant, because they help differentiate optimal algorithms from non-optimal ones. In each NIPS conference, hundreds of new algorithms were proposed, but over the years, only a small fraction of them will pass the test of time. It is therefore desirable to develop algorithms that are optimal, which are more promising to pass the test of time. But to do that, we have to derive a sharp lower bound as a benchmark and this is particularly important since community detection on hypergraphs attracts increasing attention. For a problem, an optimal algorithm and a sharp lower bound are two sides of coin, and both are indispensable. From a practical viewpoint, advancements in each of two aspects are valuable.", " Thank you for your detailed response. I realize that I did not understand the test that was used in the numerical experiments, and I recognize that this is an interesting contribution of this paper. Nevertheless, I am not planning on changing my score (which is Weak Accept), since I think that what I wrote about the practical significance of the result still stands.", " Thanks for your valuable comments. Below is a point-to-point response (please also see our response to all reviewers above, where we clarify the motivations and practical side of the paper). \n\nThanks, Yuan et al (2021) is a remarkable paper. We especially like the idea of using loose loop counts for testing, and the results on the power and limiting null there.  We have already included some comparison with the paper, and we would be happy to add more comparisons. \n\nAs for the model, despite some notational differences, we are using the same model as in the literature (see more discussion below). For example, the setting of Yuan et al (2021) is a special case of our $m$-DCMM model if we let (a) $\\theta_1 = \\theta_2 = \\ldots =  \\theta_n = \\alpha_n$, (b) all communities have the same size, and (c) ${\\cal P}^{(m)}_{k_1k_2\\cdots k_m} = 1$ if $k_1=k_2=\\cdots =k_m$ and $\\rho$  otherwise. Moreover, in this special case,   $|\\mu_2^{(m)}| = |1-\\rho|$ and $l_m=\\\\|\\theta^{(m)}\\\\|_1^{m-2}\\\\|\\theta^{(m)}\\\\|^2(\\mu_2^{(m)})^2\\asymp n^{m-1}\\alpha_n^m(1-\\rho)^2$, so $\\ell_m \\to 0$ is equivalent to that $n^{m-1}\\alpha_n^m(1-\\rho)^2\\to 0$; the latter is the lower bound given by Yuan et al (2021). On the other hand, our paper is for the much broader non-uniform tensor model, and our results are more general. Even for the SBM case, note that our results show that symmetric SBM and asymmetric SBM have very different Region of Impossibility: degree-based $\\chi^2$ is powerless in the former but may have power in the latter. Such insight was not discovered before. \n\nOur model is the same as that in the literature. One reason you may get confused is because we use a different notational system. For example, consider the network case without mixed-memberships. In this case,  ${\\cal A}  = {\\cal Q} - \\mathrm{diag}(\\Omega)$ and $\\Omega = \\Theta \\Pi P \\Pi' \\Theta$, where $\\Theta = \\mathrm{diag}(\\theta_1, \\theta_2, \\ldots, \\theta_n)$ and $\\Pi = [\\pi_1, \\pi_2, \\ldots, \\pi_n]'$. This is exactly the DCBM by Karrer and Newman (2010) for social networks.  Such a matrix form for $\\Omega$ is highly preferred since the information bound critically depend on the eigenvalues of $\\Omega$. Another reason that you may get confused is because we require $P$ to have diagonal entries (while SBM model does not have to). This is because DCBM is broader, so we need a constraint to make sure that $(\\Theta, \\Pi, P)$ are uniquely determined by $\\Omega$. In fact,  for any positive numbers $d_1, d_2, \\ldots, d_K$, let $D = \\mathrm{diag}(d_1, d_2, \\ldots, d_K)$ and $\\widetilde{\\Theta} = \\mathrm{diag}(\\tilde{\\theta}_1, \\tilde{\\theta}_2, \\ldots, \\tilde{\\theta}_n)$, where $\\tilde{\\theta}_i = d_k^{-1}  \\theta_i$ if node $i$ belongs to community $k$. It follows that \n$$\\Omega =  \\Theta \\Pi P \\Pi' \\Theta = \\widetilde{\\Theta} \\Pi (D P D)  \\Pi' \\widetilde{\\Theta}.$$ \n\nTherefore, for any $(\\Theta, \\Pi, P)$ and $\\Omega = \\Theta \\Pi P \\Pi' \\Theta$, we can always rewrite $\\Omega = \\widetilde{\\Theta} \\Pi \\widetilde{P} \\Pi'  \\widetilde{\\Theta}$ and  $\\widetilde{P} = D P D$. But if we restrict $P$ to have diagonal entries, then $D$ has to be the identify matrix, so $(\\Theta, \\Pi, P)$ are uniquely defined. \n\nLast, note that in our response to all reviewers, we have presented an example where we consider an order-3 tensor with two equal-size communities where $\\theta_1 = \\theta_2 = \\ldots = \\theta_n = \\alpha_n$ and where the two slices of the tensor ${\\cal P}$ are \n\\begin{equation} \n\\left[\n\\begin{array}{ll} \n.5 + \\epsilon & .5 - \\epsilon  \\\\\\\\\n.5 - \\epsilon  & .5 + \\epsilon  \\\\\\\\\n\\end{array} \n\\right], \\qquad \\mbox{and} \n\\qquad \\left[\n\\begin{array}{ll} \n.5 - \\epsilon & .5 + \\epsilon   \\\\\\\\ \n.5 + \\epsilon  & .5 - \\epsilon \\\\\\\\ \n\\end{array} \n\\right], \n\\end{equation} \nrespectively. We conjecture that in this case, no polynomial-time test may achieve the lower bound. In case that you are not sure why this is true, let us note that the power of the test in Yuan et al (2021) depends on the quantity delta (defined in equation (10) of their paper), but delta = 0 in this particular example, so the test loses power. However, our proposed test (which is not polynomial-time) is optimal in this case. We hope this helps convince you that the setting we consider here is indeed much more complicated than that in Yuan et al (2021).  \n\n\n\n", " \nThank you for your reading and your careful summary. We are glad that you think our results are nice and our topic is relevant for the conference. Below is a point-to-point response (please also see our response to all reviewers above, where we discuss the motivation and \npractical side of our paper). \n\n1.The null model. \n\nWe must clarify that our null model is not the Erdor-Renyi model. In our null model, the probability for a hyper-edge $\\{i,j,k\\}$ is equal to $\\theta_i\\theta_j\\theta_k$. Degree heterogeneity still exists in the null model.  \n\nYour guess is correct that a test using the Erdos-Renyi null is usually not effective on real data applications. This is the motivation of our work: Degree-corrected models are preferred in real applications, but it is unclear to what extent its statistical limit differs from that for SBM in hypergraphs. Our study helps answer this question. \n\n\n2.Whether the result is interesting to only a small group of attendees. \n\nWe must clarify that community detection is a problem of interest to many researchers in social network data analysis (e.g., Peter Bickel and Bin Yu at UC-Berkeley, and Stephen Fienberg and Eric Xing at CMU used to work in this area). It is true that there are interests from the random graph and SBM community, but the main users of community detection algorithms are from machine learning, statistics, and social science. Network analysis is a well-accepted and indispensable area in machine learning, and many NIPS papers discuss network analysis.  \n\nOur problem is also of interest to researchers who study tensor data. Hypergraph community detection is indeed a clustering problem with Bernoulli tensor data. It has close connections to clustering with Gaussian tensor data, an area that have a lot of publications in NeurIPS.  \n\nFor these reasons, we think our problem will be interesting to a broad group of attendees. \n\n\n3.Practical relevance and real applications. \n\nHere is an example of why our theory is useful in practice. Suppose we are interested in knowing whether a group of NeurIPS authors have latent clusters. We can build a coauthorship hypergraph on these authors using their NeurIPS papers in recent years and test whether $K=1$. Suppose a data analyst suggests to compute the node degrees $d_1,d_2,\\ldots,d_n$ and look at the chi-square statistic $\\sum_{i=1}^n (d_i-\\bar{d})^2$. Our theory immediately tells that this is a wrong approach, because any degree-based test will not have power.\n\nA concrete real application is in our ongoing work (see our response to all reviewers above; where we discuss a recent data set on the publication of statistics, which contains bibtex (title, author, keywords, abstract, references) and citation data of 83,331 papers published in 36 journals, spanning 41 years (1975-2015)).   We can apply our test to measure the collaboration diversity of statisticians. We build a coauthorship hypergraph. Then, for each author, we extract a personalized hypergraph (also called an ego-hypergraph) by restricting the node set to this author and his/her coauthors. We then apply the test on this personalized hypergraph to get a p-value. This p-value is used to measure the collaboration diversity of this author. We can also apply our test for hierarchical community detection. This is also discussed in our response to all reviewers above.  \n\n\n\n4.The algorithm in simulations. \n\nWe clarify that the test used in our numerical experiments is not an \"entirely different\" algorithm. Our test can be re-expressed as $\\max_{S} \\{X(S)\\}$, where the maximum is taken over all $(m+1)$-partitions of nodes and $X(\\cdot)$ is an empirical utility function defined through lines 337-345 (given any $S$, if we follow the computation in lines 337-345, the resulting number is denoted by $X(S)$). \nThe definition of $X(\\cdot)$ is the core idea behind our test. In simulations, we replace the exhaustive search over $S$ by $\\hat{S}$ from a spectral clustering algorithm, and we use $X(\\hat{S})$ as the test statistic. Here, we use $\\hat{S}$ to approximate the maximizer of  $X(\\cdot)$. It is a proxy of our original test, not something \"entirely different.\"  \n\nThere is a rationale for this proxy. Using the proof of Theorem 3.1 (and some regularity conditions), we can show that the maximizer of $X(\\cdot)$ is a non-splitting partition, i.e., each group is equal to either a true community or the union of a few true communities, with high probability. This inspires the use of $\\hat{S}$ from spectral clustering. \n\nAlternatively, we may replace the exhaustive search by a greedy search over sub-sampled $S$. We tried it numerically and found that the current proxy was better. \n\n\n5.Our algorithmic contributions. \n\nThere is no available test for the global testing under tensor-DCMM. The test we propose in Section 3.1 is entirely new. As a byproduct,  we also introduce a new iterative algorithm for estimating degree parameters in the null DCMM and a proxy test (see Point 4 above) that runs fast and works satisfactoriy simulations. These are all our algorithmic contributions. The proxy test has a great potential for real applications. Please see our response to all reviewers above (especially the last part, where we discuss three points on the algorithm). \n\n\n", " Thank you for your very helpful comments. Below is our point-by-point response (please also read our response to all reviewers above, \nwhere we explain the motivation and practical side of our paper). \n\n\n1.The challenges and difficulties that arise due to the use of a richer mode:\n\nCompared with the study of tensor-SBM, we encounter 4 major challenges as follows:\n\nFirst, in lower bound studies, a key step is to figure out the \"least favorable configuration\" -- how to pair the null and alternative hypotheses so that testing is most difficult. A naive extension of the least favorable configuration from tensor-SBM to tensor-DCMM will suggest matching the degree parameters $\\theta_1,\\ldots,\\theta_n$ of two hypotheses. Unfortunately, this fails to lead to a sharp lower bound. We find out that the correct approach is matching expected node degrees, not matching the degree parameters. These approaches happen to be the same for tensor-SBM but they are different for tensor-DCMM. This was not known before. \n\nSecond, there is no existing approach for matching the expected node degrees (because this is not needed for tensor-SBM). We tackle it by borrowing the tensor scaling literature (generalization of Sinkhorn-type theorems from matrices to tensors) to propose a degree matching strategy. This is new and non-obvious. \n\nThird, the analysis of the $\\chi^2$-divergence is challenging. Tensor-DCMM has many more parameters than tensor-SBM, and so the $\\chi^2$-divergence is more complicated. It requires a lot of efforts to bound terms with different random patterns, and it is non-trivial to figure out that $\\\\|\\theta\\\\|_1^{m-2}\\\\|\\theta\\\\|_2^m\\mu_2^m$ is the correct quantity driving the asymptotic behavior.  \n\nLast, showing the tightness of the lower bound is much more challenging than we anticipated. (a) Even with the degree parameters given, there is no straightforward testing idea that attains the lower bound. Several tests that work satisfactorily for tensor-SBM suffer from signal cancellation under tensor-DCMM (because tensor-DCMM, which has a large number of free parameters, is more general) and thus lose power. (b) It is hard to estimate the degree parameters when $m\\geq 4$. Several estimates that work satisfactorily for $m\\leq 3$ have non-negligible biases for $m\\geq 4$. We have to propose an iterative algorithm for bias correction (see Section 3.1). No such issue exists for tensor-SBM. \n\nIt is a pity that the paper is too compact due to that NIPS has a strict page limit and that we have to present many results to make the story complete. We would like to clarify all the technical difficulties in the revision (by moving some content to the supplement).  \n\n\n2.Why $\\pi_i$ appears in the expected degree:\n\nThe expectation here is conditional on $\\pi_i$. Our degree matching requires ``for fixed $i$ and every possible realization of $\\pi_i$, it holds that $\\mathbb{E}_1[d_i|\\pi_i]=\\mathbb{E}_0[d_i]$. We will add a clarification there. \n\n\n3.An intuitive explanation of how degree matching affects $\\chi^2$-divergence: \n\nGreat suggestion. We will add an explanation. Intuitively speaking, the $\\chi^2$-divergence has many terms, each being the sum (or a function) of terms in a Taylor expansion. Therefore, we can roughly call these terms first-order term, second-order term, and so on and so forth.  When expected node degrees are not matched between the null and the alternative, the first-order term dominates, and correspondingly, a degree-based $\\chi^2$-test may have power; see Section 2.4.   When the expected degrees are matched, the first-order term vanishes as we have hoped,   and the degree-based $\\chi^2$-test loses power. As a result, the second-order term in the $\\chi^2$-divergence now dominates.  The asymptotic behavior of the second-order term is controlled by $\\\\|\\theta\\\\|_1^{m-2}\\\\|\\theta\\\\|_2^m \\mu_2^m$; thus, it gives a sharp lower bound. \n\n\n4.Expanding Section 2.4 and Section 3.1:\n \nWe are sorry that due to strict page limit, we have to present the two sections in really compact form. We would be glad to expand them in the revision.\n\nIn our response to your Point 1, we try to explain the challenges of finding the test in Section 3.1. We hope it is useful.  ", " We thank all reviewers for valuable comments. We are glad that all of them find our paper technically sounding, and especially, some reviewers find our results new, original, important, and that our paper solves a fundamental problem in graphical model. \n\nHowever, since that there is a strict page limit  and that we have to present many results to make the \nstory complete, we could not explain the motivation and practical relevance of our problem with more details, and as a result, there are significant misunderstandings on these.  We wish to take a chance to clarify.  \n\n\n\nWhy the paper fits NeurIPS (area, application examples, data, and algorithm):\n\nFirst, some reviewers think our work grew out of the probability area of random graphs and stochastic block models, and thus our results are only of interest to mathematical probabilist. We do not agree, for our problem grew out of the area of social network analysis, which is a well-accepted and indispensable area in machine learning (e.g., Peter Bickel and Bin Yu at UC-Berkeley, and Stephen Fienberg and Eric Xing at CMU used to work in this area; all these people are well-known in the machine learning community). Also,  in recent years, there are many NeurIPS papers discussing network community detection and related problems, with many interesting applications. The authors have been working in network analysis for many years and the current paper grew out from this area.  \n\nIn fact, our paper points out that probabilistic random graphs and stochastic block models are sometimes too idealistic to be useful in real applications, so we propose to consider a much broader and more realistic model. Our model extends the well-known Degree-Corrected Block Model (DCBM) by Karrer and Newman (2010) for network analysis to tensor setting and allows for mixed-memberships. It is important to use such a broader model. For example,  in Section 2.4, we show that a test with good powers for the block model setting turns out to be powerless in the broader setting. \n \nAlso, as partially explained in Lines 14-16, our paper is directly motivated by a large-scale high-quality data set on the publications of statisticians,  where we have the bibtex (title, author, abstract, keywords, references) and citation data of 83,331 papers in 36 journals, 1975-2015. Since a paper may have $m$ authors,  $m = 1, 2, \\ldots$,  the co-authorship relationship is best modeled with a non-uniform hypergraph.  We are interested in (a) how to use the hypergraph to build a community tree (and so to visualize the coauthor topology of statisticians), and (b) how to measure the diversity of an individual author.  Our study can help (a)-(b). For (a), note that a recursive clustering algorithm (like a tree) has many layers, and for each identified community in each layer, a \ncrucial problem is to decide whether we should  further divide it into several communities or not. This is exactly the global testing problem we study in this paper. For (b), to measure the diversity of an author, we consider his/her personalized (or ego) hypergraph, where each node is a co-author, and we think his/her co-authorship as diverse if and only if his/her personalized hypergraph has more than one community (so he/she co-authors with people from at least two different groups or research areas). Therefore, this is also the global testing problem we discuss. We have done both (a)-(b) in the network setting with the data set mentioned above. However, since this data set is best modeled with a non-uniform tensor, instead of a network (e.g., a paper may have 3 or more authors; it is better to model the co-author relationship as a hyper-edge instead of an edge), we have to attack the problem with a tensor model as in the current paper.    \n\nLast, additional to a tight lower bound (which, as all reviewers agree, solves a difficult problem), we also present an upper bound. It is true the algorithm we propose is not polynomial time, but we have the following comments. \n\n(i) We conjecture that unless we put a hard-to-check and unrealistic constraint on the model, \npolynomial-time algorithm that is optimal does not exist: only non-polynomial time algorithm \ncan be optimal in such a broad setting. For example, consider an order-$3$ uniform tensor with $2$ communities,  where (a) two communities have equal sizes, (b) $\\theta_1 = \\theta_2 = \\ldots = \\theta_n$, and (c) the two slices of  tensor ${\\cal P}$ (each of them is a 2 by 2 matrix)  are \n\n$\n\\left[ \n\\begin{array}{ll} \n.5 + \\epsilon, & .5 - \\epsilon \\\\\\\\\n.5 - \\epsilon, &  .5 + \\epsilon \\\\\\\\ \n\\end{array}  \n\\right], \\qquad \\mbox{and} \\qquad  \n\\left[ \n\\begin{array}{ll} \n.5 - \\epsilon, & .5 + \\epsilon \\\\\\\\\n.5 + \\epsilon, &  .5 - \\epsilon \\\\\\\\   \n\\end{array}  \n\\right], \n$\n\nrespectively, where $\\epsilon$ depends on $n$. Our preliminary study of the computational lower bound suggests that there does not exist a polynomial time algorithm that is optimal (of course, our proposed test is optimal in this case, though it is not polynomial time). On one hand, such a result says our proposed algorithm (though not polynomial time) is practically relevant, as we won't have any polynomial-time algorithm.  On the other hand, if we really need polynomial time algorithm that is optimal, we must put a constraint \non our model. Of course, we wish to have a constraint that is as weak as possible, but how to identify such a constraint is a highly nontrivial problem. This problem has never been discussed before in the literature, and we intend to study it in the near future.  \n\n(ii). While our  algorithm is not polynomial time, it does not mean it is easy to construct. In fact, even if we allow algorithms that are not polynomial time, no existing method is optimal and it is especially challenging to construct one when $m \\geq 4$ ($m$ is the order of tensor):  to match with the lower bound, we have to develop a new algorithm to estimate $\\theta_1, \\theta_2, \\ldots, \\theta_n$. \n\n(iii) At the same time, in Section 3.1, we have proposed a greedy version of our algorithm, which approximates our \ntesting statistics. The algorithm is fast, so we do have some contribution on this front.  \n", " \nThank you for your comments and your recognition of our theoretical contributions. Below is a point-to-point response (please also see our response to all reviewers above, where we explain the motivation and practical side of our paper).  \n\n1.Appropriateness for NeurIPS. \n\nWe have clarified this point in the response to all reviewers (see above). \n\n\n2.Where the model arises and why the results are relevant to practice.\n\nIn social networks, it has been recognized that the stochastic block model is often too idealized for real networks, and the degree-corrected stochastic block model (DCBM) (Karrer and Newman, 2010) is the state of art. Our tensor-DCMM model is a natural extension of DCBM from graph networks to hypergraph networks. It was observed in [15] that this model fits real data very well and is more suitable than tensor-SBM. In short, our model follows the spirit of the famous DCBM for graph networks. Its practical relevance has been well justified (see the references in [15]). \n\n\nWe now give an example for relevance of our problem in practice. Suppose we are interested in knowing whether a group of NeurIPS authors have latent clusters. We can build a coauthorship hypergraph on these authors using their NeurIPS papers in recent years and test whether $K=1$. Suppose a data analyst suggests to compute the node degrees $d_1,d_2,\\ldots,d_n$ and look at the chi-square statistic $\\sum_{i=1}^n (d_i-\\bar{d})^2$. Our theory immediately tells that this is a wrong approach, because any degree-based test will not have power. \n\n\n3.Sample application and real data set\n\nNote that in our response to all reviewers above, we have introduced a data set on the publication data of statisticians, and two application examples. For example, in our ongoing work, we can apply this hypergraph testing to measure the collaboration diversity of statisticians. We build a coauthorship hypergraph using publications in statistics journals. Then, for each author, we extract a personalized hypergraph (also called an ego-hypergraph) by restricting the node set to this author and his/her coauthors. We then apply the test on this personalized hypergraph to get a p-value. This p-value is used to measure the collaboration diversity of this author. We also refer to Gao and Lafferty (2017) for a similar application on coauthorship graphs. Also, see our response to all reviewers for how our problem of global  testing is related to hierarchical/recursive community detection (a problem of great interest in network and tensor analysis). \n", "This paper studies a binary hypothesis testing problem that considers whether a given hypergraph has one or more than one community. Under a general class of degree-corrected mixed-membership model for hypergraphs, it identifies conditions under which it is impossible for any test to distinguish between the null and alternative hypothesis. The above results, which are first derived for uniform hypergraphs, are then extended to non-uniform hyper-graphs by viewing a non-uniform hypergraph as a collection of uniform hypergraphs. The paper also proposes a statistical test that successfully solves the above-mentioned hypothesis testing problem under conditions that can get logarithmically close to the impossibility regime. Experiments on simulated data validate some of the theoretical findings.   1. This paper argues that it is the first to consider significantly more flexible models that support mixed-membership and degree-heterogeneity in hypergraphs. The key idea is degree matching, which allows construction of a difficult-to-distinguish alternative hypothesis. It would be better to present an intuitive explanation for the technical difficulties that arise due to the use of a richer model. What techniques have been borrowed from the existing literature and why are existing techniques not applicable in the current setting?\n\n2. Why is $\\pi_{i_1}$ still present while computing the expected degree of node i_1 under the alternative hypothesis? Under the alternative hypothesis, $\\pi_{i_1}$ is a random quantity.\n\n3. Degree matching aims at matching the expected degrees of nodes under the null and alternative model. Thus, it appears reasonable to argue that a degree-based test would fail in this case. However, to argue that \"any\" test would fail, the paper shows that $\\chi^2$-divergence between the null and alternative distribution goes to zero asymptotically. An intuitive explanation for how one goes from arguing that expected degree matches to arguing that $\\chi^2$-divergence is zero would be very helpful in making the paper more accessible.\n\n4. Sections 2.4 and 3.1 are difficult to understand. Consider providing additional details and more intuition.\n Yes", "This paper considers the community detection problem in the hypergraph setting: determine whether there is a single community (null hypothesis) or K > 1 communities (alternate hypothesis). The main contribution is the development of lower bounds via bounding the \\Chi^2-divergence, which is a standard method, although quite technically demanding in this setting due to the many parameters involved. Overall, this is a clear contribution to the theory literature on stochastic block models and community detection. It is a bit harder to judge its appropriateness for NeurIPS.  On the technical side, the main contribution of this paper is pushing through the challenging technical analysis for the \\Chi^2-divergence to demonstrate a lower bound that is tight up to logarithmic factors. I believe this is a valuable theoretical contribution, that extends the community detection work from the random graph to the random hypergraph setting. The paper is a dense read, due to the need to capture all of the notation and technical details within the page limit. I did not check the proofs carefully, but the results seems correct.\n\nIt does not devote much space or effort to justifying where this form of hypergraph model might arise and where the obtained theoretical understanding might lead to some useful algorithms. This would have significantly strengthened the paper with respect to NeurIPS, and might give the broader audience an appreciation for why such these results might be needed as part of a larger body of work. As the paper currently stands, it is an interesting contribution to a seemingly narrow set of technical questions arising from the stochastic block model community, and might be a better fit in a venue that is more targeted towards that community. To be clear, I do not see a clear reason to reject this paper from NeurIPS but I do not see a compelling reason to include it either. The paper would have been much stronger with a sample application and a real dataset, to justify the complexity of the proposed hypergraph models. I completely agree that this is a challenging technical problem, but I would prefer to see how and why this level of complexity might be needed some day in a data-driven setting. I did not have any concerns on this front.", "This paper introduces and studies a hypergraph community detection model called RMM-DCMM. In this model, each vertex may have a different degree (degree-corrected) and may also belong to many different communities (mixed-membership).\n\nA main takeaway from the paper is that possibility/impossibility of testing whether this complicated model has K = 1 or K > 1 communities boils down to the quantity ||\\theta||^2 ||\\theta||_1 \\mu_2^2. \n\nThe main results are:\n1) Region of impossibility result for RMM-DCBM (a subset of RMM-DCMM). This shows that given any:\n\t(a) degree-correction factors \\theta for a 1-community model\n\t(b) tensor P representing connection strengths between subsets of the K communities,\n\t(c) distribution F over mixed membership vectors, supported on {e_1,...,e_K},\n\tif ||\\theta||^2 ||\\theta||_1 \\mu_2^2 --> 0 (where \\mu_2 is the second eigenvalue of the matricization of P), then there is a choice of degree-correction factors \\theta^* such that the K-community model given  by (P, \\theta^*, F) is information-theoretically indistinguishable from the 1-community model given by \\theta\n\n2) Region of possibility result for RMM-DCBM. For any K-community model (P,\\theta,F) with K > 2, such that ||\\theta||^2 ||\\theta||_1 \\mu_2^2 / log(n)^{1.1} --> \\infty, there is a test (based on estimating the degree-correction vector \\theta assuming that the model is null) to distinguish it from any 1-community model.\n\nThese results extend to RMM-DCMM under some additional conditions.  The paper appears technically correct, and the connection to tensor scaling is especially nice. Furthermore, it is nice that the paper boils down the study of a complicated testing problem to whether a simple quantity ||\\theta||^2 ||\\theta||_1 \\mu_2^2 tends to 0 or to infinity. Finally, the RMM-DCMM model introduced by the paper is a natural community detection model.\nOverall, this paper is of interest to researchers specializing in community detection, so it seems on-topic for the conference.\n\nNevertheless, I am on the fence about acceptance, since I think that the result may only be of interest to a very small group of attendees.\nFrom a practical point of view, the testing algorithm to distinguish H0 (1 community) from H1 (more than one community), seems to depend heavily on the null distribution being Erdos-Renyi, and my best guess is that it will not be effective on real-world data (I could be convinced otherwise if the paper had experiments on real-world data). Furthermore, the testing algorithm is computationally inefficient, as it involves an exponentially-large sum. Because of this, the experiments implemented by the paper use an entirely different algorithm -- a standard spectral algorithm based on the matricization of the adjacency tensor.\n\nThus, the significance of the paper appears to be mainly theoretical. The main theoretical insight that I gleaned was that if the expected degrees of the K > 2-community model and the K = 1 community model match, then their information-theoretic distinguishability depends on the quantity ||\\theta||^2 ||\\theta||_1 \\mu_2^2, and whether it tends to 0 or to infinity sufficiently quickly.\n\n\nMinor comments\n\n* \"hypergraph\" in the title should be capitalized for consistency\n* typos: indicies, hypergraphse, \"can not\" --> \"cannot\", \"the simply quantity\"\n\n* Line 127, \\theta^{(m)}, P^{(m)} \\mu_2^{(m)} have not yet been defined. Please either define them or add a remark that they will be defined later and are analogues of the 3-hypergraph case.\n\n* Line 233, write log(n)^{1.1} instead of log(n)^{1+delta}\n\n* Line 286, \"[M; b1, · · · , bK]\" should be \"[M; b1, · · · , bm]\"\n\n* Lines 320, 321: (3.16) and (3.15) are flipped Yes.", "The current paper explore information-theoretic results for testing the existence of community underlying an observed hypergraph. The nodes of hypergraph are degree hetergeneous extending homogeneous results in literature. Both uniform and non-uniform hypergraphs are considered.\n\n  The results are new, original, solving a basic problem in graphical model. The results are important, and proving deeper theoretical insights on the nature of the testing problem. There is a lack of comparison with existing results such as Yuan et al. (AoS, 2021) who explored testing limits under homogeneous setting, i.e., $\\theta_1=\\cdots=\\theta_n$. Can the authors compare their results? For instance, setting all $\\theta_i$'s to be equal, can they recover the results of   Yuan et al. (AoS, 2021) ?\n\nAlso, I am a bit confused by the definition of degree-corrected model which seems a bit different from literature. Can the authors explain why they consider such a different version?"], "review_score_variance": 0.25, "summary": "The paper's main contribution is to provide a crisp theoretical characterization of the feasibility of detecting multiplicity of underlying communities in a degree-corrected mixed membership hypergraph model. \n\nIt has been recognized by all reviews that this is a significant achievement that will be of interest to researchers working on stochastic block models and related inference questions. The reviewers did not give very high marks on the basis that the topic may be of interest to only a limited subset of the NeurIPS community, also pointing to the fact that the paper could be made more attractive if a compelling application was brought forward. \n\nThe authors' reply to these comments is an argument that the problem they tackle is important in network science, an area which is relevant for NeurIPS, and a description of concrete applications to hypergraphs of co-authorships in scientific articles. The authors further point to the usefulness of their result to hierarchical clustering, and explain in greater detail how their non-polynomial time test informs the design of the polynomial test used in the experiments. \n\nI believe that these answers by the authors alleviate the concerns expressed by the reviewers, and suffice to justify acceptance of the paper to the conference. \n\n\n\n\n\n", "paper_id": "nips_2021_bYIddUC7AYO", "label": "train", "paper_acceptance": "accept"}
{"source_documents": ["Machine learning models based on Deep Neural Networks (DNNs) are increasingly being deployed in a wide range of applications ranging from self-driving cars to Covid-19 diagnostics. The computational power necessary to learn a DNN is non-trivial. So, as a result, cloud environments with dedicated hardware support emerged as important infrastructure. However, outsourcing computation to the cloud raises security, privacy, and integrity challenges. To address these challenges, previous works tried to leverage homomorphic encryption, secure multi-party computation, and trusted execution environments (TEE). Yet, none of these approaches can scale up to support realistic DNN model training workloads with deep architectures and millions of training examples without sustaining a significant performance hit. In this work, we focus on the setting where the integrity of the outsourced Deep Learning (DL) model training is ensured by TEE. We choose the TEE based approach because it has been shown to be more efficient compared to the pure cryptographic solutions, and the availability of TEEs on cloud environments. To mitigate the loss in performance, we combine random verification of selected computation steps with careful adjustments of DNN  used for training. Our experimental results show that the proposed approach may achieve 2X to 20X performance improvement compared to the pure TEE based solution while guaranteeing the integrity of the computation with high probability (e.g., 0.999) against the state-of-the-art DNN backdoor attacks.", "It seems that the paper is focusing on the privacy preserving training of deep neural networks by developing a Learning-as-a-Service framework. It assumes that all resources may be penetrated by adversaries except the TEE. In this paper, the authors leverage random verification to detect the attacks and shows how gradient clipping can defend against attacks.\n\nAs an avid user of cloud training, I can agree upon the motivation of the paper that the integrity of training models in the cloud. However, I want to write about some issues that I have about the paper.\n\nFirst, it seems that the main contribution of the paper is two folds (1) using randomized verification over full verification and (2) verifying gradient clipping to defend against the attacks. However, mere randomized runs of verifications speak for a rather humble contribution. It would be interesting to see whether there can be other more intelligent methods that can identify them. It would add to the novelty and merit if there were some evaluations too for this.\n\nFurthermore, to verify the real effectiveness of the gradient clipping as the defense mechanism, it would be nice to see some theoretical analysis on this front.\n\nFurthermore, I have some issues with the evaluation. First, the paper compares to a full verification in Figure 2. However, this information is given without any breakdown of \"actual training\" vs. \"verification\". This makes it hard for the readers to see exactly where the speedups come from and where.\n\nThe paper does not seem to demonstrate any evaluation n terms of the potential accuracy loss for large datasets. While F.3 seems to demonstrate this on CIFAR10, this cannot fully verify that such gradient clipping wont hurt the accuracy of the models.\n\nI am open to reevaluating the paper given the authors' feedback.\n\n============================================\n\nI thank the authors' for their response.\nI am satisfied with the answer regarding gradient clipping. As such, I raised my score to 5.\n\nHowever, I still cannot get away from the thought that random verification seems to speak for a rather humble a contribution. Combined with the lack of breakdown of \"actual training\" vs. \"verification\", I am rather hesitant to give a score higher than this. Regarding the experimentation on ImageNets, I understand that there was lack of time to add more experiments on this front. However, it would provide a more concrete message if the paper includes these additional experiments.", "Paper Summary:\n\nThe paper presents new techniques to enable secure neural training in the TEE+GPU paradigm. This is a natural extension of previous work like Slalom which only handles the inference case. The authors propose a two-step approach. First, they clip the gradients during training to force the attacker to insert multiple deviations to influence the model. They then randomly verify the integrity a subset of the gradient updates to check for tampering. Combining these two ideas the authors demonstrate a 2-20x improvement over a TEE only benchmark.\n\nScore Rationale:\n\n- The paper presents two novel ideas\n  - The use of probabilistic checks on a random subset of the gradient update steps\n  - The use of gradient clipping to force the attacker to tamper with multiple update steps to affect the final model\n- While the core idea of verifying the linear layers is a direct extension of previous work, the probabilistic checks offer a significant improvement in performance\n\nStrengths:\n\n- The paper presents a detailed empirical evaluation of the required verification rate as function of multiple hyper-parameters such as the style of the injected error, aggressiveness, and timing of the poisoning\n- The paper demonstrates that gradient clipping has minimal impact on the network accuracy if a suitable learning rate is chosen\n\nWeaknesses:\n\n- The core contribution of this work relies on the effectiveness of clipping procedure in forcing the attacker to introduce a large number of faults for a successful attack. However, this conclusion is not directly supported by empirical evidence, i.e. there is no ablation study of the verification rate required in the absence of clipping.\n\nMinor Comments/Questions:\n\n- There is a typo in the caption of Fig. 6 which should specify the CIFAR-10 dataset rather than ImageNet\n", "I have questions about the model. It is explained that the attacker is full control of CPU and GPU etc. but it is also assumed the attacker runs the code on GPU as expected. I would prefer a complete black box model, where the system can input model, batches, parameters etc. to the GPU and get whatever it wants back, i.e. gradients, activations, whatever you desire and given an input the attacker can decide to return whatever you want.\n\nResponse: \nModel is initialized within the SGX. Every time an SGD step is performed, the gradients are returned to the SGX for clipping, and updates are applied within the SGX (TEE we used in our experiments). Model’s hash is always authenticated whenever an update happens, and a new authentication code is generated for the new model weights. The only way the attacker can influence the model is through sending wrong updates. To prevent that we randomly check (Poll) the SGD step within the SGX. For that, we only need the input batch, parameters values (from last SGD update) and current reported gradients to see if the reported values match the computed ones inside the SGX. However, it is theoretically possible to check the computation at a finer grain such as a specific activation neuron, parameters within a kernel in some layers, etc. In fact, it might give a better theoretical bound since the more computational steps (dividing the SGD steps into smaller sub-steps) for a fixed number of malicious steps might mean the less verification! However, our proposed approach allows a much easier implementation and analysis. \n\nIt seems the attacker is seemingly completely oblivious to the actual network, and parameters being used, and only measures his current success rate, and seems very naive\n\nResponse: \n\nPlease note that our defense mechanism is strategy proof since we randomly check we do not care how and when the attacker attacks, we only care that such an attack step occurs not so rarely.  In other words, the simple random verification strategy advocated by our work does not need to run experiments with sophisticated attackers, hence our defense mechanism will not be impacted at all by the different attacks that require a similar number of modification/attack steps. In our theoretical analysis, we show the detection rate based on the number of attack steps and the verification rate.  \n\nWe would like to emphasize that we are the first ones to evaluate the simple random verification approach combined with gradient clipping to substantially improve the training time while preserving integrity with high probability, and provide an important baseline against any type of attack and future work.\n\nFinally, the time is measure compared to a pure based TEE solution which is stated in the paper as completely unsatisfactory, and should instead be compared with training time without using any form of verification as this is the target.\n\nResponse: \n\nWithout any verification, the GPU based training would be 50-200X times faster than pure TEE based solution. Please note that our goal is to improve the state of art in integrity preserving training in cloud settings. Hence, we compare our solution to a pure TEE based solution that provides integrity.\n", "However, mere randomized runs of verifications speak for a rather humble contribution. It would be interesting to see whether there can be other more intelligent methods that can identify them. It would add to the novelty and merit if there were some evaluations too for this.\n\nResponse: \n\nIn our experiments, we logged the norm of  gradients (before clipping) and how it changes when the attack is starting. If we use these signals to devise a defense, the attacker can also adapt its attack to hide these signals. Therefore, we believe that random verification is robust to any type of attack. Our only requirement, as we showed using probabilistic analysis, is that the attacker needs to attack at multiple steps. Hence, simple random verification advocated by our work is strategy proof against different attacks.  Therefore, this simple strategy is more robust as long as the attack requires multiple steps. \n\n\nFurthermore, to verify the real effectiveness of the gradient clipping as the defense mechanism, it would be nice to see some theoretical analysis on this front.\n\nResponse: \n\nAs we discuss in our response to reviewer 1, no clipping clearly makes the attacker’s job easier. It can even match the desired model parameters that allow successful attack in a single step.  With new experiments added, we show empirically that low clipping rates would require more steps. Please see the updated experimental section.\n\nIntuitive reasoning for low clipping rate is that imagine the attacker wants some model parameters to be $\\theta’$ for successful attack. Given the current model parameters $\\theta$, it can only move towards $\\theta’$ bounded by the clipping value and learning rate. Hence, at each step, it can move the model parameters less with a lower clipping rate.  \n\nThe paper does not seem to demonstrate any evaluation in terms of the potential accuracy loss for large datasets. While F.3 seems to demonstrate this on CIFAR10, this cannot fully verify that such gradient clipping won't hurt the accuracy of the models.\n\nResponse:\n\nWe agree that the accuracy may be lower in some certain cases. In our experiments, we used a fixed learning rate schedule and on CIFAR and other datasets we tried we did not observe any loss.  Our framework suggests possible solutions to this issue. For example, higher clipping rates could be combined with higher verification inside TEE steps.\n", "The authors illustrated how to do probabilistic verification: randomly decide to verify the computation over each batch. But how is a batch verified inside the TEE? It is not clearly stated in the paper\nResponse:\nSame algorithms (layers, forward and backward steps) that run on the GPU are repeated within the enclave. Please note that SGX has the seed to derive the input batch, plus the randomness for layers needed. If the computed gradients in the verification round do not match the reported ones, it identifies a misstep. We also have updated the figure caption and main text to reflect this point.\n\nLarge dataset experimentation will be amended\n\nResponse:\n\nWe added experiments with ImageNet as well. Also, please note that irrespective of the dataset, the random verification strategy improves the overall run time since only some subset of the steps would be verified inside the slow TEE.\n \nThe random verification strategy is based on the observation that it is unnecessary to verify all the computation steps. Where does the observation come from? More details, data, or citations will help.\n\nResponse:\n\nThis is the observation of our work. Basically, gradient clipping is important to prevent the attacker from attacking in a few steps. For example, if the target model parameter is $\\theta’$ and the regular training learns model parameters $\\theta$, then it is clear that without clipping the attacker can easily move $\\theta$ to $\\theta’$ in one step. Hence, clipping is needed.  Please see the updated experiment section for more experiments on the impact of clipping.\n\nAs far as I understand, the use of gradient clipping is intended to reduce the bias introduced by the adversary (which might be missed by random verification ) so that the adversary tends to launch multiple attacks? It would be better to explain that clearly in the paper.\n\nResponse: \n\nWe mention this aspect in the introduction.\n\nIt would be good to see some results about Verification Rate v.s. clipping rate.\n\nResponse: \n\nWe chose very small values for clipping rate for our analysis. Initial experiments with larger clipping rates made the attack easier, hence it required much higher verification rates. So, we only focused on training and experimenting tight clipping intervals for the majority of experiments. However, for the case of ImageNet, we chose two values: high( 2.0) and low (0.0005) which are significantly different in magnitude and investigate their potential impact under different attack scenarios. Our results reported in the update experimental section indicate that low clipping rate requires more attack steps by the attacker.\n", "Gradient clipping is important to prevent the attacker from attacking in a few steps. For example, if the target model parameter is $\\theta’$ and the regular training learns model parameters $\\theta$, then it is clear that without clipping, the attacker can easily move $\\theta$ to $\\theta’$ in one step. Given the plethora of trojan attacks, it would be difficult to come up with a mechanism to detect faulty gradient updates in case the attacker decides to send huge updates and modify the model in O(1) attempts.\n\nTo show the importance of gradient clipping, we conducted various experiments on ImageNet. We show the impact of gradient clipping on the case where there is no attack and where there is attack. Please see the updated experimental section for the ablation study. The results indicate that clipping is important for convergence in a no-attack setting, and requires the attacker to attack in more steps while minimal impact on accuracy.\n\nWe fixed all the typos pointed out.\n", "The paper discuss how to detect erroneous steps in gradient descent on a non-trusted GPU using a separate slower trusted execution environment,\nby randomly deciding in each step whether to check the values returned by the GPU, as well as using small learning rates and clipping the gradients to ensure all updates are small.\n\nThe reason for checking this is based on the assumption that since GPU calculations are out sourced there may be trust issues and attackers with control of the\nvalues returned by the GPU can alter the final network in subtle ways. \n\nThe paper includes experiments and shows that this approach is faster than just running everything on the trusted execution environment.\nThe experiments test an attack approach where the attacker tries to inject some bad samples to get some success of making the network being trained output some particular class on a particular image kind.\n\n\nI have questions about the model. It is explained that the attacker is full control of CPU and GPU etc. but it is also assumed the attacker runs the code on GPU as expected. I would prefer a complete black box model, where the system can input model, batches, parameters etc. to the GPU and get whatever it wants back, i.e. gradients, activations, whatever you desire and given an input the attacker can decide to return whatever you want.\n\nIt seems the attacker is seemingly completely oblivious to the actual network, and parameters being used, and only measures his current success rate, and seems very naive. \n\nFinally, the time is measure compared to a pure based TEE solution which is stated in the paper as completely unsatisfactory, and should instead be compared with training time without using any form of verification as this is the target.\n\nIn my opinion this paper is an implementation of a straight forward idea and the theorems for setting the probability parameters  are basic probably computations.\nThere is no theoretical contribution in the paper and all the arguments are heuristically based on some intuition about training dynamics.\n\nIn short, in my opinion the paper is simply not relevant or strong enough to warrant acceptance at ICLR.\n\n", "The paper targets security challenges of deep neural networks. While solutions can hardly scale up to support realistic DNN model training workloads, the authors propose GINN to support integrity-preserving DL training by random verification of stochastic gradient steps inside trusted execution environments (TEE). GINN combines random verification and gradient clipping to achieve good performance. The experimental results show that GINN achieves 2x-20x performance improvement compared with the pure TEE based solution while guaranteeing the integrity with high probability.\n\n#######################\nPros\n1.\tThe security of deep neural networks is a very interesting and challenging topic.\n2.\tThe paper is well structured, and the assumptions are clear.\n3.\tThe authors provide extensive evaluation results to demonstrate their work. \n\n#######################\nCons\n\n1.\tThe authors illustrated how to do probabilistic verification: randomly decide to verify the computation over each batch. But how is a batch verified inside the TEE? It is not clearly stated in the paper. \n2.\tWhen discussing the limitation of existing solutions, the authors claim that they are evaluated with small datasets (MNIST, CIFAR10). But there is not much evaluation on large datasets for GINN either. Maybe adding an experiment on a large dataset in Figure 3 will be helpful? \n3.\tThe random verification strategy is based on the observation that it is unnecessary to verify all of the computation steps. Where does the observation come from? More details, data or citations will help.\n\n#######################\nMinor comment\n1.\tAs far as I understand, the use of gradient clipping is intended to reduce the bias introduced by the adversary (which might be missed by random verification ) so that the adversary tends to launch multiple attacks? It would be better to explain that clearly in the paper.\n2.\tIt would be good to see some results about Verification Rate v.s. clipping rate.\n\n"], "review_score_variance": 2.1875, "summary": "While all reviewers agree the problem of TEEs for model training is well motivated, the reviewers remain divided on whether the concept of randomly selecting computations to verify has sufficient novelty, and whether the proposed gradient clipping method is well motivated.\n", "paper_id": "iclr_2021_tkra4vFiFq", "label": "train", "paper_acceptance": "withdrawn-rejected-submissions"}
{"source_documents": ["An object can be described as the combination of primary visual attributes. Disentangling such underlying primitives is the long objective of representation learning. It is observed that categories have the natural multi-granularity or hierarchical characteristics, i.e. any two objects can share some common primitives in a particular category granularity while they may possess their unique ones in another granularity. However, previous works usually operate in a flat manner (i.e. in a particular granularity) to disentangle the representations of objects. Though they may obtain the primitives to constitute objects as the categories in that granularity, their results are obviously not efficient and complete. In this paper, we propose the hierarchical disentangle network (HDN) to exploit the rich hierarchical characteristics among categories to divide the disentangling process in a coarse-to-fine manner, such that each level only focuses on learning the specific representations in its granularity and finally the common and unique representations in all granularities jointly constitute the raw object. Specifically, HDN is designed based on an encoder-decoder architecture. To simultaneously ensure the disentanglement and interpretability of the encoded representations, a novel hierarchical generative adversarial network (GAN) is elaborately designed. Quantitative and qualitative evaluations on four object datasets validate the effectiveness of our method.", "\n6). About AdaIN. \nAdaIN is proposed for style transfer task and be introduced to the image translation recently. It can change the input image in different degrees. E.g. changing the facial attributes as we did, or translating a dog to a cat as the work of Huang et al did in ECCV 2018 (MUNIT). Therefore, we think AdaIN is not the limitation of our method. \n\n7). About noise in cGANs\nGenerally speaking, the noise in conditional GANs plays the role of generating background or other not encoded information by the conditions. As for our method, we have disentangled an object into two main parts, the commonality part F_1 and unique ones R_l (1<l<L). R_l controls the variations of generated images, which play the role of conditions in cGANs. F_1 involves the information of discriminability irrelevant information, which has already encoded the background information of the image, i.e. the role of noise is played by F_1 actually. \n\nRef.\nTaihong Xiao, Jiapeng Hong, and Jinwen Ma. ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes. In ECCV 2018\nWonjoon Goo, Juyong Kim, Gunhee Kim, and Sung Ju Hwang. Taxonomy-regularized semantic deep convolutional neural networks. In ECCV 2016\nXun Huang, Ming-Yu Liu, Serge J. Belongie, and Jan Kautz. Multimodal unsupervised image-to-image translation. In ECCV 2018.\n", "Thank you for your constructive comments about our method and experiments. We have considered your comments carefully and respond to them in the following.\n1). Comparison with disentangled methods [2], [3] in terms of the metric in [1]. \nWe follow your advice and carefully read [2] and [3]. However, [2] is designed for time-sequential data such as video and speech. Besides, neither [2] or [3] has released their codes or models. Authors of [2] claimed that they do not have the rights to release their work which belongs to Disney. We emailed the authors of [3] but have not received a response. To make a comparison with a flat disentanglement work as much as possible, we searched recent works carefully and found an ECCV 2018 disentangling work called ELEGANT which is proposed by Taihong Xiao et al. We reproduced their work on CelebA for multiple attributes disentanglement. Results are shown in Sec.A.6 of the revision. \nAs for the 6 disentangling metrics described in [1], we find that they were used for evaluating unsupervised disentanglement methods on several toy-like datasets, esp. for VAE-based ones. Our method is designed on the cGAN framework and trained on data with complex objects. We read their released codes and computed the Separated Attribute Predictability (SAP) score which is the average difference of the prediction error of the two most predictive latent dimensions for each factor. Specifically, we used disentangled representations and attribute labels of the whole training set (about 120K images) and test set (about 13K images) for computing. The SAP result is 0.0604, which we find is relative satisfactory compared with methods reported in [1] on different datasets (Fig.13 in [1]). We’d like to measure SAP for ELEGANT. However, the latent feature of it is too high-dimensional (8192D, ours is 24D), which is not feasible for large scale metrics as done in [1]. Therefore, we compared ELEGANT in terms of the quality of generated images controlled by disentangled features in Sec.A.6.\n\n2). Needs for supervision … not applicable for existing data.\nIt is noted that the focus of this paper is to interpret the hierarchical structure within data and expect to extract meaningful and robust representations for discriminative tasks. Therefore, we heuristically construct hierarchical structures based on human priors to verify our method. One can also automatically obtain reasonable hierarchical annotations using machine learning technologies such as unsupervised clustering as Wonjoon Goo et al. ECCV2016 suggests.\n\n3). Ablation of some choices of the method.\nWe have made the ablation studies about the usage of local brother categories, image/feature reconstruction loss. Results can be found in Sec.A. 5. Results reveal that usage of local brother influence the disentanglement significantly, as we expected and introduced in Sec.3.2. As for the reconstruction losses, they would make the training more stable and thus generate images of better quality. \n\n4). Tab.1 metric for disentanglement, which may fool classifiers.\nWe agree with you that purely relying on the classification metric would not reflect the whole of features, as this is a many to one problem. It can not find the adversarial samples which fool the classifier. However, please note that we also provided other experimental results, such as 2D visualization, image generation conditioned on these features, to further validate the disentanglement. Besides, we further measure the image quality in terms of IS, FID and LPIPS to avoid adversarial samples, i.e. if features fool the classifier, they may appear as artifacts in generated images and thus decrease the quality of images. We think these comprehensive measurements together could validate the disentanglement. \n\n5). Failure case analysis, e.g. ImageNet. \nIn the revision, we provide the results in terms of classification metric, 2D visualization and conditional generation on ImageNet animal data we preprocessed, which can be found in the last Sec. of Appendix. We think the incomplete disentanglement on this dataset mainly due to two aspects. For one thing, there exists too much information that can be leveraged for classification, classifiers can easily find “shortcuts” and extract only partial primitives constituting the objects in that level. For another, the poor image quality is partially owing to the capacity of GAN. Generating high-quality images on ImageNet is a well-known tough problem that has not been well addressed by GANs until now due to the much complex data distribution. In our framework, in order to disentangle semantics, it is required to synthesize some nonexistent categories combined by semantics from different categories, which further makes distribution fitting harder for the discriminator. We believe that HDN could be improved on ImageNet dataset by disentangling a large scale of categories organized in a hierarchical structure, given a powerful enough generative framework.\n\n", "We thank you for your great efforts to review our work. In the following, we respond to your main concerns. \n1). About the term “granularity” in the abstract and Sec.1\nIn our opinion, the meaning of granularity is not completely equal to the concept of hierarchy. Granularity means the division degree of objects, while the hierarchy tends to express the structure of relationship. In other words, granularity corresponds to the concept of “level” in the hierarchy. Sometimes, we may be confused about the difference between them. To make it more consistent, we have modified the usage of granularity to the hierarchy level. We wish the modifications could fulfill your concerns. \n\n2). The clarify of R_l in Sec.3.2. \nWe feel sorry for the unclear formulation of R_l in Sec.3.2, and more accurate introduction about that has been made in the revision. \n\n3). The transition from Sec.4.1 to Fig.3. \nWe have added a necessary explanation in the main paper for the results of Fig.3 at the begining of Sec.4.1 of our revision.\n\n4). Your suggestions to polish the writing of the paper. \nWe are sorry for these inaccurate expressions which have influenced your reading experience and thank you for your careful reading and good suggestions. We have carefully modified corresponding writings one by one in our revision. \n", "Thank you for your constructive comments about our method and experiments. We have considered your comments carefully and respond to them in the following.\n1). Compare with GANs: IS, FID, SSIM.\nSince our method belongs to the family of conditional GAN, esp. for image-to-image translation methods, the image quality is better to general GANs (noise to image). For a fair comparison, apart from StyleGAN, we compared our method with one popular translation method StarGAN (Yunjey Choi et al. CVPR 2018). Besides, recently many GANs find that the Learned Perceptual Image Patch Similarity (LPIPS) (Zhang et al., CVPR 2018) is more consistent with human’s perception than SSIM. Therefore, we use IS, FID and LPIPS for evaluations. \nResults are shown in Tab. 6 (StyleGAN is time cost, which is on-training and the best results will be added in the final revision). We can see that our method achieves comparable and even slightly better semantics compared with the state-of-the-art image-to-image translation method, which demonstrates that HDN can not only extract primitives of objects for discriminative tasks but also be applied to such graphical applications\n\n2). About the interpolation in Fig.5. \nWe are sorry for the details loss of this result in the main paper. We adopt the linear interpolation (with 5 equally spaced interpolation coefficients range from 0.1 to 0.9) of disentangled features between the source (first columns in each case) and target (last columns of each case). The last columns represent the target images, but not generated results. For clarity, such introductions have been added to our revision. \n\n3). Image retrieval: use the latent of a pre-trained GAN as the baseline. \nWe added the retrieval performance of the pre-trained StarGAN (supervised) and StyleGAN (unsupervised) in Tab.2. The obvious advantages of our method in this experiment over them can be found, mainly owing to the discriminative disentangling mechanism we proposed. \n\n4). Relationship with the work of Kaneko et al. 2018\nThanks for notifying us of this related work. The work of Kaneko et al. 2018 did a good job to generate an image in a coarse-to-fine manner which is controlled by the disentangled hierarchical representations. Their experiments also validate that their method can synthesize images with higher quality and controlled the image appearances to be more and more specific. \nOur work has similarities with the work of Kaneko et al. 2018 on motivations. Both aim to disentangle the variations within data in a hierarchical manner. \nNevertheless,  the work of Kaneko et al. 2018 is indeed different from ours. Specifically, the detailed goals of leveraging hierarchical relationships are different. The work of Kaneko et al. 2018 aims to maximize the mutual information between conditioned representation and data in each level, i.e. study how the appearance of data varies with more and more narrow conditions and thus synthesize data with more fine-grained details. Our method focuses more on how humans distinguish objects from categories in different hierarchical levels and wish such manner of understanding objects can be applied to the machine, i.e. learn the commonality and individuality of categories in nature. Therefore, the disentangled features of our method are mainly served for downstream discriminative tasks such as semantic retrieval, open-world unseen category recognition as we have attempted in the experiments. Besides, thanks to the disentangled commonality, our method can further realize the semantic translations between images by exchanging the individualities, which has been a popular application in the real world. \nWe have cited this work and discussed the difference in Sec.3.3 in our revision. \n\nRef.\nYunjey Choi, Min-Je Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. In CVPR 2018.\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR 2018.\n\n", "We thank you for your valuable comments and suggestions. We have followed your suggestions and conducted experiments which would make our work more interesting.\n1). Table.3 evaluated on multiple unseen categories.\nDuring rebuttal, we tried our best to collect and preprocess more unseen categories in each dataset. Specifically, we observed all 40 attributes on CelebA and finally used the bald and gray hair which are unseen leaf-level hair colors. We further investigated the 205 categories of ShapeNet and preprocessed 3D models for kinds of new tables and sofas which are regarded as the unseen leaf-level categories for ShapeNet-C. Finally, objects with other poses for ShapeNet-P are also considered in this experiment. Typical examples of these unseen categories are shown in Fig.11 in the Appendix. \nThe new test results are updated in Tab.3 in the revision. We find that the performance of super-categories decreased to some extent due to more categories added. Overall, our method still has considerable generalization ability for unseen but similar objects.\n\n2). Cross dataset evaluations.\nThanks for this interesting and valuable comments. We preprocessed a facial expressions dataset called RAF (Li et al., CVPR 2017) and a fine-grained cars dataset called CompCars (Yang et al., CVPR2015). These two datasets are quite challenging compared we have used for training our model and have different leaf-level annotations. Similar to the evaluation of our method for unseen categories but within the same dataset, we also test the performance of hierarchical prediction and semantic translation. Detailed results can be found in Sec.A.4. \nIt is found that the performance would become relatively poor due to the large domain shift. Even so, we can still extract and transfer some meaningful information in high-level, e.g. the gender, smiling on Faces, the poses on Cars, which demonstrates the robustness and advantages of hierarchical feature learning compared to the flat ways. \n\n3). Compare with non-hierarchical retrieval models and obtain their high-level performance using the corresponding parent level categories.\nIn fact, the compared hashing methods in Tab.2 are all non-hierarchical retrieval models and we trained them in each level. Training them using the annotations in each level can make them perform best in that level, but a little inefficient. Following your advice, we only trained them in the leaf-level and used the learned features to test on high-level using corresponding parent level categories. Results are added to Tab.2 (methods with “-S” postfix). Besides, we also added pre-trained GANs as baselines for comparison in this experiment. We find that using only one model trained in leaf-level is efficient but the mAP drops. This also proves both efficient and effective of our HDN for disentangled features. \n\n4). Typos.\nWe are sorry for these inaccurate expressions which have influenced your reading experience and thank you for your careful reading and good suggestions. We have carefully modified corresponding writings one by one in our revision.\n\nRef.\nShan Li, Weihong Deng, and JunPing Du. Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild. In CVPR 2017\nLinjie Yang, Ping Luo, Chen Change Loy, and Xiaoou Tang. A large-scale car dataset for fine-grained categorization and verification. In CVPR 2015\n", "This paper proposes a method for solving the following problem: given an Image I from a labelled dataset with a label hierarchy as a tree of depth L, produce a set of vector representations {F_1, F_2 ... F_L}, such that a) the set can be used to reconstruct I as well as possible and b) each representation in the set only contains information about the label at the corresponding level in the label tree.\n\nWhile I am not extremely familiar with work in disentangled representation learning, the authors claim is true to my knowledge that most work on disentangling factors does not explicitly take into account hierarchical structure as this work does. Therefore, this work appears novel and interesting to me. I will leave the assessment of the degree of novelty to other reviewers/AC who may be more familiar with the literature.\n\nThe approach is also effective, and the authors demonstrate through visualizations and experiments that the proposed model can be trained and accomplishes its objectives reasonably well. My overall decision is to accept this paper, but there are some improvements I'd like to see since I found it difficult to understand in some places. \n\n- There is repeated use of the term \"granularity\" in the abstract and Sec. 1 which is undefined. What, according to the authors, is the difference between having a hierarchical structure and multi-granularity? I suggest clarifying what is meant by this, or avoid using the term (used hierarchy instead).\n\n- In Sec. 3.2, it appears that what is meant by R_l is actually the set {R_1, ..., R_L}. This would imply that the R's from different levels are randomly combined, and the number of representations combined is always L. Is this correct? In either case, what happens here should be made much clearer. It took me several readings to arrive at this interpretation.\n\n- It took me a while to infer how the results in Figure 3 are generated. There is a sudden switch in Sec. 4.1 from model training details to its use for semantic translation, which was not explained.\n\nMinor suggestions:\n\n- Please use parenthetical citations throughout the paper where appropriate (use \\citep{}) to avoid breaking the flow of reading.\n- Pg. 1, last line: \"us human\" -> \"humans\"\n- Pg. 2, line 1: \"with others\" -> \"to others\"\n- Pg. 2, line 2: \"hierarchy structure\" -> \"hierarchical structure\"\n- Pg. 2: \"multi-granularity nature\" -> \"multi-granular nature\" or \"hierarchical nature\"\n- Pg. 7, line 2: \"an significant\" -> \"a significant\"\n- Pg. 7, line 4: \"At the last\" -> \"At the end\"\n- Pg. 7, \"it can be reached\" -> \"it can be seen that\"\n- Pg. 7: \"Table.4.1 gives the evaluation results\" -? \"Table 1 gives ...\"\n- Pg. 7, Please revise: \"which is deserved to make more efforts\"\n- Pg. 7: \"to applied\" -> \"to be applied\"\n- Pg. 8, line 1: Remove \"quite\"; typo in \"leraning\"", "This paper studies the problem of learning disentangled representation in a hierarchical manner. It proposed a hierarchical disentangle network (HDN) which tackles the disentangling process in a coarse-to-fine manner. Specifically, common representations are captured at root level and unique representations are learned at lower hierarchical level. The HDN is trained in a generative adversarial network (GAN) manner, with additional hierarchical classification loss enforcing the disentanglement. Experiments are conducted on CelebA (attributes), Fashion-MNIST (category), and CAD Cars (category & pose).\n\nOverall, this paper’s contribution seems quite outdated and presentation is not very clear.\n\n(1) Learning hierarchical representation using GAN has been explored in Kaneko et al. 2018 but not even mentioned in the paper. \n\nGenerative Adversarial Image Synthesis with Decision Tree Latent Controller. Kaneko et al. In CVPR 2018.\n\nAs far as reviewer understands, the bottomline for publication at ICLR is to demonstrate significant improvement/technical novelty compared to prior art. \n\nThis paper should also compare against GANs or other state-of-the-art generative models with flat representation (especially on face generation) in terms of SSIM, inception score, and FID score. Without such comparisons, it is unclear what is the value of hierarchical representation proposed here.\n\n-- Glow: Generative Flow with Invertible 1x1 Convolutions. Kingma and Dhariwal. In NeurIPS 2018.\n-- Progressive Growing of GANs for Improved Quality, Stability and Variation. Karras et al. In ICLR 2018.\n-- A Style-based Generator Architecture for Generative Adversarial Networks. Karras et al. In CVPR 2019.\n\n(2) The interpolation results (see Figure 5) look a bit strange as the transition between last columns are not very smooth. Also, please provide details about this experiment: are you applying linear interpolation? What’s the interpolation parameter for each of the column?\n\n(3) For image retrieval experiment, it is not clear if the proposed method is better than any state-of-the-art generative models with flat representations. One strong baseline is to use the latent representation of a pre-trained GAN model as comparison.\n", "This paper proposed the hierarchical disentangle network (HDN) that leverages hierarchical characteristics of object categories to learn disentangled representation in multiple levels. Their coarse-to-fine manner approach allows each level to focus on learning specific representations in its granularity. This is achieved through supervised learning on each level where they train classifiers to distinguish each particular category from its ‘sibling’ categories which are close to each other. Experiments are conducted on four datasets to validate the method.  \n\nExploiting object hierarchy to learn disentangled representation is a promising direction but I lean towards rejecting this submission due to\n1. No results on commonly used disentanglement metrics (e.g. see [1])\n2. No comparison with existing supervised/unsupervised methods on disentangled representations (e.g. [2][3])\n3. The needs for full supervision on each level and manually designed fixed hierarchy require labels for the full hierarchy and make it not applicable to many existing data. This probably is why the proposed approach did not work well for more complex datasets like ImageNet.\n\n\nThese also should be addressed:\n1. The choice of adaptive instance normalization should be discussed. AdaIN could be used to account for small changes like color or local changes, but can it be used for larger and more global change (for example from animal category to human). If not, is it a limitation of this method?\n2. Justification for several choices made in the method, for example in the form of qualitative/quantitative ablation studies - usage of local ‘brother’ categories, image/feature reconstruction losses\n3. Can the metric in Table 1 prove disentanglement is achieved? What if E and G learned some way to fool the classifiers\n4. Authors use conditional generative adversarial networks but it seems that there is no noise.\n5. Discussion of failure cases. For example, the authors mentioned that the proposed approach did not work well for ImageNet. Why is this the case?\n\n\nMinor comments:\n- some citations are not properly formatted\n\n[1] Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations, Locatello et al.\n[2] Disentangled Sequential Autoencoder, Li and Mandt\n[3] Exploring Disentangled Feature Representation Beyond Face Identification, Liu et al.\n\n", "This paper proposes an algorithm for supervising networks for image classification and reconstruction with the object's hierarchical categories in mind. The claimed benefits are the improved generalizability and interpretability. The paper reports per-category-level analysis on the semantic image reconstruction task and retrieval on seen and unseen object categories.\n\nI am currently leaning towards weak accept because I find the paper's claim and technical details generally convincing and simultaneously extracting low-level and high-level features trained using the hierarchical levels of categories is novel. Generalization to unseen categories tends to be a good proxy for real world performance and directly learning the high level categories is a useful idea for doing so.\n\nAlthough I am leaning towards weak accept, I think this paper is close to borderline because the findings do not seem experimentally well validated. It would be more interesting to see Table 3 on multiple unseen categories instead of one special case per dataset. Another idea for experiments is doing cross-dataset evaluations where different datasets may have different leaf categories but shared high level ones. I think it may also be interesting to compare with a non-hierarchical retrieval model and then obtain their high-level prediction accuracy using the corresponding parent level categories.\n\nThe paper generally needs polishing. Minor typos I found:\n\nPage 5: classificaiton, classifers\nPage 6: intuitional->intuitive\nPage 7: an significant\nPage 8: leraning\nPage 1: human\nFigure 3: arbitary"], "review_score_variance": 9.5, "summary": "The authors propose a new method for learning hierarchically disentangled representations. One reviewer is positive, one is between weak accept and borderline and two reviewers recommend rejection, and keep their assessment after rebuttal and a discussion. The main criticism is the lack of disentanglement metrics and comparisons. After reading the paper and the discussion, the AC tends to agree with the negative reviewers. Authors are encouraged to strengthen their work and resubmit to a future venue.", "paper_id": "iclr_2020_rkg8xTEtvB", "label": "train", "paper_acceptance": "reject"}
