{"source_documents": ["With a mortality rate of 5.4 million lives worldwide every year and a healthcare cost of more than 16 billion dollars in the USA alone, sepsis is one of the leading causes of hospital mortality and an increasing concern in the ageing western world. Recently, medical and technological advances have helped re-define the illness criteria of this disease, which is otherwise poorly understood by the medical society. Together with the rise of widely accessible Electronic Health Records, the advances in data mining and complex nonlinear algorithms are a promising avenue for the early detection of sepsis. This work contributes to the research effort in the field of automated sepsis detection with an open-access labelling of the medical MIMIC-III data set. Moreover, we propose MGP-AttTCN: a joint multitask Gaussian Process and attention-based deep learning model to early predict the occurrence of sepsis in an interpretable manner. We show that our model outperforms the current state-of-the-art and present evidence that different labelling heuristics lead to discrepancies in task difficulty.", "I've read the rebuttal and I'd like to keep my score as is. My main concern is the questionable role of attention in making the model more interpretable (which is the main contribution of the paper).\n\n###########################\n\nThe paper proposes a new model for automated sepsis detection using multitask GP and attention-based GP. The sepsis detection problem is of paramount importance in the clinical domain and the authors rightly emphasized that point. Also the paper tries to combine interpretability with prediction accuracy by using attention mechanism. \n\nThe paper is generally well written and well motivated; however, in terms of technical novelty and empirical evidence the paper can be further improved. \n\nThe MGP-AttnTCN model is mostly a minor modification of the model proposed by Moor et al. 2019 and has the additional attention element to be more interpretable. Unfortunately, it’s not easy for an ICLR reader without any medical background to evaluate the validity of the interpretability results provided in the paper. Furthermore, the recent works in NLP have agued against the value of attention for interpretability (see for instance “Attention is not Explanation” by Jain & Wallace 2019). That said, I believe the paper is probably a better fit for a machine learning in healthcare venue (such as MLHC). \n\nIn terms of empirical evidence of the prediction accuracy the paper only compares with Moor et al 2019 (which does not show a significant improvement in the realistic setting) and a much older InSight paper (2016). This would have been typically enough for a paper with major technical novelty; however, for this paper, I believe adding more recent baselines and discussing the advantages of the method over these baselines would be necessary. \n\nMinor:\nCaption in Figure 1 can be more informative and useful for the reader if you add more details on different parts of the model. \n“Graphically, once can observe” should be “one can observe” . ", "Thank you for taking the time to read through our paper and share your thoughts. We are going to respond to your comments in the following.\n\nInterpretability:\nAn approach into the validation of interpretability of the results is given in Figure 5: the time-points closer to the sepsis onset are deemed more important by the model, which is intuitively where you would see a patient’s health worsening.\nAs per the relevance of the attention mechanism, Wiegreffe and Pinter argued against the point made by Jain and Wallace (“Attention is not not Explanation”). We believe a fully fleshed proof of the explainability of attention is beyond the scope of this paper.\n\nBaselines:\nRegarding comparison to baselines, to the best of our knowledge, the work of Moor et al. is the state of the art. Moreover, we decided to also compare our work to Insight, as this interpretable model is the most advanced one validated by the clinical community (as it is now undergoing clinical trials). Our improvement over Moor et al. is not only in the actual performance (at least on their labels) but mostly in the interpretability of the model.\nMinor:\nWe updated Figure 1 to make the caption more useful.\n\nIf you have any further suggestions on how to improve the paper, please let us know.\n", "Thank you for taking the time to read through our paper and share your thoughts. We are going to respond to your comments in the following.\n\nNew sepsis labels:\nRegarding the different labelling of the data, we will make the code available once the paper will be reviewed - as it is now on a (non anonymous) github repository. As such, the labelling is part of the contributions of the paper.\nThe decision to have zero contributions reflects that we assume a patient is healthy unless proven otherwise. This is in line with the official Sepsis-3 guidelines, which claim that “the baseline SOFA score can be assumed to be zero in patients not known to have preexisting organ dysfunction.” (https://jamanetwork.com/journals/jama/fullarticle/2492881, Box 3). Our assumption is that the authors of the Sepsis-3 guidelines are aware of possible clinical practices and have taken them and other clinical biases into consideration when fleshing out their recommendations. \nMoreover, as opposed to Moor et al who ignore cases that do not have complete data, our labels will also contain patients with fewer records and hence ‘noisier’ time series, but also a broader and more realistic use-case. On the other hand, only looking at well documented patients is not in line with the aim of the research stream: if a patient is already well attended, then doctors are already well aware of their health conditions and a diagnostic support tool would only bring marginal benefit.\n\nInterpolation in Figure 5:\nYour point on Figure 5 can be explained by the Multitask nature of the Gaussian Process: even if there is no input for that specific covariate, the model is able to infer its value from the other values it is able to record.\n\nMGP samples:\nRegarding the MC samples y_{MGP}, as written in the original paper, they are taken from the posterior over t’ defined by \\mu and \\Sigma in equation (6) in order to approximate its distribution. Could you be more specific about which part is unclear? \n\nDimensionality of the latents:\nz_j and z_j’ should be N x (M+Q), we amended the paper. Thank you for spotting that.\n\nIf you have any further suggestions on how to improve the paper, please let us know.\n", "Thank you for taking the time to read through our paper and share your thoughts. We are going to respond to your comments in the following.\n\nNumerical performance results:\nWe added the table with the numerical results in Appendix C.4.\n\nChoice of covariance times:\nIn response to the time covariances, we initially started with one covariance for all variables, which performed worse than the presented model. We also clustered the different features solely based on data sampling frequency, and found that two clusters were optimal: the clusters have low wss and are intuitive to the medical staff. It is especially given the latter point that we decided for two clusters instead of treating the number of covariances as a hyperparameter.\n\nInterpolation:\nAs per the interpolation of the signal, we would like to point out that even the most frequently sampled data is sampled every 15 minutes. As such, micro-movements in data would already be missing. However, in order to limit the amount of smoothing, we decided to use a kernel that has no moments greater than two. As such, these kernels are able to capture ‘jumpier’ behaviours.\n\nIf you have any further suggestions on how to improve the paper, please let us know.\n", "The authors present a Sepsis-3 compliant labeling of the MIMIC-III dataset and a sepsis-prediction model largely based on MGP-TCN that uses attention mechanisms to enable explainability.\n\nIt is not entirely clear what the authors mean by MC samples from Y_MGP, are these simply samples from the posterior in (6)?\n\nIf z_j and z'_j are M-dimensional, how does one apply (8) and (9) for W_{\\alpha,0}, W_{\\alpha,1} being (M+Q)-dimensional or W_{\\beta,0}, W_{\\beta,1} being matrices?\n\nThe labelling of the data, largely following Johnson & Pollard (2018) and Moor et al (2019), is only different to Moor et al (2019) in the assumption that in the SOFA calculation missing values have zero contribution. Unless the authors provide evidence that this is reasonable, it is not necessarily clear whether labels resulting from the proposed scheme will be biased and affected by differences in clinical practice at different sites or data collection practices. That being said, it is not clear whether the proposed labeling is a contribution from the work.\n\nThe fact that the proposed labels are harder to fit does not imply that the proposed labels are better or more reasonable. This provided that is difficult to know (without ground truth) whether the difficulty originates from a broader use case (not as easy as Moor et al (2019)) or labels being noisy, imperfect proxies for sepsis diagnosis. I understand the author's motivation for doing it, however, their approach is not sufficiently justified. I also agree that predicting sepsis in a realistic setting is harder than suggested in prior work, however, the proposed labeling does not necessarily yields evidence of that being the case.\n\nThe interpretation of the covariance matrices of the MGP is interesting, though not surprising considering that covariates in green are measured regularly while blue covariates are ordered sparingly.\n\nFigure 5 is interesting, though raises questions about of interpretability of the model. How should unobserved covariates be interpreted (INR in Figure 5)?\n\nIn summary, the contributions of the present work are not sufficiently justified (labeling), the novelty of the proposed model is minor, relative to MGP-TCN, and the added value of the attention mechanism as a means to interpret predictions in terms of the journey of a patient is not clear.\n\nMinor:\n- Figure 1 needs a better caption. Being in page 2 makes it very difficult to understand.\n- TCN is used before being defined.\n- In (1) it should be t_{p,i,k} not t_{p,k,i}", "The authors consider a combination of an Gaussian model and neural network learned together to be able to find specific Gaussian features that would predict sepsis in a more intuitive, interpretable way for a medical experts. Also, they have provided a new labelling for the MIMIC-III dataset, which is of great value.\n\nUsually constraining the feature space reduces the accuracy, as one tends to miss important features. However, here, one is using a Gaussian model to generate a feature space that is easier to train with a neural network (by filling the sparse data by Gaussian process interpolation) but also augment the dataset.\n\nThe author report that his improves prediction. Unfortunately, I did not find tables of solutions where one could the actual impact. The authors should include a numerical table of their result comparison results.  Now there is only a narrative in section 5.3 and an image showing that at different covariance times, different feature groups starts to interpret the results. Obviously, a question arises if the model would perform even better with a combination of covariance times, or is there some covariance time range that is missing that would improve the result even more.\n\nThe Gaussian model creates smooth interpolation of data spaces and also forces the training to look at corresponding smoothened features - that are very good for human eyes. However, there are situations (like the detecting heart beat from a video from a head moving with a recoil from the blood rushing to the brain) in where the signal is too weak for human to see, but is definitely there for a computer.  I would state that this as interpretable,  as an explicitly visible signal would be. Even  shorter time constant signal might be valuable as well, but it would not be visible here...  It seems that in sepsis, it was a good idea, as it improves the result compared to the situation of not using the Gaussian model. \n\nOne has to be careful to state that this would address the interpretability of the results. Gaussian process by itself is not giving understanding nor interpretability as it is too general. But it can make the provided solution \"teachable\" to a human expert by showing what visible features one can track.\n\nCompare this to a situation where one is using physical model to regularize detection. It has the analogous two model structure like the one in the manuscript. In https://xbpeng.github.io/projects/SFV/index.html the authors of the paper report that that one can achieve a better pose estimation by constraining the pose to only those that are achievable by a physical model based policy trained by reinforcement learning. This one is able to \"interpret\" the pose.\n\nAs a summary, the authors have done solid and valuable work in improving the accuracy detection. They should have a more formal way to present the results and baseline comparisions as tables. On the explainability and interpretability, there remains a lot of interpretations and one has lots of explaining to do, even after this manuscript.\n\n\n\n\n"], "review_score_variance": 8.666666666666666, "summary": "The problem of introducing interpretability into sepsis prediction frameworks is one that I find a very important contribution, and I personally like the ideas presented in this paper. However, there are two reviewers, who have experience at the boundary of ML and HC, who are flagging this paper as currently not focusing on the technical novelty, and explaining the HC application enough to be appreciated by the ICLR audience. As such my recommendation is to edit the exposition so that it more appropriate for a general ML audience, or to submit it to an ML for HC meeting. Great work, and I hope it finds the right audience/focus soon. ", "paper_id": "iclr_2020_rJgDb1SFwB", "label": "train", "paper_acceptance": "reject"}
{"source_documents": ["Deep Neural Networks (DNNs) have achieved high accuracy in various machine learning applications in recent years. As the recognition accuracy of deep learning applications increases, reducing the complexity of these neural networks and performing the DNN computation on embedded systems or mobile devices become an emerging and crucial challenge. Quantization has been presented to reduce the utilization of computational resources by compressing the input data and weights from floating-point numbers to integers with shorter bit-width. For practical power reduction, it is necessary to operate these DNNs with quantized parameters on appropriate hardware. Therefore, systolic arrays are adopted to be the major computation units for matrix multiplication in DNN accelerators. To obtain a better tradeoff between the precision/accuracy and power consumption, using parameters with various bit-widths among different layers within a DNN is an advanced quantization method. In this paper, we propose a novel decomposition strategy to construct a low-power decomposable multiplier-accumulator (MAC) for the energy efficiency of quantized DNNs. In the experiments, when 65% multiplication operations of VGG-16 are operated in shorter bit-width with at most 1% accuracy loss on the CIFAR-10 dataset, our decomposable MAC has 50% energy reduction compared with a non-decomposable MAC.", "This paper proposes to shorten the shift-addition operations in the straightforward configurable MACs (Sharma et al., 2018), to an addition-shift style. The authors claim that the new design is able to lower the energy consumption in the matrix multiplication. In the experimental analysis, the authors demonstrate the effectiveness of the proposed method.\n\nThis paper should be rejected since it proposes exactly the same architecture with the following published work:\nBitBlade: Area and Energy-Efficient Precision-Scalable Neural Network Accelerator with Bitwise Summation\n\nAlso, the authors do not provide a valid approach for the auto-selection of quantization bits, which is more significant in my opinion.", "This paper presents a decomposable MAC unit for low-power computation for matrix-multiplication operation for neural networks.\n\nAlthough I believe I understood the idea of this technique, I strongly believe this paper should be submitted to an architecture of design automation conference instead of ICLR. I am also not in the position to assess the experiments, which were conducted with synthesis of Synopsis 28nm library. The paper discussed systolic array, MAC in detail, without too many algorithmic elements inside, therefore may be a paper not toward the audience of this conference.", "Summary: \n\nThis paper proposes a novel decomposition strategy for matrix multiplication to aim for less energy consumption. They demonstrate that energy consumption is reduced on several CNNs. It is an interesting work but it is not sure that there is a difference in contribution when compared to previous work.\n\nThe main argument to impact to the score:\n\n    1. A similar idea is addressed in [1]. It is highly recommended to show the difference in contribution.\n    2. In Figure 7,  the performance and energy consumption of systolic arrays of decomposable MAC  is shown. However, the only energy consumption is shown in section 4.2 when the method is applied to DNNs. It is better to show both performance and energy consumption.\n    3. It is inappropriate to quantize VGG16, VGG19, DenseNet-121, DenseNet-169 and MobileNet based on CIFAR10 dataset since all models are designed for ImageNet dataset. It is appropriate to quantize these models on ImageNet dataset.\n\n\n[1]     Sungju Ryu, Hyungjun Kim, Wooseok Yi and Jae-Joon Kim. BitBlade: Area and Energy-Efficient Precision-Scalable Neural Network Accelerator with Bitwise Summation. DAC. 2019.\n\nMinor comments not to impact the score: \n    1. \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" has been accepted by ICLR 2014. It is better not to use arXiv preprint on citations unless there is a reason.\n    2. It is recommended to put citations on CIFAR-10 dataset.", "This paper proposes decomposable MAC and a systolic array-based accelerator for varible bitwidth of weights/activations in CNN. The core insight is that the partial results of a multiplication which have the same shift distance can be composed together before shifted so as to reduce area and energy consumption.\n\nHowever, exactly the same idea has been seen in previous work BitBlade [1], which was also an incremental work based on Bit Fusion [2]. The difference is that BitBlade adopted a 2D-mesh architecture instead of systolic array. \n\nI think the authors should cite BitBlade in the paper and reconsider the contributions. \n\nThanks.\n\n\n\n[1] Sungju Ryu, Hyungjun Kim, Wooseok Yi, Jae-Joon Kim. BitBlade: Area and Energy-Efficient Precision-Scalable Neural Network Accelerator with Bitwise Summation. DAC'2019\nhttps://dl.acm.org/citation.cfm?id=3317784\n\n[2] Hardik Sharma et al. Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Networks. ISCA'2018"], "review_score_variance": 0.8888888888888888, "summary": "This paper presents an energy-efficient architecture for quantized deep neural networks based on decomposable multiplication using MACs. Although the proposed approach is shown to be somehow effective, two reviewers pointed out that the very similar idea was already proposed in the previous work, BitBlade [1]. As the authors did not submit a rebuttal to defend this critical point, I’d like to recommend rejection. I recommend authors to discuss and clarify the difference from [1] in the future version of the paper. \n\n[1] Sungju Ryu, Hyungjun Kim, Wooseok Yi, Jae-Joon Kim. BitBlade: Area and Energy-Efficient Precision-Scalable Neural Network Accelerator with Bitwise Summation. DAC'2019\n", "paper_id": "iclr_2020_Hye-p0VFPB", "label": "test", "paper_acceptance": "reject"}
{"source_documents": ["Most research on lifelong learning applies to images or games, but not language.\n      We present LAMOL, a simple yet effective method for lifelong language learning (LLL) based on language modeling.\n      LAMOL replays pseudo-samples of previous tasks while requiring no extra memory or model capacity.\n      Specifically, LAMOL is a language model that simultaneously learns to solve the tasks and generate training samples.\n      When the model is trained for a new task, it generates pseudo-samples of previous tasks for training alongside data for the new task.\n      The results show that LAMOL prevents catastrophic forgetting without any sign of intransigence and can perform five very different language tasks sequentially with only one model. \n      Overall, LAMOL outperforms previous methods by a considerable margin and is only 2-3% worse than multitasking, which is usually considered the LLL upper bound.\n      The source code is available at https://github.com/jojotenya/LAMOL.", "\nSummary:\n\nThe paper proposes to use the same language model to learn multiple tasks and also to generate pseudo-samples for these tasks which could be used for rehearsal while learning new tasks. The authors demonstrate that this idea works well compared to other SOTA lifelong learning methods for learning various NLP tasks using a single model.\n\n\nMy comments:\n\n1. Please change the title! Language modeling is NOT all you need for lifelong language learning. Also, not every NLP task is a QA task. I do not want more papers to over-trivialize NLP by following Bryan McCann and Socher, 2018. I will not increase my scores until the title is changed.\n2. A relevant model architecture based method is Sodhani et al. 2018 (Towards Training Recurrent Neural Networks for Lifelong Learning) who use Net2Net to do zero-shot expansion of the model parameters.\n3. Section 3.2 - you mention that any pseudo-example which does not have only one ANS token is discarded. Can you comment on how much discarding is needed to generate the required number of pseudo-samples?\n4. Why is it that every task was trained only for 9 epochs?\n5. On page 5, you mention k=20. What is k? Where is this introduced?\n6. On page 5, you mention that MTL is used to determine whether forgetting is caused by a lack of model capacity. I am not sure if it is correct. Can you explain?\n7. Why not compare the approach with models like GEM? Keeping very few examples is ok. Even though you don’t beat GEM, it is good to see the comparison.\n8. Page 7: Is there any reason why you choose to go from large to small tasks? I feel like this is a favorable order. I would like to see how the model performs if you do the reverse order.\n9. Please remove the last line.\n10. I assume that the authors will release the code upon acceptance of the paper.\n\nMinor comments:\n\n1. Page 2, 4th contribution: check the spelling for “pseudo-samples”\n2. Page 2, 5th last line: “After a completing a task” - fix it.\n3. Table 1: I think the description is not correct. 1fEM is for wikiSQL, not WOZ. Also, it is better if you can describe these metrics in detail in the appendix.\n\n==================================\n\nAfter rebuttal:\n\nI am happy with the authors' response and name change. I am increasing my score.\n", "We thank the reviewer for the review, comments, and constructive feedback. We provide responses to the weakness of our paper below.\n\nQ1: One weakness of the paper is in the experimental results especially in section 5.4. The statistical significance of the results in table 5 is missing. As the authors have discovered, the performance is highly dependent on implementation. In addition, the resulting performance might have a high variance. From the data, it is hard to argue that LAMAL is better than MBPA++ by a significant margin.\n\nA1: Since we only have a limited computational resource, we can only run one more time for the four orders with MBPA++ (our impl.) and $\\text{LAMOL}_{\\text{TASK}}^{0.2}$ and we perform paired $t$-test on the two sets of eight numbers. The $p$-value is smaller than 1%, which shows that there is a significant margin between MBPA++ and our method. Here is the screenshot of the updated Table 5: https://drive.google.com/file/d/1J6KWh2M7WAj8NvGFQxRaRxV90ALlXVdL/view?usp=sharing\n\nQ2: I recommend having a more elaborate figure instead of Figure 2. The structure of the network might be particularly interesting for people who are not in this field. \n\nA2: Generally speaking, any language model can be used, so we only show our conceptual framework in Figure 2 instead of showing the network structure. Also, due to the page limit, we do not have more space for the network structure of GPT-2, so we refer readers to previous papers.\n\nWe thank the reviewer again and sorry for submitting our responses lately because we ran additional experiments until the last minute.", "We thank the reviewer for the review, comments, and constructive feedback. We provide responses to the weak points below:\n\nQ1: The assumption of modeling all tasks as QA might be strong;\n\nA1: This is also mentioned by Reviewer 1. We admit the assumption might be too strong, so we changed the title to \"LAMOL: LAnguage MOdeling for Lifelong Language Learning\". We also rephrase the paper to show that our major intent is to follow the datasets and metrics in DecaNLP instead of claiming to solve all kinds of NLP tasks.\n\nQ2: The baseline from using real data is missing;\n\nA2: We added the result from using real data in Table 3 (https://drive.google.com/file/d/1EI72CVvkRHxPeF4HR5eueOvURZwgIgaz/view?usp=sharing), Figure 3 (https://drive.google.com/file/d/1E_52Mh5_D6ERjc43z7cLloC23o2xkS0q/view?usp=sharing), and Figure 5 in Appendix B (https://drive.google.com/file/d/1NE3r_wlvsKQ-IW4mC1Awa9-o-YiLVpME/view?usp=sharing). Since Reviewer 1 suggested using the reverse order of five tasks, we also ran the experiments using various methods, including using real data with LAMOL (https://drive.google.com/file/d/1Y9ACQIAMH8tXFrS-ezLjmAIo1gHDvYDl/view?usp=sharing).\n\nQ3: There are many components that are missing from the discussion, such as the complexity of the language model, etc. For instance, when the model complexity is high,  TopK sampling could be expensive.\n\nA3: Generally speaking, any language model can be used, so it is difficult to discuss the complexity of the language model. \nIn each time step of sampling, Top-$k$ sampling only retains the probabilities of the top $k$ words, re-normalizes these $k$ probabilities, and samples one word from these $k$ words. Thus, top-$k$ sampling does not do beam search and its complexity is the same as direct, multinomial sampling in big-O notation.\n\nWe thank the reviewer again and sorry for submitting our responses lately because we ran additional experiments until the last minute.\n", "We thank the reviewer for the review, comments, and constructive feedback. We provide answers to the comments below.\n\nQ1: Please change the title!\n\nA1: We understand our title over-trivialized NLP tasks, so we changed it to: \"LAMOL: LAnguage MOdeling for Lifelong Language Learning\". We change the title in the revised PDF but we can't change the title on OpenReview for now. We also rephrase the paper to show that we merely use the datasets and metrics from decaNLP.\n\nQ2: A relevant model architecture based method is Sodhani et al. 2018.\n\nA2: Thank you for your reminder. We added the following paragraph to Section 2.2:\nTraining Recurrent Neural Networks for Lifelong Learning (Sodhani et al., 2018) unifies Gradient episodic memory (Lopez-Paz et al., 2017) and Net2Net  (Chen et al., 2015a). Using the curriculum-based setting, the model learns the tasks in easy-to-hard order. The model alleviates the forgetting problem by GEM method, and if it fails to learn the current task and has not been expanded yet, the model will expand to a larger model by the Net2Net approach.\n\nQ3: Can you comment on how much discarding is needed to generate the required number of pseudo-samples?\n\nA3: After 9 epochs of training for each task, the discarding happens for only 0.5% to 1% of all generated examples.\nWe added this information to Section 3.2 the last line of the first paragraph.\n\nQ4: Why is it that every task was trained only for 9 epochs?\n\nA4: There are two reasons. First, we want to compare our method to multitasking in a fair manner, so the total update steps on each example in each task should be exactly the same. If we train multitask for $T$ epochs, then in LLL, we should train each task for the same number of epochs $T$. Secondly, we find that $T = 9$ epochs were enough for every task we chose to converge to a satisfying performance, as shown in Table 2, the single task performance. Some tasks may have slightly higher performance if trained for more epochs, but we only have limited computational resource so we think $T = 9$ is a good balance.\n\nQ5: What is k? Where is it introduced? \n\nA5: It is the $k$ of the top-$k$ sampling, introduced in Section 3.2. We made it clearer in Section 4.2 paragraph 2: In all experiments, $k= 20$ in top-$k$ sampling and $\\lambda = 0.25$ for weight of the LM loss.\n\nQ6: On page 5, you mention that MTL is used to determine whether forgetting is caused by a lack of model capacity. Can you explain?\n\nA6: In many papers (for example: Learning without Forgetting), the performance of MTL is viewed as an upper bound of the performance of lifelong learning because MTL has access to old data while lifelong learning can only access current data.* Therefore, we assumed if we could not get acceptable performance on MTL, we don’t even need to consider the model’s capability in lifelong learning.\n\n* Sometimes other methods of training multiple tasks may have better overall performance than MTL. This is because (1) training all tasks together can make optimization much harder and (2) if there are unbalanced datasets, multitasking may ignore smaller datasets during training; however when we are averaging the final scores of all tasks, the weight of every task is the same.\n\nQ7: Why not compare the approach with models like GEM?\n\nA7: We added the comparison to GEM in Section 5.2 for the SST, QA-SRL, and WOZ tasks. The results are updated in the paper in Table 3 (https://drive.google.com/file/d/15S0qtl7TeR_a4dTvuYEa6qrWY-c8qp9f/view?usp=sharing)\nand Figure 5 in Appendix B (https://drive.google.com/file/d/1NE3r_wlvsKQ-IW4mC1Awa9-o-YiLVpME/view?usp=sharing).\nThe performance of GEM is only slightly better than fine-tuned, which is similar to that of EWC and MAS.\nWe do not run GEM on larger datasets because it is too time-consuming to solve the Quadratic Programming.\n\nQ8: Is there any reason why you choose to go from large to small tasks? How about other orders?\n\nA8: On the five DecaNLP tasks, with a limited computational resource, we decided to explore this order at first. On the three small tasks SST, QA-SRL, and WOZ, we compared all 6 orders as shown in Table 3.\nNow, we also completed the reversed order (WOZ → QA-SRL → SST → WikiSQL → SQuAD) experiments using following methods: (1) Fine-tuned, (2) MAS, (3) $\\text{LAMOL}_{\\text{GEN}}^{0.05}$, (4) $\\text{LAMOL}_{\\text{GEN}}^{0.2}$, (5) $\\text{LAMOL}_{\\text{TASK}}^{0.05}$, (6) $\\text{LAMOL}_{\\text{TASK}}^{0.2}$, (7) $\\text{LAMOL}_{\\text{REAL}}^{0.05}$, and (8) $\\text{LAMOL}_{\\text{REAL}}^{0.2}$. Again, because it is too time-consuming for solving the Quadratic Programming, we did not run GEM.\nThe results are shown in Appendix C (https://drive.google.com/file/d/1Y9ACQIAMH8tXFrS-ezLjmAIo1gHDvYDl/view?usp=sharing).\nWe can clearly see that our method performs much better than Fine-tuned and MAS. In this case, $\\text{LAMOL}_{\\text{TASK}}^{0.2}$ even performs better than multitasking, possibly due to the reason stated in the annotation in A6.\n\n---- to be continued ----\n", "\nQ9: Please remove the last line.\n\nA9: We removed the last line in the revised paper.\n\nQ10: I assume that the authors will release the code upon acceptance of the paper.\n\nA10: Yes, we will release the code on Github. To prove this, the Google Drive link is here: https://drive.google.com/file/d/1arQD40NfkbD_cS2vW2LwWtESYeO13SQt/view\n\nA to Minor comments: We fixed all issues. We also added more metrics description in Appendix A. The detail description of lfEM and dsEM is quite complicated, so we decide to leave it out.\n\nWe thank the reviewer again and sorry for submitting our responses lately because we ran additional experiments until the last minute.", "The paper presents a new NN architecture designed for life-long learning of natural language processing. As well depicted in Figure 2, the proposed network is trained to generate the correct answers and training samples at the same time. This prevents the \"catastrophic forgetting\" of an old task. Compared to the old methods that train a separate generator, the performance of the proposed method is noticeably good as shown in Fig 3. This demonstrates that the new life-long learning approach is effective in avoiding catastrophic forgetting.\n\nThe motivation of the paper is clear. The comparison to old methods seems fair. The proposed method is clearly different from previous methods.\n\nOne weakness of the paper is in the experimental results especially in section 5.4. The statistical significance of the results in table 5 is missing. As the authors have discovered, the performance is highly dependent on implementation. In addition, the resulting performance might have a high variance. From the data, it is hard to argue that LAMAL is better than MBPA++ by a significant margin.\n\nI recommend having a more elaborate figure instead of Figure 2. The structure of the network might be particularly interesting for people who are not in this field. \n\nOverall, the results are very interesting and worth a publication.", "This paper studies the problem of lifelong language learning. The core idea underlying the algorithm includes two parts: 1. Consider the NLP tasks as QA and then train a LM model that generates an answer based on the context and the question; 2. to generate samples representing previous tasks before training on a new task. \n\nIn experiments, the authors demonstrate the efficiency and effectiveness of the proposed models based on the following perspectives:\n1. Compare the proposed method with existing baselines. However, it seems that keep real data is missing from Table 3. \n2. Preliminary studies with 3 tasks and study the task oder.\n3. Performance of the training epochs.\n4. Hyper parameter tuning gamma. \n\nThe weak points of this paper are as following:\n1. The assumption of modeling all tasks as QA might be strong;\n2. The baseline from using real data is missing;\n3. There are many components that are missing from the discussion, such as the complexity of the language model, etc. For instance, when the model complexity is high,  TopK sampling could be expensive.\n"], "review_score_variance": 2.0, "summary": "This paper proposes a new method for lifelong learning of language using language modeling. Their training scheme is designed so as to prevent catastrophic forgetting. The reviewers found the motivation clear and that the proposed method outperforms prior related work. Reviewers raised concerns about the title and the lack of some baselines which the authors have addressed in the rebuttal and their revision.", "paper_id": "iclr_2020_Skgxcn4YDS", "label": "train", "paper_acceptance": "accept-poster"}
{"source_documents": ["In this paper, we introduce a novel method of weight compression. In our method, we store weight tensors as sparse, quantized matrix factors, whose product is computed on the fly during inference to generate the target model's weight tensors. The underlying matrix factorization problem can be considered as a quantized sparse PCA problem and solved through iterative projected gradient descent methods. Seen as a unification of weight SVD, vector quantization and sparse PCA, our method achieves or is on par with state-of-the-art trade-offs between accuracy and model size. Our method is applicable to both moderate compression regime, unlike vector quantization, and extreme compression regime.", " **W4.** First of all, we admit there is a mistake in the range limits in quantizers (5) and (6). As elements of $\\mathbf{C}$ and $\\mathbf{Z}$ are not required to be positive, the correct definition is (also different quantization scales are used for $\\mathbf{C}$ and $\\mathbf{Z}$):\n\n$C_{q,ij}=\\mbox{clamp} \\left( \\left\\lfloor \\frac{C_{ij}}{s_{z,i}} \\right\\rceil , -2^{b_c-1}, 2^{b_c-1} - 1\\right)s_{c,i},$\n\n$Z_{q,ij}=\\mbox{clamp} \\left( \\left\\lfloor \\frac{Z_{ij}}{s_{z,j}} \\right\\rceil , -2^{b_z-1}, 2^{b_z-1} - 1\\right)s_{z,j}$,\n\nWe use the sequence of the following variables in order to calculate the derivative w.r.t $\\mathbf{C}$ and $\\mathbf{Z}$:\n\n$F= \\frac{1}{m}\\sum_{i=1}^m ||Y_i- O_i]||^2_F$,\n\n$F_i=||Y_i- O_i||^2_F$,\n\n$O_i=\\tilde{W}*X_i$,\n\n$\\tilde{W}=C_{q} Z_{q}$.\n\nThe derivatives of interest are defined as follows:\n\n$\\frac{\\partial F}{\\partial O_i}=2(Y_i-O_i)$,\n\n$\\frac{\\partial \\tilde{W}}{\\partial C_q}= Z_q^T$,\n\n$\\frac{\\partial \\tilde{W}}{\\partial Z_q}= C_q^T$.\n\nThe derivatives of $\\mathbf{C}_q$ can be expressed as follows (as Latex cases are not supported in this discussion, we have to use single line expressions): \n\n$\\frac{\\partial C_{q,ij}}{\\partial C_{ij}}=1\\mbox{ if } -s_{c,i}2^{b_c-1} \\leq C_{ij} \\leq s_{c,i}(2^{b_c-1}-1) \\mbox{, 0 otherwise}$;\n\n$\\frac{\\partial C_{q,ij}}{\\partial s_{c,i}}=-\\frac{C_{ij}}{s_{c,i}}+\\lfloor\\frac{C_{ij}}{s_{c,i}}\\rceil\\mbox{ if  }-2^{b_c-1}\\leq\\frac{C_{ij}}{s_{c,i}}\\leq 2^{b_c-1}-1, -2^{b_c-1}\\mbox{ if } \\frac{C_{ij}}{s_{c,i}} < -2^{b_c-1}, 2^{b_c-1}-1\\mbox{ if } \\frac{C_{ij}}{s_{c,i}} > 2^{b_c-1}-1$.\n\nNote that in the equations above, we used STE estimator for the rounding operation, i.e. the gradient of the quantizer is computed as if the rounding operation is bypassed in the backward pass.\nUsing the equations above, the derivative of the objective $F$ w.r.t. $\\mathbf{C}$ can be expressed as follows:\n\n$\\frac{\\partial F}{\\partial C}=\\sum_{i=1}^{m} \\frac{\\partial F_i}{\\partial O_i}\\frac{\\partial O_i}{\\partial \\tilde{W}}\\frac{\\partial \\tilde{W}}{\\partial C_q}\\frac{\\partial C_q}{\\partial C}$.\n\nThe gradients of $Z$ can be expressed in a similar manner. We are ready to include the full derivation and the corrections to the paper once it is possible to upload a new revision.\n\n**W5.** We understand that having the source code available can aid the review process. Unfortunately, open-sourcing anonymous source code is not equally easy for every research institute due to legal and intellectual property considerations. In our case, the institute’s process did not enable us to release the source code on time for review, but we plan to release to code with the final paper.", " For the response to W4, I don't think \"The STE gradient for the rounding operation is defined as...\" is a clarification. I expect the authors to write down the update rule for $\\mathbf{C}$ and $\\mathbf{Z}$ explicitly and clearly. \n\nFor the response to W5, I don't understand why the authors cannot upload the source code during this reviewing phase. I apologize for my late response, but the authors can still reply to me with sharing a link to an anonymous repository for the code. ", " Thank you for the clarification. I think adding this to the paper would be nice.", " While the main intention of our method is to reduce the model size and memory bandwidth, we confirm that the compute cost increases with our approach.\n\nThe complexity of a forward pass for a convolutional layer is $f_{in}f_{out}hwp$, where $f_{in}$ and $f_{out}$ are the number of input and output channels, $h \\times w$ is the spatial size of the filter, and $p$ is the number of pixels in the input feature map assuming \"same\" padding. The overhead for the filter reconstruction is the product of two matrices using $f_{in} f_{out}hwk$ multiply-add operations (MACs), where $k$ is PCA rank. So the relative overhead in the number of operations is $k/p$. Practically, for Resnet18, a value of $k$ equals for example 128, while the spatial size of the feature map for an ImageNet goes down from $56\\times 56$ to $7\\times7$ in the later layers. So, the relative overhead for a single forward pass is varying between 4\\% and 261\\% extra operations for different layers. While the total relative overhead for the full model is 85\\% extra MACs. However, with an increasing batch size or when doing multiple inferences this because neglectable.\nPlease also note that the above comparison is based on MACs, however in our approach, most computations contain low bit tensors. If we do the same comparison in bit operations (BOPs) the overhead is significantly reduced. We are going to add a clarification on the computational overhead to the paper.\n", " Thank you for your comments. As we mentioned, we never claim that the problem we are solving is convex. While the purpose of Proposition A.1. is to describe the conditions on the objective which would guarantee the convergence despite we perform a projection on a non-convex set. Again, we do not claim this is applicable to the problem we are solving, the purpose of adding this appendix to the paper is rather to provide intuition on convergence in some settings with a non-convex projection.  \n\nStraight-through estimator (STE) is a technique used for back-propagation through the rounding operation which is non-differentiable. The STE gradient for the rounding operation is defined as $\\frac{\\partial \\lceil p\\rfloor}{\\partial p}=1$, we added this to the paper in section 3.1.1. STE was introduced by Bengio et al. 2013 and is commonly used for training quantized models (Krishnamoorthi et al. 2018, Esser et al. 2019, Bhalgat et al. 2020, Fan et al. 2020).\n\nStep 5 of algorithm 1 is returning both updated $C, Z$ and their quantized versions $C_q, Z_q$ which are computed using equations 5 and 6. The computation of the updated $C, Z$ involves back-propagation through equations 5 and 6, so the computation includes the forward pass which also computes $C_q, Z_q$. The description of Algorithm 1 is fully correct. \n\nWe hope our comments help to clarify the paper. ", " The authors do not need to provide intuition about the theoretical guarantee of projected gradient descent while the function is convex and projection is performed on a convex set---simply for \"well-behaved\" problems. These are well-known results. What the authors are solving is a nonconvex problem (objective function) with nonconvex projection (for the sake of argument, I am not even considering the quantization steps, the hard thresholding alone is a nonconvex projection on the set of sparse matrices) and NO THEORETICAL GUARANTEE of projected gradient descent on convex set with \"well behaved\" problems holds there. This is a faulty and vastly misleading mathematical claim, and I will keep my score intact just for that. \n\nNew questions: \n\nThe authors said in reply to my comment: \"we use the straight-through estimator in order to backpropagate through the quantization operation.\" What is that? Can the authors please explain further? \n\nI request the authors to check Algorithm 1. If what the authors are claiming is correct then step 5 of Algorithm 1 is not correct in its present form. What Step 5 of Algorithm 1 is returning is C, Z and they are not the same as quantized C, Z as the authors defined in equations 5 and 6, respectively. The pseudocode in Algorithm 1 is not in conformity with the process defined. \n\nIn summary, there is a lot of mathematical jargon, and most of them are incorrect and misleading. ", " Thank you for your response. \n\nTo clarify my question Q2: If the NN is e.g. used for classification, does it take longer to classify a single input using a compressed network vs an uncompressed network? Basically, does a forward pass in the compressed NN take longer than a forward pass in the uncompressed NN? It seems like it should take more time, since the weight tensors have to be reconstructed from the factors and then reshaped into the correct shape. This additional cost may be negligible in practice, but I was just curious to hear if you looked at this.\n\nAfter reading the other reviews and author responses, I'm leaving my score as it is for now. ", " We thank the reviewer for the detailed feedback. We answer the comments and suggestions below.\n\n**W1**. As opposed to a straightforward combination, e.g. applying quantization on existing SVD decomposition, we solve the joint optimization problem (8). This problem of jointly optimizing the quantized values and the sparsity mask is a non-convex integer program and is NP-hard. Practically, it is challenging to solve. To the best of our knowledge, this type of problem was never considered in the literature. We look at the problem on the data manifold and solve it by introducing the alternating optimization method (Algorithm 1). The challenge of dealing with discrete quantized values is tackled using a straight-through estimator (STE).\n\nIn order to compare the solution obtained with our method to a direct combination of the two methods (simply quantizing SVD-compressed network), we present the results in Table 1. For example for 30\\% sparsity (see the pre-FT results) we obtain a network with 63.0\\% top-1 accuracy (see iterative HT with 30 iterations) while the naive combination of the methods gives a network with 53.3\\% accuracy (see \"no data-aware opt.\" row). \n\n**Q1.** As our method is based on per layer weight optimization (Algorithm 1), its computational cost is much lower than network training/fine-tuning. Practically, it takes 1-2 minutes to compress an ImageNet model on a GPU.\n\n**Q2.** The sparse representation is only used when the sparsity level is higher than $1/b$ to ensure the total size of weights in memory does not increase. In practice, this holds for most of the layers in our experiments.\n\n**Q3.** In our experiments, we flatten the weight tensor in the order of dimensions $f_{out}, f_{in}, h, w$. Then we reshape it into $n \\times d$, where $d$ is PCA dimensionality and $n$ is the number of tiles. We clarified it in the paper.\n\n**Q4.** Yes, this notation signifies rounding to the nearest integer. We added a clarification on this to the paper.\n\n**Q5.** Correct. Fixed in the paper, thank you for pointing it out.\n\n**Q6.** Exactly. No optimization is performed in this case.\n\n**Q7.** The experiments are implemented in PyTorch. We define a custom gradient operation for the quantizers where we use a straight-through estimator. \n\n**Q8.** By MAC we mean multiply-accumulate operation. We added a clarification to the paper.\n\nThank you for mentioning the typos, we corrected the paper accordingly.", " We thank the reviewer for their comments and suggestions. We give our replies below.\n\n**W1.** Our method can be viewed as a generalization over SVD compression, sparse PCA, and vector quantization. Depending on the choice of quantizers and sparsity constraint we can arrive at any of three methods while we depart from a single unified formulation as we show in section 3.1.2. While additive quantization Babenko \\& Lempitsky (2014) only consider sums of a fixed number of codewords for vector quantization which is a very restricted version of our method. Our formulation yields more expressive linear combinations which are in agreement with our experiments.\n\nAs opposed to a straightforward combination, e.g. applying quantization on existing SVD decomposition, we solve the joint optimization problem (8). This problem of jointly optimizing the quantized values and the sparsity mask is a non-convex integer program and is NP-hard. Practically, it is challenging to solve. To the best of our knowledge, this type of problem was never considered in the literature. We look at the problem on the data manifold and solve it by introducing the alternating optimization method (Algorithm 1). The challenge of dealing with discrete quantized values is tackled using a straight-through estimator (STE).\n\nIn Table 1 we present the results on our joint optimization method versus the naive baseline  (simply quantizing SVD-compressed network). For example for 30\\% sparsity (see the pre-FT results) we obtain a network with 63.0\\% top-1 accuracy using the joint optimization (see iterative HT with 30 iterations) while the naive combination of the methods gives a network with 53.3\\% accuracy (see \"no data-aware opt.\" row). \n\n**W2.** Our motivation for sparsifying matrix Z is inline\nwith previous works on sparse PCA. We represent every row of weight matrix Z using a sparse linear combination of codewords. Practically, factor Z has a larger size in memory, so it is more important to compress Z versus compressing the codebook C (we explain the details in section 3.2).\nRegarding sparsity, we penalize the number of nonzero elements in Z as a (kn)-dimensional vector. We added a clarifying note to our formula which introduces the L0 norm.\n\n**W3.** In our work, we follow the conventional sparse PCA in the sense that we use dense codebook $C$ and a sparse set of linear coefficients $Z$. Although the order of $C$ and $Z$ is indeed reversed, i.e. the first factor would be sparse in the conventional PCA while the second factor would be dense. Strictly speaking, we perform sparse PCA on $W^T$, and should use $W^T$ in our notation, however, we deliberately omit the transposition to avoid unnecessary notation clutter (we assume the matrix W can always be transposed prior to the computation if needed).\n\n**W4.** The STE gradient for the rounding operation is defined as $\\frac{\\partial \\lceil p\\rfloor}{\\partial p}=1$, we clarified this in the paper.\n\n**W5.** We agree that publishing the source code would help to ensure the reproducibility of our method. And we plan to release the source code upon acceptance. As a side note, this should not be a limiting factor for the paper according to the conference guidelines.\n\n**Reply to summary.** We can not provide theoretical guarantees for this non-convex problem, however, we give intuition on the convergence of Algorithm 1 in a simplified setting in Appendix A.\n\n**Other comments.** Thank you for the suggestions provided in \"other comments\", we addressed each of them in the updated manuscript accordingly.", " We thank the reviewer for the comments and we address the concerns below.\n1. Thank you for mentioning the paper by Vogels et. al. This method uses dense factorization in the context of gradients compression, while quantization and sparsity are not used for further size reduction. We added the reference to related work.  \n2. Thank you for mentioning this. Indeed, we only use a limited subset of training data for optimization, and $m$ is the size of the subset. We clarified this in the paper.\n3. While solving the problem (10), we obtain the quantized values of $C$ and $Z$ as outputs of $Q_c$ and $Q_z$, so they do not require further projection. While the values of $C$ and $Z$ are not quantized, we use the straight-through estimator in order to backpropagate through the quantization operation. This allows us to compute the updates for $C$ and $Z$.\n4. Regarding severely faulty mathematics. First of all, as we mentioned in the paper, \nthe theoretical support holds only  \"for well behaved optimization problems\" (see above 3.1.2). The goal of Appendix A is to provide intuition behind the choice of iterative projection methods and not proving convergence results for it. \nRegarding Proposition A.1 and the reviewer's comment, if we understand the comments correctly, we do not assume projection onto a convex set, and this is not mentioned anywhere in the paper.    \nBesides, we never mentioned that the theory applies to the problem (10). It only provides an intuition for the chosen algorithm.\nThe claim regarding stationary point is indeed correct.  \nIf $\\mathbf{x_\\star}$ is in $\\mathcal{D}$ with  $\\Delta_f(\\mathbf{x_\\star})=0$,  then we have\n\n    $\\Pi_{\\mathcal{D}} (\\mathbf{x}_\\star-\\Delta_f(\\mathbf{x_\\star}))=$ \n\n    $=\\Pi_{\\mathcal{D}} (\\mathbf{x}_\\star)=\\mathbf{x}_\\star$\nwhich means that $\\mathbf{x}_\\star$ is the stationary point of the projection step.  \nNote that we are not providing general convergence results for general non-convex optimization. It is just shown that only under certain conditions on the projection step, we can get convergence results. We do not claim anywhere in the paper that these conditions hold for our problem. The same holds for $L$-Lipschitz analysis.   \nTo summarize, the theoretical statements of Appendix A are fully correct, and their goal is just to provide intuition about the algorithm 1 and not to prove any result about it.   \n\nWe hope this clarified your concerns and hope you are willing to review the full paper.", "The authors proposed a matrix factorization-based method for deep neural network weight compression. The weight tensors are factorized as two low precision quantized matrices, out of which one is sparse.  1. I request the authors to discuss a significant low-rank factorization technique used for deep neural network gradient compression---PowerSGD by Vogels et al. NeuRIPS, 2019. PowerSGD uses power iteration to decompose the original gradient matrix $M$ into two $r$-rank matrices $P$ and $R$. This work is relevant and similar to the present work and requires mentioning. \n\n2. In the expected loss minimization problem, what is $m$? Is it the number of observations in the training set? I did not find it before. \n\n3. I have questions about solving the problem (10). How in (10), the authors arrive at quantized, $C_q, Z_q$? By using gradient descent, one can reach the factors, $(C, Z)$, and then it requires further projection by using quantization operations (as defined in (7)) to find quantized, $C_q, Z_q$. Running gradient descent algorithm directly on (10) does not guarantee quantized, $C_q, Z_q$. In the present form, it is misleading, and the treatment and evolution of the problem throughout the draft are highly faulty.  \n\n4. **Severely faulty mathematics.** Proposition A.1 is not correct. Please allow me to explain. The projection is not on a convex set; projection onto the set of sparse matrices (by using hard threshold operation) is a nonconvex projection. Moreover, the function that the authors defined in (10) is the same as the function, $f(x)$ defined in (14), which is a nonconvex function. First of all, for nonconvex functions proving convergence in terms of the optimum is impossible because one cannot guarantee the existence of a global minimizer $x_*$. Although the authors applied the projected gradient descent technique to nonconvex optimization, the convergence proof techniques used in the convex case do not apply directly to nonconvex problems. Therefore, the projection claim that the authors provided is mathematically incorrect. Please note that the claim that the authors made about the stationary point is erroneous. Moreover, the convergence rate with the derivation of the L-Lipschitz to justify their faulty analysis in Proposition A.1, is severely flawed. For (local linear) convergence of gradient descent with nonconvex projection, please see [1,2,3]. \n\n5. SVD stands for Singular Value Decomposition. Therefore, “SVD decomposition” does not make any sense. \n\n[1] Lewis et al. Local linear convergence for alternating and averaged nonconvex projections. Foundations of Computational Mathematics, 9(4):485–513, 2009.\n[2] Lewis and Malick. Alternating projections on manifolds. Mathematics of Operations Research, 33(1):216–234, 2008.\n[3] Dutta et al., A Nonconvex Projection Method for Robust PCA, AAAI, 2019. \n\n\n\n I did not read the paper after Section 3.1.1. The article is erroneous and the results are misleading. Please refrain from misleading the readers. This alone makes me recommend a STRONG REJECTION of the paper. ", "This paper introduces a novel method of weight compression. Weight tensors are stored as sparse, quantized matrix factors, and the underlying matrix factorization problem can be considered as a quantized sparse PCA problem and be solved through iterative projected gradient descent methods. The authors' method  is applicable to both moderate and extreme compression regimes, and is claimed to achieve or be on par with state-of-the-art trade-offs between accuracy and model size. Strengths:\nThe authors empirically demonstrate that their method is the only method to achieve SOTA compression-accuracy trade-offs in both low and high compression regimes. \n\nWeakness:\n1. The method seems to be a simple combination of tensor factorization and vector quantization methods. The idea of using (sparse) PCA in vector quantization methods has also been studied, e.g., in Babenko & Lempitsky (2014). Therefore, the novelty in this submission seems to be quite limited. \n2. The authors should provide more explanations about the motivation and benefits of assuming the factor matrix $\\mathbf{Z}$ to be sparse. In addition, the authors should clarify that Z is sparse in what sense? For example, row-sparse, column-sparse, or sparse while considering $\\mathbf{Z}$ as a $(kn)$-dimensional vector? In (8), the norm $\\||\\cdot\\||_0$ is used without any explanation. \n3. The authors should be careful to claim that their method for factorizing $\\tilde{\\mathbf{W}}$ in (1) is sparse PCA (ignoring the quantization). For conventional sparse PCA, the factor matrix $\\mathbf{C}$, instead of $\\mathbf{Z}$, is assumed to be column-sparse.\n4. The gradient descent update on (10) is important and should be clearly presented in this submission, rather than simply citing another paper. \n5. All source codes required for conducting experiments should be included in a code appendix.\n\nOther comments:\n1. The terminology \"quantized sparse PCA\" should be fixed, and the authors should not also use \"sparse quantized PCA\" arbitrarily (e.g., in the second dot point of the contributions). \n2. The relation $k < d < n$ should be made clear when these parameters first appear in Section 3.1 (instead of mentioning it in Section 3.2). \n3. In Algorithm 1, line 5, equation 10 should be equation (10). In Appendix A, problem 14 should be problem (14). Although the method proposed by the authors seems to be empirically promising, it seems to be a direct combination of existing methods, and the novelty seems to be limited. The authors provide no theoretical guarantee/intuition for their method. In addition, some terminologies/notations are vague or confusing. My impression is that this submission requires some major revisions. ", "The paper proposes a method for compression of neural network weights. The proposal turns weight tensors into matrices, factorizes these matrices into a rank-k factorization via PCA, applies quantization to the factor matrices, and additionally makes the right (latent) matrix sparse. An algorithm is presented, with two different thresholding options (per-iteration, or one-shot). In experiments, these ideas lead to an accuracy/compression tradeoff which is either competitive with or better than previous state-of-the-art across all compression ratios investigated. # Strenghts\nS1. The paper is clearly written and the proposed method and results are convincing. The method is simple yet seems to work well.\n\nS2. The topic of weight compression should be of interest to a large proportion of the deep learning/ML community.\n\nS3. Although I’m not familiar with the literature in this area, the authors seem to do a good job of discussing related work (in Sec. 2) and later on also discussing how their proposal relates to other previous methods (in Sec. 3.1.2).\n\n# Weaknesses\nW1. I mentioned the simplicity of the method as a strength. However, the simplicity of the method comes from the fact that it just combines several previous ideas (PCA, sparsity, quantization) into a new method, which could be seen as incremental/less exciting. \n\n# Questions\nBelow are some questions I have for the authors. \n\nQ1. The paper discusses the accuracy/compression trade off, but focuses less on time/computational cost. What can you say about the time it takes to compress a network? Is it negligible compared to, e.g., the time it takes to train the network? \n\nQ2. Does having the weights in the sparse quantized PCA format increase the inference time compared to the non-compressed network? \n\nQ3. How is the reshaping of the weight tensor $W$ into $\\tilde{W}$ done? Do you always put $f_{\\text{out}}$ along the rows, and $f_{\\text{in}}$, $h$, $w$ along the rows? You should mention this in the paper.\n\nQ4. What does the $\\lfloor \\cdot \\rceil$ notation in Eqs. (5) and (6) signify? Round to nearest integer?\n\nQ5. Should it be $2^{b_c}$ in Eq. (5) and $2^{b_z}$ in Eq. (6) rather than just $2^b$?\n\nQ6. At end of 1st paragraph in Sec. 4.3.1, you mention doing “one-shot hard-thresholding without data-aware optimization.” What do you mean by “without data-aware optimization”? Are you just doing an SVD of $\\tilde{W}$ and then quantizing the $C$ and $Z$ matrices?\n\nQ7. How can quantization be implemented in practice? Do you implement your experiments in Python? \n\nQ8. What does “MAC operations” stand for (mentioned in the introduction)?\n\n# Typos, other minor things\nT1. Above Eq. (11), “while $Z$ encodes the of indices” should be “while $Z$ encodes the indices.”\n\nT2. Sec. 3.2, 2nd paragraph: Should the size of $W$ be $256 \\times 256 \\times 3 \\times 3$ rather than $256 \\times 256 \\times 9 \\times 9$ since $w=h=3$?\n\nT3. Sec. 4.3.2, 2nd paragraph, 1st sentence, there’s a period missing before “In this figure”.\n\nT4. In 3rd sentence after the proof of Proposition A.1, should it be $\\Delta_{f}(x) = \\alpha \\nabla f(x)$, i.e., without a $\\star$ on the $x$ on the left hand side?\n\nT5. In Sections B.2 and B.3 in the appendix, it would be nice if you at least added one sentence per section that refers to the relevant table so that we can know which table belongs in which section. Right now, the section headings aren’t really doing much.\n The paper is well written and covers a topic that should be of broad interest to the deep learning/ML community. The authors do a good job of putting their proposal in context and explaining previous work. The experiment results are encouraging. Overall, a nice paper that I think is suitable for publication in ICLR."], "review_score_variance": 8.222222222222221, "summary": "Reviewer rRp9 expressed concerns regarding the theoretical results included in Appendix A. In the discussion (not visible to the authors), the AC and Reviewer zn4a agree that the exposition in the original manuscript was confusing and could lead readers to assume these results were valid for the proposed algorithm. Also, in the original manuscript the presentation of the theoretical results in the appendix was quite poor (e.g. Proposition A.1). Having said that, the contributions and main points of the work are not affected by these observations as it is mainly an empirical study.\n\nFollowing from the previous point, Reviewers rRp9 and zn4a pointed out that the overall presentation of the method, particularly the mathematical presentation could be improved. \n\nReviewer zn4a points out that the method is not particularly novel, this was also indicated as a weakness by Reviewer iyVU. The main contributions of the work are to simultaneously solve the tensor factorization and vector quantization problems usinga form of projected gradient descent (with hard-thresholding). While the empirical results seem promising, are somewhat limited. The authors could make them stronger by studying other applications on top of image classification (e.g. semi-supervised setting, object detection or segmentation).\n\nIn the discussion (not visible to the authors), Reviewer iyVU stated in light of the other reviews, he/she does not oppose rejecting the work.\n\nOverall, the method is technically sound and produces promising results. In its current form, however, the paper is not yet ready for publication. The AC encourages the authors to incorporate the feedback and resubmit the work to a different venue.", "paper_id": "iclr_2022_kK3DlGuusi", "label": "train", "paper_acceptance": "Reject"}
{"source_documents": ["Variational inference is a fundamental problem in Variational AutoEncoder (VAE). The optimization with lower bound of marginal log-likelihood results in the distribution of latent variables approximate to a given prior probability, which is the dilemma of employing VAE to solve real-world problems. By virtue of high-dimensional geometry, we propose a very simple algorithm completely different from existing ones to alleviate the variational inference in VAE. We analyze the unique characteristics of random variables on spheres in high dimensions and prove that Wasserstein distance between two arbitrary data sets randomly drawn from a sphere are nearly identical when the dimension is sufficiently large. Based on our theory, a novel algorithm for distribution-robust sampling is devised. Moreover, we reform the latent space of VAE by constraining latent variables on the sphere, thus freeing VAE from the approximate optimization of posterior probability via variational inference. The new algorithm is named Spherical AutoEncoder (SAE). Extensive experiments by sampling and inference tasks validate our theoretical analysis and the superiority of SAE.", "Thank you for your response.\n\nI went through the rebuttal and the revised version of the paper, and most of my original concerns remain unaddressed:\n\n- The positioning of the paper with respect to VAE, variational inference is confusing and even misleading.\n\n- Attributing generation issues in VAE to the fact that “the posterior q(z|x) is incapable of matching the prior distribution p(z) well” is not correct. In VAE we are not expecting q(z|x) to match p(z) well as this would result in useless inference (q(z|x) ignoring x).  Please refer to my original comments for more details. In their answer A3, the authors mentioned about Wasserstein autoencoder (WAE). I would like to emphasize that in WAE the objective is to match the “aggregated” posterior q(z) with the prior p(z), where  q(z) = \\int q(x)q(z|x)dx, with q(x) denoting the empirical data distribution.  Indeed, we want q(z), but not q(z|x), to perfectly match p(z). \n\n- Assumptions under which Theorem 2 holds should be stated clearly. The sampling at random assumption is not enough if z, z’ are from empirical distributions on the sphere with overlapping supports. \n\n- While the result of Theorem 2 may justify “distribution robust-sampling”, it is not clear how and why would this Theorem justify improvement in inference when projecting z onto a hypersphere. \n\n- The revised version includes qualitative results for VAE and the proposed SAE on a new dataset (CelebA), as well as for hyper-Spherical VAE on MNIST. However, quantitative experiments remain weak.\n\n- The writing has been revised making it better than the initial version, yet the paper is still hard to follow, and further improvements are necessary.", "\nQ1: “An important claim in this paper is that the proposed approach “alleviates variational inference in VAE”. However, this requires clarification as well as theoretical/empirical justifications”\nQ2: “Moreover, why and how would Theorem 2 justify improved inference when projecting latent samples onto a hypersphere?”\nA1 and A2: These two questions and related comments might be due to our inappropriate use of “alleviate” and the extensive meaning of inference beyond probability. Actually, there is no posterior inference and any priors involved in our SAE algorithm. It is the vanilla autoencoder subject to the spherical constraint shown in equation (10). So we said “thus freeing VAE from the approximate optimization of posterior probability via variational inference” and “Our algorithm is geometric and free from posterior probability optimization”. Indeed, “alleviates variational inference in VAE” is an inappropriate use in this scenario. We will correct this in the revised version. \n\nBesides, we use “inference” to refer to inferring (obtaining) z from the encoder, not only for “variational” inference or “probabilistic” inference. This might cause misunderstanding with habitual thinking in this field. This misunderstanding might be avoided by using “geometric inference”. We will note this meaning clearly in the revised version.\n\n\nQ3: “However, in VAE we do not expect the posterior to match the prior perfectly, as this would result in useless data representations or inference.” \nA3: We  understand your viewpoint about the model distribution and the prior distribution . “match the prior perfectly” does not mean the point-to-point correspondence. We refer to fitting distributions. The word “match” is also used in Wasserstein autoencoder (https://arxiv.org/abs/1711.01558), which is the same scenario to ours.\n\nQ4: “Theorem 2 (on the convergence of the Wasserstein distance (W2) on high dimensional hyperspheres) does not seem to hold if, for instance, P and P’ are empirical distributions with overlapping supports.”\nQ5: “Further, even when the above Theorem holds, the W2 distance may be relatively high since it is proportional to the square root of the number of samples.”\nA4: To make our theory much easier to understand, we directly gave the computational definition of Wasserstein distance in (8) and (9) rather than its original integral form. Thus, Theorem 2 is the direct result by substituting the conclusion of Lemma 1 into (8). It is very easy. About the correctness of Lemma 1, please refer to the elegant proof at http://faculty.madisoncollege.edu/alehnen/sphere/hypers.htm.\n\nMost theorems only hold under some conditions. Both Lemma 1 and Theorem 2 need a basic condition. The condition is that the points are drawn from spheres at RANDOM. To satisfy the condition, we use the operation of centerization in our SAE algorithm, which is motivated from central limit theorem in probability. \n\nIn fact, it is straightforward to design the case to deny Lemma 1 and Theorem 2 if we bypass the condition. For instance, let Z1 be the set sampled from the spherical part in the open positive orthant and Z2 sampled from the spherical part in the open negative orthant. The third set Z3 is derived from Z2 by the small perturbation. Both Lemma 1 and Theorem 2 do not hold for the dataset { Z1, Z2, Z3}. But such samping violates the randomness needed. For SAE, the centerization is used to prevent such cases. \n\nA5: “the W2 distance may be relatively high since it is proportional to the square root of the number of samples.” is correct. However, it is logically wrong to use it to deny our theory, because all the W2 distances between two arbitrary random datasets still converge to be the same constant in Theorem 2 when the number of samples increases. The conclusion still holds in our paper.\n\nQ6: “Improve experiments by including more datasets and baselines (e.g., hyperspherical VAE [1]), as well conduct more targeted experiments to give more insights regarding the effect of the L2 normalization on inference and generation. ”\nA6: We failed to get the convergent results of hyper-Spherical VAE (S-VAE) on FFHQ faces of size 128x128. So we did not compare it in the current version. We are now running it on MNIST. The results will be updated in the revised version within several days.\n", "\nQ1: “Then, to clarify the algorithm, it seems necessary to provide the formulation of objective functions.”,  “Is the objective still valid or reasonable even it is derived from the equation (10) without posterior inference?”\nA1: The objective function will be provided in the revised version. It is the reconstruction loss || x - \\tilde{x} || subject to the spherical constraint on z (equation (10)). There are no posterior inference and no KL-divergence involved in our algorithm. It is very simple. \n\nQ2: “How does the objective change when centerization and spherization are applied to the GAN?”\nA2:  There is no extra objective when applied to GANs. Only centerization and spherization are needed. \n\nQ3: “Compared with using von Mises-Fisher distribution in the vanilla VAE, the advantage of the proposed method is not clear. To my understanding, the main difference seems to be whether using lower bound with posterior inference or deterministic framework without such approximation. However, there are no theoretical or empirical results to show the benefit of the proposed method.”\nA3: This might be the misunderstanding caused by that we didn’t explicitly write the objective function in the paper. We explain this in Q1. Our SAE algorithm is essentially different from S-VAE (hyper-Spherical VAE). The S-VAE is established on the principle of VAE. So, S-VAE has the drawbacks posed by VAE such as the approximation of posterior inference, the prior dependence, and the reparameterization trick for random variables. But SAE is distribution-agnostic with respect to Wasserstein distance, which is rigorously guaranteed by Theorem 2. \n\nActually, we failed to get the convergent results of S-VAE on FFHQ faces of size 128x128. We are now running it on MNIST. The results will be updated in the revised version within several days.\n\n\nQ4: “Compare to ProGAN and StyleGAN, is the contribution of the paper to applying centerization to GAN and centerization and spherization to autoencoder?”\nA4: Both GAN and autoencoder need to use centerization and spherization on random variables. For ProGAN and StyleGAN, the authors empirically applied spherization on z in their code, which motivated our work. We also made it clear in the context of equation (3).  \n\nQ4: “What dimension do you use as latent dimension in the experiments?”\nA4: We followed the experimental setting of StyleGAN. The 512-dimensional latent vectors are used for StyleGAN, VAE, and SAE on the face datasets including FFHQ and CelebA. For MNIST, we take the 10-dimensional latent codes. \n\nQ5: “Does the choice of prior distribution affect the experimental results? If so, is there any compatible reason with the intuition of SAE?”\nA5: This question might be another misunderstanding caused by Q1. Actually, there are no any priors involved in SAE during training. We used different priors to test the robustness of SAE and VAE after training was completed. We will make it clear in the revised version.\n", "The revised version has been updated. We revised the submission from the following eight aspects according to Reviewers' advice.\n\n1. We explicitly wrote the objective function of SAE in equation (11). The reconstruction loss and the spherical constraint in equations (10) and (11) are all operations in SAE. There are no variational inference, no probabilistic optimization, and no priors involved in SAE during training.\n\n2. To make the meaning of the word \"inference\" clear, we named the inference in SAE as the spherical inference. As opposed to the variational inference, the spherical inference  is deterministic during training. But the decoder of SAE is rather robust to various priors for sampling after training. We made this expression clear in the paper to avoid misunderstanding with the variational inference.\n\n3. We added the discussion for Wasserstein autoencoder, adversarial autoencoder, and beta-VAE in the related work.\n\n4. We also compared VAE and SAE on CelebA.  We need to note that the quality of generated faces by VAE and SAE will be both improved if we use the face images of only cropping facial parts for the experiments. But such data are not sufficient to test the robustness of the algorithms against the variations of the entire facial features.\n\n5. We compared hyper-Spherical VAE (S-VAE) with SAE on MNIST using the official code at https://github.com/nicola-decao/s-vae-tf.\n\n6. We provided the visualization results of latent codes from VAE, S-VAE, and SAE on CelebA and MNIST. This visualization clearly shows the superiority of the spherical inference in SAE.\n\n7. We re-arranged images in Figure 5 to save more space for the new contents. The experimental results in this Figure are kept the same as the previous version.\n\n8. We corrected the typos, polished the writing,  and made the paper more readable.\n\nAll the complementary experimental results were attached in Appendix.", "\nQ1: “is this a variant of the Wasserstein auto-encoder?”\nA1: Our SAE algorithm is not a variant of WAE proposed in the following paper.\n\nWasserstein Auto-Encoders\nhttps://arxiv.org/abs/1711.01558\n\nWAE minimized Wasserstein distance between the model distribution and the prior distribution. The algorithm reduces to adversarial learning via a discriminator in the latent space, which is similar to the following paper\n\nAdversarial Autoencoders\nhttps://arxiv.org/abs/1511.05644\n\nLike VAE, both Wasserstein autoencoder and adversarial autoencoder need a prior distribution to match. However, there is no loss imposed on the latent space to optimize for SAE. SAE does not need priors either. The object function is the reconstruction loss || x - \\tilde{x} || with the spherical  constraint shown in equation (10). It is much simpler than Wasserstein autoencoder.\n\nIn order to elucidate the unique property of random variables on spheres, we leveraged Wasserstein distance to derive Theorem 2. The Wasserstein distance here serves to establish a circumstance that the algorithm with the spherical constraint can be distribution-agnostic. We did not use Wasserstein distance for computation in SAE.  \n\nBoth Wasserstein autoencoder and adversarial autoencoder are very interesting and inspiring algorithms. We like these two works very much.\n\n\nQ2: “the image quality of VAE (CelebA) is not that bad in other VAE papers, maybe tuning the \\beta-VAE can also achieve the same quantitative and qualitative results.”\nQ3: “Can you visualize the latent space (z) for the CelebA dataset, also comparing with the results from VAE?”\nA2: For data factors, the image quality of VAE depends on the image size and the image diversity. For face images, the large image size and more backgrounds in the image will make the data difficult to fit. We used the more challenging data of FFHQ available at  https://github.com/NVlabs/stylegan and the image size we used is 128x128. \n\nA3: We are conducting the experiment on CelebA of size 64x64 according to your advice. We will update the results when this complementary experiment is completed.\n", "This paper proposed an interesting idea that by regularizing the structure of the latent space into a sphere, we can free VAE from variantional inference framework.\n\nHowever, here are several concerns about this paper:\n1. is this a variant of the Wasserstein auto-encoder?\n2. the image quality of VAE (CelebA) is not that bad in other VAE papers, maybe tuning the \\beta-VAE can also achieve the same quantitative and qualitative results.\n3. Can you visualize the latent space (z) for the CelebA dataset, also comparing with the results from VAE?", "Summary\n\nThis paper considers the L2 normalization of samples “z” from a given prior p(z) in Generative Adversarial Netowks (GAN) and autoencoders. The L2 normalization corresponds to projecting samples onto the surface of a unit-hypersphere. Hence, to attempt to justify this normalization, the authors rely on some already established results regarding high dimensional hyperspheres. In particular, the focus is on the fact that, the Euclidean distance between any given point on a hypersphere and another randomly sampled point on the hypersphere tends to a constant, when the number of dimensions goes to infinity. This result is then used to show that the Wasserstein distance between two arbitrary distributions on a hypersphere converges to a constant when the number of dimensions grows. Based on this result, the authors claim that projecting the latent samples onto the surface of a hypersphere would make GAN less sensitive to the choice of the prior distribution. Moreover, they claim that such normalization would also benefits inference, and that it addresses the issue of variational inference in VAE.\n\nMain comments. \n\nThis paper is hard to follow and requires substantial improvements in terms of writing, owing to several grammatical and semantic issues. Moreover, there is a lack rigor; some important claims are supported neither by experiments nor by theoretical analysis. Experiments in the main paper are also weak. I can therefore not recommend acceptance. My detailed comments are below.\n\n- An important claim in this paper is that the proposed approach “alleviates variational inference in VAE”. However, this requires clarification as well as theoretical/empirical justifications.\n- In the introduction, it is stated that generated samples from VAE may deviate from real data samples, because “the posterior q(z|x) cannot match  the prior p(z) perfectly”. However, in VAE we do not expect the posterior to match the prior perfectly, as this would result in useless data representations or inference. Generation issues in VAE may rather be explained by the fact that, in this context we optimize a lower bound on the KL-divergence between the empirical data distribution and the model distribution. The latter objective does not penalize the model distribution if it puts some of its mass in regions where the empirical data distribution is very low or even zero.\n- Theorem 2 (on the convergence of the Wasserstein distance (W2) on high dimensional hyperspheres) does not seem to hold if, for instance, P and P’ are empirical distributions with overlapping supports. Further, even when the above Theorem holds, the W2 distance may be relatively high since it is proportional to the square root of the number of samples.\n- Moreover, why and how would Theorem 2 justify improved inference when projecting latent samples onto a hypersphere?\n- Please consider revising the following statement in the introduction: “The encoder f in VAE approximates the posterior q(z|x)”. The encoder “f” in VAE parametrizes the variational posterior.\n- Some typos,\n\t- Abstract, “… by sampling and inference tasks”  -- “on sampling …”\n\t- Introduction second paragraph after eq 2. “… it also causes the new problems” – “ … causes new problems”\n\t- Section 2.1, “For convenient analysis …” – “For a convenient …” \n\t- Second paragraph after Theorem 1. “… perform probabilistic optimizations … ” – “… optimization …” \n\t- Section 5.2, second paragraph. Is it Figure 9?\n\nThe main recommendations I would make are as follows.\n- Consider revising the paper to improve its writing.\n- Provide rigorous theoretical analysis and discussions to support the main claims. \n- Improve experiments by including more datasets and baselines (e.g., hyperspherical VAE [1]), as well conduct more targeted experiments to give more insights regarding the effect of the L2 normalization on inference and generation. \n\n[1] Davidson, Tim R., et al. \"Hyperspherical variational auto-encoders.\" UAI, 2018.\n", "This paper proposes a novel autoencoder algorithm, named Spherical AutoEncoder (SAE). In this paper, the authors argue that the sphere structure has good properties in high-dimensional. To leverage the properties, proposed algorithm centerizes latent variables and projects them onto unit sphere. To show the empirical performance of the proposed approach, the authors perform image reconstruction and generation using FFHQ dataset and MNIST dataset.\n\nComments:\nI think the proposed approach, using spherical latent space, is interesting and make sense.\n\n- As mentioned in section 3.2, the proposed algorithm is reduced to standard autoencoder since it is free from posterior inference. Then, to clarify the algorithm, it seems necessary to provide the formulation of objective functions.\n- Is the objective still valid or reasonable even it is derived from the equation (10) without posterior inference?\n- How does the objective change when centerization and spherization are applied to the GAN?\n- Compared with using von Mises-Fisher distribution in the vanilla VAE, the advantage of the proposed method is not clear. To my understanding, the main difference seems to be whether using lower bound with posterior inference or deterministic framework without such approximation. However, there are no theoretical or empirical results to show the benefit of the proposed method. If theoretical or empirical results with reasonable intuition is provided, it will make the proposed algorithm more valuable.\n\nQuestions:\n- Compare to ProGAN and StyleGAN, is the contribution of the paper to applying centerization to GAN and centerization and spherization to autoencoder?\n- What dimension do you use as latent dimension in the experiments?\n- Does the choice of prior distribution affect the experimental results? If so, is there any compatible reason with the intuition of SAE?\n\nTypo:\nUnder equation (10) in page 5: \\tilde{z} should be \\hat{z}.\n", "Thanks for your very insightful comments, Alex.\n\n1) About covering\n\nThe case you raised is really challenging. We choose the vector centerization to afford randomness on the sphere. But how well this operation enforces z_i to cover the spherical surface as uniformly as possible is an important topic to study for spherical autoencoder (SAE).\n\nThe vector centerization presented in our paper does have flaw. For example, the points on the spherical surface falling into the open positive orthant (z_i > 0) cannot be sampled. Considering that there are all 2^512 orthants in R^512, however, the open positive orthant only takes 1/2^512 part of the whole sphere. So, the negative effect is nearly trivial.\n\nWe also figured out another way of sampling on the sphere to circumvent this problem. For any {z_1,...,z_i,...,z_n} drawn from arbitrary distributions, we can first project them on the sphere by z_i <-- z_i/norm(z_i). The projected points probably lie on some specific regions on the sphere.  Then we can randomly rotate these points on the sphere by a series of orthogonal matrices that are obtained by orthogonalizing random matrices via Gram–Schmidt process. In this way, we can get {z_1,...,z_i,...,z_n} that distributes randomly on the sphere as long as the rotation manipulations are sufficient.\nHowever, this method is not friendly to end-to-end learning for autoencoder. We do not use it in this paper.\n\nThe vector centerization and spherization is the simplest way we get to realize our idea, even though it is not perfect. What is most important is that it is very easy to use in the end-to-end architecture of autoencoder.\n\n2) About inductive bias\n\nTheorem 2 tells that SAE is distribution-agnostic with respect to Wasserstein distance. In other words, it has distributional inductive bias. However, it is very inspiring about your conjecture \"Perhaps the inductive bias of the neural network makes this type of issue unlikely\".\n\nActually, your conjecture leads to the connection between random variables on the sphere and universal approximation theorem (UAT).  We also think that this is an alternative way of further exposing the deep reason why the simple spherical constraint can outperform the traditional variational inference. You may refer to the following paper, if interested.\n\nSpherical approximate identity neural networks are universal approximators\n Zarita Zainuddin, Saeed Panahian Fard\nICNC, 2014\n\nYour thoughts are quite inspiring. We will consider the topics you raised seriously for our future work.", "I think this is an interesting paper and I was quite impressed by the results and elegance of the approach.  I have some thoughts about it from a conceptual point of view though.  \n\n1.  I'm curious if the SAE loss going to zero guarantees good samples in a theoretical sense.  I'm not sure if this is the case because during training the z's are always projected onto the sphere, but there is no requirement that they cover all of the points on the sphere.  So I could imagine a way of packing all of the z's seen during training into a small region on part of the sphere, having these points decode well, and then having all of the other regions decode to bad points.  \n\nPerhaps the inductive bias of the neural network makes this type of issue unlikely - in either case it makes it interesting that it seems to work so well.  (if it's the case it reminds me a bit of the cyclegan, where there is technically a way for the model to do something bad, but it doesn't happen as a result of the inductive bias from the architecture).  \n\nI think I have a particular construction that you might find interesting.  Let’s say that each real data point is binary, for example x in N^784 (as with binary MNIST).  I can encode this digit in a single number xb by laying out the digits: 0.0011010…1 (with each decimal point corresponding to that pixel position).  \n\nNow let c = sqrt((1 - 2*xb^2) / 2)\n\nThen let’s say z = [xb, -xb, c, -c].  \n\nRegardless of xb, so long as it is between (-sqrt(0.5), sqrt(0.5)), this z will be centered and on the sphere.  I realize that this is extremely unlikely to be learnable by a NN, especially due to smoothness and its inductive biases.  However I still think it's at least interesting to think about.  \n\n2.  I think one reason SAE might work so well in practice is due to the asymmetry in the KL-divergence in the VAE objective, where you have KL(q(z|x) || p(z)).  It becomes unbounded and large if q(z|x) ever has support but p(z) doesn't have support.  By pushing q(z | x) onto the sphere, and because samples from p(z) are essentially always on the sphere, you guarantee that the KL is at least bounded.  In practice this might be enough to make KL(q(z|x)||p(z)) sufficiently small, especially because the SAE doesn’t have any incentive to concentrate the encoded points in particular parts of z-space, even though it hypothetically could.  "], "review_score_variance": 5.5555555555555545, "summary": "This paper proposes to improve VAE/GAN by performing variational inference with a constraint that the latent variables lie on a sphere. The reviewers find some technical issues with the paper (R3's comment regarding theorem 3). They also found that the method is not motivated well, and the paper is not convincing. Based on this feedback, I recommend to reject the paper.", "paper_id": "iclr_2020_rJx2slSKDS", "label": "train", "paper_acceptance": "reject"}
{"source_documents": ["Top-$k$ predictions are used in many real-world applications such as machine learning as a service, recommender systems, and web searches. $\\ell_0$-norm adversarial perturbation characterizes an attack that arbitrarily modifies some features of an input such that a classifier makes an incorrect prediction for the perturbed input. $\\ell_0$-norm adversarial perturbation is easy to interpret and can be implemented in the physical world. Therefore, certifying  robustness of top-$k$ predictions against $\\ell_0$-norm adversarial perturbation is important. However, existing studies either focused on certifying $\\ell_0$-norm robustness of top-$1$ predictions or  $\\ell_2$-norm robustness of top-$k$ predictions. In this work, we aim to bridge the gap. Our approach is based on randomized smoothing, which builds a provably robust classifier from an arbitrary classifier via randomizing an input. Our major theoretical contribution is an almost tight $\\ell_0$-norm certified robustness guarantee for top-$k$ predictions. We empirically evaluate our method on CIFAR10 and ImageNet. For instance, our method can build a classifier that achieves a certified top-3 accuracy of 69.2\\% on ImageNet when an attacker can arbitrarily perturb 5 pixels of a testing image. ", "This paper provides an almost tight l0-norm certified robustness guarantee for top-k predictions against adversarial perturbations, which extends certified radius of the top-1 prediction from Levine & Feizi (2019) to that of the top-k predictions, and the l2-norm certified radius from Jia et al. (2020) to the l0-norm certified radius. The experiments on CIFAR10 and ImageNet show that the proposed method substantially outperforms state-of-the-art for top-k predictions. In total, this is a good work both in the theoretical perspective and the experimental perspective. The authors provide a solid proof on the l0-norm certified robustness guarantee for top-k predictions, which is different from previous works of Levine & Feizi (2019) and Jia et al. (2020). The detailed experiments show its consistent improvement in certified top-k accuracy on CIFAR-10 and ImageNet and the ablation study provides a view on how these hyperparameters affect the performance. The author derives an almost tight l0-norm certified robustness guarantee of top-k predictions against adversarial perturbations for randomized ablation. The corresponding theoretical analysis show both the l0-norm certified radius and the tightness. The empirical results demonstrate that the l0-norm certified robustness is substantially better than those transformed from l2-norm certified robustness. ", " Thanks for the review and insightful comments. \n\nThank you for your suggestions. We conduct experiments for 3D deep learning. In particular, we evaluate our methods on the ModelNet40 benchmark dataset and use PointNet as the model. We set $e=16$, $n=10,000$, and $\\alpha$=0.001. When the number of modified points is 10, 20, 30, 40, and 50, the certified accuracies for top-$1$ prediction are 0.779, 0.743, 0.700, 0.649, and 0.570; and the certified accuracies for top-$3$ prediction are 0.901, 0.867, 0.829, 0.803, and 0.775. We have added the discussion to Section C in Appendix. \n", " Thanks for the review and insightful comments. \n", " Thanks for the review and insightful comments.\n\n1. We have revised this part as suggested. Please refer to our revised paper for details. \n\n2. Thanks for pointing this out. We have clarified that the certified radius is (almost) tight is true when no assumption is made on the base classifier.\n\n3. Sorry for the confusion. Theoretically, our method is the same or better than Levine & Feizi + SimuEM. We found that our method has the same certified accuracy as Levine & Feizi + SimuEM on CIFAR10 and ImageNet datasets. So we didn’t show the results for our method in the tables. We have added them to Table 1 and 2 in our revised paper. \n\n4. We note that the method in Chiang et al. (2020) is only applicable for top-$1$ prediction. We compare with the method on the CIFAR10 dataset for top-$1$ predictions using the publicly available code. The results are as follows. When the number of perturbed pixels is  1, 2, 3, 4, and 5, the certified accuracies for our method are respectively 0.746, 0.718, 0.690, 0.660, and 0.636. In contrast, the certified accuracies of Chiang et al. are 0.400, 0.369, 0.342, 0.312, 0.308. As the results show, our method is better than Chiang et al. (2020). We note that our certified robustness guarantee is probabilistic while Chiang et al. (2020) can give a deterministic certified robustness guarantee. We have cited the paper in related work and have included experimental results in Section D in Appendix. \n", " Thanks for the review and insightful comments.\n\n1. The key challenge in proving the tightness of certified robustness guarantee is that we are not able to find regions in the discrete space that satisfy certain conditions. The reason is that we cannot arbitrarily divide the discrete space into different regions. To address the challenge, we propose to relax the conditions when finding the regions to prove that the certified robustness guarantee is almost tight. The idea in our proof is very general and we hope our proof can inspire future research in proving the (almost) tightness for $\\ell_0$-norm certified robustness guarantee. We have added the discussion to the end of the proof of our Theorem 2 in the Appendix.\n\n2. We compare with the methods that are designed for $\\ell_2$-norm certified robustness because we aim to show that it is suboptimal to derive $\\ell_0$-norm certified robustness for top-$k$ predictions by leveraging the relationship between $\\ell_2$-norm and $\\ell_0$-norm. We have clarified this in our revised paper. \n", "In this paper the authors extend the tightness guarantees of other works on randomized smoothing to the $\\ell_0$ case and also tighten the guarantees given by previous works by noting the discrete nature of the predictive distribution induced by randomized ablation. In addition, they extend previous $\\ell_0$ methods to certify top $k$  predictions rather than just top $1$ predictions which is typically the case for smoothing.  There are several key pros of the paper that contribute to my favorable opinion. In particular, the authors do a good job of highlighting recent and related works and ensuring that readers understand the contribution they are making relative to those works that it builds on. Further, the advances that the authors suggest in terms of improving tightness are stated in an intuitive way which allows for greater appreciation of the relatively simple modification they make. Finally, I think the introduction of top $k$ predictions rather than top 1 prediction is an interesting contribution which extends the applicability of randomized ablation (i.e. $\\ell_0$ smoothing). \n\nThe primary cons of this paper are in its potentially incremental impact. I think the clear exposition of prior works may make this paper's contributions seem simple or incremental, but I think ultimately that these are important developments/extensions of prior work that constitutes a strong enough contribution for acceptance. \n\nThe second primary con (and perhaps the more noteworthy one) is the applications that this method is tested on. It is rare for the $\\ell_0$ metric to be tested on images in practice. However, for graph neural networks and 3D deep learning this is a primary threat model. I think it would greatly strengthen the paper if such networks were certified and would add yet another point of contribution for this paper. \n\n\n Though one might see the contributions of this paper as incremental over other works in smoothing, I think these are valuable and important developments in the field of smoothing which can enable further applications to gain guarantees via smoothing. ", "The paper proposed a tighter L0 verification method for Top-K predictions of classifiers based on randomized smoothing. I appreciate the theoretical analysis and the impressive empirical results in the paper. \nConcerns and questions:\n* The \"Deriving the certified radius for the smoothed classifier\" sections is rather difficult to understand. I recommend the authors to reorganize this section and define the notations separately in a clearer way.\n* The paper states that for $k=1$ the certified radius is tight. But this statement is only true if there is no assumption on the base classifier is made, and this is not true for a given base classifier, therefore this statement may be overclaimed.\n* In Table 1&2, the comparisons of Top-1 prediction for 'Our methods' are missing.\n* I recommend the author to compare with another L0 verification methods like https://openreview.net/pdf?id=HyeaSkrYPH.  I appreciate the theoretical analysis and the impressive empirical results in the paper but the written is not clear. I also have some questions on the experiments.", "The paper provides both theoretical progress and experimental results on certified robustness of top-$k$ predictions. Specifically, on the theory side, the paper shows that the randomized ablation (Levin & Feizi) has $\\ell_0$-norm certified radius of top-$k$ predictions. Also, the paper proves that the certified radius is tight for k = 1 and almost tight for k > 1. (The certified radius cannot be larger than “the derived radius plus 1”.) On the experiment side, the paper compares the proposed method to existing competitors and explored the impact of the related parameters. [Strength]\n1.\tThe writing is concise and easy to understand in general.\n2.\tThe paper makes nontrivial theoretical progress. Also, the authors provided enough explanation about how this progress can be distinguished from other existing works, so the contribution looks to be unique.\n\n[Weakness]\n1.\tI don’t see any severe weakness (to reject this paper) in this work.\n\n[Comments]\n1.\tI enjoyed reading this paper, and I think that the approach is “kind of good”. For the following reasons, I’d not put a score of 8 for this paper.\nA.\tThe suggested proof approach is not easily generalizable, so it would be hard to say that this is a major discovery in the theory of certified robustness. (If possible, some discussion on the possible uses of the proof strategy would make the paper better.)\nB.\tI might have missed some potential weaknesses that I should have found. For this comment, I want to discuss with other reviewers during the review process.\n2.\tI can understand how the authors compared their method ($\\ell_0$-certified robustness) to existing counterparts (some of them are $\\ell_2$-certified robustness) and I can see there are not that many possible ways to compare them. However, I’m still not convinced whether it is a fair comparison or not, and I’m not even sure whether we can directly compare those methods or not. Also, in my personal opinion, there could be better experiment questions than just showing “our method is excelling other methods”.\n I vote for accepting. First, the paper proved a nontrivial result about the certified robustness of top-$k$ predictions. Also, the paper provided enough justifications for its unique contribution to the defenses using randomized smoothing & randomized ablation type, so I believe this paper can be distinguished from other existing works. While I believe that the work itself is publishable, I’m a little bit reluctant to give a score of 8 for this paper yet due to the reasons I provided in Comments."], "review_score_variance": 0.1875, "summary": "Thank you for your submission to ICLR.  The reviewers ultimately have mixed opinions on this paper, but reading in a bit more depth I don't feel that the critical comments raised by the sole negative reviewer really raise valid points.  Specifically, the fact that this reviewer directly asks e.g. for comparisons to Levine and Feiz 2019, when the paper (before its revisions) contains an entire section devoted to exactly this comparison, strikes me as not sufficient for a thorough review.\n\nHowever, while I'm thus going to recommend the paper for acceptance (it does present a notable, if somewhat minor, advance upon the state of the art in randomized smoothing), I also feel the paper is generally rather borderline for more straightforward reasons.  Specifically, given the _very_ narrow focus of the proposed improvements (improvements to the bounds of randomized smoothing, for L0 perturbations, for Top-k accuracy), I ultimately don't think the paper presents that significant an advance in the field.  The paper could go other way, thought definitely not doing so due to the issues that the sole critical reviewer takes.", "paper_id": "iclr_2022_gJLEXy3ySpu", "label": "test", "paper_acceptance": "Accept (Poster)"}
