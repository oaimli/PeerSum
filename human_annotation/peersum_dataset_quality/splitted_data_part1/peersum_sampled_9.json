{"source_documents": ["The efficacy of the width of the basin of attraction surrounding a minimum in parameter space as an indicator for the generalizability of a model parametrization is a point of contention surrounding the training of artificial neural networks, with the dominant view being that wider areas in the landscape reflect better generalizability by the trained model. In this work, however, we aim to show that this is only true for a noiseless system and in general the trend of the model towards wide areas in the landscape reflect the propensity of the model to overfit the training data. Utilizing the objective Bayesian (Jeffreys) prior we instead propose a different determinant of the optimal width within the parameter landscape determined solely by the curvature of the landscape. In doing so we utilize the decomposition of the landscape into the dimensions of principal curvature and find the first principal curvature dimension of the parameter space to be independent of noise within the training data.", "Q8) On the other hand, as the authors used the spectrum properties of the Fisher information matrix, there are some recent works by Amari which can be cited.\n\nA8) Based on your suggestion, we found the paper \"Pathological spectra of the Fisher information metric and its variants in deep neural networks\" by Karakida, Akaho and  Amari [2]. This is very exciting work which we will now include in this paper. Thank you for this helpful recommendation. Unfortunately this paper was only uploaded to ArXiv on the 14th October 2019 (after this conference's deadline) and as a result we could not include it in our original submission. None the less, this is a welcome opportunity to further contextualize our work in this paper.\n\n[1] Zhang, Yao, et al. \"Energy–entropy competition and the effectiveness of stochastic gradient descent in machine learning.\" Molecular Physics 116.21-22 (2018): 3214-3223.\n[2] Karakida, Ryo, Shotaro Akaho, and Shun-ichi Amari. \"Pathological spectra of the Fisher information metric and its variants in deep neural networks.\" arXiv preprint arXiv:1910.05992 (2019).", "We thank the reviewer for their constructive feedback.\n\nQ1) On the point of the poor writing quality.\n\nA1) We apologize for this and are working hard to improve the literary standard of the paper.\n\nQ2) In eq.(1), the authors equate the Fisher information matrix (which is an expected Hessian) to the Hessian matrix, this is subject to conditions which must be clearly given right before/after the equation.\n\nA2) Thank you for pointing this out. We will correct this error in the updated version.\n\nQ3) In the first equation in A.1, what is the subindex \"j\", \"Utilizing Laplace Approximation of the integral\": such approximations have conditions that must be clearly stated.\" and \"It is not clear how one can get the last approximation in page 12 from the previous equations.\"\n\nA3) The merits of the Laplace Approximation are discussed in [1]. We are adapting the discussion from [1] for the updated version of Appendix A. We are in the process of improving the general quality of Appendix A, including the discussion on the assumptions of the derivation and making the link between certain steps in the derivation more explicit. Thank you for the feedback and assistance in improving this Appendix.\n\nQ4) As a theoreiritical contirbution, the authors did not manage to converge to some simple and clear statements (theorems or equvalent).\n\nA4) We believe Reviewer #2 provides an excellent summary statement, and one which we have included in the paper. Namely, \"The authors provide theoretical arguments and claim that there exists an optimal width beyond which generalization can be poor\".\n\nQ5)  It is hard to observe anything new, given the poor writing and organization.\n\nA5) We apologise if this was unclear and have made this clearer throughout the paper. In addition, we point to the last paragraph of the Introduction beginning at the bottom of Page 1 where we outline what we perceive to be our 3 main contributions. In summary:\n1) We reflect that a correlation exists between energy and entropy as opposed to a competition or trade-off as was first presented in [1].\n2) We reflect that an optimal level of curvature exists within the\nlandscape which does not necessarily occur at the point in the landscape with the least curvature. We provide the novel perspective that the propensity of the model to find points of minimal curvature is a direct result of the model's propensity to overfit the training data.\n3) We show that at the point in the landscape which corresponds to the Jeffreys prior the test error of the model reaches its minimum value and at this point the dimension of principal curvature of the model is at its maximum entropy. In doing so we also reflect the noise invariance of the dimension of principal curvature.\n\nQ6) The first 4 pages are mainly introductions of previous works.\n\nA6) We acknowledge that our work does rely heavily on past work and provides a detailed exposition of these past works, however, we view this as being necessary as we utilize a number of different field in this work. Namely Objective Bayes statistic, Information Theory, Differential Geometry and Machine Learning. We believe it necessary to not only provide sufficient background information for each field separately but also to illustrate the necessary overlap of the different concepts in these field for the full impact of this work to be seen. For example, a reader who is aware of the Jeffreys prior from an Objective Bayesian perspective may be unaware of its use as a right Haar measure in Differential Geometry. Thus, we aim to reflect the key fact that the Fisher Information, and as a result the Jeffreys Prior, is the commonality between the fields and guides our argument from the Objective Bayesian perspective of Section 3 to the Information Geometry perspective in Sections 4 and 5. We are, however, working at reducing the excess information in the paper, such as the overlap between the MDL property and Bias-Variance Dilemma, and restructuring aspects of our arguments to ensure that our contributions are clearer.\n\nQ7) The authors used information geometry and minimum description length to explain the generalization of deep learning. This is a small area. It is hard to miss closely related works by simple searching. Instead, the authors only cited Rissanen (1978).\n\nA7) Given the fact that the Minimum Description Length Principle can be equally phrased in light of the Bias-Variance trade-off which is also discussed in this work we see this as an opportunity to reduce the length of this work closer to 8 pages in line with the Conference standards and will, thus, rephrase our argument more in term of the Bias-Variance trade-off.\n\n", "We thank the reviewer for their constructive feedback, and kind words.\n\nQ1) In Figure 1 values outside the optical cluster at 0.0 appear nonetheless. I am not sure how to judge the amount of spread I see, and what effect they have on the performance of the network.\n\nA1) The topic of Figure 1 and our experimental procedure was raised also by another reviewer and, thus, we answered this in a general comment: \"General Comment on Previous Experimental Procedure\". Please see this above, however, in summary this is due to the fact that networks will learn both true signal from the data as well as noise simultaneously while training. This distorts the results of our experiments when a significant degree is modelling early in training.\n\nQ2) In general I would like to see experiments on datasets and with architectures that are at least somewhat close to what people use in practice (at least in terms of the size of the task and the capacity of the net).\n\nA2) We are updating our experimental procedure to be more general and include larger models. We have made a general comment describing the new procedure above, titled: \"General Comment on Updated Experimental Procedure\". Thank you for this suggestion. We would value any further feedback or suggestions you may have for the new procedure. ", "We are grateful for the time taken by the reviewers in helping to improve the quality of this paper and our work. The most common concern raised was that our experimental procedure was not sufficiently exhaustive to provide a compelling case for our theoretical arguments. We agree with these comments and in some cases the concerns raised with our experimental procedure are necessary and we aim to clarify their necessity in this general comment. We do, however, acknowledge that a more general and realistic experimental procedure was required. We have, thus, updated our experimental procedure and discuss this in the second general comment below. \n\nFigure 1:\nThe most common concerns appear to be as a result of Figure 1. Thus, we will provide a brief summary of this Figure and clarify some terminology, such as \"the intersection of the likelihoods\", which were raised by Reviewer #2. The purpose of Figure 1 is to reflect the number of training steps between where the network achieves its minimum test set error and where the Maximum A Posteriori parametrization for the network using the Jeffreys prior is found (for brevity we will call this the Jeffreys prior parametrization). From the discussion on Page 8, and in particular using Equation 10, we reflect that the Jeffreys prior parametrization will result in the likelihood of the training neural network generating the training data and the likelihood of the true data distribution generating the training data being equal. In other words the Jeffreys prior results in a model with the same training error or variance as the true data distribution.\n\nThere are, however, two means by which the training network may reduce its error and increase its likelihood. Firstly it can model the true signal in the data, and secondly it can model the noise in the data. In reality, while training, the model will learn both noise and signal simultaneously. Conceptually if the model were to learn pure signal only it would model the true distribution identically, achieve a test error of $0$ (as no noise was added to this data) and obtain a training error equal to that of the true distribution. The model would then proceed to reduce training error by modelling the only information remaining, namely the noise and we would then see an increase in the test error. Unfortunately a normal training procedure is not as separable and as a result the network will model noise before learning all of the true signal in the data. Depending on the relative quantity of noise to signal being modeled two cases will occur. Case 1: The quantity of noise learned will dominate the true signal learned. In this case we will see the test error reach its minimum prior to the likelihoods intersecting. Case 2: The degree of true signal being learned dominates the degree of noise being learned. In this case the test error will continue decreasing slowly and, since true signal remains to be learned when the likelihoods intersect, the test error will be minimized after the likelihoods intersect. In summary, the Jeffreys prior parametrization equates the model and true distribution likelihoods. The fact that modelling the noise also increases the model likelihood distorts the point where the minimum test error is found. In cases where high quantities of noise are learned early in the training we observe the points outside the optical cluster at 0.0 (as well as an observably higher test error) as observed by Reviewer #2. We believe the symmetry of the density estimation of Figure 1 to be the main highlighting factor reflecting the fact that equating the likelihoods and using the Jeffreys prior parametrization results in minimum test error. In essence the difficulty of separating noise from true signal in data is the problem of overfitting, and one we aim to address in future work. We do, however, believe a contribution of this work to be the novel theoretical placement of where the minimum test error can be obtained in the loss landscape.", "We thank the reviewer for their constructive feedback.\n\nQ1) The authors should discuss the architecture design choices used for the synthetic data-generating model.\n\nA1) In light of the requests of Reviewer #3 we have updated our experimental procedure to be more general and utilize larger networks with more variance in their design. Please see the general comment on our updated experimental procedure above, titled \"General Comment on Updated Experimental Procedure\", as this was also raised by another reviewer. We will be more explicit about our design choices in the updated version of the paper and agree that this requires more discussion. Our aim, however, with the generating model was to create a complicated function for the training network to model. As a result we began with non-linear sigmoidal layers to create a complex function. The linear layer in the output on the generating model was then used to obtain the scalar output for the regression task. In our updated experimental procedure we utilize more general and larger generating networks. We also discuss this aspect in the general comment on our updated experimental procedure. \n\nQ2) Why are the last 3 layers of the larger model comprise of linear mappings?\n\nA1) This was merely to over-parametrize the model. Naturally any consecutive linear layers can equally be compressed into a single layer, however, the addition of more linear layers does increase the expressive power of the model and aids in overfitting. The impact of this design decision on the loss landscape is evident in the work on alpha-scaling [1] in which it is shown that by placing more weight on one layer of linear parameters while proportionally decreasing the weight on the following linear layer it is possible to move to an area in the landscape with different width but without changing the model behaviour. This is a direct result of the linear layers being over-parametrized and when parameter weight is spread over more linear layers wider landscapes will occur. In line with our work, we believe the wider areas to overfit more and, thus, the inclusion of more linear layers will help enforce that the training network overfits the training data.\n\nQ3) Fig 1 is not clear. What does n=23 signify in the caption?\n\nA3) n represents the number of datapoints, separate trainings run, in generating the figure. We will include this in the caption.\n\nQ4) More discussion is needed to describe \"intersection of the likelihood values\", \"Difference in update Step\" and \"density is placed around 0\" in section 5.\n\nA4) Thank you for pointing this out. We will expand on these points in the paper. We have elaborated on these points in another general comment above: \"General Comment on Previous Experimental Procedure\". In summary, however, we use the phrase \"intersection of the likelihood values\" to express the point at which the training network has the same error as the true data generating network on the noisy training data. We believe this to be the point at which the Jeffreys Prior parametrization is found in the loss landscape. To test our assertion that the Jeffreys Prior parametrization provides the optimal test performance we observe the number of parameter updates between where the Jeffreys Prior parametrization is found and where the minimum test error is found. We referred to this as the \"Difference in update step\". We then plot a histogram and kernel-density estimation (KDE) of the difference in update step from repeated trainings. We found a significant portion of the KDE was situated around the difference in update step of $0$ and said that \"the density is placed around 0\".\n\n[1] Dinh, Laurent, et al. \"Sharp minima can generalize for deep nets.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.", "We are grateful for the time taken by the reviewers in helping to improve the quality of this paper and our work. Further, we acknowledge that the experimental networks in the original submission were too small and as a result we are obtaining experimental results on larger network architectures for Figure 1, with more variety in depth, width and the activation function used. We will update Figure 1 with these new results in the updated version. The new procedure is as follows. We create a randomly generated True network with depth between $5$ and $15$ layers. The widths of the model layers are randomly sampled between $5$ and $100$ neurons. The layers are sorted in descending order of width (so we have no encoder-type layers). We then prepend the $100$ neuron input layer and append the $1$ neuron output layer. At the moment all layers except the last are sigmoidal. This is the model used to *generate* data. We then randomly initialize our training network. The number of layers in this network is randomly chosen from the range of $[True Network Size+5, 25]$ to ensure we obtain a sufficiently large network to overfit the data. The widths of this network's layers are sampled from the range of $[True Networks Smallest Layer, 100]$. This is again to ensure the model is over-parametrized. The True networks parameter values as well as the Training networks initial values are sampled uniformly from $[-1.0, 1.0]$ with a random positional bias added to the True network parameters in the range of $[-0.5, 0.5]$. This bias is to ensure the Training network starts with a significant degree of error. Finally, we utilize randomly sampled values between $[0.0, 1.0]$ as input to the models, with a training batch size of $50$ datapoints and a test batch size of $500$. This data is input to the True network and we obtain the corresponding data labels as output. Lastly we add Gaussian noise to the Training data only (while the Test data remains clean) with a mean of $0$ and variance of $0.2$. The Training network is then trained to model the True network using this data and we observe the points where their likelihoods are equal and where the test error is minimized.", "We are in the process of running experiments on considerably larger networks, as requested by both Reviewer #2 and Reviewer #3, the details of which have been posted in another general comment below. While our new results are consistent with those provided in the original work, it appears the problems highlighted above were as a result of the training model being under-parametrized to fully learn the true signal and the noise. Thus, a trade-off occurred. The deeper models appear to be giving better results. The use of synthetic data is, however, still necessary for our experimental procedure. As stated above, we need to determine the point where the model variance is equal to the true distribution variance on training data (point where the likelihoods are equal). In all real-world datasets such ground truth information is not obtainable. Thus, the synthetic data afforded us the ability to precisely determine where the intersection of the likelihoods occurred, as well as the point where a minimum was reach in the error on the very large test set.\n\nFigure 2 and Figure 3:\nThese experiments relying on the calculation of the Hessian matrix, however, are only computationally feasible (at least by our hardware constraints) on the smaller network. This is due to the Hessian of the network being an exceptionally large matrix for even a small network. Naturally there are techniques which aim to mitigate the size of the Hessian. These are, however, approximations and we believe the trade-off of using a smaller network to calculate the full and precise Hessian for our results as being worth-while.", "This paper targets at a deep learning theory contribution based on information geometry. This contribution is tightly based on Zhang et al. (2018) and explains the generalization of deep learning from a Bayesian perspective. The main contribution the authors claimed is an optimal degree of curvature exist which gives the best generalization guarantees, which is in contrast to the commonly perceived \"the wider the better\".\n\nFirst of all, the writing (including language etc) is of poor quality, to the extent that the submission is very difficult to read and can be rejected merely based on this, with unusual expressions, missing  punctuations, super long sentenses, and wongly used words. The reviewer won't list example here because they are everywhere.\n\nWhat is even worse is the conceptral errors and defected derivations. For example, in eq.(1), the authors equate the Fisher information matrix (which is an expected Hessian) to the Hessian matrix, this is subject to conditions which must be clearly given right before/after the equation. As their results are largely based on the correctness of eq.(2), let's examine the derivations in appendix A.1.  In the first equation in A.1, what is the subindex \"j\"?  \"Utilizing Laplace Approximation of the integral\": such approximations have conditions that must be clearly stated. It is not clear how one can get the last approximation in page 12 from the previous equations. In summary, their eq.(2) is a loose approximation which is subject to a set of conditions (that are not given), and the derivation is of poor quality.\n\nAs a theoreiritical contirbution, the authors did not manage to converge to some simple and clear statements (theorems or equvalent). Instead, the contribution is largely *explanatory*. It is hard to observe anything new, given the poor writing and organization. The first 4 pages are mainly introductions of previous works.\n\nThe authors used information geometry and minimum description length to explain the generalization of deep learning. This is a small area. It is hard to miss closely related works by simple searching. Instead, the authors only cited Rissanen (1978). On the other hand, as the authors used the spectrum properties of the Fisher information matrix, there are some recent works by Amari which can be cited.", "The paper argues that the widest minimum in the loss landscape is not the best in terms of generalization. The authors provide theoretical arguments and claim that there exists an optimal width beyond which generalization can be poor. Synthetic simulations are presented to support these claims.\n\nThe authors employ Fisher Information to characterize the optimal width or the curvature around the minimum. The fact that the determinant of the Fisher Information Matrix is invariant to parametrization, under certain conditions, serves as the motivation to design an objective Bayesian prior called Jeffrey's prior. \n\nThe motivation and the theoretical arguments are interesting, but the paper lacks in presentation and sufficient empirical evidence is also lacking to get fully convinced by the claims. \n\nThe authors should discuss the architecture design choices used for the synthetic data-generating model. Why are the last 3 layers of the larger model comprise of linear mappings?\n\nFig 1 is not clear. What does n=23 signify in the caption? More discussion is needed to describe \"intersection of the likelihood values\", \"Difference in update Step\" and \"density is placed around 0\" in section 5.\n\n\n", "The paper conjectures that the so called Jeffreys prior over the parameters of a neural network is the prior leading to the best generalization performance. The authors test this conjecture on an artificial task using a small neural network, and investigate the sensitivity of the results to noise.\n\nI like the general idea of the paper and appreciate the very detailed exposition placing it in the context of other works. In particular, I enjoyed the summary showing the sometimes conflicting evidence for better generalization in either broader or sharper minima, and how it relates to the Jeffreys prior.\n\nHowever, as I understood the paper, the main claim in page 5 Equation 4 “Thus we conjecture that a correct prior for a model would be:” is an *assertion* that Jeffreys prior is the correct prior to use over the parameter space of neural networks. While it is a possibility, the amount of empirical evidence presented does not (at least to me) provide strong enough justification.\n\nOn page 7, you say “This model was a neural network composed of one, 5 neuron, hidden layer which utilized a sigmoid activation function in its hidden layer and a linear activation in its scalar output layer.“, describing your experiment. I don't think this experiment is sufficiently large to convince me.\n\nFurthermore, in Figure 1 values outside the optical cluster at 0.0 appear nonetheless. I am not sure how to judge the amount of spread I see, and what effect they have on the performance of the network.\n\nIn general I would like to see experiments on datasets and with architectures that are at least somewhat close to what people use in practice (at least in terms of the size of the task and the capacity of the net). That would give me more confidence that your conjecture is true. While I appreciate your detailed theoretical exposition, I think the amount of empirical evidence you provide is insufficient to back the claims. Considering the explicit instruction to judge papers exceeding 8 pages with a higher standard, I believe that the lack of a greater amount of empirical evidence is a significant deficiency of your otherwise very interesting work.\n\nI encourage you to expand this paper and resubmit to another venue -- I believe it has a great potential.\n"], "review_score_variance": 5.555555555555556, "summary": "There has been significant discussion in the literature on the effect of the properties of the curvature of minima on generalization in deep learning.  This paper aims to shed some light on that discussion through the lens of theoretical analysis and the use of a Bayesian Jeffrey's prior.  It seems clear that the reviewers appreciated the work and found the analysis insightful.  However, a major issue cited by the reviewers is a lack of compelling empirical evidence that the claims of the paper are true.  The authors run experiments on very small networks and reviewers felt that the results of these experiments were unlikely to extrapolate to large scale modern models and problems.  One reviewer was concerned about the quality of the exposition in terms of the writing and language and care in terminology.  Unfortunately, this paper falls below the bar for acceptance, but it seems likely that stronger empirical results and a careful treatment of the writing would make this a much stronger paper for future submission.", "paper_id": "iclr_2020_HygXkJHtvB", "label": "test", "paper_acceptance": "reject"}
{"source_documents": ["This paper investigates the network load balancing problem in data centers (DCs) where multiple load balancers (LBs) are deployed, using the multi-agent reinforcement learning (MARL) framework. The challenges of this problem consist of the heterogeneous processing architecture and dynamic environments, as well as limited and partial observability of each LB agent in distributed networking systems, which can largely degrade the performance of in-production load balancing algorithms in real-world setups. Centralised training and distributed execution (CTDE) RL scheme has been proposed to improve MARL performance, yet it incurs -- especially in distributed networking systems, which prefer distributed and plug-and-play design schemes -- additional communication and management overhead among agents. We formulate the multi-agent load balancing problem as a Markov potential game, with a carefully and properly designed workload distribution fairness as the potential function. A fully distributed MARL algorithm is proposed to approximate the Nash equilibrium of the game. Experimental evaluations involve both an event-driven simulator and a real-world system, where the proposed MARL load balancing algorithm shows close-to-optimal performance in simulations and superior results over in-production LBs in the real-world system.", " Dear Authors, Thank you for responding to my questions. I appreciate your detailed explanation and find the clarification helpful. The additional results also help to alleviate my concerns. I will increase my rating to Boarderline Accept.\n ", " Dear Reviewer Y8vv:\n\nThank you so much for raising the score from 3 to 6. We appreciate this valuable discussion with you, which has definitely made our paper stronger.\n\n## Wikipedia Replay\n\nIn terms of the Wikipedia replay technique, we managed to use exactly the same setup as in [1], and we have documented the implementation details in Sec C.2.3 in the supplementary materials already.\n\n## Scaling Experiment\n\nAs promised, though given limited time, we have conducted additional experiments using 6 load balancers + 20 servers testbed with different traffic rates from low to high. Under low traffic rates, when servers are all under utilized, the advantage of our proposed method is not obvious because all resources are very much over-provisioned. With the increase of traffic rates (till servers are 100% saturated), our methods outperforms the best classical methods. Please check the 4 tables below, comparing both the 99th percentile and average task completion time for both Wiki pages and static pages.\n\n### Wiki Pages - QoS - 99th percentile task completion time (s)\n| Traffic Rate (queries/s) |  731.534 | 1097.3 | 1463.067 | 1828.834 | 2194.601 | 2377.484 | 2560.368 | 2743.251 | 2926.135 |\n|---:|---:|---:|---:|---:|---:|---:|---:|---:|---:|\n| LSQ  | 0.175 +/- 0.015 | 0.212 +/- 0.025 | 0.249 +/- 0.043 | 0.342 +/- 0.121  | 0.827 +/- 0.572 | 2.103 +/- 0.654 | 10.662 +/- 2.557 | 17.656 +/- 0.714 | 17.999 +/- 0.253 |\n| SED  | 0.201 +/- 0.022 | 0.261 +/- 0.079 | 0.322 +/- 0.099 | 0.360 +/- 0.088 | 0.618 +/- 0.268 | 2.175 +/- 1.328 | 11.444 +/- 3.861 | 22.086 +/- 4.892 | 22.727 +/- 5.632 |\n| Distr-LB [VBF] | **0.160 +/- 0.010** | **0.205 +/- 0.036** | **0.248 +/- 0.086** | **0.284 +/- 0.113**   | 0.567 +/- 0.306 | **1.276 +/- 0.647** | 7.005 +/- 1.147  | 10.560 +/- 1.042 | 15.745 +/- 0.254 |\n| Distr-LB [VBF+logVBF] | 0.161 +/- 0.008 | 0.216 +/- 0.052 | 0.249 +/- 0.068 | 0.348 +/- 0.122    | **0.439 +/- 0.121** | 1.533 +/- 0.670 | **4.427 +/- 0.443**  | **9.391 +/- 0.329** | **15.347 +/- 0.572** |\n\n### Static Pages - QoS - 99th percentile task completion time (s) \n| Traffic Rate (queries/s) |  731.534 | 1097.3 | 1463.067 | 1828.834 | 2194.601 | 2377.484 | 2560.368 | 2743.251 | 2926.135 |\n|---:|---:|---:|---:|---:|---:|---:|---:|---:|---:|\n| LSQ  | 0.014 +/- 0.001 | 0.015 +/- 0.000 | 0.015 +/- 0.000 | 0.018 +/- 0.003 | 0.217 +/- 0.305 | 0.856 +/- 0.554 | 11.066 +/- 3.095 | 16.874 +/- 0.391 | 17.155 +/- 0.217 |\n| SED  | 0.014 +/- 0.000 | 0.015 +/- 0.000 | 0.016 +/- 0.001 | 0.018 +/- 0.001 | 0.071 +/- 0.066 | 1.252 +/- 1.489 | 11.272 +/- 3.975 | 21.941 +/- 5.970 | 20.708 +/- 5.423 |\n| Distr-LB [VBF] | 0.014 +/- 0.000 | 0.015 +/- 0.000 | 0.016 +/- 0.001 | **0.017 +/- 0.000** | **0.041 +/- 0.025** | **0.338 +/- 0.364** | 6.670 +/- 1.152  | 9.743 +/- 0.863  | 15.506 +/- 0.056 |\n| Distr-LB [VBF+logVBF] | 0.014 +/- 0.000 | 0.015 +/- 0.001 | 0.016 +/- 0.000 | 0.018 +/- 0.002 | 0.072 +/- 0.087 | 0.465 +/- 0.403 | **3.970 +/- 0.545**  | **8.782 +/- 0.187**  | **15.095 +/- 0.497** |\n\n\n### Wiki Pages - QoS - avg. task completion time (s)\n| Traffic Rate (queries/s) |  731.534 | 1097.3 | 1463.067 | 1828.834 | 2194.601 | 2377.484 | 2560.368 | 2743.251 | 2926.135 |\n|---:|---:|---:|---:|---:|---:|---:|---:|---:|---:|\n| LSQ  | 0.048 +/- 0.002 | 0.055 +/- 0.003 | 0.059 +/- 0.003 | 0.069 +/- 0.008 | 0.131 +/- 0.070 | 0.643 +/- 0.325 | 1.910 +/- 0.269 | 2.873 +/- 0.215 | 3.545 +/- 0.146 |\n| SED  | 0.054 +/- 0.001 | 0.061 +/- 0.004 | 0.068 +/- 0.004 | 0.080 +/- 0.004 | 0.117 +/- 0.025 | 0.660 +/- 0.396 | 1.718 +/- 0.366 | 2.767 +/- 0.207 | 3.482 +/- 0.189 |\n| Distr-LB [VBF] | 0.047 +/- 0.001 | 0.054 +/- 0.003 | 0.059 +/- 0.005 | **0.066 +/- 0.007** | 0.105 +/- 0.035 | **0.266 +/- 0.139** | 1.465 +/- 0.115 | 2.047 +/- 0.145 | 2.704 +/- 0.108 |\n| Distr-LB [VBF+logVBF] | 0.047 +/- 0.001 | 0.054 +/- 0.004 | 0.059 +/- 0.004 | 0.069 +/- 0.008 | **0.084 +/- 0.009** | 0.413 +/- 0.249 | **1.183 +/- 0.063** | **1.838 +/- 0.083** | **2.513 +/- 0.105** |\n\n### Static Pages - QoS - avg. task completion time (s)\n| Traffic Rate (queries/s) |  731.534 | 1097.3 | 1463.067 | 1828.834 | 2194.601 | 2377.484 | 2560.368 | 2743.251 | 2926.135 |\n|---:|---:|---:|---:|---:|---:|---:|---:|---:|---:|\n| LSQ  | 0.004 +/- 0.001 | 0.004 +/- 0.000 | 0.003 +/- 0.000 | 0.004 +/- 0.000 | 0.018 +/- 0.023 | 0.252 +/- 0.234 | 1.455 +/- 0.258 | 2.426 +/- 0.207 | 3.080 +/- 0.136 |\n| SED  | 0.003 +/- 0.000 | 0.004 +/- 0.001 | 0.004 +/- 0.000 | 0.004 +/- 0.000 | 0.006 +/- 0.003 | 0.284 +/- 0.308 | 1.283 +/- 0.374 | 2.322 +/- 0.226 | 3.041 +/- 0.188 |\n| Distr-LB [VBF] | 0.004 +/- 0.000 | 0.004 +/- 0.000 | 0.004 +/- 0.000 | 0.004 +/- 0.000 | **0.005 +/- 0.001** | **0.055 +/- 0.070** | 1.039 +/- 0.144 | 1.617 +/- 0.135 | 2.277 +/- 0.096 |\n| Distr-LB [VBF+logVBF] | 0.004 +/- 0.000 | 0.004 +/- 0.000 | 0.004 +/- 0.000 | 0.004 +/- 0.000 | 0.006 +/- 0.004 | 0.116 +/- 0.114 | **0.750 +/- 0.063** | **1.413 +/- 0.083** | **2.076 +/- 0.096** |\n", " Thank you for your response. I will update my score based on this discussion. \n\nI will also suggest authors to write the details of wikipedia replay technique in supplementary materials (or refer the readers to the exact section of the paper if it is exactly the same as [1]). Based on my reading of [1], section 6.c, the methodology used in that paper makes sense. ", " Dear Reviewer Y8vv,\n\nThank you so much for your reply. We are glad to see that the reviewer is happy with our additional experiments regarding QoS. We also would like to point out that we included the SOTA RL-based load balancing method from NeurIPS 2021 (RLB-SAC), whose results are shown in the revised pdf. We hope that the reviewer could merit these contributions as well.\n\n## Re: Scaling Experiments\n\nThank you for pointing out the experiments that you would like us to conduct. As mentioned in our answer to the **Common Concern 1**, the sets of experiments we conducted have the configurations that correspond to real-world setups, where servers are heavily loaded. We have shown in the current version that scaling up the number of load balancers from 2 to 6, our proposed method still shows superior performance than SOTA load balancing methods. We could understand that the reviewer would like to see how the proposed method behave under stress tests. However, given the remaining time is limited for us to configure the testbed, run additional experiments, and analyze as well as plot the results, we could only try our best and investigate how the performance could potentially drop when facing increasing traffic rates with fixed testbed scale (6 load balancers + 20 servers). We hope that this set of experiments can address your concern regarding scaling experiments. If more time is allowed, we would be happy to conduct more extensive experiments to analyze the system sensitivity. We will post new experimental results as soon as possible and we hope that you would take a chance to reconsider your rating.\n\nThough, we respectfully argue that this paper do have contributions on the ML side, including the theoretical formulation of network load balancing problem and the design of potential functions. Specifically, the choice of fairness metric as potential function is not straightforward, only the Variance Based Fairness (VBF) can satisfy the Markov potential game assumption and serve as the individual reward function for each LB, with proofs in paper. Moreover, we show that although single-agent RL and vanilla cooperative MARL algorithm like QMIX can be applied for this problem, they do not perform as well as the decentralized learning method we proposed. The theoretical insights also contribute a lot in providing good performance.\n\n## Re: Real Traffic\n\nWe are sorry that we forgot to point you to our answer to the **Common Concern 1** as well -- where we cited the paper [1] that says the traffic that we used is from a real-world traffic analysis [2] -- so that our rebuttal on the point of \"real-world traffic and testbed configuration\" will be complete.\n\n## Realism\n \nBy overlayed optimization techniques in the data plane, we meant _e.g._ the kernel bypassing technology as in DPDK [3], loop-unrolling and pre-fetching techniques used in VPP [4]. These techniques are typically used for optimizing high performance network data planes, and they are also employed in our paper when implementing our MARL-based load balancing method. Bursty traffic and long-tail distributed workloads are reflected in our experiments by way of the real-world traffic that we used as in [1, 2]. Heterogeneous architecture exists in cloud computing [5], which motivated us to use 2 groups of servers with different processing capacities (2-CPU vs. 4-CPU). Dynamically changing server processing capacities are evaluated experimentally in Sec. E.2.3 in the supplementary material.\n\nIn the paper, we implemented both an event-based simulator and a physical-machine-deployable testbed. In the simulator, these factors are hard to be captured. Therefore, we used the simulator only for evaluating the theoretical distance between our methods and the theoretical optimal solution. Then we used the experimental testbed deployed on physical machines with real traffic to test the performance of our proposed algorithm under the overlayed constraints from different layers in the network stack.\n\n---\n\nReference:\n\n[1] Desmouceaux, Yoann, et al. \"6lb: Scalable and application-aware load balancing with segment routing.\" IEEE/ACM Transactions on Networking 26.2 (2018): 819-834.\n\n[2] Urdaneta, Guido, Guillaume Pierre, and Maarten Van Steen. \"Wikipedia workload analysis for decentralized hosting.\" Computer Networks 53.11 (2009): 1830-1845.\n\n[3] https://www.dpdk.org\n\n[4] https://wiki.fd.io/view/VPP\n\n[5] Kumar, Adithya, et al. \"The fast and the frugal: Tail latency aware provisioning for coping with load variations.\" Proceedings of The Web Conference 2020. 2020.」", " Scaling experiments\n---\nThank you for pointing out that there were 6 LBs in table 3. However, by a scaling experiment, one really is trying to indentify at what point does the algorithm break (or does not perform better). In table 3, for example one could scale the LBs from 1 to 12 (or 6 if the resources is limited) and #requests from some nominal value of 100 to 100K. Authors are trying to show that they are better than other algorithms (which is okay and required) but they also need to show that for some scale, configuration etc, the algorithm/methodology does break. It is a common practice for a system's work. Given that this paper falls under the category of ML4systems (no innovation on ML side), I would expect this analysis irrespective of what was done before by previous authors in the previously published articles. \n\nReal traffic\n--- \nI do not see any details wrt real workload in common concerns 2. I do not see why the traffic used is a real world traffic. I would ask authors to drop the idea of calling it a real workload unless they can cite some paper that says that the arrival rate of queries is X based on real user traffic analysis. \n\nrealism\n---\ne.g., overlayed optimization techniques in the data plane and the network protocol stack, bursty and workloads of long-tail distribution, heterogeneous and dynamically changing server processing capacities.\n\nI dont see how these experiements were done and how methodological these experiments were?\n\n\nWhat about QoS (99th percentile behavior, an important metric to evaluate for LBs)?\n----\nThank you for these additional experiments. \n", " Dear Reviewer WwLw,\n\nThank you again for your review.\n\nWe are sorry that we have not heard back from you since we submitted our rebuttals. The discussion period with the authors is coming to an end and we would not like to miss your response due to time difference. Would you mind please giving us an update on your new interpretation of our work given the revision and the information from our first response?\n\nWhat would it take for us to get a good recommendation from you at this point?\n\nThank you very much in advance for your time and help!\n\nSincerely,\n\nPaper10297 Authors", " Dear Reviewer yzYr:\n\nThanks again for your review with constructive comments. We hope our answers could increase your confidence. As the discussion period is close to the end and we have not yet heard back from you, we would be glad to see if our rebuttal response has addressed your questions/concerns.\n\nWe are more than happy to discuss if you have any further questions, please kindly let us know. Thank you for your time and help!\n\nSincerely,\n\nPaper10297 Authors", " Dear Reviewer cVpL,\n\nThanks again for your review. We hope our rebuttal response has addressed your questions/concerns. We have also made appropriate modifications in the revised pdf to highlight the fact that we did compare with classical load balancing methods.\n\nWe hope you would take a chance to reconsider your rating in light of the points raised in support of the merits of our work.\n\nWe are more than happy to discuss if you have any further concerns. Please kindly let us know your feedback. \n\nThank you for your time!\n\nPaper10297 Authors", " Dear Reviewer Y8vv:\n\nWe hope you are doing well.\n\nWould you mind please giving us an approximate time window as to when you would be responding to our reply? The discussion period with the authors is coming to an end and we would not like to miss your response due to time difference. We have been standing by.\n\nWe spared no effort in out first response to address your concern (_e.g._ by including a new baseline in our evaluations, clarifying several concepts that might have caused your 4 major misunderstandings). Corresponding modifications have also been made in our revised pdf. We hope you would take a chance to reconsider your rating in light of the points raised in support of the merits of our work. \n\nWhat remaining doubts/questions do you have? We'd be happy to clarify and/or discuss.\n\nThank you very much in advance for your information.\n\nSincerely,\nPaper10297 Authors", " Dear Reviewer XheU,\n\nThank you very much for your motivating feedback. \n\nLayer-4 load balancers play a significant role in data center networks. We believe that our work has advanced the important application of MARL on this problem from both theoretical and empirical perspectives, with clearly superior performance than SOTA load balancing algorithms -- not only heuristic ones in production (_e.g._ ECMP and WCMP are implemented based on the Maglev paper from Google [1]), and RL-based one that was proposed last year in NeurIPS [2].\n\nThough some reviewers seem to have misunderstandings on our paper - which we totally understand given the time constraints for reviewing huge amount of papers, we are confident with our contributions in this work and we hope to see its acceptance.\n\nThank you again for the pleasant discussion.\n\n[1] Eisenbud, Daniel E., et al. \"Maglev: A fast and reliable software network load balancer.\" 13th USENIX NSDI 16. 2016.\n[2] Yao et al. \"Reinforced Workload Distribution Fairness.\" NeurIPS 2021", " Dear authors,\n\nThank you for your response. The paper definitely has merits, both theoretically and experimentally. I will continue the discussion with fellow reviewers and AC. I feel the paper is slightly above the acceptance bar, mostly for its modelling/experimental part. ", " To justify the setups (*e.g.*, scale, traffic) of our experiment satisfies the “real-world” requirement, we present a brief survey of real-world DC setup based on a set of SOTA load balancing research papers (also added in Section C.2.6). \n\nA modern data center may comprise thousands of servers and hundreds of load balancers (as *mentioned by Reviewer **yzYr***). However, each independent service is exposed in a modular way at one or several virtual IP (VIP) addresses to receive requests, running over a cluster of servers. Each server in the cluster can be identified by a unique direct IP (DIP). Traffic and queries from the clients destined to a VIP are load-balanced among the DIPs of the service. The development of virtualization, where computers are emulated and/or sharing an isolated portion of the hardware by way of Virtual Machines (VMs), or run as isolated entities (containers) within the same operating system kernel, has accelerated the commoditization of computing resources. Therefore, **the gigantic in-production data center network is typically partitioned into small pods**, where different services (VIPs) are hosted. This partition in data center networks makes services more fault tolerant — e.g., when one server cluster (pod) behind a VIP fails, the service is still available on other pods. This scheme also largely motivates the design of our passive feature collection mechanism with POSIX shared memory partitioned by VIPs and DIPs (see Sec. C.2.4 in the supplementary material). Therefore, although our experiments have 2-6 LBs, we believe they already demonstrated a certain level of scalability in a real-world network system.\n\nIn networking studies, especially Layer-4 server load balancing problems, “real-world” experiments are conducted at per-VIP granularity. The testbed configurations from SOTA load balancers are summarized in the table below:\n\n| Related Work | Real-World Testbed Scale | Note |\n| --- | --- | --- |\n| 6LB [1] | 2 load balancers + 28 servers (2 CPU each) | Our paper uses the same network trace as input traffic. |\n| Ananta [2] | 14 load balancers for 12 VIPs | The exact number of servers per VIP and the in-production traffic is not documented in the paper. |\n| Beamer [3] | 2 load balancers + 8 servers as small scale and 4 load balancers + 10 servers as larger scale | Large scale experiments are conducted with 700 active HTTP connections max. |\n| Duet [4] | 3 software load balancers + 3 hardware load balancers + 34 servers | Synthetic traffic is applied so that the server cluster behind VIP processes 60k (identical) packets per second. |\n| SilkRoad [5] | 1 hardware load balancer or 3 software load balancers per VIP hosted on PoP (point of presence) cluster. | Real-world PoP traffic is applied, where one server cluster behind VIP processes on average 309.84 active connections per second. |\n| Cheetah [6] | 2 load balancers + 24 servers | A python generator creates 1500-2500 synthetic requests/s as input traffic. |\n\nIt’s not practical to always conduct research experiments on dozens or hundreds of servers. Based on the survey above, we believe that the experiments conducted in this paper have met the same standard as in the SOTA load balancers from top venues in networking domain. Using 2 physical servers with 48 CPUs each, we have made our best effort to find a configuration that allows us to conduct experiments similar to real-world setups. Based on the survey above, we believe that the experiments conducted in this paper have reasonable scale — not only in terms of number of agents (2/6 load balancers) and servers (7/20 servers), but also in terms of traffic rates (more than 2k queries per second per VIP and more than 1150.76 concurrent connections in large scale experiments) — and are representative of real-world circumstances.\n\nWe would especially like to draw the attention of Reviewer **Y8vv** respectfully, who **may have misunderstood or overlooked our scaling experiments in the main paper**. We would appreciate it if you could reconsider your rating regarding our evaluations in light of the points in support of the substantial efforts for conducting real-world evaluation in our paper.\n\n---\n\nReference:\n\n[1] Desmouceaux, Yoann, et al. \"6lb: Scalable and application-aware load balancing with segment routing.\" *IEEE/ACM Transactions on Networking*\n\n[2] Patel, Parveen, et al. \"Ananta: Cloud scale load balancing.\" *ACM SIGCOMM Computer Communication Review*\n\n[3] Olteanu, Vladimir, et al. \"Stateless datacenter load-balancing with beamer.\" *15th USENIX NSDI 18*. 2018.\n\n[4] Gandhi, Rohan, et al. \"Duet: Cloud scale load balancing with hardware and software.\" *ACM SIGCOMM*\n\n[5] Miao, Rui, et al. \"Silkroad: Making stateful layer-4 load balancing fast and cheap using switching asics.\" *SIGCOMM*. 2017.\n\n[6] Barbette, Tom, et al. \"A High-Speed Load-Balancer Design with Guaranteed Per-Connection-Consistency.\" *17th USENIX NSDI20*. 2020.", " (cont'd)\n\nBased on the additional experiment above, we can also see that delayed measurement and communication can cause degraded system state observation. We haste to point out that our proposed load balancing mechanism observes the networking features by passively processing networking packet headers, instead of actively probing servers (***which is a misunderstanding by Reviewer Y8vv***). Based on the collected networking features, we then infer system states with neural networks. We humbly consider this feature collection mechanism as a minor contribution in this paper and we described this mechanism in Sec. C.2.4 with Fig. 6 in the supplementary material. The technical details including the feature extraction and collection mechanism at per-VIP granularity are as follows. We implemented reservoir sampling (for collecting task duration and task completion time) and multi-buffering (for collecting the number of ongoing tasks) with POSIX shared memory, in a high-performance kernel-bypassing programmable data plane (VPP). Observations related to different VIPs are organized in separate POSIX shared memory (shm) files, to manage applications independently. Within each VIP, observations related to each egress equipment (*e.g.*, link or server) are also organized independently. Updating (adding or removing) services (VIPs) and their associated equipment can be achieved by managing different shm files in a scalable way using this design, incurring no disruption on data planes. More detailed technical details can be found in our revised paper in **Sec. C.2.4 in the supplementary material**.\n\nTo further demonstrate the performance of the passive feature collection mechanism which incurs absolutely **zero communication overhead**, an additional experiment is conducted to compare the feature collection latency. The latency overhead of the passive feature collection process in our paper using [POSIX shared memory](https://man7.org/linux/man-pages/man7/shm_overview.7.html) is compared with different active probing techniques. The idle communication latency is compared using both KVM and Docker containers between two hosts either deployed on the same machine (local) or on two neighbor machines (remote). To compare with the shortest latency possible of a hardware-based SDN controller directly connected to the agent, a loopback test is conducted using a NetFPGA [7] connected to the machine via both Ethernet and PCIe. Aquarius parses features stored in the local shared memory with a simple Python script without generating control messages. Its median processing latency outperforms typical VM- and container-based VNF probing mechanisms [8-10] by more than 94.18*μ*s (also visualized in Figure 10 in the revised supplementary material).\n\n---\n\nReference:\n\n[7] Zilberman, Noa, et al. \"NetFPGA SUME: Toward 100 Gbps as research commodity.\" *IEEE micro* 34.5 (2014): 32-41.\n\n[8] [ETSI Network Functions Virtualization (NFV) Architectural Framework](https://www.etsi.org/deliver/etsi_gs/NFV/001_099/002/01.02.01_60/gs_NFV002v010201p.pdf). 2014.\n\n[9] [OpenStack Project Portal](https://www.openstack.org/). 2019.\n\n[10] [OPNFV. Open Platform for NFV (OPNFV) Project Portal](https://www.opnfv.org/). 2019.", " We believe that it will be helpful to better motivate reduced communication overhead in real-world data center networks. Therefore, in addition to our analysis in Sec. E.2.2 in the supplementary material as well as the background information about real-world data center networks described above in **Common Concern 1**, we will address this point by clarifying the impact of communication overhead both qualitatively and quantitatively, with additional experiments discussed as follows.\n\nCommunication overhead in data center networks can be discussed in two folds: throughput and latency.\n\n- Throughput: Active signaling (*e.g.*, periodically probing, or sharing messages) is an intrinsic way to observe and measure system states so that informed decisions can be made to improve performance [1-3]. Higher communication frequency gives more relevant and timely observations yet there is a trade-off between communication frequency and additionally consumed bandwidth. Especially, in large distributed systems like data center networks, services are organized by multiple server clusters scattered all over the physical data center network in the era of cloud computing. Thus, management traffic among different nodes can cascade and plunder the bandwidth for data transmission in high-tier links. To demonstrate the trade-off between measurement quality and throughput overhead, we have conducted experiments to evaluate (i) the relevance of collected server utilization information to the actual server utilization information with root mean square error (RMSE) and Spearman’s Correlation in our testbed on physical servers. When a controller VM periodically probes a server cluster via TCP sockets (In the 69-byte control packet emitted by the server, the 24-byte payload consists of the server ID, CPU and memory usage, and the number of busy application threads), as shown in the table below, the visibility over system states (relevance between measurements and ground truth) correlates with the probing frequency (also visualized in Figure 9 in the revised supplementary material).\n\n    |Probing Frequency (/s)|CPU (%) RMSE|#Job RMSE|CPU Spearman's Corr.|#Job Spearman's Corr.|2LB-7server (Kbps)|6LB-20server (Kbps)|\n    |---:|---:|---:|---:|---:|---:|---:|\n    |2.2222|48.3335|2.0675|0.2827|0.4651|2.15|18.40|\n    |2.8571|44.5611|1.8466|0.3964|0.5644|2.76|23.66|\n    |4.0000|39.8433|1.6076|0.5197|0.6642|3.86|33.12|\n    |6.6667|32.6542|1.3109|0.6768|0.7729|6.44|55.20|\n    |20.0000|21.9652|0.9056|0.8525|0.8892|9.32|165.60|\n\n    Additional management traffic within a single service cluster — behind one virtual IP (VIP) — can exceed the 90th percentile of per-destination-rack flow rate (100kbps as depicted in Fig. 8a in [4]) in the Facebook data center in production. As depicted in Fig. 9a in the supplementary material, to synchronize and communicate observations among load balancer agents, a substantial amount of data need to be transferred, which can break full-bisection bandwidth (an important throughput-related performance metric) in data center networks [5].\n\n- Latency: The impact of communication overhead in terms of the increased latency is studied in the supplementary material and depicted in Fig. 9b in Sec. E.2.2. It is measured for per-packet round trip time (RTT) between two directly connected network nodes. While normal RTT is 0.099ms +/- 0.014ms in such a setup, with additional communication overhead, RTT can grow more than 10x. This is not considered as low additional latency (*mentioned by Reviewer **WwLw***), especially not in high-performance networking systems. In elastic and cloud computing contexts and real-world setups, load balancers can be deployed in different racks [6]. There can be multiple hops between two nodes and one connection consists of tens of hundreds of packets, which can lead to cascaded high latency.\n\n(to be continued)\n\n---\n\nReference:\n\n[1] S. Zhang et al, “Efficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control”, NeurIPS 2019. \n\n[2] Z. Ding et al, “Learning Individually Inferred Communication for Multi-Agent Cooperation”, NeurIPS 2020. \n\n[3] T. Wang et al, “Learning Nearly Decomposable Value Functions Via Communication Minimization”, ICLR 2020.\n\n[4] Roy, Arjun, et al. \"Inside the social network's (datacenter) network.\" *SIGCOMM*. 2015.\n\n[5] Zhang, Jiao, et al. \"Load balancing in data center networks: A survey.\" *IEEE Communications Surveys & Tutorials* 20.3 (2018): 2324-2352.\n\n[6] Gandhi, Rohan, et al. \"Duet: Cloud scale load balancing with hardware and software.\" *ACM SIGCOMM Computer Communication Review*  44.4 (2014): 27-38.", " We agree with Reviewer **XheU** that it is natural to apply a game theoretical approach to tackle resource allocation problems. We appreciate the pointer to the reference [1], which studies the theoretical bounds for the worst-case Nash equilibrium in a basic link load balancing setup.  But we would like to clarify that the network (Layer-4 server) load balancing problem is different from the scheduling problem studied in [1] or the link load balancing problem studied in [2] (the difference between server load balancing and link load balancing problem is also discussed in [3] in the context of data center networks), and it is a challenging problem.\n\nAs Reviewer **Y8vv** pointed out, the assumptions in theoretical studies typically have strong assumptions. We summarize the following challenges of applying Markov potential game to Layer-4 server load balancing problem which is studied in the paper:\n\n- As studied in [1, 2, 4], theoretical bounds are derived based on the assumption that resources (links/servers) share identical capacities, and the system is relatively static. With the rise of elastic and server-less computing, where tenants in a data center can share physical resources (*e.g.,* CPU, disk, memory), servers can have different processing capacities, which may also change over time — because of e.g., updated server configuration (upgrading an AWS EC2 a1.xlarge instance to a1.4xlarge, which increases the server capacity by 4x — product detail can be found [here](https://aws.amazon.com/ec2/instance-types/a1/)) or resource contention (co-located workloads) [5].\n- Real-world networking systems consist of multiple layers of technology and overlayed optimization techniques, *e.g.*, random access (a simplified version is studied in [1]), batch processing, loop-unrolling, congestion control, etc. These can hardly be modeled theoretically, which motivates us to implement our framework in a real-world system on physical machines using the flexible learning-based approach.\n- Unlike the assumption in [1] where each agent is aware of its workload $w_i$ to be distributed, or in [4] where every job is identical, Layer-4 server load balancers are agnostic to the information of jobs which they will distribute [3]. Besides, real-world traffic is bursty (*e.g.*, one of the data centers studied in [6] exhibits a median flow inter-arrival time lower than $250\\mu$s) and consists of long-tail distribution [7]. Therefore, we exploit recurrent neural networks to condition the policy on the historical data for modeling the distributions and handle this sequential and stochastic problem.\n\nMean-field theory (*mentioned by Reviewer **Y8vv***) is an interesting approach to handle MARL problem [8], yet it is designed for circumstances where a large number of similar agents co-exist. However, as discussed above, typical real-world setups have less than 15 load balancer agents (which is far less than 100, 500, or 1000 as in [8]). The mean-field treatment for agents with not a very large number may lead to other problems, especially when asymmetric agents are required. Besides, load balancer agents may be different from each other in terms of their infrastructure (*e.g.*, software, commodity switch, or ASICs [9, 10]) or in terms of their placement in the physical data center (whether load balancers are deployed within the same rack as servers [9]). Though mean-field theory does not fit the network load balancing problem, we believe that this clarification is important and will discuss these points in our revision.\n\n---\n\nReference:\n\n[1] Koutsoupias, et al. \"Worst-case equilibria.\" 1999.\n\n[2] Sen, Siddhartha, et al. \"Scalable, optimal flow routing in datacenters via local link balancing.\" *CoNEXT*. 2013.\n\n[3] Zhang, Jiao, et al. \"Load balancing in data center networks: A survey.\" *IEEE Communications Surveys & Tutorials* 2018\n\n[4] Goren, Guy, Shay Vargaftik, and Yoram Moses. \"Distributed dispatching in the parallel server model.\" *arXiv preprint arXiv:2008.00793* (2020).\n\n[5] Guo, Jing, et al. \"Who limits the resource efficiency of my datacenter: An analysis of alibaba datacenter traces.\" IEEE/ACM IWQoS. 2019.\n\n[6] Benson, et al. \"Network traffic characteristics of data centers in the wild.\" *ACM SIGCOMM IMC*. 2010.\n\n[7] Roy, Arjun, et al. \"Inside the social network's (datacenter) network.\" *ACM SIGCOMM*. 2015.\n\n[8] Yang, Yaodong, et al. \"Mean field multi-agent reinforcement learning.\" *ICML*. 2018.\n\n[9] Gandhi, Rohan, et al. \"Duet: Cloud scale load balancing with hardware and software.\" *ACM SIGCOMM*. 2014\n\n[10] Miao, Rui, et al. \"Silkroad: Making stateful layer-4 load balancing fast and cheap using switching asics.\" *SIGCOMM*. 2017.", " (cont'd)\n\n> Robustness against dynamic changes in network setup is discussed to some degree, but it’s unclear how significant this issue is in a real-world environment. Even in a large-scale setup, the number of LBs/servers is likely to remain fairly constant at the timescales considered in this work (i.e., minutes). Given this, it seems that the paper should at least discuss trade-offs with a longer training time, which could impact the relative benefits of various approaches.\n> \n\n***Response:*** With the rise of elastic and server-less computing, where tenants in the data center can share physical resources (*e.g.,* CPU, disk, memory), servers can have different processing capacities, which may also change over time dynamically — because of e.g., updated server configuration (upgrading an Amazon EC2 a1.xlarge instance to a1.4xlarge) or resource contention (co-located workloads) [4]. According to [5], there are 32% of server clusters in a data center that update more than 10 times per minute based on the measurements collected over 432 minutes up time in a month. 3% of clusters have more than 50 updates per minute. Therefore, dynamic changes prevail in real-world data center networks. The episode length — because of the variance in traffic rate and load balancing decisions — varies from 45s-75s, which captures a good representation of various scenarios, meanwhile allowing for efficiently collecting playback trajectories. We have carefully tuned the training time in our paper so that different load balancing methods converged (as depicted in Fig. 4 in the main paper) and no significant improvement happens with longer training time.\n\n---\n\nGiven the clarifications and additional experimental results detailed above, we kindly ask the reviewer to re-evaluate our paper and consider recommending it for acceptance. \n\nWe thank you in advance for your reply. We hope our responses and edits help convince you of the merits of this paper. Please do not hesitate in contacting us during the rolling discussion and let us know if there is anything else that we can revise.\n\n---\n\nReference:\n\n[4] Guo, Jing, et al. \"Who limits the resource efficiency of my datacenter: An analysis of alibaba datacenter traces.\" IEEE/ACM IWQoS. 2019.\n\n[5] Miao, Rui, et al. \"Silkroad: Making stateful layer-4 load balancing fast and cheap using switching asics.\" *SIGCOMM*. 2017.\n", " Dear Reviewer WwLw,\n\nThank you for your time to review our paper and we appreciate your thorough and constructive comments.\n\n> Representativeness of baselines used for evaluation: The baselines used for evaluation do not seem to accurately represent the state-of-the-art in CTDE e.g., [1-3].\n> \n\n***Response:*** We appreciate the reviewer for pointing us to very interesting CTDE works [1-3]. These works target cooperative setups and propose to exploit (minimized) communications among agents during the “distributed execution” phase to improve performance and reduce communication overhead. However, in deployed load balancing systems in real-world data center environments — or distributed execution phase, no direct communication happens between agents [6-10], because load balancers are either fully distributed (self-managed) or directly managed by a centralized controller to reduce management and operational overhead in data center networks. Even if a direct inter-load-balancer channel was to be established, in elastic data centers, two load balancers may be deployed in different racks [9], which can cause (i) delayed signal (>1ms) because of multi-hop distance while inter-task arrival time can be < 200 microseconds, and (ii) additional bandwidth consumption in high-tier links in physical data centers (discussed later). Besides, [2] defines the field of view for each agent, which does not apply in data center network load balancing setups, and [3] introduces another parameter (message drop rate) to tune for achieving optimized performance. Presumably, if we were to implement these CTDE algorithms, more sensitivity analysis needs to be conducted to study the impact of *e.g.*, frequency of communication among agents, and types of information communicated, which we consider will hinder the clarity of the paper. However, this comment can be really valuable for the MARL application in mobile networks.\n\nFollowing the suggestion of reviewer Y8vv, we have conducted additional experiments using RLB-SAC (NeurIPS 2021) — the latest RL-based solution for network load balancing. We respectfully refer the reviewer to our answer to reviewer Y8vv as well as our updated results in the revised paper, to check the results, which demonstrate that our proposed method has superior performance.\n\n> Benefits in terms of reduced communication overhead could also be more strongly motivated. Presumably, communication between agents could be done over purpose-built inter-LB links, thus avoiding QoS degradation due to contention on links between LBs and servers. Even without inter-LB links, the increase in latency demonstrated in Appendix E.2.2 appears relatively low.\n> \n\n***Response:*** This is a very valuable remark. We have provided more motivation for reduced communication overhead and argued that the additional latency is not relatively low in our answer to **Common Concern 2** in our **Meta Response** above.\n\n(to be continued)\n\n---\n\nReference:\n\n[1] S. Zhang et al, “Efficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control”, NeurIPS 2019. \n\n[2] Z. Ding et al, “Learning Individually Inferred Communication for Multi-Agent Cooperation”, NeurIPS 2020. \n\n[3] T. Wang et al, “Learning Nearly Decomposable Value Functions Via Communication Minimization”, ICLR 2020.\n\n[6] Eisenbud, Daniel E., et al. \"Maglev: A fast and reliable software network load balancer.\" *13th USENIX NSDI 16*. 2016.\n\n[7] Barbette, Tom, et al. \"A High-Speed Load-Balancer Design with Guaranteed Per-Connection-Consistency.\" *17th USENIX NSDI*. 2020.\n\n[8] Olteanu, Vladimir, et al. \"Stateless datacenter load-balancing with beamer.\" *15th USENIX NSDI*. 2018.\n\n[9] Gandhi, Rohan, et al. \"Duet: Cloud scale load balancing with hardware and software.\" *ACM SIGCOMM*  2014\n\n[10] Desmouceaux, Yoann, et al. \"6lb: Scalable and application-aware load balancing with segment routing.\" *IEEE/ACM Transactions on Networking* 2018\n", " (cont'd)\n\n> Other future work directions:\n> \n> \n> > This work addresses load balancing in a single data center but I think it can easily be extended to a multi-DC setup by capitalizing on the low-observability approximation.\n> > \n> \n> > Have you considered including some offline learning as the initial state?\n> > \n> \n> > Simulator not as complex as real-world (addressed in paper). Still allows testing parts of the system without stochastic network parameters.\n> > \n\n***Response:*** We appreciate the insightful comments and inspiring suggestions. \n\n- Multi-DC setup or inter-DC use cases can be interesting, especially for traffic optimization and balancing traffic among PoP (point of presence) data centers.\n- Offline training has been used by AWS EC2 to learn from history data *e.g.*, diurnal traffic patterns. We believe that it is also possible to conduct a similar pre-training process to learn different types of hardware. However, there are also recent works that challenge the assumption of consistent patterns over time [2]. And there are more challenges to tackle including the dynamically changing environment (frequently and elastically updated server clusters [3].\n- We agree that more sensitivity analysis (e.g., by varying CPU/IO, traffic characteristics) will be interesting. However, to thoroughly evaluate what happens with extensive parameter tuning in terms of testbed configuration requires additional modeling, evaluation, and analysis, which, we estimate, would hinder the clarity of this paper.\n\nWe have already open-sourced both our simulator and testbed in case more researchers would like to join forces for future work concerning these points.\n\n> What is the rationale behind using 2/3 agents in Fig 2b?\n> \n\n***Response:*** In Fig. 2b, we wanted to demonstrate with a simplistic case, that in real-world data centers where multiple load balancers distribute workloads among a server cluster, partial observation will degrade the performance of classical methods. Therefore, we proposed this MARL framework with a carefully designed potential function to handle the partial observation issue and achieved improved performance when compared with SOTA load balancer methods.\n\n---\n\nGiven the clarifications, we kindly ask the reviewer to re-evaluate our paper and consider recommending it for acceptance. We hope to have answered your questions sufficiently and helped you see the merits of this paper in a new light. We look forward to your feedback.\n\n---\n\nReference:\n\n[2] Jajoo, Akshay, et al. \"A case for task sampling-based learning for cluster job scheduling.\" *19th USENIX NSDI 22*. 2022.\n\n[3] Miao, Rui, et al. \"Silkroad: Making stateful layer-4 load balancing fast and cheap using switching asics.\" *SIGCOMM*. 2017.", " Dear Reviewer yzYr,\n\nThank you for taking the time to review our paper and we appreciate your insightful comments.\n\n> DCs typically have high bandwidth for internal communication. The paper states that centralized communication leads to heavy overhead which is not convincing in the main paper. The evaluation section mentions it in passing as being evaluated in the appendix but I feel it would be helpful to show in the main paper.\n> \n\n> Need for low communication overhead in DC is not motivated strongly. Centralized methods (QMix for example) still show comparable performance in some application setups.\n> \n\n***Response:*** This is a very valuable remark. We respectfully refer the reviewer to our answer to **Common Concern 2** in the **Meta Response** to all reviewers. \n\n> Not sure if I agree with \"large-scale\" DC networks having only 20 servers. The largest of data centers have thousands of servers and load balancers. This makes the real-world setup slightly less impressive.\n> \n\n> Nitpick: Specs for the real world setup would be helpful to compare to existing DCs\n> \n\n***Response:*** This is a reasonable comment. We have addressed this comment in our answer to **Common Concern 1** in our **Meta Response** above. We have also added a brief survey as specs for the real-world setup in DCs.\n\n> Fault tolerance not evaluated in the paper (in terms of failed requests leading to incorrect job completion estimates for the next time period, network partitions, etc.)\n> \n\n***Response:*** This is a valuable remark. We agree that fault tolerance is worth proper investigation. However, to thoroughly system robustness and fault tolerance requires additional evaluation and analysis (which can be a standalone topic to study as in [1]), which, we estimate, would hinder the clarity of this paper. Methods like robust RL would be considered or compared in this setting. In this paper, we have conducted a system robustness evaluation, which will be described in the answer to the following question.\n\nIn terms of collecting measurements of job completion time which might be corrupted by failed requests, we refer the reviewer to our contribution described in Sec. C2.4. in the revised supplementary material. We use reservoir sampling to gather task completion time (and task duration). The basic idea is to have a fixed-sized (size k = 64 in the paper) buffer for each feature for each server behind a given service (or virtual IP). Whenever a new data point is received at time $t$ (*e.g.*, when a request finished with a TCP FIN packet), the task completion time — together with the timestamp $t$ — is inserted in a random bucket in the buffer, replacing the previously stored value. Therefore, given a Poisson stream of events with an arrival rate $\\lambda$, reservoir sampling gathers an exponentially distributed number of samples over a time window — the expectation of the number of samples that are preserved in the buffer after $n$ steps is $E = \\lambda(\\frac{k-1}{k})^{\\lambda n}$. At every time step, the reservoir buffer is summarized by calculating its mean, standard deviation, discounted average, percentiles, etc. as input features. Thus, a couple of failed requests will have little impact on the overall measurement representation. However, this reservoir sampling mechanism still allows for collecting timeouts in the buffer and detecting issues if most requests fail because of overloaded servers.\n\n> The paper doesn't seem to address elastic setups even though their motivation included both heterogeneous and elastic infrastructures.\n> \n\n***Response:*** We agree that it is important to evaluate elastic setups, which also, in part, motivated us to add a detailed discussion about elastic DC setups in our answer to **Common Concern 1** in our **Meta Response**. To study the system in dynamic environments in the context of elastic computing, we have presented experimental evaluations in Sec. E.2.3 in the supplementary material, where server processing capacities change over time. We consider evaluating more elastic setups (*e.g.*, adding or removing servers with action masking) for future work.\n\n(to be continued)\n\n---\n\nReference:\n\n[1] Ghaznavi, Milad, et al. \"Fault tolerant service function chaining.\" *SIGCOMM*. 2020.\n", " Dear Reviewer cVpL\n\nThank you for taking the time to review our paper and we appreciate your insightful comments.\n\n> Are all servers allocated jobs by all LBers? What do the results look like with partial overlaps? This seems to be a harder problem.\n> \n\n***Response:*** Yes, all servers are allocated jobs by all load balancers. Because in the context of data center networks, the aim is to achieve as high throughput as possible with multiple load balancers, so that the uplink bandwidth matches the server-facing bandwidth. For instance, given a $12$ $1$Gbps server cluster, we need a $3$ $4$Gbps load balancer so that the load balancers will not become a bottleneck in the network even when all servers go full throttle. Therefore, in all SOTA load balancing mechanisms in data centers, load balancers are connected to all servers as a complete bipartite graph [1-5]. However, partial overlaps can indeed be an interesting research topic, *e.g.*, for mobile networks.\n\n> The experimentation doesn’t include comparison with classical methods such as LSQ. It would be interesting to see how a distributed, blind, greedy LSQ compares to the distributed MARL method proposed here, especially since the computation costs are so vastly different.\n> \n\n***Response:*** We respectfully refer the reviewer to our results in Table 1 (L259) and 3 (L293) in the revised main paper, and our results in Sec. E in the supplementary material, where we **compared our proposed framework with three classical methods** — *i.e.*, WCMP, **LSQ**, and SED — in the real-world testbed. We will improve the clarity of the paper and make our comparison with classical methods more explicit in our revision.\n\n---\n\nGiven the clarifications, we kindly ask the reviewer to re-evaluate our paper and consider recommending it for acceptance. We hope to have answered your questions sufficiently and helped you see the merits of this paper in a new light. We look forward to your feedback.\n\n---\n\nReference:\n\n[1] Eisenbud, Daniel E., et al. \"Maglev: A fast and reliable software network load balancer.\" *13th USENIX NSDI 16*. 2016.\n\n[2] Barbette, Tom, et al. \"A High-Speed Load-Balancer Design with Guaranteed Per-Connection-Consistency.\" *17th USENIX NSDI20*. 2020.\n\n[3] Olteanu, Vladimir, et al. \"Stateless datacenter load-balancing with beamer.\" *15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 18)*. 2018.\n\n[4] Gandhi, Rohan, et al. \"Duet: Cloud scale load balancing with hardware and software.\" *ACM SIGCOMM Computer Communication Review*  44.4 (2014): 27-38.\n\n[5] Desmouceaux, Yoann, et al. \"6lb: Scalable and application-aware load balancing with segment routing.\" *IEEE/ACM Transactions on Networking* 26.2 (2018): 819-834.", " (cont'd)\n\n## 2. Assumption\n\n> the assumption made in this paper is that each server is capable of processing a certain amount of workload v_j, which is a number only dependent on server capabilities and not on the request (or traffic type itself). […] In other words, v_j should be stochastic and not fixed on just the server but also on the request characristics.\n> \n\n***Response:*** We agree with the reviewer that it is important to take into account the variance of network requests.  However, **there seems to be a misunderstanding about the notations in the paper**. We respectfully refer the reviewer to L102, where we defined $w_i(t) \\in W$ (instead of $v_j$) as the notation of workloads (*e.g.*, it can be instantiated as 2s or 20s request), which has **no dependency on server capabilities but on the workload distribution** $W$ itself. **$v_j$, on the other hand, denotes the max processing speed of each server (defined at L108)**, which is dependent on server capabilities. Given $\\alpha_{ij}(t)$ as the probability mass of assigning tasks from load balancer $i$ to server $j$, we then represent the residual workload on server $j$ at each timestamp as $\\max\\{0, \\sum_{i=1}^M w_i(t)\\alpha_{ij}(t) - v_j\\}$ (described at L110). The actual processing times for different requests are indeed different in our system.\n\n> previous assumption invalidates most of the derivation presented later in the paper.\n> \n\n***Response:*** We hope that we have addressed this concern by our answer to the question above.\n\n> Another assumption is that active probing is impractical. However, it is okay for LBs to communicate with severs to observe the server state, why? There is no citation or experiment showing that indeed that is a reasonable assumption. All of the work is based on this key assumption.\n> \n\n***Response:*** There seems to be **a misunderstanding about the way load balancers (LBs) observe server states**. We do agree that autonomously managing network services by active probing servers is impractical in data center networks. We haste to point out that our proposed load balancing mechanism observes the server states by passively processing networking packet headers, instead of actively probing servers. \n\nWe did not stress this feature collection mechanism — which is one of our key contributions that made possible the deployment of the MARL algorithm in real-world experimental environments — in the main paper because we considered that it might hinder the clarity of this paper. But based on your constructive comments, we believe that appropriate modifications need to be made in the main paper to clarify the fact that our proposed load balancing mechanism collect server state observations passively and incurs no communication overhead. We have put detailed description about our feature collection mechanism in Section C.2.4 in the revised supplementary material.\n\nWe respectfully refer the reviewer to our answer to **Common Concern 2** in our **Meta Response** to all reviewers above.\n\n---\n\n## 3. Insights\n\n> why Markov Potential Games for handling the stated problem? Why not use mean-field theorem to approximate the behavior of all the other multi-agents using mean or median behavior.\n> \n\n***Response:*** \n\nThis is a valuable remark. We respectfully refer the reviewer to our answer to **Common Concern 3** in our **Meta Response** to all reviewers. \n\n> What happens if RL makes bad decisions (safety of RL: Towards safe online reinforcement learning in computer systems, NeurIPS 2021).\n> \n\n***Response:*** Thank you for this insightful remark and we will clarify this point in our revision. The proposed framework makes load balancing decisions and assigns server j based on the form $j = \\arg \\min_{k\\in[N]}\\frac{q_{ik}(t)+1}{a_{ik}(t)}$, where $q_{ik}$ is the observed queue length on server $k$ and $a_{ik}$ is the agent’s action. By design, $a_{ik}$ is within a positive action range. The worst case RL decisions will potentially overload a subset of servers while starving another subset of servers, by assigning bad weights ($a_{ik}$) to servers. This can also happen to classic load balancing algorithms if server weights are misconfigured (see Fig. 2b in the main paper), leading to increased task completion time. We appreciate that the reviewer mentions this problem, but the safety of RL itself is an unsolved research topic, and our work is on a specific application. Therefore, we would put this as our future work instead part of the current paper.\n\n---\n\nGiven the clarifications and additional experimental results detailed above, we kindly ask the reviewer to re-evaluate our paper and consider recommending it for acceptance. \n\nWe thank you in advance for your reply. We hope our responses and edits help convince you of the merits of this paper. Please do not hesitate in contacting us during the rolling discussion and let us know if there is anything else that we can revise.", " Dear Reviewer Y8vv:\n\nThank you for your time to attentively review our paper and we appreciate your thorough and constructive comments.\n\nThere seem to be a couple of misunderstandings, regarding:\n\n- notations and assumptions in our paper,\n- scaling experiments,\n- real-world traffic, and\n- the way we gather networking features and system state observations.\n\nWe will try to rectify what we can here. We will address the comments — point by point — by grouping them into 3 categories: (1) evaluation, (2) assumption, and (3) insights.\n\n---\n\n## 1. Evaluation\n\n> No scaling experiments (i.e., increasing number of LBs or servers). Only 2 LBs in evaluation.\n> \n\n***Response:*** We do agree with the importance of scaling experiments and we respectfully refer the reviewer to our results with 6 load balancers (LBs) and 20 servers in Table 3 in the main paper (description can be found at L293), as well as the overhead analysis in the supplementary material (Sec. E.2.2). We have added the number of LBs and servers to Table 6 in the supplementary material (as LB System parameter) as well for making it more prominent and explicit.\n\n> No real traffic/workload. The supposedly real benchmark is a mocked-up small testbed that does not mimic the real distribution of traffic or scale.\n> \n\n***Response:*** We appreciate the reviewer for the prospect of real traffic/workload. We did use real traffic and workload and we have clarified our real-world testbed in detail in our answer to the **Common Concern 2** in our **Meta Response** to all reviewers. \n\n> Experimentally weak: no variation in traffic and limited variation of IO/CPU\n> \n\n***Response:*** We agree that more sensitivity analysis (*e.g.*, by varying CPU/IO, traffic characteristics) will be interesting. However, to thoroughly evaluate what happens with extensive parameter tuning in terms of testbed configuration requires additional modeling, evaluation, and analysis, which, we estimate, would hinder the clarity of this paper. We have already open-sourced both our simulator and testbed in case more researchers would like to join forces for future work.\n\nWe have tried to balance our contribution between theoretical modeling and applied evaluation by way of both simulation and experiments running on real machines, which allow for verifying the performance under realistic constraints — *e.g.*, overlayed optimization techniques in the data plane and the network protocol stack, bursty and workloads of long-tail distribution, heterogeneous and dynamically changing server processing capacities. \n\nWe have varied the traffic in the simulation with 3 types of workloads, i.e. 100% CPU intensive application, 75% CPU & 25% IO application, and 50% CPU and 50% IO application.\n\nWe have varied the traffic in the experimental testbed on physical servers with real-world traffic on two different scales, under different traffic rates, using trace replay from the different hours of the day.\n\nWe have also varied server configuration over time (Sec. E in the supplementary material)\n\nHowever, this comment is well taken, and it is considered for future work.\n\n\n> What about QoS (99th percentile behavior, an important metric to evaluate for LBs)?\n> \n\n***Response:*** Additional evaluation on a real-world testbed is conducted regarding the 99th percentile of task completion time (QoS). These results are included in the revised supplementary material (Table 9, 10, 11), where our proposed MARL-based load balancing method achieves superior performance than SOTA load balancing methods. Please don't hesitate to check.\n\n> No comparison with other ML-based approaches: No evaluation of the overhead of the RL vs MARL solution in terms of performance as well as overhead (to justify MARL is needed over RL). There are several solutions proposed such as RLB-SAC (Neurips 2021) which reports similar high performance, and Park: An Open Platform for Learning-Augmented Computer Systems.\n> \n\n***Response:*** We thank the reviewer for providing pointers to these interesting related works. We have added additional experiments to compare our proposed method against the latest solution RLB-SAC (NeurIPS 2021), whose results are also presented in the 99th percentile QoS comparison above. The performance of RLB-SAC is not as good as the proposed distributed method in both moderate-scale and large-scale settings. Our proposed MARL load balancing methods achieve superior results because of (i) a well-designed MARL framework with a carefully selected potential function, and (ii) the use of the recurrent neural network to handle load balancing problem as a sequential problem.\n\n(to be continued)", " Dear Reviewer XheU,\n\nThank you for taking the time to review our paper and we appreciate your insightful comments. \n\n---\n\n> The result of the proposed framework to be a Markov potential game is not very surprising as load balancing games are known to be potential games (see [Koutsoupias, Papadimitriou 99]).\n> \n\n> What was the main technical difficulty that did not allow the authors to provide theoretical guarantees for their proposed algorithm?\n> \n\n***Response:*** We appreciate the pointer to the reference. We respectfully refer the reviewer to our answer to **Common Concern 3** in our **Meta Response** above. The authors did not provide theoretical guarantees because in the proposed problem settings the theoretical guarantees would require strong assumptions, which typically fail to capture the reality. It would be encouraged to try to get rigorous theoretical analysis for the algorithm, but due to function approximation and multiple constraints under real-world concerns, it could be hard to do. And the our focus is not on the theoretical-side analysis of the algorithm (convergence bound, etc). The focus of this work is proposing an empirical algorithm to solve the real problem, with a certain level of theoretical insights.  \n\n---\n\nWe thank you in advance for your reply. We hope our responses and edits help convince you of the merits of this paper. Please do not hesitate in contacting us during the rolling discussion and let us know if there is anything else that we can revise.", " We thank the reviewers for catching these nits. They have been all fixed. All abbreviations have been checked so that they are introduced on their first appearances.", " First of all, we would like to take this chance to thank all our reviewers for the valuable reviews. We have put in a major effort to address these comments, questions, and concerns, which we believe has brought the paper to a better level.\n\nThe revised paper is uploaded, where all major modifications in the pdf have been highlighted in blue to facilitate the reading. We will also keep updating both the main paper and the supplementary material in the following days during the discussion period if applicable. \n\nMajor paper updates as per the reviewer's requests are listed below:\n\n- Added real-world DC specs in the supplementary material (Sec. C.2.6, Table 5) to justify that our testbed configuration meets the “real-world” requirement\n- Added motivation for reduced communication overhead (Sec. E.2.2, Figure 9)\n- LB system configuration hyperparameter is updated so that the testbed scale (number of load balancer agents and servers) is clear in the supplementary material (Table 6)\n- Added technical details regarding the passive feature collection mechanism and complexity analysis in Section C.2.3 in the supplementary material (Figure 6, Algorithm 3, Table 4).\n- Added 99th percentile QoS evaluation in the supplementary material (Table 9, 10)\n- Added RL-based load balancing method — RLB-SAC (Yao et al. Reinforced Workload Distribution Fairness, *NeurIPS* 2021) — as a baseline in both moderate- and large-scale real-world testbed evaluations (Table 1, 3, 8, 9, 10)\n- Writing issues are fixed (*e.g.*, abbreviations introduced, notation $M$, Alg.2 H for Horizon)\n\nWe will provide our point-by-point responses to each of your concerns raised in this first set of reviews. However, we have also summarized **4 common concerns** from multiple reviewers, which we would like to address in this thread beforehand. We have noted the reviewers who raised the corresponding concern, yet we would also like to welcome all reviewers to join the discussion.", " This paper focuses on the network load balancing problem in data centers using multi-agent RL paradigm. The main goal in load balancing problems is to minimize the makespan\nThe authors prove various properties of the setting with the main result to be that such setting is a Markov Potential Game. They showed this result via properly defining a workload distribution fairness potential function. Moreover, using facts established in Leonardos et al, they design a distributed algorithm to approximate Nash equilibrium policies. The authors provide an extensive experimental section that suggest that the proposed algorithm is effective. Pros\nThe paper is interesting, with both theoretical and applied merits and an interesting modeling of the network load balancing prblems in data centers as a MARL system. \nCons: The result of the proposed framework to be a Markov potential game is not very surprising as load balancing games are known to be potential games (see [Koutsoupias, Papadimitriou 99]).\n What was the main technical difficulty that did not allow the authors to provide theoretical guarantees for their proposed algorithm? This work has no negative societal impact as far as the reviewer can forsee.", " This paper proposes MPG-based MARL solution for the load-balancing problem. Applying RL directly for load-balancing is not favorable as the load balancers (i.e., multiple-agents) need to synchronize observations as well as the action space grows with number of agents (requiring re-training etc.) **Strengths**\n1. Does not require re-training with increasing number of multiple agents as the proposed approach decomposes\nthe joint state and action space.\n2. Does not require synchronization between the load balancers\n\n\n**Weakness**\n1. Poor evaluation\n  * No scaling experiments (i.e., increasing number of LBs or servers). Only 2 LBs in evaluation. If the paper had scaled the experiments more they would find that their approach may not be practical for real DCs (discussed later). \n  * No real traffic/workload. The supposedly real benchmark is a mocked up small testbed that does not mimic real distribution of traffic or scale. \n  * Experimentally weak: no variation in traffic and limited variation of IO/CPU\n  * What about QoS (99th percentile behavior, an important metric to evaluate for LBs)?\n2. Invalid and inconsistent assumptions wrt problem statement.\n  * In the intro, the paper claims that \"existing algorithms are not adaptive to due to dynamic environments\" yet the assumption made in this paper is that each server is capable of processing a certain amount of workload v_j, which is a number only dependent on server capabilities and not on the request (or traffic type itself). For example, a GET request of type X can take 2s whereas a another GET request of type Y can take 20s. The paper mentions collided elephants and yet does not provide any experiments that the proposed technique can handle such situations. In other words, v_j should be stochastic and not fixed on just the server but also on the request characristics. \n  * previous assumption invalidates most of the derivation presented later in the paper. \n\n  * Another assumption is that active probing is impractical. However, it is okay for LBs to communicate with severs to observe the server state, why? There is no citation or experiment showing that indeed that is a reasonable assumption. All of the work is based on this key assumption. \n\n3. Limited insights:\n  * why Markov Potential Games for handling the stated problem? Why not use mean-field theorem to approximate the behavior of all the other multi-agents using mean or median behavior. Overall the paper reads as an application of MPG rather than \"there is a nice solution\" to the load-balancing problem. Insights and analysis of different approaches are missing. \n * No evaluation of the overhead of the RL vs MARL solution in terms of performance as well as overhead (to justify MARL is needed over RL). There are several solutions proposed such as RLB-SAC (Neurips 2021) which reports similar high performance, and Park: An Open Platform for Learning-Augmented Computer Systems. \n * What happens if RL makes bad decisions (safety of RL: Towards safe online reinforcement learning in computer systems, NeurIPS 2021).\n\n4. Writing needs significant improvement esp. introduction and related work section.  \n     * citations on key assumptions/claims or experiments to make those statements\n    * Abbreviations introduced without describing what it stands for?: e.g. NE for MPG\n\n Please clarify the questions and limitations raised in the weakness section above.  * Limited evaluation\n* Strong assumptions for a practical system\n* No comparison with other ML-based approaches. ", " This paper considers the load balancing problem in a network of multiple heterogenous servers, and multiple load balancers.  The authors formulate the problem as a multi agent reinforcement learning problem, and specifically consider a Markova potential game.\n\nThe settings is that of multiple load balancers, each responsible for sending jobs to a set of servers.  There might be overlaps in the set of servers the various load balancers serve, and the load balancers thus have partial observability of the system state.  Using the cumulative total fairness as the potential function, where fairness is defined as either variance fairness or product fairness, the authors show that the job allocation game where the objective is to minimize the makespan while maximizing the variance fairness or product fairness, is a Markov potential game.   A network with multiple load balancers managing load to multiple and overlapping servers is a complex problem.  The interactions between the load balancers is such that a closed form solution to the balancing problem is not evident.  This approach of setting a potential game within a RL environment is interesting and seems novel.\n\nThe authors propose a distributed load balancing method where each agent independently learns a policy, through policy gradient methods. The reward function is set to be per-LB variance or product fairness.  The authors show that maximizing for these local fairness metrics is sufficient tor minimizing makespan, a global metric. \nThe exact model, with respect to overlap of servers among the load balancers, is not clear.  Are all servers allocated jobs by all LBers? What do the results look like with partial overlaps? This seems to be a harder problem.  \n\n Are all servers allocated jobs by all LBers? \nWhat do the results look like with partial overlaps? This seems to be a harder problem.   The experimentation doesn’t include comparison with classical methods such as LSQ.  It would be interesting to see how a distributed, blind, greedy LSQ compares to the distributed MARL method proposed here, especially since the computation costs are so vastly different.", " This paper proposes a distributed Multi-agent Reinforcement Learning based approach for load balancing at the network layer formulated as a Markov Potential game.\n\nCurrent network load balancers have limited observability over the workloads and servers performance and are prone to misconfiguration due to heterogeneity and elasticity. Centralized approaches (CTDE) incur an additional overhead from centralized communication. This work addresses this by using a local variance-based fairness function in each load balancer which, when maximized, can minimize the potential function of the Markov potential game. This approximates the Nash equilibrium of the game.\n ## Strengths\n* Significant gains by using proposed design over current in-production load balancing algorithms\n* Strong theoretical foundation of formulating load balancing as a multi-agent RL-based Markov potential game\n* Well written paper that puts the pieces of the design in an easy to understand order\n\n## Weaknesses\n* DCs typically have high bandwidth for internal communication. The paper states that centralized communication leads to heavy overhead which is not convincing in the main paper. The evaluation section mentions it in passing as being evaluated in the appendix but I feel it would be helpful to show in the main paper.\n* Not sure if I agree with \"large-scale\" DC networks having only 20 servers. The largest of data centers have thousands of servers and load balancers. This makes the real-world setup slightly less impressive.\n* Fault tolerance not evaluated in the paper (in terms of failed requests leading to incorrect job completion estimates for next time period, network partitions etc.)\n * This work addresses load balancing in a single data center but I think it can easily be extended to a multi-DC setup by capitalizing on the low-observability approximation. Have you considered this?\n* Have you considered including some offline learning as the intial state? System logs from data centers usually provide good information about request patterns and existing data center hardware.\n* What is the rationale behind using 2/3 agents in Fig 2b?\n* Did you consider other kinds of traffic other than wiki (database heavy) and static? (video for example)\n* Are the numbers in Tables 1/2 mean or median?\n* Nitpick: Page 3, line 100 should say [M] denotes the set of LB agents {1,...,M} instead of set of servers\n* Nitpick: Specs for the real world setup would be helpful to compare to existing DCs\n * The paper doesn't seem to address elastic setups even though their motivation included both heterogeneous and elastic infrastructures.\n* Simulator not as complex as real-world (addressed in paper). Still allows to test parts of the system without stochastic network parameters. These could be synthetically injected though.\n* Need for low communication overhead in DC is not motivated strongly. Centralized methods (QMix for example) still show comparable performance in some application setups.\n", " The paper explores the task of multi-agent network load balancing via formulation as a Markov potential game, using workload distribution fairness as a potential function. A MARL algorithm is proposed based on this formulation and provides for fully-decentralized learning. The paper further develops an event-based simulator which, along with a real-world network setup, is used to evaluate the proposed algorithm against several MARL baselines.  Strengths:\n+ Rigorous formulation of network load balancing as MPG with proofs that appear sound.\n+ Generally interesting and well-motivated application for MARL with promising potential\n\nWeaknesses:\n- Concern regarding representativeness of baselines used for evaluation\n- Practical benefits in terms of communication overhead & training time could be more strongly motivated\n\nDetailed Comments:\n\nOverall, the paper was interesting to read and the problem itself is well motivated. Formulation of the problem as an MPG appears sound and offers a variety of important insights with promising applications. There are, however, some concerns regarding evaluation fairness and practical benefits.\n\nThe baselines used for evaluation do not seem to accurately represent the state-of-the-art in CTDE. In particular, there have been a variety of recent works that explore more efficient strategies (e.g., [1-3]) and consistently outperform QMix with relatively low inter-agent communication. Although the proposed work appears effective as a fully-decentralized approach, it is unclear how well it would perform against more competitive CTDE baselines. Comparison against these more recent works would greatly improve the strength of evaluation.\n\nBenefits in terms of reduced communication overhead could also be more strongly motivated. Presumably, communication between agents could be done over purpose-built inter-LB links, thus avoiding QoS degradation due to contention on links between LBs and servers. Even without inter-LB links, the increase in latency demonstrated in Appendix E.2.2 appears relatively low.\n\nRobustness against dynamic changes in network setup are discussed to some degree, but it’s unclear how significant this issue is in a real-world environment. Even in a large-scale setup, the number of LBs/servers is likely to remain fairly constant at the timescales considered in this work (i.e., minutes). Given this, it seems that the paper should at least discuss trade-offs with a longer training time, which could impact the relative benefits of various approaches. \n\nSome confusion in notation:\n-\tAlgorithm 2, L8 should be t = 1,…,H (for horizon)?\n-\tL100, [M] denotes the set of LBs?\n\nMinor notes:\n-\tSome abbreviations are not defined, e.g., “NE” on L73\n-\tSuperscript notation in Eq 6 is not defined until much later (L166), which hindered understanding in an initial read.\n\n[1] S. Zhang et al, “Efficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control”, NeurIPS 2019.\n[2] Z. Ding et al, “Learning Individually Inferred Communication for Multi-Agent Cooperation”, NeurIPS 2020.\n[3] T. Wang et al, “Learning Nearly Decomposable Value Functions Via Communication Minimization”, ICLR 2020.\n - Is there any reason why the work cannot compare against more recent CTDE schemes?\n- Comments on other concerns?\n n/a"], "review_score_variance": 0.24, "summary": "The paper received an uniformly positive evaluation, although all the scores are in the \"borderline / weak accept\" range. The authors included a long and comprehensive rebuttal and actively participated in the discussion, which made some of the reviewers updating their scores.\n\nI recommend the paper to be accepted, but I understand the decision could be reverted when comparing the paper with the other candidates.", "paper_id": "nips_2022_s_mEE4xOU-m", "label": "train", "paper_acceptance": "Accept"}
{"source_documents": ["Transfer reinforcement learning (RL) aims at improving learning efficiency of an agent by exploiting knowledge from other source agents trained on relevant tasks. However, it remains challenging to transfer knowledge between different environmental dynamics without having access to the source environments. In this work, we explore a new challenge in transfer RL, where only a set of source policies collected under unknown diverse dynamics is available for learning a target task efficiently. To address this problem, the proposed approach, MULTI-source POLicy AggRegation (MULTIPOLAR), comprises two key techniques. We learn to aggregate the actions provided by the source policies adaptively to maximize the target task performance. Meanwhile, we learn an auxiliary network that predicts residuals around the aggregated actions, which ensures the target policy's expressiveness even when some of the source policies perform poorly. We demonstrated the effectiveness of MULTIPOLAR through an extensive experimental evaluation across six simulated environments ranging from classic control problems to challenging robotics simulations, under both continuous and discrete action spaces.", "The paper introduces a novel multi-source policy transfer problem, where we want to utilize policies from multiple source domains with different dynamics to improve the performance of the policy on our target dynamics. \n\nThe paper addresses the problem by adaptively aggregating the deterministic actions produced by source policies to maximize the expected return in the target environment. The method further trains an auxiliary network to predict a residual to revise the predicted action when some source policies are not useful or even adversarial.\n\n In my understanding, the paper assumes that the source and target domain shared the same task (reward structure) but only differs in dynamics. Also, the two domains share similar state and action spaces since the policy accepts the target states and predict actions in the target environment. This may limit the usage of the method.\n\nThe paper proposes to use residual learning as an auxiliary to compensate for the sub-optimal expressiveness of the source policies, which is novel and interesting.\n\nThe paper performs experiments on multiple environments. But the source and target domains only vary in some parameters of the agents. The domain gap seems small for these experiments. The paper needs a metric to measure the domain gap between source and target dynamics and report how the domain gap influences the proposed method and the baselines according to the metric.\n\nOne of my major concerns is that the limitation of the method with the same state and action space of the source and target domains. Also, there is no theoretical or intuitive analysis of how large the domain gap can be. This problem can be impractical for real-world applications with restrict limitations.\n\nPost-rebuttal:\n\nMy major concern is that the method is a naive combination of previous works and the paper is more like an engineering work. The method is also a weighted sum of source policies. There is no insight why the combination can work.\n\nNo assumption on source policies is given. That means I can get any random policy to learn a combination. This is like learning a policy from scratch by reinforcement learning. With better source policies, we can achieve better initialization for RL.\n\nThe paper is also related to hierarchical reinforcement learning, where the target reinforcement learning step is like building a high-level policy. \n\nThe work still requires lots of steps to train in the target domain, which does not fit to the real application of transfer RL. We hope transfer learning can adapt to the target environment fast.", "This paper presents a transfer reinforcement learning method that learns from existing source policies. The method aggregates deterministic actions produced by a collection of source policies to maximize expected return in the target environment. Unlike prior work it does not assume access to source environments nor source policy performance.\n\nThe method is intuitive and simple (simply a weighted sum over the actions of source policies). The paper is well-written in that it clearly explains the method and intuitions. The authors show results on a collection of different environments that include continuous and discrete action spaces. I appreciate the additional work put in to evaluate the distribution of performance. The method is well-ablated and addresses variants in which there is no reweighting and in which the residual is estimated independently of the state.\n\nI have some questions regarding the experiments:\n\n- In Table 1, do the authors have intuitions for why sometimes RPL is worse than MLP?\n- I'd like to see results comparing MULTIPOLAR with only bad sources with a randomly initialized policy\n- Given that source policies are needed for this to work, I'd like to see comparisons in which one continues to finetune an existing source policy. I know that the assumption here is that one does not have access to the internals of the source policies, but it would be nice to see how the performance compares.\n\nMy main concern has to do with the applicability of this method, since it seems to make strong assumptions on how different the domain dynamics are between source and target environments.", "Thank you authors for your detailed response. I am  my score to Accept. Good luck!", "We would like to thank all the three reviewers for their overall positive and constructive comments.  Based on their feedback, we have made the following modifications to our paper:\n\n1. Revised the introduction to include the real-world applications of our method, which address some of the Reviewer 1 and 3 concerns.\n\n2. Revised Appendix B.2 to include the additional experiment on MULTIPOLAR with randomly initialized policies, suggested by Reviewer 1.\n\n3. Added the URL of video replays of source policies, as well as MULTIPOLAR vs. baseline MLP in the Ant environment. This would clarify how different the source and target tasks are and confirm that our problem setting is challenging, which addresses some of the Reviewer 3 concerns.\n\n4. Fixed a few typos.\n\n5. Sectionalized Appendix B.\n\nWe did not include our other new experiment (suggested by Reviewer 1) that fine-tuned a randomly selected source policy in target environment instances. This is because fine-tuning an existing policy contradicts our primary assumption of not having access to the internals of the source policies.\n\nWe would also like to point out that in our initial submission, we made our code available and explained all the experimental details in our manuscript, which makes it possible to reproduce our experiments.\n\nFinally, please find the detailed responses to each of the reviewers below.", ">>> Q: \"the source and target domains only vary in some parameters of the agents. The domain gap seems small for these experiments.\" “The paper needs a metric to measure the domain gap between source and target dynamics and report how the domain gap influences the proposed method and the baselines according to the metric.” \"There is no theoretical or intuitive analysis of how large the domain gap can be. This problem can be impractical for real-world applications with restrict limitations.”\n\nA: We are aware that quantifying domain gaps is an important step in transfer learning and domain adaptation methods in supervised learning settings. However, it is not common to do so in the transfer RL domain [1]. This is mainly because source/target data samples in transfer RL are collected only by agents interacting with source/target environments, which means that there are not static datasets that can be used to measure the gap between source and target environmental dynamics. \n\nMoreover, analyzing \"how the domain gap influences the proposed method and the baselines\" is also a nontrivial problem. For instance, Ammar et al. propose a data-driven measurement called RBDist to measure the domain gap [3], but the paper only shows that the proposed metric could be an indicator to predict transfer performance with some limited and preliminary examples. It’s not obvious how such metrics would work for the various tasks that we did, and how it could measure gaps when multiple source environments exist.\n\nTo intuitively demonstrate the domain gap between source and target environment instances, we would like to share some videos of environment instances that were used in our Ant experiments https://www.youtube.com/watch?v=3b0mGeT3sLo. As shown in the videos, source and target robotic ants have different dynamics (e.g., different leg lengths, and frictions), which makes our problem very challenging.\n\nMoreover, as explained in our appendix, we designed the range of the environment dynamics/kinematics parameters for Roboschool environments following [6], which tackles a challenging problem in transfer RL with real-world applications.\n\nThat being said, we don't mean that measuring the gap between source and target environments is pointless, and we find it an interesting question that should be addressed in the future work, especially to transfer policies to real-world environments.\n\n[1] Matthew E. Taylor and Peter Stone. Transfer Learning for Reinforcement Learning Domains: A Survey. Journal of Machine Learning Research, 10(Jul):1633–1685, 2009.\n[3] Haitham Bou Ammar, Eric Eaton, Matthew Taylor, Decebal Constantin Mocanu, Kurt Driessens, Gerhard Weiss, and Karl Tuyls. An Automated Measure of MDP Similarity for Transfer in Reinforcement Learning. In Workshops at the AAAI Conference on Artificial Intelligence, 2014.\n[6] Tao Chen, Adithyavairavan Murali, and Abhinav Gupta. Hardware Conditioned Policies for MultiRobot Transfer Learning. In Advances in Neural Information Processing Systems, pp. 9355–9366, 2018. ", "We would like to express our gratitude to the reviewer for giving many comments and also recognizing the strengths of our work, such as \"The paper proposes to use residual learning as an auxiliary to compensate for the sub-optimal expressiveness of the source policies, which is novel and interesting.\"\n\n>>> Q: “In my understanding, the paper assumes that the source and target domain shared the same task (reward structure) but only differs in dynamics. Also, the two domains share similar state and action spaces since the policy accepts the target states and predict actions in the target environment. This may limit the usage of the method.” \"One of my major concerns is that the limitation of the method with the same state and action space of the source and target domains\"\n\nA: We would respectfully disagree with the reviewer's concern that our problem setting, which assumes source and target environments to be different only in their dynamics, limits the usage of our method. In fact, this problem setting is recognized as one of the common challenges in transfer RL, as introduced in a transfer RL survey [1], and has been studied extensively as we introduced briefly in Section 5 [2,3,4,5,6,7,8]. For instance, Yu et al. [8] presented their work in the last ICLR, which attempted to transfer policies between simulated and real robotic agents with different dynamics (https://openreview.net/pdf?id=H1g6osRcFQ), which we believe is a promising direction. Our work has advanced this line of research by proposing a new task and solution that could leverage multiple source policies without getting access to source environments. This contribution is recognized and supported by Reviewer 2: \"an innovative contribution that pushes the needle on the transfer RL literature.\"\n\nBesides, our assumptions do not limit the real-world applicability of our method. For example, we could apply MULTIPOLAR to sim-to-real tasks [9], industrial insertion tasks [10] (different dynamics comes from the differences in parts), and wearable robots (with different users) [11]. \n\nHaving said that, we observe that our current introduction may not be sufficient to show how this problem setting is common and promising in the transfer RL domain, which leads to the reviewer's concern. We promise to clarify this point in the final version of our paper.\n\n\n[1] Matthew E. Taylor and Peter Stone. Transfer Learning for Reinforcement Learning Domains: A Survey. Journal of Machine Learning Research, 10(Jul):1633–1685, 2009.\n[2] Alessandro Lazaric, Marcello Restelli, and Andrea Bonarini. Transfer of Samples in Batch Reinforcement Learning. In International Conference on Machine Learning, pp. 544–551, 2008.\n[3] Haitham Bou Ammar, Eric Eaton, Matthew Taylor, Decebal Constantin Mocanu, Kurt Driessens, Gerhard Weiss, and Karl Tuyls. An Automated Measure of MDP Similarity for Transfer in Reinforcement Learning. In Workshops at the AAAI Conference on Artificial Intelligence, 2014.\n[4] Jinhua Song, Yang Gao, Hao Wang, and Bo An. Measuring the Distance Between Finite Markov Decision Processes. In International Conference on Autonomous Agents and Multiagent Systems, pp. 468–476, 2016.\n[5] Andrea Tirinzoni, Andrea Sessa, Matteo Pirotta, and Marcello Restelli. Importance Weighted Transfer of Samples in Reinforcement Learning. In International Conference on Machine Learning, pp. 4936–4945, 2018.\n[6] Tao Chen, Adithyavairavan Murali, and Abhinav Gupta. Hardware Conditioned Policies for MultiRobot Transfer Learning. In Advances in Neural Information Processing Systems, pp. 9355–9366, 2018. \n[7] Hao Wang, Shaokang Dong, and Ling Shao. Measuring Structural Similarities in Finite MDPs. In International Joint Conference on Artificial Intelligence, pp. 3684–3690, 2019.\n[8] Wenhao Yu, C. Karen Liu, and Greg Turk. Policy Transfer with Strategy Optimization. In International Conference on Learning Representations, 2019.\n[9] Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen1, Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent Vanhoucke. Sim-to-Real: Learning Agile Locomotion For Quadruped Robots. In Robotics: Science and Systems, 2018\n[10] Gerrit Schoettler, Ashvin Nair, Jianlan Luo, Shikhar Bahl, Juan Aparicio Ojea, Eugen Solowjow, Sergey Levin. Deep Reinforcement Learning for Industrial Insertion Tasks with Visual Inputs and Natural Reward Signals. In International Conference on Machine Learning Workshop, 2019\nURL: https://openreview.net/pdf?id=ryg5E-gy3E\n[11] Juanjuan Zhang, Pieter Fiers, Kirby A. Witte, Rachel W. Jackson, Katherine L. Poggensee, Christopher G. Atkeson, Steven H. Collins. Human-in-the-loop optimization of exoskeleton assistance during walking. Science 356.6344, pp. 1280-1284, 2017.", ">>> Q: “Given that source policies are needed for this to work, I'd like to see comparisons in which one continues to finetune an existing source policy. I know that the assumption here is that one does not have access to the internals of the source policies, but it would be nice to see how the performance compares.”\n\nA: This is also an intriguing question. Based on this request, we conducted an extra experiment that fine-tuned a randomly selected source policy in target environment instances. Following our experimental procedure, the mean of average episodic rewards (over 3 random seeds and 3 random choices of a single source policy to be fine tuned in 100 environment instances) in the Hopper environment are:\n\n==========================================================================================\nPolicy                      |               0.5M               |               1M               |               1.5M               |               2M\n--------------------------------------------------------------------------------------------------------------------------------------------------\nMLP (FineTuned)          997 (945,1049)            1209 (1157,1261)         1338 (1286,1390)           1428 (1378,1479)\n==========================================================================================\n\nWe observe that the fine-tuned policies achieved a very high sample efficacy because they were able to 'learn' to interact with a target environment appropriately from the very early episodes to get a high return. With that said, this result does not affect our main contributions and conclusions that the proposed MULTIPOLAR worked well in the settings where source policies are hand-engineered or \"one does not have access to the internals of the source policies,\" as recognized by the reviewer. To further investigate the comparison, we also visualized the average learning curves of fine-tuning an existing source policy, MULTIPOLAR, RPL, and baseline MLP, which is available here: https://www.dropbox.com/s/zosq4pd0bfooykl/all.pdf?dl=0\n\nThese results suggest that MULTIPOLAR slightly outperforms finetuning an existing source policy in terms of final episodic rewards while finetuning is significantly more sample efficient than MULTIPOLAR.\n\n\n---\n>>> Q: “My main concern has to do with the applicability of this method, since it seems to make strong assumptions on how different the domain dynamics are between source and target environments.”\n\nAs the reviewer concerns, our method assumes that the environment structure (state/action space) is similar between source and target environments, while dynamics/kinematics parameters are different. However, please note that due to the nonlinearity of the environment dynamics, even little parameter changes would make a completely different environment  [2]. For example, a slight difference in dynamics parameters severely affects the low-level torque control in the robotic environments [2]. Hence, even with our assumption, the problem is still challenging.\n\nTo intuitively demonstrate the difficulties, we would like to share some videos of environment instances that were used in our Ant experiments https://www.youtube.com/watch?v=3b0mGeT3sLo which is also available in our code repository. As shown in the videos, source and target robotic ants have different dynamics (e.g., different leg lengths, and frictions), which makes our problem very challenging.\n\nBesides, our assumptions do not limit the real-world applicability of our method. For example, we could apply MULTIPOLAR to sim-to-real tasks [3], industrial insertion tasks [4] (different dynamics comes from the differences in parts), and wearable robots (with different users) [5].\n\nLastly, as explained in our appendix, we designed the range of the environment dynamics/kinematics parameters for Roboschool environments following [2], which tackles a challenging problem in transfer RL with real-world applications.\n\n\n[2] Tao Chen, Adithyavairavan Murali, and Abhinav Gupta. Hardware Conditioned Policies for MultiRobot Transfer Learning. In Advances in Neural Information Processing Systems, pp. 9355–9366, 2018. \n[3] Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen1, Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent Vanhoucke. Sim-to-Real: Learning Agile Locomotion For Quadruped Robots. In Robotics: Science and Systems, 2018\n[4] Gerrit Schoettler, Ashvin Nair, Jianlan Luo, Shikhar Bahl, Juan Aparicio Ojea, Eugen Solowjow, Sergey Levin. Deep Reinforcement Learning for Industrial Insertion Tasks with Visual Inputs and Natural Reward Signals. In International Conference on Machine Learning Workshop, 2019\nURL: https://openreview.net/pdf?id=ryg5E-gy3E\n[5] Juanjuan Zhang, Pieter Fiers, Kirby A. Witte, Rachel W. Jackson, Katherine L. Poggensee, Christopher G. Atkeson, Steven H. Collins. Human-in-the-loop optimization of exoskeleton assistance during walking. Science 356.6344, pp. 1280-1284, 2017.", "We genuinely thank the reviewer for the thorough, detailed, and insightful review, which also highlights the strengths of our work such as \"Unlike prior work it does not assume access to source environments nor source policy performance\", \"clearly explains the method and intuitions\", and \"the additional work put in to evaluate the distribution of performance\". We would be happy to incorporate any other suggestions the reviewer may have for our paper. Below is our response to the questions raised in the review:\n\n>>> Q: “In Table 1, do the authors have intuitions for why sometimes RPL is worse than MLP?”\n\nA: This a thoughtful and relevant question. First of all, we would like to clarify that, in our experiments, we extended the RPL method [1] to use a single source policy that was not trained or hand-engineered for a target environment dynamics. Although this is not the assumption of the original RPL papers (they assume the source to be designed to work on a target environment), we had to do so due to the lack of baseline methods for our new problem setting, where RPL is the most relevant approach presented recently. We observe this might however raise the reviewer’s confusion, and promise to clarify this point in the final version of our paper.\n\nIn our experiments, given that RPL had only a single source policy that was selected randomly from a pool of candidate source policies with diverse performance (shown in Figure 5 of the appendix A.4), it is likely that, on average, the selected source policy had a too low performance. On the other hand, MULTIPOLAR(K=4) has four source policies, which makes it more likely to have high-performing source policies helping the exploration or providing strong baseline actions.\n\n\n---\n>>> Q: “I'd like to see results comparing MULTIPOLAR with only bad sources with a randomly initialized policy”\n\nA: We evaluated MULTIPOLAR (K=4), where the sources are just randomly initialized policies, in the Hopper environment using our experimental procedure explained in section 4.1. Below is the bootstrap mean and 95% confidence bounds of average episodic rewards over various training samples for this experiment and MULTIPOLAR with four low-performing sources:\n\n===============================================================================\nMULTIPOLAR (K=4)                    |        0.5M          |        1M         |        1.5M        |       2M\n--------------------------------------------------------------------------------------------------------------------------------\n4 low performance [Table 3]            27 (26,27)        45 (44,47)        68 (66,71)        92  (88,95)\n4 randomly initialized                        27 (26,28)        47 (45,49)        73 (70,76)        101 (96,106)\n===============================================================================\n\nThis result shows that the sample efficiency of MULTIPOLAR with low-performing source policies (i.e., source policies which had low final episodic rewards in their own environments) is almost the same as with randomly initialized source policies.\n\nIn the final version of our paper, we will incorporate this experiment in the appendix due to the main text page limitation.\n\n\n[1] Tobias Johannink, Shikhar Bahl, Ashvin Nair, Jianlan Luo, Avinash Kumar, Matthias Loskyll, Juan Aparicio Ojea, Eugen Solowjow, and Sergey Levine. Residual Reinforcement Learning for Robot Control. In International Conference on Robotics and Automation, pp. 6023–6029, 2019.", "We would like to thank the reviewer for providing us with valuable comments and for recognizing the significance of our work, such as \"an innovative contribution that pushes the needle on the transfer RL literature\", \"The set of experiments covers a wide range of different standard RL tasks and they provide enough evidence that the approach works\", and \"able to extend the approach to the discrete action tasks.\" Below is our response to the question raised in the review:\n\n>>> Q: “I would however recommend providing more experimental results that provides evidence that the target policy can recover the right policy when the target environment dynamics is the same as one of the source environments.”\n\nA: We suppose that the question here is: in the case that the target environment dynamics is the same as one of the source environments, will MULTIPOLAR policy converge to a solution where $\\theta_{agg} \\approx 1$ for that source policy, $\\theta_{agg} \\approx 0$ for the rest of the source policies, and $ F_{aux}(s_t; \\theta_{aux}) \\approx 0$? In other words, does MULTIPOLAR essentially learn to mirror the output actions from the source policy that is obtained from the environment instance with the same dynamics as the target?\n\nIf we understood the question correctly, we would like to mention that, during the early stages of our work, we conducted several preliminary experiments with the suggested setup. We confirmed that MULTIPOLAR could learn a high-performing (sometimes significantly better than the source) policy very quickly; however, this was not achieved by recovering one of the source policies. The reason is twofold:\n\n(1) As discussed in our paper and shown in Figure 3, source policies are not guaranteed to work *optimally* in their environment instance. Hence, MULTIPOLAR converged to a better performing policy quickly by learning to aggregate all the source policies' actions and the residuals around them, rather than learning to recover one of the source policies.\n\n(2) In the space of policies, there may exist more than one high-performing solution for a given MDP [1].  Therefore, MULTIPOLAR, even in the case of having high-performing source policies,  may converge to any of the high-performing solutions rather than recovering a high-performing source policy. In fact, converging to a policy where $F_{aux}(s_t; \\theta_{aux}) \\approx 0$ would be quite difficult because it requires learning a function which maps every input states to a $D$-dimensional zero vector. Figure 6 of the appendix shows an example of a high-performing solution, in which MULTIPOLAR suppress the two useless low-performing policies and emphasizes the two high-performing policies, as the training progresses.\n\nPlease also note that in real-world settings, it rarely happens that the source and target environment instances have the same dynamics because even a tiny deviation in the dynamics/kinematics parameters makes a substantial difference in the environment state transition distribution [2].\n\n\n[1] Richard S. Sutton,  and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 1998.\n[2] Tao Chen, Adithyavairavan Murali, and Abhinav Gupta. \"Hardware conditioned policies for multi-robot transfer learning.\" Advances in Neural Information Processing Systems. 2018.", "In this paper authors propose a method for transfer reinforcement learning (RL). Specifically they are claiming that RL agents can transfer knowledge between each other about the environment dynamics. In order to showcase their approach they have come up with a new transfer RL task that makes use of some source policies trained under a diverse set of environment dynamics. Their key contributions to solve the task involve a decision aggregation framework that is able to build on top of relevant policies while suppressing irrelevant ones and an auxiliary network that predicts the residuals around the aggregated actions.\n\nI recommend the paper to be accepted since they have an innovative contribution that pushes the needle on the transfer RL literature although I do not think the contribution is substantial. The set of experiments covers a wide range of different standard RL tasks and they provide enough evidence that the approach works. I find it interesting that they are able to extend the approach to the discrete action tasks. \n\nI would however recommend providing more experimental results that provides evidence that the target policy can recover the right policy when the target environment dynamics is the same as one of the source environments."], "review_score_variance": 8.666666666666666, "summary": "The paper considers the case where policies have been learned in several environments - differing only according to their transition functions. The goal is to achieve a policy for another environment on the top of the former policies. The approach is based on learning a state-dependent combination (aggregation) of the former policies, together with a \"residual policy\". On the top of the aggregated + residual policies is defined a Gaussian distribution. The approach is validated in six OpenAI Gym environments. Lesion studies show that both the aggregation of several policies (the more the better, except for the computational cost) and the residual policy are beneficial. \n\nQuite a few additional experiments have been conducted during the rebuttal period according to the reviewers' demands (impact of the quality of the initial policies; comparing to fine-tuning an existing source policy).\n\nA key issue raised in the discussion concerns the difference between the sources and the target environment. It is understood that \"even a small difference in the dynamics\" can call for significantly different policies. Still, the point of bridging the reality gap seems to be not as close as the authors think, for training the aggregation and residual modules requires hundreds of thousands of time steps - which is an issue in real-world robotics.\n\nI encourage the authors to pursue this promising line of research; the paper would be definitely very strong with a proof of concept on the sim-to-real transfer task.", "paper_id": "iclr_2020_Byx9p2EtDH", "label": "train", "paper_acceptance": "reject"}
{"source_documents": ["The loss of a few neurons in a brain rarely results in any visible loss of function. However, the insight into what “few” means in this context is unclear. How many random neuron failures will it take to lead to a visible loss of function? In this paper, we address the fundamental question of the impact of the crash of a random subset of neurons on the overall computation of a neural network and the error in the output it produces. We study fault tolerance of neural networks subject to small random neuron/weight crash failures in a probabilistic setting. We give provable guarantees on the robustness of the network to these crashes. Our main contribution is a bound on the error in the output of a network under small random Bernoulli crashes proved by using a Taylor expansion in the continuous limit, where close-by neurons at a layer are similar. The failure mode we adopt in our model is characteristic of neuromorphic hardware, a promising technology to speed up artificial neural networks, as well as of biological networks. We show that our theoretical bounds can be used to compare the fault tolerance of different architectures and to design a regularizer improving the fault tolerance of a given architecture. We design an algorithm achieving fault tolerance using a reasonable number of neurons. In addition to the theoretical proof, we also provide experimental validation of our results and suggest a connection to the generalization capacity problem.", "Dear Reviewer, thank you for the review.\n\nWe provide the detailed description of the motivation of the paper here: https://iclr-2020-fault-tolerance.github.io/\n\nWeight matrices norm was proposed as regularization before [1]. Our novelty is computing the error in the continuous limit (Th1). Adversarial examples are the worst-case perturbations. We, in contrast, consider the average case. We only tangentially discuss them on page 18 of the suppl.\n[1] Gouk, Henry, et al. \"Regularisation of neural networks by enforcing lipschitz continuity.\"\n\nDropout is indeed connected to Fault Tolerance. It is in this context that this algorithm was invented [1]. Reducing overfitting came out as a bonus in Kerlirzin’s work and Dropout would, two decades later, be rebranded [2] as a regularization method. We, however, are interested in calculating the error rather than in generalization properties. While Dropout helps fault tolerance, it is not a solution to our problem: first, we do not know the $p$ exactly (suppl., p25). Next, we want to estimate the error, and not just make the error smaller.  We have added a paragraph summarizing this difference to the introduction in the final version.\n\n[1] P. Kerlirzin and F. Vallet. Robustness in multilayer perceptrons\n[2] G. E. Hinton et al Improving neural networks by preventing co-adaptation of feature detectors\n\nAs our Conclusion mentions, there are links between generalization and fault tolerance. While exploring the connection to the generalization problems would be exciting, as we mention in the conclusion, it is out of the scope of this paper. \n\nThe major solution to the problem of fault tolerance is redundancy [1]. The obvious way to apply that to neural networks is to replicate an off-the-shelf network multiple times. However, the single network itself might not utilize its neurons efficiently for fault tolerance. We improve the robustness of a single network.\n\nBatch normalization for improving fault tolerance is an interesting idea, but not a well-studied one. We run an experiment to determine the effect of batch normalization on fault tolerance. It shows that this technique does not result in major improvements for a small MNIST CNN: https://cutt.ly/0ePOco1\n[1] von Neumann, J. (1956). \"Probabilistic Logics and Synthesis of Reliable Organisms from Unreliable Components\"\n\nIndeed, table 2 is confusing. We have adjusted it in the final revision of the paper. Now we provide an explanation for the experiment and the table.\n\nThe goal is to compare fault tolerances of different networks under faults with prob. $p$. We take several train Dropout parameters $p_{train}$ and train the networks. We train with each $p_{train}$ 6 times, doing more repetitions until the variance is low enough.\n\nIt is known that when increasing $p_{train}\\in [0, p]$, the fault tolerance increases. This is also true in our experiment: the Experimental part  of Table 2 shows that crashing mean absolute error (MAE) \"correlates\" well with $p_{train}$ (as via rank loss).\n\nWe are looking for a metric which can order correctly which network is more resilient. We take crashing (for the network with faults)/correct (without faults) accuracy and MAE for test/train datasets, and also our bounds. We compute the rank loss between the metric and $p_{train}$. This shows how well the metric \"correlates\" with the resilience to faults. We only care about the correct ordering, so we use rank loss instead of correlation (more in suppl., Sec, 7.1-7.2).\n\nTwo parts of the table (left and right) have the same structure, and just describe different metrics. The alignment in the table design does not indicate that theoretical values should be compared to the experimental ones.\n\nThe table shows that only the T1 VarDelta bound has a low rank loss. This means that T1 can tell which network is more fault-tolerant, unlike the other metrics.\n\nThe algorithm lacks clarity in a sense that some constants are not explained due to the paper to the size limit. We made it more clear in the final version. Here we provide explanations on the algorithm.\n(line 6) Constants $1/3$ and $\\alpha$ are chosen in supl., Section 6.1\n(line 7) $n$ is the number of neurons at a layer with crashes\n(line 8) $R_3$ is defined below Eq. 1\n(line 9) The constant $C$, complexity, is fully defined in suppl, p18\n(line 12, 13) $n$ is the number of neurons at the layer with crashes\n(line 14) We copy the network multiple times $R$ (in hardware) with median aggregatio. We do not implement is as it is a well-established technique\n\nOur algorithm is a proof-of-concept, and it is not a major feature of the paper. In contrast, our bound (Theorem 1) is. Using it, we only show that our theoretical bound is also practical.\n\nWe consider neural networks implemented in neuromorphic hardware as an application. Thus, it is less important how expensive is the training procedure since it is done only once.\n\nDo not hesitate to write a response to us if you have more questions.", "Dear reviewer, \nThank you for your time. It seems that several points on our end would require clarification with regards to your review.\n\n(Connection to Dropout and differences) Dropout is indeed connected to Fault Tolerance. It is in this context that this algorithm was invented [1]. Reducing overfitting came out as a bonus in Kerlirzin’s work and Dropout would later be rebranded as it was crucial in the revival of neural networks [2,3] from 2012 on.  However, our problem is different from Dropout since we are interested in calculating the error rather than in generalization properties.\n\nWe note that Dropout indeed helps fault tolerance, but it cannot be seen as a solution to our problem: first, we do not know the true failure probability in hardware exactly (suppl., p25). Next, we want to bound the error and give an estimate of it. In contrast, Dropout only tries to make the error smaller.\n\n[1] P. Kerlirzin and F. Vallet. Robustness in multilayer perceptrons\n[2] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors\n[3] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks\n\n(clarity: Dropout and Fast Dropout) Dropout is a regularization technique using randomly crashing neurons during training. We study fault tolerance, which is a different problem: be resilient at the stage of inference to these randomly crashing neurons. We set our goal to calculate the error in the output. Dropout theory is aimed at explaining generalization properties and providing faster versions of Dropout. In particular, the paper on Fast Dropout considers the first-order Taylor expansion without bounding the remainder, as no guarantee is required. While we also use a Taylor expansion, in contrast, we explicitly bound the second-order remainder to guarantee robustness in all cases.\n\nWe note again that we solve a different problem compared to the study of Dropout.\n\n(Adversarial Examples and Dropout) These directions are interesting. However, we do not address the worst case of adversarial examples and we consider the average case. This would be an interesting research direction, but out of the scope of this paper.\n\n(Motivation of the paper) We have created a website briefly describing the paper's motivation, positioning and the approach: https://iclr-2020-fault-tolerance.github.io . Below we also describe the major points.\n\nOur motivation is the following: emergent neuromorphic hardware is efficient but prone to faults. We study the effect of such faults on the computation by bounding the error theoretically, and then show how to defend against them using a regularizer. Our problem that we consider (Fault Tolerance) is related to Dropout, but is distinct from Dropout.\n\n(Biological fault tolerance and squids) There is no known neurons in complex organisms where a single neuron loss would lead to a remarkable effect. Any organism whose survival or any major function would rely on a single neuron would have been cleared by the evolution rapidly.\n\nIn general, a human can sustain damage to entire regions of the brain (thousands of neurons) without loosing too much capacities. For example, a person had a nail in their brain for hours without realizing it [1]. In another case, the person had 90% of his brain slowly cease to exist, without any symptoms [2].\n\nFrom an evolutionary point of view, single cell die quite easily -- just because neural activity is extremely cytotoxic [3]. For instance visual cortex pyramidal neurons need to be aneuploid [4,5] just to sustain the stress induced by their activity. For example, for C. elegans’s it was found that the neural system rewires itself under stress, thus it is fault-tolerant [6].\n\n[1] https://www.theguardian.com/world/2012/jan/21/man-survives-shooting-nail-brain\n[2] https://www.sciencealert.com/a-man-who-lives-without-90-of-his-brain-is-challenging-our-understanding-of-consciousness\n[3] https://link.springer.com/article/10.1007/s12640-009-9051-z\n[4] https://www.pnas.org/content/102/17/6143\n[5] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5490080/\n[6] https://www.sciencedirect.com/science/article/pii/S0092867418316386 \n\n(Speed of Light and Neuromorphic hardware) Electrical signals are transmitted to the speed close to the speed of light.  Depending on the parameter of material called velocity factor that speed can range from 50% to 99% speed of light [1]. Compared to the current load-offload cycle of thousands nanoseconds (thousands of instructions, at a clock frequency of ~1GHz) in best of conditions of CPUs/GPU for a single propagation step, the speed of light computation is exponentially faster.\n[1] https://en.wikipedia.org/wiki/Speed_of_electricity\n\n(content: Citation format) Thank you for the note, we make the style compatible with ICLR in the final version.\n\nDo not hesitate to ask us for more clarification, if that is needed.", "Dear Reviewer, \n\nThank you for your time. We thank you for your review. The error is corrected in the final version. We will remain at your disposal for any additional clarifications, if the need for them to arise for you.", "\n\nReview: This paper considers the problem of dropping neurons from a neural network.  In the case where this is done randomly, this corresponds to the widely studied dropout algorithm.  If the goal is to become robust to randomly dropped neurons during evaluation, then it seems sufficient to just train with dropout (there is also a gaussian approximation to dropout using the central limit theorem called \"fast dropout\").  \n\nI think there are two directions I find interesting that are touched on by this paper.  One is the idea of dropping neurons as an adversarial attack, which I think has been studied empirically but not theoretically (to my knowledge).  However then it would be important to specify the budget of the attack - how many neurons can they remove and how precisely can they pick which neuron to remove?  Another would be studying the conditions for dropout to be useful as a regularizer (and not underfit), which are again somewhat understood experimentally but could deserve a more theoretical treatment.  \n\nHowever I don't think this paper solved a sufficiently clear problem and the motivation is somewhat confusing to me, especially when it seems like an analysis of dropout, and that isn't even mentioned until the 7th page.  \n\nNotes: \n  -Paper considers loss of function from loss of a few neurons.  \n\n  -Idea is to study more rigorously the fault tolerance of neural networks to losing a subset of the neurons.  \n\n  -In terms of impact of the work, one thing to consider is that even if a normally or arbitrarily trained network doesn't have perfect fault tolerance to dropping neurons, a neural network *trained* with dropping networks could learn hidden states which become more fault tolerant.  \n\n\nMinor Comments: \n  -I'm a bit unhappy with the argument about the brain losing neurons unless it has better referencing from neuroscience.  I imagine it's true in general but I wouldn't be surprised if some neurons were really essential.  For example squid have a few giant neurons that control propulsion.  It's just the first sentence so maybe I'm nitpicking.  \n\n  -  It also seems weird that the opening of the paper doesn't give more attention to dropout, since it's a well known regularizer and seems rather closely related conceptually.  \n\n  - In the intro it says neuromorphic hardware would pass information at the speed of light.  Is this really true?  My understanding is neuromorphic hardware would still generally use electrical or chemical signals but not pass things at the speed of light.  \n\n  - The citation format is not valid for ICLR. \n", "This contribution studies the impact of deletions of random neurons on prediction accuracy of trained architecture, with the application to failure analysis and the specific context of neuromorphic hardware. The manuscript shows that worst-case analysis of failure modes is NP hard and contributes a theoretical analysis of the average case impact of random perturbations with Bernouilli noise on prediction accuracy, as well as a training algorithm based on aggregation. The difficulty of tight bounds comes from the fact that with many layers a neural network can have a very large Lipschitz constant. The average case analysis is based on wide neural networks and an assumption of a form of smoothness in the values of hidden units as the width increases. The improve fitting procedure is done by adding a set of regularizing terms, including regularizing the spectral norm of the layers.\n\nThe robustness properties can be interesting to a wider community than that of neuromorphic hardware. In this sense, the manuscript provides interesting content, although I do fear that it is not framed properly. Indeed, the introduction and conclusion mention robustness as a central concern, which it is indeed, but the neuron failures are quite minor in this respect. More relevant questions would be: is the approach introduced here useful to limit the impact of adversarial examples? Does it provide good regularizations that improve generalization? I would believe that the regularization provided are interesting by looking at their expression; in particular the regularization of the operator norm of layers makes a lot of sense. That said, there is some empirical evidence that batch norm achieves similar effects, but with a significantly reduced cost.\n\nAnother limitation of the work is that it pushes towards very wide networks and ensemble predictions. These significantly increase the prediction cost and are often frowned upon by applications.\n\nIt seems to me that the manuscript has readability issues: the procedure introduced is quite unclear and could not be reimplemented from reading the manuscript (including the supplementary materials). Also, the results in the main part of the manuscript are presented to tersely: I do not understand where in table 2 dropout is varied.\n\nThe contributed algorithm has many clauses to tune dynamically the behavior of the regularizations and the architecture. These are very hard to control in theory. They would need strong empirical validation on many different datasets.\n\nIt is also very costly, as it involves repeatedly training from scratch a neural network.\n\nThe manuscript discusses in several places a median aggregation, which gives robustness properties to the predictor. I must admit that I have not been available to see it in the algorithm. This worries me, because it reveals that I do not understand the approach. The beginning of section 6.1, in the appendix, suggests details that are not understandable from the algorithm description.\n\nFinally, a discussion of the links to dropout would be interesting: both in practice, as dropout can be seen as simulating neuron failure during training, as well as from the point of view of theory, as there has been many attempts to analyze theoretically dropout (starting with Wager NIPS 2013, but more advanced work is found in Gal ICML 2015, Helmbold JMLR 2015, Mianjy ICML 2018).", "This paper investigates the problem of fault telorance\non NN: basically how the predictions are affected by\nfailure of certain neurons at prediction time. The analysis\nis theoretical under the classical assumption of lipschitz\nbounded non-linearities and looking at the limit case\nof an infinite number of neurons. The paper is well\nwritten and comes with public code that allows to replicate\nthe experiments illustrating the theoretical derivations.\n\nThe paper is well motivated and addresses a timely matter.\n\nTypos\n\n- \"the error of the output of\" -> \"the error of the output\"\n"], "review_score_variance": 8.666666666666666, "summary": " This paper focuses on the problem of robustness in the network with random loss of neurons.  However, reviewers had issues with insufficient clarity of the presentation, and lack of discussion about closely related dropout approach.\n\n \n ", "paper_id": "iclr_2020_rkl_f6EFPS", "label": "train", "paper_acceptance": "reject"}
{"source_documents": ["Belanger & McCallum (2016) and Gygli et al. (2017) have shown that an energy network can capture arbitrary dependencies amongst the output variables in structured prediction; however, their reliance on gradient-based inference (GBI) makes the inference slow and unstable. In this work, we propose Structured Energy As Loss (SEAL) \nto take advantage of the expressivity of energy networks without incurring the high inference cost. This is a novel learning framework that uses an energy network as a trainable loss function (loss-net) to train a separate neural network (task-net), which is then used to perform the inference through a forward pass. We establish SEAL as a general framework wherein various learning strategies like margin-based, regression, and noise-contrastive, could be employed to learn the parameters of loss-net. \nThrough extensive evaluation on multi-label classification, semantic role labeling, and image\nsegmentation, we demonstrate that SEAL provides various useful design choices, is faster at inference than GBI, and leads to significant performance gains over the baselines.\n", " Thank you for your prompt response! We will reflect your suggestions into our camera ready.", " Thank you for the detailed responses. My questions were primarily based on the added complexity of GBI for model training and the provided clarifications are really useful. I see the significance of the proposed technique and have accordingly raised my score.\n\nHowever, I want to stress that it will be very useful to add the clarification responses in your rebuttals to the main paper. For example, adding discussions on why alternating minimization seems to be stable because of the cross-entropy loss (it would be useful to see sensitivity study w.r.t. $\\lambda_1$ for that), a forward reference to the training time tables or direct inclusion in the main paper, better discussion on relation with InfNet, etc. For me, the comments were useful to helping understand the paper's impact and may help other readers as well.", " > The authors measure the performance of their model using F1 scores for classification. While the results are promising and valid, would it be possible to compute the AUC ROC too? This would provide a measurement agnostic to the cut thresholds, providing a better assessment of the classification performance.Furthermore, clarifications should be provided regarding how the cut threshold is computed to obtain the F1 score.\n\n\nFor F1 calculation, we use a fixed threshold of 0.5 for all the models, following previous literature of SPEN, DVN, and InfNet.\n\nWe compute the ‘samples’ version of [average precision score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\n), which is a threshold-free metric similar to AUC ROC.  As noted [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\n), for the ‘samples’ version, the mean of average precision values is computed across samples. The table below, which we plan to add to the appendix, shows the mean average precision for various models. Best results per dataset are annotated with asterisks; best results within a framework (energy network with GBI, SEAL-static, SEAL-dynamic) are marked in bold. Note that the models were not chosen with the best MAP score but chosen with the best F1 validation score.\n \n### MAP Performance for feature-based MLC datasets\n\n|                   |  Use of samples | **bibtex** | **delicious** | **genbase** | **cal500** | **eurlexev** | **expr_fun** | **spo_fun** | **Average** |\n|-------------------|---|:----------:|:-------------:|:-----------:|:----------:|:------------:|:------------:|:-----------:|-------------|\n| **cross-entropy** | x |     54.95 |        37.24 |      75.61 |     50.59 |       47.39 |      * **47.42** |      40.13 |    50.47   |\n|    energy only    |   |            |               |             |            |              |              |             |             |\n| **SPEN**          | x |     35.07 |    **25.36** |      42.75 | **36.93** |   **38.25** |   **40.05** |  **30.83** | **35.61**  |\n| **DVN**           | x | **36.68** |        17.57 |  **72.13** |     31.53 |       20.02 |       17.85 |      14.03 | 29.97      |\n| **NCE**           | o |      6.81 |         4.99 |      10.98 |     27.22 |        0.13 |       15.16 |       7.03 | 10.33      |\n|    SEAL-Static   |   |            |               |             |            |              |              |             |             |\n| **margin**        | x | **56.15** |    **39.77** |      66.21 |     50.96 |       47.45 |       47.07 |  **39.79** | 49.63      |\n| **regression**    | x |     54.40 |        34.31 |      98.80 |     50.58 |   * **47.65** |   **47.24** |      38.84 |  53.12 |\n| **NCEranking**    | o |     54.55 |        36.36 |  **98.94** | **51.49** |       47.53 |       46.63 |      39.29 |  **53.54** |\n|        SEAL-Dynamic       |   |            |               |             |            |              |              |             |             |\n| **margin**        | x |     55.06 |        36.63 |      98.82 |     49.07 |       40.17 |       46.42 |      37.60 | 51.97      |\n| **regression**    | x |     56.62 |        38.84 |      98.98 |     51.15 |       45.44 |   **47.33** | * **40.17** |  54.08 |\n| **regression-s**  | o | * **56.67** |    * **40.25** |      98.90 | * **51.51** |   **47.16** |       46.56 |      37.76 |  * **54.11** |\n| **NCEraking**     | o |     56.65 |        37.76 |      98.91 |     47.33 |       44.84 |       46.32 |      37.76 | 52.80      |\n| **ranking**       | o |     54.37 |        39.36 |  * **99.05** |     43.36 |       45.75 |       47.16 |      39.29 | 52.62      |\n\n\n### AAPD MAP Performance\n\n| method | MAP |\n| --- | --- |\n| BERT (cross-entropy) | 82.59 |\n| SEAL-dynamic-NCE | **83** |\n\n### MAP Performance for other text-based datasets in Appendix\n\n| method \\ datasets | BGC | NYT |\n| --- | --- | --- |\n| cross-entropy | 91.17 | 87.40 |\n| SEAL-dynamic-NCE | **91.53** | **88.11** |\n\n", " Thank your for taking your time and providing thoughtful reviews. \n\n> Nevertheless, we miss code to ensure reproducibility or a statement ensuring such code would be made available upon acceptance.\n\nA [link]([https://anonymous.4open.science/r/SEAL/README.md](https://www.google.com/url?q=https://anonymous.4open.science/r/SEAL/README.md&sa=D&source=docs&ust=1658967677872626&usg=AOvVaw3EnzLD_QGBB6UeaCRw_-jp)) to the anonymized code is provided in our submitted draft (footnote 3). We will also make our code public upon acceptance.\n\n \n> IOU metric vs. other metrics\n> \n\nCOCO metrics evaluate the precision and recall of multiple predicted objects per image in the task of instance detection and segmentation, where each image usually contain several objects; they are not suitable for the Weizmann Horse task, which is a binary segmentation task of a single object per image. For fine-grained evaluation of segmentation quality in the Weizmann Horse task, we follow DVN to calculate mean IoU between ground truth and predicted masks, unlike COCO evaluation during which IoU values are only utilized to filter out (segmentations with IoU values under a certain threshold) frivolous predictions.\n\n> Furthermore, would it make sense to execute additional experiments regarding image segmentation on a well-known benchmark dataset (e.g., PASCAL VOC)?\n> \n\nWe defer the study of task-net and loss-net architectures for more datasets including PASCAL VOC to future work, due to limited resources and as we think the set of experiments we present is already a handful to convey the effectiveness of SEAL framework.\n\n> In Table 1, the authors report the inference speed, but no information is provided regarding the hardware on which such models were executed for inference. Could such data be provided? We consider hardware specifications to provide context to put execution times in perspective.\n> \n\nWe used TitanX with 14GB CPU memory for the inference speed reports and for feature-based MLC dataset. For other experiments, we use GPUs with larger memory like M40 and RTX8000.", " Thank you for your time and thoughtful review.\n\n> The loss function of task-net is a weighted average with parameters λ1 and λ2, the authors fix the latter to 1 and perform bayesian search to determine the best λ1. The variability of this parameter is high and for certain tasks or energy losses it sets to be quite small.\n\nThe loss functions from cross entropy and energy network can come from wildly different function families and hence can have very different shape and steepness. For this reason, the weights for these losses cannot be compared on the same scale. Moreover, because the loss function coming from energy is learnt, there can be high variability in $\\lambda_1$ based on data and task. We agree that gaining further understanding of the loss functions coming from the energy network is important and leave it as future work.\n\n> For example, Table 7 in Appendix C.2.1 shows that for genbase (the best performing task for the presented model) the influence of loss-net is quite small compared to simple BCE. Nonetheless, the model performs substantially better than CE.\n> \n\nAs we stated above, it’s hard to exactly decompose the effect of loss-net. That being said, noticing the out-of-norm behavior of genbase, we analyze its peculiar data distribution in appendix E.3.1.\n\n> Given the closeness of the results for SEAL-static and SEAL-dynamic which one would the authors recommend for use? Is there any advantage in one above the other based on context?\n> \n\nWe recommend SEAL-dynamic as it usually performs better and as it also simplifies the engineering procedure because SEAL-static requires separate training of loss-net prior to training of task-net whereas SEAL-dynamic does not require such procedure and can start from scratch.\n\n> What’s the behavior in training of SEAL-dynamic? I imagine that given the alternating minimization approach training would be highly variable in early stages. Does it require more epochs to be trained due to its alternating minimization nature? Could the author provide details in this regard? I could not find any mention of it either in the main text nor the appendix.\n> \n\nOwing to the cross entropy term in our loss function in $L_F$ (eq.1), the learning procedure is pretty stable. We did not observe any significant difference between the number of epochs it takes to train SEAL-dynamic vs. cross-entropy loss. While the number of epochs varies by dataset, the average number of epochs is slightly smaller for SEAL-dynamic. \n\nFor more specific training-time analysis, please refer to runtimes (Table 19) & model parameter sizes (Table 17) in the appendix. \n\n> how is batch size selected? It seems it should be a crucial parameter in stabilizing training and performances but it is not mentioned.\n> \n\nSEAL-dynamic trains without stability issues on most hyper-parameter settings, including various batch sizes, due to the presence of cross-entropy in the loss. Below we mention the procedure we use to find the hyper-parameters (including the batch size).\n\n1. We optimized hyper-parameters like feedforward network sizes and batch size on CE models (baseline feedforward models) first and fixed those hyper-parameters for other models giving an advantage to the baseline. \n2. After that, we only searched for additional hyper-parameters like loss-net size and loss weight. \n\nNote that even when we do not optimize every possible hyper-parameter for the SEAL framework, this framework outperformed both feedforward and energy network models in a stable manner.", " Thank your for taking your time and providing thoughtful feedback.\n\nBefore answering specific questions, we wanted to clarify the central premise of our paper: “energy network as a loss”.  Please note that SEAL-dynamic does not require any kind of gradient-based inference (GBI) steps, neither in training nor in inference time. The energy network (loss-net) provides a differentiable loss function that can be use to backprop all the way to the feedforward network. \n\nTo put the previous work in context, Tu et. al. claimed that the feedforward network is mimicking inference-time behavior of energy network, which leads one to believe that one first obtains the GBI output $\\mathbf{\\hat y}$ and then makes the feedfoward output $\\mathbf{\\tilde y}$ follow $\\mathbf{\\hat y}$. However, this is not what happens in Tu et al., nor in our paper as the energy network is used as loss to update feedforward network without calculating $\\mathbf{\\hat y}$. By reinterpreting energy network as  loss, we enable SEAL to utilize various different loss formulations and be not limited by adversarial formulation of InfNet.\n> Limitations should be better discussed: The use of a feedforward network to map the input to the GBI output of the energy based network can be thought of as \"learning to do gradient-based inference\". It is unclear to what extent the strategy can be expected to generalize.\n> \n\nAs we stated above, we want to clarify that we are not learning to perform GBI, but instead we are utilizing energy network as a learnable loss function that can capture dependency across structured multivariate output. \n\nWe believe SEAL framework can be applied to any structured prediction problem where we can relax the discrete output space to a probability simplex. We also showed that our method can be applied to different network structures (MLP, BERT, CNN), to different tasks, and to different data characteristics. If these are not the axis of generalization that the reviewer was looking for, please let us know so we can better address your concerns.\n\n> Is the InfNet model in Table 1 from Tu et al. (2020)? That is, is the updated and closer to the current method version of the baseline being used for comparison?\n\nYes, it is 2020 model with cost-augmented layer. \n\n> Relation with InfNet\n\nWhat we tried to establish in our paper in relation to InfNet is two folds.  \n\n1. We are generalizing as we show that loss for feedforward network, -E(x,y), and energy loss does not have to hold specific relationship such as an adversarial relationship used in Tu et al., 2019, 2020, i.e.  $\\min_\\Theta \\max_\\Phi f(x,y;\\Theta, \\Phi)$ .  This opens up many other loss functions to be applied into our propoesd SEAL framework as we show with the regression and NCE loss.\n2. We are reinterpreting the mechanism of InfNet. As we stated above, we wanted to clearly identify what is the success behind InfNet. We hypothesized the reason of success was because the energy network was used as a loss function and provided experimental results  that supports our hypothesis. Experiments with SEAL-static especially well exhibits this behavior as loss-net is trained once and fixed as a  loss function.\n\n> How expensive is SEAL's training? How does it fare in comparison to other methods? I suspect that twice the model size (due to a Feedforward network and an energy based network)\n\nWe compare the SEAL-dynamic’s runtimes (Table 19) take & model parameter sizes (Table 17) in the appendix. It is correct that it takes longer on train time and more memory. However, at inference time it is faster than energy network that runs GBI and takes the same memory and same speed as the conventional feedforward network.\n\n> plus having to run GBI to get the energy-based network's output will yield a significant training cost.\n\nWe do not run GBI during training of feedforward network in SEAL framework. Please refer to the first two paragraphs of this response for details.\n\nFor SEAL-static, we do have extra memory consumption for loss-net. Please refer to the Table 17, 19. \n\n> Alternating minimization optimization problems can often be sensitive and hard to tune. Did that cause any difficulty for the dynamic model? Regardless, I think this point should be discussed.\n\nWe believe the cross-entropy term in the loss term stabilizes the learning process. Because of this we did not observe any stability issues in SEAL dynamic.\n\n> The global energy in Equation 2 does not have a dependence on x. Is this correct?\n\n\nThat is correct. This is the same equation used in Belanger and McCallum (2016), Gygli et al. (2017), and Tu et al., (2019) for multi-label classifcation. The local term takes care of energy between input x and each label $y_i$, while the global energy term takes care of energy across multivariate $y_1, y_2, \\dots, y_L$.\n\nThe only exception to this formulation was binary image segmentation following Gygli et al. (2017) for a fair comparison.", " The authors propose using the structured energy network as a loss to achieve faster and better performance on structured prediction problems. They demonstrate that their approach achieves competitive results, where a dynamic component of the loss can adapt to specific characteristics of the problem at hand. The authors present a novel framework where a trainable loss function is used to train a network to perform the inference through a forward pass. The paper is well structured, and the main concepts are clearly conveyed to the reader. The authors created many experiments and considered the ceteris paribus principle to ensure the results could be attributed to a particular change or aspect of their framework, ensuring the quality of research. They demonstrated their approach on various tasks and datasets, obtaining good results. Nevertheless, we miss code to ensure reproducibility or a statement ensuring such code would be made available upon acceptance. 1) The authors measure the performance of their model using F1 scores for classification. While the results are promising and valid, would it be possible to compute the AUC ROC too? This would provide a measurement agnostic to the cut thresholds, providing a better assessment of the classification performance. Furthermore, clarifications should be provided regarding how the cut threshold is computed to obtain the F1 score.\n\n2) The authors report the mean IoU metric for image segmentation. It would make sense to compute some additional metrics (e.g., https://cocodataset.org/#detection-eval) to make it easier to compare the results against other well-known models? Furthermore, would it make sense to execute additional experiments regarding image segmentation on a well-known benchmark dataset (e.g., PASCAL VOC)? \n\n3) In the description provided for Table 1, please consider: \"are marked with underline\" -> \"are underlined\"\n\n4) The authors may be interested in the following research work, at least for the literature review section: Bechtle, Sarah, et al. \"Meta learning via learned loss.\" 2020 25th International Conference on Pattern Recognition (ICPR). IEEE, 2021.\n\n5) In Table 1, the authors report the inference speed, but no information is provided regarding the hardware on which such models were executed for inference. Could such data be provided? We consider hardware specifications to provide context to put execution times in perspective. We did not find potential negative societal impacts in the present work. The authors did not provide details regarding the limitations of the proposed approach.", " Energy based networks have to run gradient based inference (GBI), which can be quite expensive. In this paper, the authors propose to use an energy based network to develop a structured prediction loss. Thereafter (or simultaneously for the dynamic approach), a feedforward network is learned that tries to match the energy network's output (modeling structure) and a cross-entropy loss w.r.t. ground truth outputs. Combined, the proposed technique gives the inference efficiency of feedforward networks, yet is able to model structure similar to an energy network. \n ### Strengths\n\nThe proposed method is quite simple, yet effective. The idea is fairly obvious, but getting the details right is laudable. The paper is decently well-written and well-motivated. \n\n### Weaknesses\n\n* Limitations should be better discussed: The use of a feedforward network to map the input to the GBI output of the energy based network can be thought of as \"learning to do gradient-based inference\". It is unclear to what extent the strategy can be expected to generalize. \n\n* Relation with InfNet: The authors argue InfNet is a special case of their proposed method. While I understand that there is a train-test mismatch for InfNet, the follow-up by Tu et al. (2020) where two separate networks are used for learning an adversarial sampler and for performing inference gets very close to the proposed method. It would be helpful to focus more on the discussion separating these methods. Currently, the discussion makes it seem like the proposed SEAL method is akin to a slight extension. \n\nPost rebuttal: The authors justifiably addressed my concerns and I have accordingly raised my scores.\n * Is the InfNet model in Table 1 from Tu et al. (2020)? That is, is the updated and closer to the current method version of the baseline being used for comparison?\n\n* How expensive is SEAL's training? How does it fare in comparison to other methods? I suspect that twice the model size (due to a Feedforward network and an energy based network) plus having to run GBI to get the energy-based network's output will yield a significant training cost. \n\n* Alternating minimization optimization problems can often be sensitive and hard to tune. Did that cause any difficulty for the dynamic model? Regardless, I think this point should be discussed.\n\n\n#### Minor comments:\n\n* The global energy in Equation 2 does not have a dependence on x. Is this correct? It will be valuable to discuss the limitations of using a feedforward network to \"learn to do GBI\". ", " The paper proposes the combined use of structured prediction energy networks (loss-net) as a loss function that considers relations in the output. This loss function is then used to train a feed forward network (task-net) to speed up inference at test time. They proposed both a static and dynamic approach to train these two networks.  The paper is well written and easy to follow, it also does a good job in placing itself in the literature and, as the authors state themselves, it is a generalization of previous work [Tu et al, 2020] aimed at reducing inference time for energy networks. Results show remarkable improvement over the state of the art.  \n- The loss function of task-net is a weighted average with parameters $\\lambda_1$ and $\\lambda_2$, the authors fix the latter to 1 and perform bayesian search to determine the best $\\lambda_1$. The variability of this parameter is high and for certain tasks or energy losses it sets to be quite small. For example, Table 7 in Appendix C.2.1 shows that for genbase (the best performing task for the presented model) the influence of loss-net is quite small compared to simple BCE. Nonetheless, the model performs substantially better than CE. Could authors provide an intuition of why this is happening? It seems to be consistently true for margin as an energy loss across tasks and there are other interesting patterns for different tasks. A discussion on this might help in future usability of the proposed approach. \n- Could the author provide confidence intervals for the results?\n- Given the closeness of the results for SEAL-static and SEAL-dynamic which one would the authors recommend for use? Is there any advantage in one above the other based on context? \n- What’s the behavior in training of SEAL-dynamic? I imagine that given the alternating minimization approach training would be highly variable in early stages. Does it require more epochs to be trained due to its alternating minimization nature? Could the author provide details in this regard? I could not find any mention of it either in the main text nor the appendix. \n- Similar to the question above, how is batch size selected? It seems it should be a crucial parameter in stabilizing training and performances but it is not mentioned. \n See questions. "], "review_score_variance": 0.22222222222222224, "summary": "This work proposes using structured energy networks as loss functions for training feed forward networks to solve structured prediction tasks. The reviewers find the paper to be well written and easy to follow. The contribution is well positioned with respect to the literature and empirical results are strong. During the discussion period the authors addressed the concerns of the most negative reviewer sufficiently for them to increase their score. I can therefore recommend accepting this paper.", "paper_id": "nips_2022_F0DowhX7_x", "label": "train", "paper_acceptance": "Accept"}
{"source_documents": ["In recent years, neural implicit surface reconstruction methods have become popular for multi-view 3D reconstruction. In contrast to traditional multi-view stereo methods, these approaches tend to produce smoother and more complete reconstructions due to the inductive smoothness bias of neural networks. State-of-the-art neural implicit methods allow for high-quality reconstructions of simple scenes from many input views. Yet, their performance drops significantly for larger and more complex scenes and scenes captured from sparse viewpoints. This is caused primarily by the inherent ambiguity in the RGB reconstruction loss that does not provide enough constraints, in particular in less-observed and textureless areas. Motivated by recent advances in the area of monocular geometry prediction, we systematically explore the utility these cues provide for improving neural implicit surface reconstruction. We demonstrate that depth and normal cues, predicted by general-purpose monocular estimators, significantly improve reconstruction quality and optimization time. Further, we analyse and investigate multiple design choices for representing neural implicit surfaces, ranging from monolithic MLP models over single-grid to multi-resolution grid representations. We observe that geometric monocular priors improve performance both for small-scale single-object as well as large-scale multi-object scenes, independent of the choice of representation. ", " Thank you very much for your reply and for increasing your score. We are happy to change the title if the reviewers and AC recommend this.", " Thanks to the authors for addressing my concerns. After reading the rebuttal, I still find the experiments on architectural choices distracting to the main contribution of this paper. In essence, the results showed that different architectures respond differently to adding monocular depth and normal cues. If the objective is to maximize the accuracy *with monocular cues*, then the experimentation should be leaned towards that scenario, rather than simply comparing the four architectures for the purpose of constructing a baseline. After all, the architecture is another topic.\n\nI'm willing to increase my score as my other concerns have been addressed. My rating is borderline accept as I think the contribution/improvement is largely due to the leverage of pre-trained omnidata model for monocular depth and normal. The paper also has a gap between the title and the content. The title says it's exploring \"monocular geometric cues\" while the paper is actually resorting to extermal models with monocular depth and normals, which is more like a prior and not what I'm expecting after reading the title.", " Thanks for the kind response.", " Dear Reviewer D9of,\n\nThank you very much for your reference and for increasing your score.\n\nFirst, please note that the reference paper is submitted to arXiv after the NeurIPS deadline and should be considered as concurrent work. Second, compared to this paper, our approach is not restricted to indoor scenes, and we did experiments on the challenging 3-view setting in the DTU dataset. Our approach is the first method that yields reasonable reconstruction on the large-scale Tanks and Temples dataset. Further, we demonstrated the effectiveness of monocular cues on various neural scene representations, ranging from MLP to multi-res. feature grids.\n\nRegarding comparison to other cues, we provided a comparison with Manhattan-SDF, which utilises semantic and multi-view stereo depth maps, and other baselines in our response (1). Our monocular cues significantly improve the reconstruction quality.\n\nWe believe our approach is well-qualified and brings value to the NeurIPS community, and we would be grateful if you could increase your score to acceptance.\n\nThank you very much for your time.", " Thanks for the response. Regarding the details which have partially solved my questions. I am willing to increase my score.\n\nHowever, I am still not too convinced by the proposed novelty. Whether the proposed method is incremental or not is not checking if there is anyone working on exactly the same idea or not, otherwise, anyone can find one point which is definitely not investigated by previous papers. I understood the paper wanted to show the importance of \"monocular\" cues compared to cues in general. Then it should include more experiments to show why it is critical.\n\nTherefore, I still tend to keep my negative rating.\n\nAlso, the following paper seems also uses normal supervision from monocular prediction.\nhttps://arxiv.org/pdf/2206.13597.pdf", " Dear Reviewer D9of,\n\nThanks for your reply. First, using depth maps from multi-view stereo as additional supervision fails in the indoor scenes with lots of textureless regions, see the table in our response (1). Because multi-view stereo methods use photometric cues to establish correspondences, they struggle in texture-less scenes and scenarios with sparse views. In contrast, monocular depth and normal estimators are trained on large-scale datasets in a supervised manner, and their outputs can serve as strong prior for optimizing neural implicit models.\n\nSecond, to the best of our knowledge, our approach is the first method that utilizes monocular depth and normal cues to neural implicit surface models and has demonstrated significant improvements over baseline methods on several challenging datasets. We are not aware of prior work that studied mono cues in the context of Neural implicit surface reconstruction and would be grateful if you could provide more details regarding prior work that investigated this setting.\n\nThank you very much for your time.", " Hi, thanks so much for the very long response to my questions.\nHowever, I still think the contribution is relatively incremental and weak, and I disagree with your response.\nI don't think there is a fundamental difference between using additional \"cues\" either from multi-view reconstruction or monocular predictions. Both have pros and cons, and there are several papers that have already done that.", " \nDear Reviewer grib,\n\nThank you again for your review. We hope that our rebuttal could address your questions. As the discussion phase is only 4 hours left, we wondered if you might still have any concerns we could address.\n\nThank you for your time.", " \nDear Reviewer Ak1a,\n\nThank you again for your review. We hope that our rebuttal could address your questions and concerns. As the discussion phase is only 4 hours left, we would be grateful to hear your feedback and wondered if you might still have any concerns we could address.\n\nThank you for your time.", " Dear Reviewer D9of,\n\nThank you again for your review. We hope that our rebuttal could address your questions and concerns. As the discussion phase is only 4 hours left, we would be grateful to hear your feedback and wondered if you might still have any concerns we could address.\n\nThank you for your time.", " Thanks for sharing this additional results.\n\nIt is pretty clear now the proposed network out-performs previous ones even with similar number of parameters (I am comparing MLP 12 layers v.s. multi-res 2^15). The visual results of rendered RGB also looks reasonable, and would be nice to include more discussion about this in the revision.\n\nI don't have any further concerns and will keep my rating as acceptance.\n", " \nDear Reviewer grib,\n\nThank you again for your review. We hope that our rebuttal could address your questions and concerns. As the discussion phase is nearing its end, we would be grateful to hear your feedback and wondered if you might still have any concerns we could address.\n\nThank you for your time.", " Dear Reviewer c8RS,\n\nThank you again for your review. We hope that our rebuttal could address your questions and concerns. As the discussion phase is nearing its end, we would be grateful to hear your feedback and wondered if you might still have any concerns we could address.\n\nThank you for your time.", " Dear Reviewer Ak1a,\n\nThank you again for your review. We hope that our rebuttal could address your questions and concerns. As the discussion phase is nearing its end, we would be grateful to hear your feedback and wondered if you might still have any concerns we could address.\n\nThank you for your time.", " Dear Reviewer D9of,\n\nThank you again for your review. We hope that our rebuttal could address your questions and concerns. As the discussion phase is nearing its end, we would be grateful to hear your feedback and wondered if you might still have any concerns we could address.\n\nThank you for your time.", " Thank you for your recognition and your time. We are very glad that you consider this paper a technically strong paper with novel ideas and excellent impacts, evaluation, resources. We address your remaining comments below.\n\n**Ablation on the number of input images and monocular geometric cues.**\n\nWe ran the experiments with a different number of input images and monocular geometric cues. Please refer to https://imgur.com/a/jqkYC1w for a comparison. We found that adding the monocular geometric cues leads to consistent improvements. We will add these results to our revised paper.\n", " Thank you for your insightful comments. We appreciate that you find that our approach is simple and effective, our experimental results are thorough, and our paper is well written. We now address the remaining comments in the following.\n\n**Number of parameters of each network.**\n\nWe list the number of parameters of each network used in the following table::\n\n|Method|MLP| Dense SDF Grid | Single-res. Feature Grid | Multi-res. Feature Grid|\n|-|-|-|-|-|\n|Num. Params | 0.7M | 6.4M | 33.8M | 12.8M|\n\nAs reviewer D9of asked for a more detailed comparison of different architectures, we ran additional experiments with different architecture configurations. More specifically, we consider MLPs with varying numbers of layers, and Multi-res. Feature Grids with varying sizes of hash tables, to evaluate the performance under different model capacities. \n\nWe list the number of learnable parameters under different architecture configurations in the table below and also show their performance over the optimization process in https://imgur.com/a/btDDxEC. \n\n|Model configuration | Num. Params|\n|-|-|\n|MLP (2 layers) | 0.15M |\n|MLP (4 layers) | 0.26M |\n|MLP (8 layers) | 0.63M |\n|MLP (12 layers) | 0.8M |\n|Multi-res. Feature Grids (hash table size $2^{13}$)| 0.41M |\n|Multi-res. Feature Grids (hash table size $2^{15}$)| 1.1M |\n|Multi-res. Feature Grids (hash table size $2^{17}$)| 3.67M |\n|Multi-res. Feature Grids (hash table size $2^{19}$)| 12.67M |\n\nOur experiments show that using monocular geometric cues improves reconstruction quality and convergence speed independent of the network configuration, which supports our claims in the paper. We will add this experiment and the results to the revised version of the paper. \n\n**Visualizing the texture of the surface.**\n\nPlease refer to https://imgur.com/a/1dYdvXE for comparing novel view synthesis results on the DTU dataset with three input views. We further a provide quantitative comparison as follows:\n\n|Method|MLP| MLP w/ cues|\n|-|-|-|\n|PSNR | 17.65 | **23.64** |\n\nUsing monocular geometric cues improves novel view synthesis results significantly. We will add both qualitative and quantitative results to our revised paper.\n\n**Showing actual image views or ground truth rendered images.**\n\nThanks for the great suggestion. We will add the ground truth images to our revised paper. We also provide an updated comparison (https://imgur.com/a/19oXNTJ and https://imgur.com/a/LLtMc4E) which includes the ground truth mesh for reference.\n", " Thank you for your valuable comments and constructive feedback. We address your concerns in the following. \n\n**The exploration of the \"monocular geometric cues\" is relatively weak. It is curious to see how different depth estimators may affect the improvement when incorporating the additional prior.**\n\nThanks for the question. We ran additional experiments to compare different depth estimators, yielding the following results:\n\n|Method|MLP| w/ MiDaS depth [1] | w/ LeReS depth [2]| w/ Omnidata depth|\n|-|-|-|-|-|\n|F-score | 64.2 | 68.6 | 72.6 | 86.7|\n\nAs shown in the table, adding monocular depth cues improves our performance over a single MLP, independently from the depth predictors we use. Unsurprisingly, better depth predictors lead to better performance, with the state-of-the-art Omnidata model giving the best results. We thus believe that the development of better depth cues will further improve the performance of our approach. \n\nMoreover, we tested our model with different monocular normal predictors:\n\n|Method|MLP| w/ Tilted normal [3] | w/ Omnidata normal|\n|-|-|-|-|\n|F-score | 64.2 | 78.3 | 92.2 |\n\n\nWe find that both monocular normal predictors improve the results, and similarly to our observations above, using normals predicted by the state-of-the-art Omnidata model leads to the best performance. We will add these results to the final paper.\n\n**It is also peculiar that this paper chose depth and normal as the only two monocular cues to experiment. The Omnidata is able to generate high quality ground truth for 19 more more tasks.**\n\nWe use depth and normal cues as they are directly related to geometry and naturally integrate into the volume rendering formulation (see L166-L178 in the main paper). Our experiments show that using these two cues already significantly improves performance, even compared to Manhattan-SDF, which uses depth, planarity, and semantic cues. We agree that exploring other monocular cues such as occlusion edges and curvature is an interesting future direction.\n\n**Appearance aspect of the reconstruction.** \n\nThanks for the suggestion! Please refer to https://imgur.com/a/1dYdvXE for comparing novel view synthesis results on the DTU dataset with three input views. We further provide quantitative results: \n\n\n|Method|MLP| MLP w/ cues|\n|-|-|-|\n|PSNR | 17.65 | **23.64** |\n\nUsing monocular geometric cues improves novel view synthesis results significantly. We will add both qualitative and quantitative results to our revised paper.\n\n\n**Experiments for the architectures are confusing.**\n\nWe first evaluate the different architectures without the cues to establish baseline results in order to measure the improvements obtained with the cues. Without using monocular cues, we find that Multi-res. Feature Grids perform better than MLPs, while after using monocular geometric cues, both architectures improve and achieve similar results on the Replica dataset. We agree that this is an interesting finding. We hypothesize that for the MLP, the model capacity is better allocated around surface regions of the 3D geometry if additional prior information is used during optimization, while the Multi-res. Feature Grids have enough model capacity for both cases, optimization with and without prior information. Further, we find that MLPs perform better than Multi-res. Feature Grids in the datasets with noisy observations (e.g., motion blur in image) and noisy camera poses such as ScanNet. Generally, a single MLP is robust to noises but tends to yield smooth surfaces, while Multi-Res. Feature Grids can capture details and converge fast but are less robust to noise and ambiguities in the input images. We will add this discussion to the paper.\n\n**I'm wondering what is the performance comparison between the proposed monocular priors vs self-supervised depth+normal estimation?**\n\nWe ran an experiment using a monocular depth map from a pretrained state-of-the-art self-supervised indoor depth estimator [4] in our framework: \n\n|Method|MLP| w/ self-supervised depth [4] | w/ Omnidata depth|\n|-|-|-|-|\n|F-score | 64.2 | 45.6 | 86.7 |\n\nThe self-supervised depth estimator degrades performance. We hypothesize that this is due to the weaker performance of the self-supervised model which is also trained with an RGB loss and hence suffers from the under-constrained problem of recovering geometry from multi-view images. We will add these results to the paper.\n\n**References**\n\n[1] Ranftl et al., Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer, T-PAMI 2022\n\n[2] Yin et al., Learning to Recover 3D Scene Shape from a Single Image, CVPR 2021\n\n[3] Do et al., Surface Normal Estimation of Tilted Images via Spatial Rectifier, ECCV 2020\n\n[4] Li et al., StructDepth: Leveraging the structural regularities for self-supervised indoor depth estimation, ICCV 2021", " **How robust is the monocular cue supervision to the model, e.g., when there are noise or large errors in the monocular predictions**\n\nThanks for the question. We did not observe any significant errors in the monocular predictions in the real-world datasets (ranging from object to large-scale indoor scenes). Even for ScanNet dataset where motion blurs or noises are present in the RGB images, the predicted monocular depths and normals are still of high quality. However, we do observe that the model predicts infinite depth in some region of the Replica dataset (e.g. windows or doors with completely black colors). We filter out these images if the maximum depth is ten times larger than the minimum depth in the image. \n\nTo further analyze the robustness of our approach to monocular geometric cues of different levels of quality, we further tested our model with different depth predictors: \n\n|Method|MLP| w/ MiDaS depth [6] | w/ LeReS depth [7]| w/ Omnidata depth|\n|-|-|-|-|-|\n|F-score | 64.2 | 68.6 | 72.6 | 86.7|\n\nFor all three methods, adding monocular depth improves performance over a single MLP without cues. Unsurprisingly, better depth predictors lead to better performance, with the state-of-the-art Omnidata model giving the best results. We thus believe that the development of better depth cues will further improve the performance of our approach.\n\nWe also tested our model with different monocular normal predictors and obtain the following results:\n\n\n|Method|MLP| w/ Tilted normal [8]| w/ Omnidata normal|\n|-|-|-|-|\n|F-score | 64.2 | 78.3 | 92.2 |\n\nWe find that both monocular normal predictors improve the results, and similarly to our observations above, using normals predicted by the state-of-the-art Omnidata model leads to the best performance. We will add this discussion and both experiments to our revised paper.\n\n**How are $w$ and $q$ estimated?**\n\n$w$ and $q$ are estimated per image as each depth map is defined up to an unknown scale and shift. In our implementation, we sample one image randomly in each iteration and then sample a batch of rays within the image. Then we estimate $w$ and $q$ using this batch of rays with a least-squares criterion which has closed-form solution, see L26-L36 in the supplementary document. We will make this clear in our revised paper.\n\n[6] Ranftl et al., Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer, T-PAMI 2022\n\n[7] Yin et al., Learning to Recover 3D Scene Shape from a Single Image, CVPR 2021\n\n[8] Do et al., Surface Normal Estimation of Tilted Images via Spatial Rectifier, ECCV 2020", " **However, the methodology did not make any changes (auxiliary loss terms) compared to the case of learning standard NeRF. I would expect different choices or additional analysis due to the task of surface reconstruction instead of simply applying the loss.**\n\nThere surely are more complex ways to utilize monocular cues, e.g., to reduce the number of samples by avoiding sampling in empty space. The advantage of using auxiliary loss terms compared to a deeper integration is flexibility, allowing us to use monocular cues independently of the used scene representation. At the same time, this \"simple\" way of integrating monocular cues already leads to significant and consistent improvements in challenging scenes. We consider the combination of simple and flexible integration and strong results a strength rather than a weakness of our approach.\n\n**For example, compared to NVS, will normal cues be more important?**\n\nWe find that both depth and normal cues are complementary and improve reconstruction results. We achieve the best results with both cues, see table 2 in the main paper. Nevertheless, we indeed found that normal cues lead to relatively more improvements, especially when using an MLP instead of a multi-res. grid representation.\n\n**For (2) part, it is a bit disconnected from the main story.**\n\nWe make the general statement that incorporating monocular cues significantly improves performance. In order to verify this statement, we need to show that the cues improve performance independently of the chosen scene representation. \nThus, exploring the impact of the cues on various commonly used representations is a central part of our study. Moreover, a surprising conclusion from our experiments is that otherwise inferior MLP representations are able to attain performance on par with more recent multi-resolution feature grids when exploiting monocular cues, see table 2 in the main paper. We believe that these results are interesting and worth to be shared with the research community.\n\n\n**The findings from the paper about MLP vs. explicit grids are not surprising.**\n\nIn our experiments on ScanNet, Multi-res. Feature grids lead to noiser reconstruction compared to MLPs due to dataset low image quality (e.g., motion blur) and noisy camera poses. We argue that this was not obvious before our experiments and our results, for the first time, reveal that the MLP architecture is more robust to noisy inputs compared to Multi-res feature grids. Generally, a single MLP is robust to noises but tends to yield smooth surfaces, while Multi-Res. Feature Grids are able to capture details and converge fast but are less robust to noise and ambiguities in the input images. Using monocular cues, however, we surprisingly found that a simple MLP architecture performs best overall, demonstrating that MLPs, in principle, can represent complex scenes while converging more slowly compared to grid-based representations. We believe our findings will provide valuable insights for future work. \n\n**More comparison of different architecture configurations.**\n\nThanks for the great suggestion! We agree that exploring different configurations is interesting and thus performed experiments with different architectural configurations. In order to evaluate the performance with different model capacities, we consider MLPs with a different number of layers and multi-resolution feature grids with different sizes of the hash table.\n\nWe list the number of learnable parameters using different architecture configurations in the table below, and also show their performance over the optimization processes in https://imgur.com/a/btDDxEC. \nOur experiments show that using monocular geometric cues improves reconstruction quality and convergence speed independent of the network configuration, which is consistent with our findings in the paper. We will add this experiment to the revised version of the paper.\n\n\n|Model configuration | Num. Params|\n|-|-|\n|MLP (2 layers) | 0.15M |\n|MLP (4 layers) | 0.26M |\n|MLP (8 layers) | 0.63M |\n|MLP (12 layers) | 0.8M |\n|Multi-res. Feature Grids (hash table size $2^{13}$)| 0.41M |\n|Multi-res. Feature Grids (hash table size $2^{15}$)| 1.1M |\n|Multi-res. Feature Grids (hash table size $2^{17}$)| 3.67M |\n|Multi-res. Feature Grids (hash table size $2^{19}$)| 12.67M |\n", " Thank you very much for the constructive feedback. We particularly appreciate that you find that our approach is \"quite clean and easy to apply to any neural implicit method\". We believe that the combination of simplicity (in terms of formulation and implementation), flexibility (in terms of not being tied to a specific scene representation), and state-of-the-art results is a particular strength of our approach. Below, we address the concerns raised in the review.\n\n**The contribution is weak and relatively incremental. The entire paper can be seen as a combination of two different parts: (1) adding monocular cues as additional supervision to improve MVS; (2) exploring different architecture choices for neural implicit representations.**\n\nWe respectfully disagree that our contribution \"is weak and relatively incremental\". To the best of our knowledge, existing approaches that use depth priors, such as DS-NeRF [1], Dense Depth Priors [2], and  Manhattan-SDF [3], obtain these priors from multi-view reconstruction (either sparse point clouds from Structure-from-Motion or dense point clouds from Multi-View Stereo). However, multi-view reconstruction approaches often struggle in texture-less scenes and in scenarios with sparse views, see the table in the next answer block. \n\nIn contrast, we use monocular cues which are versatile (i.e., can be extracted from a single image using a feedforward network) and exploit the recognition ability of state-of-the-art deep neural networks as opposed to multi-view reconstruction approaches which utilize photoconsistency cues similar to the NeRF objective itself. Our insight is that photometric consistency cues used by surface reconstruction methods (such as VolSDF) and the recognition cues provided by monocular geometric networks are complementary, see L52-L59 in the paper. We show that such readily available monocular cues can be easily used to significantly increase 3D reconstruction quality, especially in challenging settings such as 3-view DTU and Tanks \\& Temples. We are not aware of any prior work based on neural implicit scene representations which yields good results for the advanced split of the Tanks \\& Temples dataset.\n\nFinally, using monocular rather than multi-view cues comes with its own challenges, most notably that depth is only defined up to an unknown scaling factor and an unknown shift per depth map and that the cues can be rather inaccurate. As such, the losses used by our approach differ significantly from the losses used by methods which integrate multi-view depth constraints (eg, DS-NeRF), by taking scale-invariance into account and modeling normal consistency. \n\n**For (1), although the idea is clean and easy to understand (as stated in strengths), it is not new in the scenario of learning neural fields. For example, several papers have already explored using predicted depth and other semantic features in learning NeRF. This work applies a very similar idea to surface reconstruction.**\n\nAs argued above, our approach uses a fundamentally different source of depth cues (monocular predictions) compared to existing work (multi-view predictions). \nGeometrically, our cues are somewhat weaker as each depth map is defined up to unknown scale and shift values (see also below), thus introducing additional parameters that need to be estimated. Our paper demonstrates that estimating these parameters during optimization is possible, leading to consistently improved 3D reconstruction results when integrating weak monocular cues.\n\nThe table below compares our approach, based on geometrically weak monocular cues, to Manhattan-SDF (which uses multi-view depth and semantic cues, as well as normals obtained from manhattan-world assumption) and multiple variants of VolSDF (used as baselines in [3]) for the ScanNet dataset. As can be seen, our monocular cues significantly improve performance compared to using multi-view cues. Please see [3] for a detailed explanation of the baselines.\n\n|Method|Chamfer-L1|F-score|\n|-|-|-|\n|VolSDF|0.267|0.364|\n|VolSDF + Colmap Depth [4]|0.164|0.431|\n|VolSDF + Colmap Depth [4] + Semantic [5]|0.104|0.474|\n|Manhattan-SDF [3] |0.070|0.602|\n|Ours|**0.042**|**0.733**|\n\n\n[1] Deng et al., Depth-supervised NeRF: Fewer Views and Faster Training for Free, CVPR 2022\n\n[2] Roessle et al., Dense Depth Priors for Neural Radiance Fields from Sparse Input Views, CVPR 2022\n\n[3] Guo et al., Neural 3D Scene Reconstruction with the Manhattan-world Assumption, CVPR 2022\n\n[4] Schönberger et al., Pixelwise View Selection for Unstructured Multi-View Stereo, ECCV 2016\n\n[5] Chen et al., Encoder-decoder with atrous separable convolution for semantic image segmentation, ECCV2018\n", " Existing neural fields-based methods fail to reconstruct high-quality surfaces for larger and complex scenes with sparse viewpoints. In this work, the authors inspect the issue as inherent ambiguity in RGB loss which provides insufficient constraints. Inspired by the area of monocular geometry prediction, this paper proposes MonoSDF, which explores the utility of depth and normal cues predicted by general-purpose monocular estimators. Experiments demonstrate the geometric monocular priors significantly improve the performance both for single and multi-object scenes. Strengths:\n\n-\tThe proposed method, which incorporates monocular predictions to ease geometry learning, is quite clean and easy to apply to any neural implicit method.\n\n\n\nWeaknesses:\n\n-\tThe contribution is weak and relatively incremental. The entire paper can be seen as a combination of two different parts: (1) adding monocular cues as additional supervision to improve MVS; (2) exploring different architecture choices for neural implicit representations.\n\n-\tFor (1), although the idea is clean and easy to understand (as stated in strengths), it is not new in the scenario of learning neural fields. For example, several papers have already explored using predicted depth and other semantic features in learning NeRF. This work applies a very similar idea to surface reconstruction. However, the methodology did not make any changes (auxiliary loss terms) compared to the case of learning standard NeRF. I would expect different choices or additional analysis due to the task of surface reconstruction instead of simply applying the loss. For example, compared to NVS, will normal cues be more important? How the noise of the prediction affects the geometry quality?\n\n-\tFor (2) part, it is a bit disconnected from the main story. It is always a nice contribution to conduct a systematic exploration of the best architecture for surface reconstruction. Although the findings from the paper about MLP vs. explicit grids are not surprising, the comparison itself can be a good topic. However, such experiments can be done at any settings with different loss functions.\n When conducting the comparison between MLP and dense grids, how is it possible to make the comparison fair? For example, MLP-based representation is known to be computationally heavy while having low storage requirements, while dense grids can consume very large memory. It is also hard to measure the “capacity” between the MLP-based model and grid (including multi-resolution grids). Simply comparing the number of learnable parameters might not be enough. Therefore, to conduct a complete exploration of different architecture choices, simple tables (like Table 2) might not be enough. For example, for each choice, a figure is required to see the performance vs.. model capacity vs. trade-offs (spatial, convergence speed).\n\nHow robust is the monocular cue supervision to the model? For example, how the results change if there are large errors in the depth or normal predictions\n\nIn Eq 13, are w and q estimated for each image separately? In Line 189, it mentioned, “per batch”. Does one batch contain multiple images?\n The paper discussed the limitations and social impacts.", " This paper proposed an approach to address 3D reconstruction from multi-view images. The paper is built upon several milestone papers/techniques and the results presented are relatively better. It started off from signed distance function (SDF), and volume rendering of implicit surfaces. The proposal is the incorporation of two losses, i.e. depth and normal consistency estimated from individual images. The estimation, or \"ground truth\" for supervision, is from a pre-trained Omnidata model [14]. The paper also explored several architectures. *Strength*\n1. The paper is easy to read and the main idea is clearly delivered. In essence, the paper took an off-the-shelf depth estimator to serve as a strong prior. \n2. The proposed approach is robust across different numbers of images. The method can not only be applied to single objects, but also large scale scenes.\n\n*Weakness*\n1. The novelty of this paper is insignificant. It pieces together several techniques (SDF, VolSDF, Omnidata etc). And many of them are well established. I think the paper can potentially compensate this weakness by addressing the points below:\n1.1 The exploration of the \"monocular geometric cues\" is also relatively weak. It is curious to see how different depth estimators may affect the improvement when incorporating the additional prior. \n1.2 It is also peculiar that this paper chose depth and normal as the only two monocular cues to experiment. The Omnidata is able to generate high quality ground truth for 19 more more tasks. \n\n2. The color aspect of the reconstruction is never demonstrated or explained. The results are mostly focused on the geometric reconstruction but not the color appearance or rendered 2D images.\n\n3. The experiments for the architectures are confusing. The paper first showed results for comparing different architectures without monocular cues and arrived at the conclusion that the best model is Multi-Res. Fea. Grids. Then after adding the proposed monocular cues, the paper concluded that MLP is the best model. This behavior is not well explained. 1. I'm wondering what is the performance comparison between the proposed monocular priors vs self-supervised depth+normal estimation?  The paper has addressed the limitations of the existing model it used for depth estimation. However, it does not address whether Omnidata is the best prior to use compared to other models. It also does not address other monocular cues to explore.", " In this work, the authors proposed a novel and powerful geometric representation using neural implicit function. Previous neural implicit functions are trained purely on RGB reconstruction loss and have difficulty in representing more complicated geometry. In this work, the authors try to address this problem from two directions: first, they propose a novel depth and normal cues that significantly improves the quality of the reconstruction. Secondly, they explored different representation functions, including dense SDF grid, simple MLP, feature grid + MLP and multi-resolution feature grid + MLP. Both of these changes significantly improve the quality of reconstructed geometry. This is a high quality work. The idea of using depth and normal cues are simple and effective. The proposed multi-resolution feature grid + MLP representation is also novel and effectively improves the reconstruction quality over MLP solution. The experimental results are thorough  and the paper is well written and easy to follow.\n\nI don’t find any particular negative point. I only have a few minor questions for the authors:\n\nIn Table 1, in addition to network type, it is also useful to report the number of parameters of each network. Normally, in neural representation, larger network parameters often lead to a better representation power, so this information is important to understand whether the improvement comes from network architecture itself, or simply a larger set of parameters. Moreover, it would be better to compare networks with similar numbers of parameters in Table 1.\n\nThe proposed neural representation also recovered the texture of the surface (color). I think it is also important to visualize them to understand the accuracy of texture recovery, even though they are not the main task of this work. It is hard for me to tell whether the network can also represent texture, or the color prediction network is simply to help to train the SDF.\n\nI also have one minor suggestion to this work:\n\nIn Figure 3, 4, it would be better to show actual image view or ground truth rendered image, as it is hard to tell whether some small reconstructed geometry are correct or not.\n\n No as far as I know.", " This paper presents a framework to utilize monocular geometric cues to improve multi-view 3D reconstruction quality, efficiency, and scalability for neural implicit surface models. A systematic comparison and detailed analysis of design choices of neural implicit surface representations including vanilla MLP and grid-based approaches has been presented. Among these representations, a simple MLP architecture performs quite well, which demonstrates that MLPs are able to represent complex scenes. I really like the idea proposed in this paper. To improve the reconstruction quality with sparse input, shape priors should be added. There are several ways to construct the priors. One solution is to construct the parametric model for some special types like face and body. This paper explores another way with the help of depth estimation from single image. Although the estimated depth and normal may contains noises or with wrong scales, the proposed method well handles these issues. One ablation study I want to see in the paper is the number of input multi-view images and monocular geometric cues. Specifically, the monocular geometric cues could help the reconstruction when the input images are sparse. If the input images are dense, the monocular geometric cues might influence the reconstruction quality due to the error contained in the monocular cues. I wonder where is the balance for the input number of images? For a reconstruction problem with different number of input images, how should I know whether the monocular geometric cues could help or not? Except the question listed in the above, I don't have other concerns to this paper."], "review_score_variance": 2.5, "summary": "There was a range of reactions to this paper from borderline reject to strong accept.  Although several of the reviewers highlighted that the contribution could be viewed as incremental, it is clearly described, and robust across different types of scenes, and I concur with the three reviewers that give positive ratings.  Therefore I am accepting this paper.", "paper_id": "nips_2022_dMK7EwoTYp", "label": "test", "paper_acceptance": "Accept"}
