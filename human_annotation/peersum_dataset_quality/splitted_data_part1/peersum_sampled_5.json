{"source_documents": ["Generative Adversarial Networks (GANs) are one of the most popular tools for learning complex high dimensional distributions. However, generalization properties of GANs have not been well understood. In this paper, we analyze the generalization of GANs in practical settings. We show that discriminators trained on discrete datasets with the original GAN loss have poor generalization capability and do not approximate the theoretically optimal discriminator. We propose a zero-centered gradient penalty for improving the generalization of the discriminator by pushing it toward the optimal discriminator. The penalty guarantees the generalization and convergence of GANs. Experiments on synthetic and large scale datasets verify our theoretical analysis.\n      ", "Thank you for increasing the rating. We have performed another experiment to compare the generalization of GANs. The detail of the experiment is as follows:\n\n1. Experiment setup \nDataset: We stack 3 MNIST images into an RGB image, resulting in a dataset of 1000 major modes. \nNetwork architecture: Generator and Discriminator/Critic are 2 hidden layer MLPs with 512 hidden neurons.\nA CNN classifier classifies each channel of the generated image into 1 of the 10 classes. The test set accuracy of the classifier is 99%. The classifier allows us to automatically count the number of modes in the model distribution.\nTo evaluate a generator, we generate 100,000 samples and count the number of modes in that generated dataset. On average, each mode contains 100 samples. To counter the inaccuracies in the classifier, a mode is counted as present if there are more than 10 samples fall into that mode.\n\n2. Result\nResults were averaged between 3 runs.\n\nThe number of modes after 10,000 generator iterations:\nGAN-0-GP with lambda = 500, 1 discriminator iterations per generator iteration: 943\nGAN-0-GP with lambda = 500, 5 discriminator iterations per generator iteration: 1000\nGAN-0-GP with lambda = 50, 5 discriminator iterations per generator iteration: 1000\nGAN-0-GP with lambda = 10, 5 discriminator iterations per generator iteration: 984\nWGAN-0-GP with lambda = 500, 5 critic iterations per generator iteration: 1000\nWGAN-0-GP with lambda = 50, 5 critic iterations per generator iteration: 1000\nWGAN-0-GP with lambda = 10, 5 critic iterations per generator iteration: 1000\nWGAN-1-GP with lambda = 500, 5 critic iterations per generator iteration: 847\nWGAN-1-GP with lambda = 50, 5 critic iterations per generator iteration: 996\nWGAN-1-GP with lambda = 10, 5 critic iterations per generator iteration: 1000\n\n0-GP improves the performance of the original GAN as well as WGAN.  The penalty weight lambda in WGAN-1-GP (WGAN-GP) is hard to tune. Larger values of lambda make WGAN-1-GP to oscillate. WGAN-0-GP performance is consistent across various settings. It also reaches 1000 modes sooner than WGAN-1-GP.\n\nIncreasing the number of discriminator iterations per generator iteration actually improve the performance of GAN-0-GP. We would like to recall that increasing the number of discriminator iterations hurts the performance of the original GAN and GAN-0-GP-sample as it makes the discriminator overfit to the training samples. \n\nFrom the result, we can conclude that 0-GP helps improving generalization of GAN. ", "Summary: \nThe paper proposes to add to the original GAN (2014) loss a zero-centered gradient penalty as the one defined in the WGAN-GP paper. It also provides an analysis on the mode collapse and lack of stability of classical GANs. The authors compare results using their penalty on a few synthetic examples and on image net dogs generations to results using the classical GAN loss with or without gradient penalties. \n\nPositive points:\nThe paper is interesting to read and well illustrated. \nAn experiment on imagenet illustrates the progress that can be achieved by the proposed penalty.\n\nPoints to improve: \n\nIf I understood correctly, the main contribution resides in the application of the GP proposed by WGAN-GP to the original setting. Why not compare results to WGAN-GP in this case? Since the proposal of GANs, many papers addressed the mode collapse problem. WGAN-GP, VEEGAN, or Lucas et al arXiv:1806.07185, ICML 2018 to name only a few. \nThe related work section looks incomplete with some missing related references as mentioned above, and copy of a segment that appears in the introduction. \nThe submission could maybe improved by segmenting the work into intro / related / background (with clear equations presenting the existing GP) / analysis / approach / experiments\nThe experiments on synthetic data could be improved: for reproducibility, many works on GANs used the same synthetic data as VEEGAN. \nThe imagenet experiment lacks details.   ", "I get the authors argument about the optimal results that couldn't be shown in Fig.1 for the Wasserstein 1-GP loss, but I still think this comparison could be interesting, and encourage the authors to add it to the camera ready version if the paper is accepted.\nNonetheless, since the authors included a comparison to Wasserstein-GP on ImageNet, I increased my rating. ", "Thank you for your comment. The encoder E could be trained jointly with D and G using an architecture similar to BiGAN [6]. \n\n[6] Adversarial Feature Learning\nJeff Donahue, Philipp Krähenbühl, Trevor Darrell", "Hi, I read your paper and it think it is quiet well written paper.\nI would like to ask 2 questions about your paper.\n\n1. Additional information about your generating performance\nIn the PDF of your paper in OpenReview, it is hard to compare generation results with other papers.\nIt would be better if the paper contains large resolution results with which resolution it is.\n\n2. About encoder E\nIn the paper, to penalize gradients on points in line segment between Y(t) and X, you proposed linear interpolation between two real and fake latent code. In appendix F, you explained how you get latent code ${z}_{x} using encoder E, however, details of E is not in the paper. How did you train E? It doesn't seem to be pretrained. Is it able to infer ${z}_{x} correctly? If so, how did you schedule the training step? Separate D and E or train at once?\n\nThanks in advance.", "Thank you for pointing out the typos. We have fixed them in our latest revision. We also added a reference to table 1 in our introduction.\nWe would like to address your other concerns as follows:\n\n1.  Figure 1 aims to illustrate the effect of our 0-GP in pushing the discriminator toward the theoretically optimal discriminator $D*$ in the original GAN, not the theoretically optimal critic $C*$ in WGAN.  \nIf we add WGAN-GP to the comparison in Fig. 1, it should be compared with the optimal critic, not the optimal discriminator in Fig. 1f. However, we are not aware of any closed-form formula for the Wasserstein-1 distance between two multivariate Gaussians. Therefore, WGAN-GP could not be added to Fig. 1. Also note that the critic can output values outside of the [0, 1] range so it is not comparable to discriminators in Fig. 1.\n\nWe also do not see the benefit of adding WGAN-GP to the comparison in Fig. 1. Our work focuses on the use of GP in improving generalization in GAN, not on the use of metric/divergence. As shown in section 5.3, 1-GP does not help to prevent overfitting in the discriminator with the original GAN loss. Similar to the discriminator, the critic in WGAN-GP can also overfit to the data and output a distance greater than the actual Wasserstein-1 (W-1) distance between $p_g$ and $p_r$. For example, $p_g$ and $p_r$ are both standard normal distribution, the W-1 distance between them is 0. However, the critic is trained to maximize the distance between the two empirical distributions can output a value greater than 0 without violating the Lipschitz constraint. More concretely, if the dataset contains a real sample $x$ and a fake sample $y$, the empirically optimal critic satisfies: \n                                   |C(x) - C(y)| = || x - y ||,\nwhile the true distance is 0. The phenomenon makes the critic and the generator to oscillate around the equilibrium [4] and slows down the improvement in sample quality. Note that, WGAN-LP [5] with the corrected GP still suffers from that same problem and performs similarly to WGAN-GP on real datasets. The problem is seen in our ImageNet experiment where GAN-0-GP and GAN-0-GP-sample outperform WGAN-GP by a large margin.\n\n2. We appreciate your suggestion on improving the paper quality. We will add more experiments if time permits. However, we believe that the current experiments are sufficient to demonstrate our main contribution which is a novel method for improving the generalization of GANs.\n\n[4] Which Training Methods for GANs do actually Converge? \nLars Mescheder, Andreas Geiger, Sebastian Nowozin.\n[5] On the regularization of Wasserstein GANs\nHenning Petzka, Asja Fischer, Denis Lukovnicov.\n", "I appreciate the authors effort put in the revised pdf.\n\nI acknowledge the comparison with WGAN-GP in the imageNet experiment.\nThe point that still bothers me in the main results of the paper (generalization) is the lack of comparison with WGAN-GP in Figure 1: the comparison with 1GP is displayed, but this is with a classical GAN objective, and not a WGAN loss. My insight is that the results should be better using the full WGAN-GP objective. Sorry for the last minute request, but this doesn't seem like a difficult comparison to make. \n\nMinor but still important to reader understanding:\n- Table 1 is nice but is not referenced in the text. The caption is not very explicit either. A ref should be added for the 0-GP sample (name only introduced in section 5). For the 1-GP a ref to WGAN-GP could be added too with precisions on the full criterion in the caption.\n- Probably will lack of time, but an interesting experiments for assessing GAN generalization could be to measure the Mean reconstruction error as in\nGuo-Jun Qi. Loss-Sensitive Generative Adversarial Networks, 2017.\n\nOther comments:\np1: typos equilibira\nGAN loss tends -> GAN loss tend\np10: add the dataset in caption of Figure 4\nRemove urls in the bib", "Thank you for your constructive review. We have updated our paper to address your concerns. The changes are summarized as follow:\n\n1. A background section is added with basic information about GANs and a definition of generalization. A table summarizing the referred gradient penalties is also added.\n\n2. We extended the Related works section to include papers which address the mode collapse problem. The writing of this part and the whole paper was revised.\n\n3. Another MNIST experiment is added to Section 6.1 to further demonstrate the effectiveness of our method in preventing overfitting. Specifically, our GAN-0-GP is the only GAN that could learn to generate realistic samples when the discriminator is updated 5 times per generator update. \n\n4. WGAN-GP is included to our ImageNet experiment. Our GAN-0-GP outperforms WGAN-GP by a large margin. \n\n5. Implementation details are added to the appendix. The code for all experiments will be released after the review process.\n\n6. We added the analysis for the 'mode jumping' problem to Section 6.2. We showed that GAN-0-GP-sample suffers from the problem. On the other hand, our GAN-0-GP is robust to the problem and is able to produce better interpolation between modes.\n\n7. A new algorithm for finding a better path between a pair of samples is added to our paper. ", "Thank you for your review and questions. We have performed additional experiments and analysis to consolidate our finding. The updates are as follows:\n\n1. A background section is added with basic information about GANs and a definition of generalization. A table summarizing the referred gradient penalties is also added.\n\n2. We extended the Related works section to include papers which address the mode collapse problem. The writing of this part and the whole paper was revised.\n\n3. Another MNIST experiment is added to Section 6.1 to further demonstrate the effectiveness of our method in preventing overfitting. Specifically, our GAN-0-GP is the only GAN that could learn to generate realistic samples when the discriminator is updated 5 times per generator update. \n\n4. WGAN-GP is included to our ImageNet experiment. Our GAN-0-GP outperforms WGAN-GP by a large margin. \n\n5. Implementation details are added to the appendix. The code for all experiments will be released after the review process.\n\n6. We added the analysis for the 'mode jumping' problem to Section 6.2. We showed that GAN-0-GP-sample suffers from the problem. On the other hand, our GAN-0-GP is robust to the problem and is able to produce better interpolation between modes.\n\n7. A new algorithm for finding a better path between a pair of samples is added to our paper. ", "Thank you again for your suggestions. We have revised our paper to address your concerns as follows:\n\n1. A background section is added with basic information about GANs and a definition of generalization. A table summarizing the referred gradient penalties is also added.\n\n2. We extended the Related works section to include papers which address the mode collapse problem. The writing of this part and the whole paper was revised.\n\n3. Another MNIST experiment is added to Section 6.1 to further demonstrate the effectiveness of our method in preventing overfitting. Specifically, our GAN-0-GP is the only GAN that could learn to generate realistic samples when the discriminator is updated 5 times per generator update. \n\n4. WGAN-GP is included to our ImageNet experiment. Our GAN-0-GP outperforms WGAN-GP by a large margin. \n\n5. Implementation details are added to the appendix. The code for all experiments will be released after the review process.\n\n6. We added the analysis for the 'mode jumping' problem to Section 6.2. We showed that GAN-0-GP-sample suffers from the problem. On the other hand, our GAN-0-GP is robust to the problem and is able to produce better interpolation between modes.\n\n7. A new algorithm for finding a better path between a pair of samples is added to our paper. ", "Thank you for your comments. We would like to address your concerns as follow.\n\n1. We do not use the gradient penalty in WGAN-GP (1-GP) to improve the original GAN. Our 0-GP, although has a similar form as 1-GP,  is motivated from a very different perspective and produces very different effects. We assume that you find our 0-GP similar to 1-GP because of the use of the straight line from a fake to a real sample. In the response to reviewer 1, we propose a more sophisticated way to find a path from a fake to a real datapoint. The new method highlights the difference between our method and 1-GP.\n\n2. The 0-GP is not the only contribution of our paper. We start by analyzing the generalization of GANs, showing the problem of the original GAN loss. Although generalizability is one of the most desirable properties of generative models, it has not been studied carefully in GAN literature. Based on our analysis, we propose 0-GP to improve the generalization of GANs. On the 8 Gaussian dataset, GAN-0-GP can generate plausible unseen datapoints on the circle, implying better generalization. We show that the original GAN loss makes GAN focuses on generating datapoints in the training dataset. 0-GP-sample proposed in [4] encourages the generator to remember the training samples. That result in the mode jumping behavior: when we perform interpolation between $z_1$ and $z_2$, the output does not smoothly transform from $x_1 = G(z_1)$ to $x_2 = G(z_2)$ but suddenly jump from $x_1$ to $x_2$. The behavior can be seen in figure 8 of BigGAN paper (https://arxiv.org/abs/1809.11096).\n\n3. We will include WGAN-GP to the baselines for the sake of completeness. However, as discussed in the previous paragraphs and in our paper, WGAN-GP and their 1-GP does not address the same problem as our 0-GP. \n\nAs discussed in our paper, 1-GP does not help improving generalization in GANs. [4] even showed that 1-GP does not help WGAN (and the original GAN as well) to converge to an equilibrium. The phenomenon can be seen in our MNIST experiment where GAN-1-GP fails to produce any realistic samples after 10,000 iterations. It has been observed that WGAN-1-GP does not converge to an equilibrium, the generator continues to map the same noise to different modes as the training continues. In our synthetic experiment, WGAN-1-GP is less robust to change in hyper-parameters than GAN-0-GP. Detailed results will be included in our revision. Please refer to [4] for more in-depth discussion about the non-convergence of WGAN-GP. \n\nWhen $p_g$ is the same as $p_r$, the gradient of the optimal discriminator in GAN and the optimal critic in WGAN must be 0. Any non-zero centered GP will not help GANs to converge to the optimal equilibrium.  Our 0-GP helps to improve both generalization and convergence of GANs. Our 0-GP can be applied to WGAN as well. \n\nSimilar to the original GAN, WGAN and WGAN-GP can overfit to the dataset: the distance output by the critic can be larger than the Wasserstein distance between the two distributions. However, overfitting in WGAN and WGAN-GP is not as severe as in GAN. This is partly because the gradient in WGAN and WGAN-GP does not explode so mode collapse is much harder to observe. \n\n4. We will include more related works to our paper. The vast body of work on GANs makes it difficult to find all related works. We only focus on some key papers on the topic.\n\nDiscussion about VEEGAN and Lucas et al. will be added to our next revision. However, we want to emphasize that our work is about improving the generalization of GANs. Reducing mode collapse is related to but is not exactly the same as generalization. As in the 8 Gaussian dataset, a GAN without mode collapse is the one that can generate all 8 modes. A GAN with good generalization should be able to generate unseen datapoints on the circle and to perform smooth interpolation between modes. \n\n5. We will add more details about the experiments to the appendix. The code for all experiments will be released after the review process. For the imagenet experiment, we used the code from [4] which is available on github. We note that [4] is a state-of-the-art method which is able to help GAN to scale to massive datasets and it is used in BigGAN paper. \n\n6.Thank you for your suggestion about the paper layout. Adding a table that summarizes referred gradient penalties is a good idea. \n\n", "Thank you for your review. We will revise our paper according to your suggestion. We would like to quickly address your question about the experiment here. For MNIST and ImageNet experiment, the whole dataset was used. For the ImageNet experiment, we used the code from [4]. Details about all experiments will be added to the appendix. We thank you for pointing the typo in Figure 3. \n\nWe will also add an in-depth discussion about our method and other related works to our next revision as suggested by other reviewers. ", "Thank you for your constructive comments. We would like to address your concerns as follow:\n1. Generalization has been defined in [1, 2, 3]. They were cited in our paper. Because of the space limit, we could not include their definition in the first version of our paper. We will add the definition from [1] to the updated version. The definition in [1] is directly related to our discussion: if the Lipschitz constant is 0, then the network has the maximum generalization capability and no discriminative power. As stated in our paper, our gradient penalty makes the network generalizable while remaining discriminative. \n\n2. We agree that the straight line is not a good option for real data like images. However, it's the cheapest way to implement our method. We are working on an improved version for the GP, which we briefly describe below. We plan to include the result in the next revision of our paper.\n\nFor all interpolated points to be in the same set with the two endpoints, the set must be convex. $supp(p_g) U supp(p_r)$ is generally not convex so linear interpolation in the data space cannot guarantee that every interpolated point to be in the support.  A solution to this problem is to force the set of latent code $z$ to be convex and perform the interpolation in the latent space. This requires an additional encoder E to encode a datapoint $x$ to a latent code $z_x$. The process of sampling a datapoint for regularization is as follow\n(i) Sample a noise vector $z ~ p_z$, generate a fake datapoint $y = G(z)$\n(ii) Sample a real datapoint $x ~ p_r$, get the latent code of $z_x = E(x)$\n(iii) Generate the interpolated latent code: $\\tilde(z) = \\alpha z_x + (1 - \\alpha) z$\n(iv) Generate the interpolated datapoint: $\\tilde(x) = G(\\tilde(z))$\n(v) Apply gradient penalty on $\\tilde(x)$\n$\\tilde(x)$ is more likely to lie on the data manifold than the weighted sum of a real and a fake sample. Regularizing the gradient w.r.t. $\\tilde(x)$ will allow better generalization and discrimination.\n\n3. We are not sure that your suggestion is correct. As discussed in our paper, gradient exploding tends to happen near the decision boundary, while the gradient near real/fake datapoints tends to vanish. We doubt that increasing the sampling rate near real/fake datapoints will lead to better result. \n\n\n[1] Generalization and Equilibrium in Generative Adversarial Nets (GANs). Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, Yi Zhang.\n[2] Do GANs actually learn the distribution? An empirical study. Sanjeev Arora, Yi Zhang.\n[3] On the Discrimination-Generalization Tradeoff in GANs. Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, Xiaodong He.\n[4] Which Training Methods for GANs do actually Converge? Lars Mescheder, Andreas Geiger, Sebastian Nowozin.\n", "The primary innovation of this paper seems focused towards increasing the generalization of GANs, while also maintaining convergence and preventing mode collapse.\n\nThe authors first discuss common pitfalls concerning the generalization capability of discriminators, providing analytical underpinnings for their later experimental results. Specifically, they address the problem of gradient explosion in discriminators. \n\nThe authors then suggest that a zero-centered gradient penalty (0-GP) can be helpful in addressing this issue. 0-GPs are regularly used in GANs, but the authors point out that the purpose is usually to  provide convergence, not to increase generalizability. Non-zero centered penalties can give a convergence guarantee but, the authors, assert, can allow overfitting. A 0-GP can give the same guarantees but without allowing overfitting to occur.\n\n\nThe authors then verify these assertions through experimentation on synthetic data, as well as MNIST and ImageNet. My only issue here is that very little information was given about the size of the training sets. Did they use all the samples? Some portion? It is not clear from reading. This would be a serious impediment to reproducibility.\n\nAll in all, however, the authors provide a convincing  combination of analysis and experimentation. I believe this paper should be accepted into ICLR.\n\nNote: there is an error on page 9, in Figure 3. The paragraph explanation should list that the authors' 0-GP is figure 3(e). They list (d) twice.\n\n", "The paper discusses the generalization capability of GAN especially from the discriminator's perspective. The explanation is clear and the method is promising. The proposed gradient penalty method that penalizes the unseen samples is novel and reasonable from the explanation, although these methods has been proposed before in different forms. \n\nPros:\n1. Nice explanation of why the training of GAN is not stable and the modes often collapse.\n2. Experiments show that the new 0-gradient penalty method seems promising to improve the generalization capability of GAN and helps to resist mode collapsing.\n\nCons:\n1. The paper does not have a clear definition of the generalization capability of the network.\n2. The straight line segment between real and fake images seems not a good option as the input images may live on low-dimensional manifolds. \n3. Why samples alpha in (7) uniformly? It seems the sampling rate should relate with its value. Intuitively, the closer to the real image the sampling point is, the larger the penalty should be.\n"], "review_score_variance": 0.22222222222222224, "summary": "The paper received unanimous accept over reviewers (7,7,6), hence proposed as definite accept. ", "paper_id": "iclr_2019_ByxPYjC5KQ", "label": "train", "paper_acceptance": "accepted-poster-papers"}
{"source_documents": ["We introduce CuLE (CUDA Learning Environment), a CUDA port of the Atari Learning Environment (ALE) which is used for the development of deep reinforcement algorithms.  CuLE overcomes many limitations of existing CPU-based emulators and scales naturally to multiple GPUs. It leverages GPU parallelization to run thousands of games simultaneously and it renders frames directly on the GPU, to avoid the bottleneck arising from the limited CPU-GPU communication bandwidth. CuLE generates up to 155M frames per hour on a single GPU, a finding previously achieved only through a cluster of CPUs. Beyond highlighting the differences between CPU and GPU emulators in the context of reinforcement learning, we show how to leverage the high throughput of CuLE by effective batching of the training data, and show accelerated convergence for A2C+V-trace. CuLE is available at [hidden URL].", "The work contributes a library emulating Atari games in GPU in parallel and allowing to speed-up the execution of reinforcement learning algorithms.\n\nI see that the paper qualifies to the conference; in particular there is listed the topic:\n\n- “implementation issues, parallelization, software platforms, hardware”\n\nHowever, this is not a research paper, and I do not really see how I should asses it. What I can say about it is that it is considerable amount of work, not only implementing the simulator but also looking at what RL methods need, and how to optimize the allocation and exchange of the data so that everything would work on GPU more efficiently.\n\nFrom the practical perspective, I am somewhat confused. The speed-up factors in the experiments are rather modest: about 4x for simulating and rendering frames, 2.5x for full RL, on a single GPU. Better with scaling to multi-GPU systems. In Table 1 the total training time per resources used differs dramatically. However if I look at the lines with A2C it is about the same time with 100-200 CPU cores + 1 GPU versus 12 cores + 1 GPU. So this is about factor 10 in the resources, versus CPU parallelization probably suffering overheads.\n\nIt appears that the maximum steed-ups are achieved for a particular type of the reinforcement learning algorithms, and using it in a general case would give a modest improvement.\n\nThe paper itself consists of introduction, related work, 1 page overview of what it means to simulate the Atari games, and experiments. So it is mostly about measuring the speedups, with several implementations / platforms.\n\nI tend to think that this work will not very much boost the research for new RL methods. It is limited to Atari games, mostly helps to sample-inefficient RL methods and if it helps, the speed-up factors are not of the order that would make experiments by the researchers otherwise impossible. \n\nI would also give priority to theoretical contributions at ICLR. In the end, we all are using CUDA and cnDNN, but presentations about how they implement things are rather given at GPU computing conferences. \n\n\n", "To better support our claim about the correctness of our implementation, and provide a more complete set of information to the reviewer and the reader, we further added a last paragraph in the Appendix (and Fig. 10)  to show that evaluation of an agent trained on CuLE data, performed on CuLE-CPU and OpenAI Gym environments, do not show significant differences.", "The comments from R2 and R1, who had to read the paper multiple times to appreciate his content (“My first reaction to this paper was, \"So what?\"; but as I read more, I like the paper more and more”), suggest that we could explain our research contributions in a better way.  Therefore we modify the Introduction to better highlight that we introduce CuLE as a tool to “both investigate and mitigate” the limitations of the existing DRL approaches, including the bottleneck analysis (contribution (1), mentioned as significant by R1), the batching strategy (contribution (2), mentioned as significant by R3), as well as the analysis of the advantages and limitations of GPU-emulation (contribution (3), mentioned as significant by R1) that eventually lead to open research questions (see the Conclusion, mentioned as significant by R3). Our paper follows the track of other works on efficient RL with additional attention to system details (e.g. Babaeizadeh et al. (2016; 2017); Stooke & Abbeel (2018a); Espeholt et al. (2018)) and, more broadly speaking, of many works aimed at making ML computationally efficient, considering not only the algorithmic aspects, but also communication issues and system level optimizations (e.g., Seunghak Lee et al.,  On Model Parallelization and Scheduling Strategies for Distributed Machine Learning, NIPS 14; Peng Jiang et al., A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication, NeurIPS 18; Michael Teng et al., Bayesian Distributed Stochastic Gradient Descent, NeurIPS 18; Jianqiao Wangni et al., Gradient Sparsification for Communication-Efficient Distributed Optimization, NeurIPS 18; ...).\n\nAs for R2’s point on giving priority to theoretical contributions, we believe that a combination of algorithmic innovations and system research (as in Babaeizadeh et al. (2016; 2017); Stooke & Abbeel (2018a); Espeholt et al. (2018)) is necessary to advance the state of the art in DRL. Coherently with this, we mention in the first paragraph of Related Work the two factors affecting the wall clock convergence time of a DRL algorithm: sample and computational efficiency. R2 notices that “the maximum speed-ups are achieved for a particular type of reinforcement learning algorithms, and using it (CuLE) in a general case would give a modest improvement,” which is indeed highlighting once more the strong interaction between a DRL algorithm and its implementation, and the need to study these two aspects together. Partially coherently with R2’s claim that CuLE “in a general case would give a modest improvement”, we do show that vanilla A2C does not benefit from CuLE’s high data throughput (see Table 3 and Figure 7), but through an ad hoc batching strategy (Fig. 6) we achieve a 2-4x speed up in terms of convergence time. On the other hand, we believe that CuLE (and GPU emulation/simulation in general) can be beneficial for a wider class of algorithms. We demonstrate (Table 5) that DQN and PPO achieve a significant speed up (in terms of raw frames per second) using CuLE, and almost linear scaling on multiple GPUs (Table 4). Further speed up can be obtained by optimizing the GPU mapping of the TIA kernel (end of the second paragraph, Section 3), or by memory compression (second paragraph, Discussion and Conclusion). Thus we demonstrate that at least the computational efficiency aspect can be improved for different classes of algorithms. The problem of leveraging the high data throughput generated by the GPU using these (and potentially others) algorithms, to achieve high sample efficiency and fast convergence at the same time, remains open, and we believe that CuLE represents a valid tool for future investigations in this direction. To this aim, the Atari suite contains a set of tasks that are sufficiently hard to be non-trivial (as discussed in the Reinforcement Learning Workshop, ICML 2017), and, at the same time, sufficiently simple to be solved in a reasonable amount of time.\n\nThe magnitude of the speed-up for A2C+V-trace is comparable with results reported in DRL literature - the fact that it is not as impressive as the ones achieved in supervised learning in the last few years, further demonstrates the high complexity of the problem of accelerating DRL, which cannot be regarded as a pure algorithmic problem, but requires the parallel investigation of both the algorithmic and system implementation details.\n\nTo better justify the description of the CuLE’s implementation in Section 3, we add a paragraph at the end of the manuscript to highlight that “CuLE’s implementation is informed by ease of debugging, need for flexibility, and compatibility with standard DRL benchmarks. These choices put a limit on the achievable speed up factor (for instance by using emulation of the Atari 2600 console instead of direct CUDA implementations of Atari games), but the analysis and insights provided in our paper furnish indications for the design of efficient simulators for DRL”.\n", "The point raised by R3 about the correctness of our implementation (and its faithful adherence to the original implementation of Atari) is indeed an important one and deserves attention. We thank the reviewer for having raised this point. We modified the caption of Fig. 7 and appended a paragraph to Section 4 to describe how we checked the correctness of our implementation: “To guarantee that CuLE is compliant with OpenAI Gym, we seed OpenAI Gym and CuLE environments with the same game state and successfully verified that the sequence of game states and frames returned by each environment are equivalent. A more subtle question to answer is if the CuLE reset procedure, based on a set of random initial states, can reduce the variability in the environments and consequently be detrimental for the learning and generalization capability of the trained agents. Our code released in [CULE URL] allows the user to test the training agent on CuLE, CuLE-CPU, and OpenAI Gym environments: in our tests the results are comparable for the three backends. Results reported in Fig. 7} further suggest that CuLE trained agents do not suffer any performance loss when tested on OpenAI Gym environments.”", "We enlarged the figures and used larger fonts in the paper to improve readability. Data in the first two sections of Table I have been taken from Table I in Horgan et al., 2018, and completed with the frames per seconds generated and consumed by each different method, when available in the corresponding paper (see caption of Table I on our paper). For the last two sections of Table I, that are fully completed, we used data from Stooke and Abbeel, 2018b, Espeholt et al., 2018, and our own experiments with CuLE; we updated Table I to use N/A to indicate meaningless metrics (e.g. training time for CuLE in emulation only mode). The typo on page 3 has been fixed. We thank the reviewer for the comments.", "This paper introduces a CUDA port of the Atari Learning Environment. The paper goes into detail examining the benefits that come from a GPU-only implementation, including much better per-GPU utilization as well as no need to run a distributed system of CPUs. They show this hardware scaling can be taken advantage of across a variety of state of the art reinforcement learning algorithms and indeed create new batching strategies to utilize their framework and the GPU better. \n\nThe paper is well written and goes into some detail describing the implementation of CuLE as well as various design decisions taken, as with splitting the emulation process across several kernels. Finally, the paper is very explicit about a number of optimizations that are not being exploited by the new framework and serve as markers for future work.\n\nA question that arises and which is not addressed in the experiments is how the authors verified their port is faithful to the original version; there is no mention of correctness in the paper.", "This paper describes a port of the Atari Learning Environment to CUDA, reports on a set of performance comparison, and provides a bottleneck analysis based communication bandwidth and various throughputs required to saturate them for training and inference.\n\nMy first reaction to this paper was, \"So what?\"; but as I read more, I like the paper more and more.  It was the bottleneck analysis that changed my mind.  It was done very thoroughly and it provides deep insight in the challenges that RL faces for both learning and inference in a variety of settings.  I especially liked the analysis of the advantages and limitations of GPU emulation.  I also thought the Discussion section was well written.\n\nThe paper would be better if:\n1) The figure fonts were larger throughout the paper.\n2) The gaps in Table 1 were explained.\n\nMinor issue:  Change \"feed\" to \"fed\" on page 3.\n"], "review_score_variance": 10.888888888888891, "summary": "The paper presented a detailed discussion on the implementation of a library emulating Atari games on GPU for efficient reinforcement learning. The analysis is very thoroughly done. The major concern is whether this paper is a good fit to this conference. The developed library would be useful to researchers and the discussion is interesting with respect to system design and implementation, but the technical depth seems not sufficient.", "paper_id": "iclr_2020_HJgS7p4FPH", "label": "train", "paper_acceptance": "reject"}
{"source_documents": ["To improve real-world applications of machine learning, experienced modelers develop intuition about their datasets, their models, and how the two interact. Manual inspection of raw data—of representative samples, of outliers, of misclassifications—is an essential tool in a) identifying and fixing problems in the data, b) generating new modeling hypotheses,\n      and c) assigning or refining human-provided labels. However, manual data inspection is risky for privacy-sensitive datasets, such as those representing the behavior of real-world individuals. Furthermore, manual data inspection is impossible in the increasingly important setting of federated learning, where raw examples are stored at the edge and the modeler may only access aggregated outputs such as metrics or model parameters. This paper demonstrates that generative models—trained using federated methods and with formal differential privacy guarantees—can be used effectively to debug data issues even\n      when the data cannot be directly inspected. We explore these methods in applications to text with differentially private federated RNNs and to images using a novel algorithm for differentially private federated GANs.", "This work presents a method for using generative models to gain insight into sensitive user data, while maintaining guarantees about the privacy of that data via differential privacy (DP) techniques. This scheme takes place in the federated learning (FL) setting, where the data in question remains on a local device and only aggregate updates are sent to a centralized server. The intended application here is to use the trained generative models as a substitute for direct inspection of user data, thus providing more tools for debugging and troubleshooting deployed models in a privacy conscious manner.\n\nPros:\nGiven the growing computational power of mobile devices and the importance of privacy for large-scale deployment of machine learning, this work is a timely contribution that could augment the ML pipeline for at-scale applications dealing with sensitive data. The authors do a good job of fleshing out the intended use cases of their training scheme, and present a pair of experiments that are well-chosen for illustrating the utility of generative models when dealing with private data.\n\nCons:\nAlthough likely of practical use, the work seems to be lacking in novelty in several respects. First, the techniques developed here represent a fairly straightforward merger of DP and FL tools without much in the way of qualitatively new offerings. While the authors do develop a new GAN training scheme that works in the FL setting, this adaptation is also pretty straightforward, and mostly follows the approach laid out in [1] for training recurrent neural nets.\n\nSecondly, this paper comes in the midst of many other works aiming to integrate different combinations of generative models, privacy, and distributed training (as pointed out in the related work section). While the particular combination of techniques here differ from those in previous work, the authors don't attempt to justify why their training scheme should be preferred over these prior methods. And although their experiments are useful for understanding the general utility of generative models trained in a private and decentralized setting, they unfortunately don't permit any direct comparison with the experiments used in these previous papers.\n\nVerdict:\nFor the reasons given above, I cannot recommend acceptance of this work.\n\n[1] H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang, Learning differentially private recurrent language models, ICLR 2018\n\n*** Follow-up after authors' rebuttal ***\n\nI'd like to thank the authors for their rebuttal, and for the significant addition to the paper in the form of an expanded Section 2. This material has helped me gain a bit better perspective on the use cases for their work, and convinced me of the potential for their methodology within real-world development of deep learning tools and services. In addition, this additional context helps to motivate the two experiments described here as fair representatives of actual debugging problems, and not simply issues that were hand-chosen to prove the authors' point.\n\nI still hold that the paper offers very little in the way of new conceptual or technical contributions, but in light of the potential utility of this privacy-conscious generative pipeline for the broader deep learning community, I have changed my score from a weak reject to a weak accept.", "Goals\nThe paper identifies a key challenge in a large class of real world federate learning problems where we also have to ensure user level data privacy. In these settings the modeler can not inspect the raw data samples from the user (due to privacy concerns) and hence all modeling tasks (from data wrangling to hypothesis generation to labeling to model class selection to validation) become far more challenging. The paper proposes that in these circumstances one may use a generative model that learns the data distribution using federated learning methods with provable differentiable privacy guarantees. The generative model can then produce data (unconditional, or conditional on some features or class labels) which can be inspected by the modeler without compromising user privacy. \n\nExperiments\nThe authors illustrate the approach using existing federated DP RNN learning methods, and using a slightly novel GAN learning algorithm for images (largely similar to other algorithms). They use these methods to provide two examples: 1) learning a language model from text (word sequences) where there is a bug in pre-processing steps (tokenization); 2) learning a GAN for images of handwriting on checks where there is a pre-processing bug that inverts the grayscale of images. These examples illustrate the potential for such methods to possibly be useful to modelers. While one may quibble about some details (see section below) the experimental set up is reasonable to illustrate the need and some of the challenges modelers are likely to face in the real world (\n\nEvaluation & Questions\n\nI'm really torn because I really enjoyed the paper very much overall but I have some strong concerns as well. \n\nPositives: the paper is well motivated and very well written (it is really a pleasure to read, and it is very clear about the details -- especially after they release the code it should be possible to reproduce the results too). The authors shine a spot light on a problem that is very important & widespread (eg while learning from condifential data on cell phones). The proposed solution is fairly simple, intuitive, and quite high level (lets use a generative model that creates phantom data that can be inspected)\n\nNegatives: I am not entirely sold on this being a realistic approach in the long term -- ie that some of the key problems will ever be solvable (I'm quite ok even if they are not solved now in the first paperr). The authors do a very good job of being transparent about several potential issues (see eg last paragraphs of main paper and appendix D). My biggest concerns are below:\n* the phantom samples generated from the model need to be very realistic in order to be useful. In other words, we need to have excellent, high fidelity generate models. Even to create proper hypothesis, create proper model classes, assess convergence, or assess whether the generative model is good enough one needs to be able to inspect the raw data -- which can not be done in the first place. This can not be entirely automated eliminating need for human inspection -- and the problem is much worse in generative models (which need to encode more information) than in discriminative models which need to encode less information (bits) almost by definition. Thus one has simply traded the problem of needing to inspect data to model the final algorithm (whcih could be discriminative) and has to deal with the problem of needing data to inspect the intermediate, generative model (which is also learned using federates, DP guaranteeing ways). It is not at all clear what one accomplished by doing this. \n* GANs are notoriously hard to train with mode collapse etc.  Setting hyper parameters of any generative model also needs access to original data and impacts the privacy guarantees. \n\n***NOTE added after author response***\nThe rebuttal has sufficiently address the quibbles I raised below. I'm leaving it here to allow traceability. I'm not fully convinced about the response to the main issue I raised above (ie if the generative model is not very representative, high-fidelity, then one cant know whether a potential bug discerned by inspecting its samples is an artefact of the generative model or whether it is truly a fundamental bug upstream -- and training a high quality generative model also requires one to inspect the raw data in the first place so the problem has simply been swept under the carpet). Nevertheless for a first paper on the topic I think the contributions and intuition provided here are quite valuable so I am ok leaving this for for future work. \n\n* quibble#1: theoretical DP bounds are not very tight. For example, in table 2 they may want to use realistic estimates instead of epsilon even to prove their high level point. I'm not sure I can buy their argument even on this illustrative problem as it stands. \n* quibble#2: You may want to at least make an effort to compare against the nearest possible methods in your experimental setup even if they are not a great match to the problem. I'm not intimately familiar with the recent literature but you mention Triascyn & Faltings (2019) so perhaps you could also use that and expand a bit more on the novelty here", "We thank the reviewer for their comments.  We answer their specific questions in turn.\n\nQuestion #1 (Re: generality of approach to bugs, choice of bugs for paper)\n\nWe respectfully disagree with the reviewer’s conclusions on the generality of this approach (e.g., “The two debugging illustrations are very specific in term of the errors introduced and the ways to achieve the debugging goal. It is not sure how they can be further generalized to other types of bugs.”), though we acknowledge this generality was not sufficiently described in our initial submission. We have significantly revised Section 2, including adding a new Section 2.1, which we hope resolves these concerns.  In particular, apart from selecting a type of generative network that best applies to the problem domain (e.g., choosing RNNs to debug a language modeling task, choosing GANs to debug an image modeling task), no further assumptions need be made by the user about the nature of the data (or any bugs or biases therein).\n\nIndeed, it is precisely because the signal we are trying to detect is unknown that we recommend a generative model.  Were the modeler to possess additional evidence that strongly indicated a particular type of bug, a simpler data analysis may be enough to detect it.  (E.g., if the modeler of the image pipeline had reason to strongly suspect a priori that some user’s images were black/white inverted, they could instead compute per-user device average pixel values and use federated computation to aggregate into a histogram.  The histogram would reveal that many devices had predominantly very white images.)  But because we assume no such a priori knowledge, we desire an approach that is as general as possible.\n\nNote that in Section 2 we now reference a recently published survey paper providing a taxonomy of faults in deep learning systems (https://arxiv.org/abs/1910.11015).  We feel this paper confirms our choice of bugs as being representative examples, as they are listed prominently (e.g., ‘text segmentation’, ‘pixel encoding’) under one of the largest subcategories of faults, ‘Preprocessing of Training Data’  (https://arxiv.org/abs/1910.11015, Figure 1).\n\n\nQuestion #2 (Re: can approach be generalized into some methodologies, what are the limits)\n\nAs mentioned, we have attempted to make the general methodology prominent in Section 2. \n\nAssessing the limits of this approach is one of the most interesting questions to explore in future work.  To make an analogy: In this paper we present the use of a new ‘sensor’ and show its promise; we hope to see the community take up research on this sensor so we can all work together to characterize its ‘signal-to-noise’ ratio. \n\nSome different limits that can be thought of are sensitivity (i.e., how much presence in the underlying data distribution is required before representative examples of a characteristic start to be synthesized) and fidelity (i.e., how realistic do synthesized examples need to be to detect a characteristic).  We’ve attempted to discuss each in the paper; we welcome the reviewer’s feedback if the current discussion in the paper could be improved.\n\nSensitivity : We empirically characterize the sensitivity limits of the approach in the paper, e.g., in Figure 4 and Table 4.  There we show, for RNN language models trained with varying degrees of presence of the concatenation bug, the varying levels of UNKs noticed in the generated content.  E.g., Figure 4 shows that when the bug is only present in 1% of sentences, the distribution of generated content is close to unchanged vs. the no-bug case; however, when the bug is present in 10% of sentences, a clear change in distribution of UNKs is noticeable.  Does the reviewer feel this a useful empirical analysis of sensitivity limits?\n\nFidelity : Section 2.1 now contains a discussion of the types of problems where lower-fidelity synthesis is ok and the problems where high-fidelity will likely be necessary. Please let us know if this addresses the reviewer’s concerns.  (Thanks to this reviewer and other reviewers’ feedback, as we realized this matter was less clearly discussed in the initial draft.)\n\nAgain, we hope this paper encourages new work in the generative modeling community, in particular to both assess current limits and hopefully push them further.  (Towards this end, we call attention to open questions about fidelity and sensitivity in the Conclusion and Open Problems sections, respectively.)  But we feel this initial paper demonstrates that there are realistic data inspection problems that exist today that can already be addressed with an approach like we describe, i.e., are useful within current limits of sensitivity and fidelity.\n", "We appreciate the reviewer’s comments.\n\nFirst, we wish to clarify our view of the contributions of the paper. The principle contribution is not the introduction of new algorithms, but of a methodology for combining existing techniques together with a careful selection procedure in order to solve a large set of ML modeling challenges when working with decentralized data. While this observation may seem straightforward in hindsight, we do not believe it has been presented in any previous works. We have revised Section 2 and added a new Section 2.1 which hopefully makes this contribution more clear. While indeed we did need to make some algorithmic contributions (training user-level DP GANs on decentralized data for the first time), this is a secondary contribution.\n\nWe think something that was missed in the initial review of our paper was the uniqueness of combining federated learning, generative models, and user-level differential privacy.  We respectfully disagree with the reviewer’s assessment of the level of previous work that’s been done at the intersection of these 3 areas.  (E.g., the reviewer states our paper “comes in the midst of many other works”; we feel this is erroneous, and revised the paper to make things more clear.)\n\nWe have significantly edited the related work section to explain how none of the existing methods directly apply to our setting (e.g., how Triascyn & Faltings 2019 uses a much weaker, empirical measure of privacy than our setting with user-level differential privacy).  In the cases where existing results are applicable, we have in fact used them directly, e.g. adopting techniques from McMahan et al. 2018 and Chen et al. 2019. \n\nDoes our revised comparison in the related work section resolve the reviewer’s concerns?\n", "We thank the reviewer for their comments and observations, and are thrilled they enjoyed the paper and found the motivating problem and proposed solution compelling.  We now address their list of ‘negatives’ in turn.\n\nNeed for Realism:\n\nWith regards to the comment that generative models “need to be very realistic in order to be useful”, it is our experience that this is not the case for many real-life problem examples, such as the pixel inversion and concatenation bugs we consider.  The measure of utility for the applications described in our paper is not realism, but rather the ability to detect the presence of distinguishing characteristics in the mimicked distribution.  We agree it is certainly the case that one could not distinguish all characteristics unless generating content to full realism, but it definitely the case that there are a broad set of characteristics that are distinguishable at well short of full realism.  We feel the two problem examples demonstrate this characteristic: while not generating extremely realistic samples, they nevertheless convey a clear ‘signal’ that is useful to the modeler, e.g., the presence of bugs. But our work far from solves the problem we address, and we hope this encourages new work in the generative modeling community.  \n\nThanks for the reviewer’s comment as we’ve updated the paper (e.g., Section 2.1) to better describe the types of problems where lower-fidelity synthesis is ok and the problems where high-fidelity will likely be necessary. Please let us know if this addresses the reviewer’s concerns.\n\nPrivacy budgets for hyperparameter sweeps (“Setting hyper parameters of any generative model also needs access to original data and impacts the privacy guarantees. ”):\n\nWe believe what you are referring to is that identifying the correct hyperparameters typically requires a ‘sweep’ of values, each of which involves data queries against the private data; the privacy budget must account for all these queries, not simply the final training run.  (If we have mistakenly interpreted your comment, we apologize, and would benefit from a clarification.)\n\nThis is absolutely true, and we made sure this paper raises this issue prominently.  In Section 3 on DP Federated Generative Models, we conclude the discussion of DP by noting “....  that since the modeler has access to not only the auxiliary generative models, but also potentially other models trained on the same data, if an (eps, delta) guarantee is desired, the total privacy loss should be quantified across these models, e.g. ...”  We also discuss the need for algorithms requiring minimal tuning as an import step for future work. Again, our contribution is primarily in highlighting this important problem, rather than solving it. Please let us know if you feel our current wording doesn’t properly convey this matter prominently enough; we certainly wish to call attention to it as we hope to see further research in this area.\n\nIt’s also true that this is a broader concern that impacts not just the generative models of our paper, but any ML (or other query-based) process that repeatedly samples from private data.  There are some mitigations typically proposed (i.e., using a different, proxy dataset to work out the hyperparameter values before training on the actual private data), but this continues to be an active research area in the larger DP community, which we applaud.  Along with benefiting everyone else working in DP ML (generative models or not, federated or not), it will definitely benefit those of us working with federated generative models.\n\nFinally, as the paper shows, GAN convergence did not take an exorbitant # of rounds (the generated image results we show are after 1000 rounds of federated training).  So the volume of data queries being performed when training federated generative models is in-line with the typical volume of data queries performed when doing any type of federated learning.\n\nQuibble #1 - DP bounds:\n\nWould the reviewer be able to clarify this comment further for our benefit?  We regretfully have had trouble parsing their meaning the first time around.  As DP gives us an upper bound on privacy loss, and we’re achieving a DP $(\\epsilon, \\delta)$ at population scale that are indicative of a tight bound, we feel we’ve shown that privacy loss is minimal?  We must be misunderstanding something in the reviewer’s comment/critique.\n\nQuibble #2 - Compare with other methods:\n\nWe have significantly edited the related work section to explain how none of the existing methods directly apply to our setting; in the cases where existing results are applicable, we have in fact used them directly, e.g. adopting techniques from McMahan et al. (2018) and Chen et al. (2019). Please let us know if this does not address these concerns.", "This paper proposes a differentially private federated learning method to learn GAN with application to data bugging situations where privacy protection is needed. The proposed method tries to leave the data at the user-end to train the discriminators, and learn the generator at the centralised server. To support the debugging data related issues as claimed, two specific examples related to text and image modeling were presented. It is the generator which is DP-protected (as the discriminators are DP-protected) makes it possible where the generated data can hint the potential bugs. \n\nThe scenario being considered is interesting and two real examples have used to illustrate the idea. However, this paper falls short in the following ways: \n- It adopts what being proposed in McMahan et al. (2018) with some modifications to achieve the goal. The novelty is more related to the proposed application which allows debugging data issues to be possible when the data is private and decentralised.\n- The two debugging illustrations are very specific in term of the errors introduced and the ways to achieve the debugging goal. It is not sure how they can be further generalized to other types of bugs.\n- The paper is well written. However, the readers should have reasonable background on DP, GAN, federated learning, and generative models, or it will be hard to read through. Having said that, the authors do provide quite comprehensive literature review on related topics. But, then not much space is left for providing the necessary background and details for  the proposed federated learning for GAN with DP (other than referring to Algorithm 1). The experiment section is good.\n\nSpecific questions:\n- Other than the tokenisation bug and the image insertion bug, can more possible examples be described?\n- Can the examples be generalised into some methodologies? And, what are the limits? Will there be data inspection needs which cannot be achieved by this approach? What are they?"], "review_score_variance": 4.222222222222222, "summary": "The paper provides methods for training generative models by combining federated learning techniques with differentiable privacy. The paper also provides two concrete applications for the problem of debugging models. Even though the method in the paper seems to be a standard combination of DP deep learning and federated learning, the paper is well-written and presents interesting use cases.", "paper_id": "iclr_2020_SJgaRA4FPH", "label": "train", "paper_acceptance": "accept-poster"}
{"source_documents": ["In this paper, we propose a generic and simple strategy for utilizing stochastic gradient information in optimization. The technique essentially contains two consecutive steps in each iteration: 1) computing and normalizing each block (layer) of the mini-batch stochastic gradient; 2) selecting appropriate step size to update the decision variable (parameter) towards the negative of the block-normalized gradient. We conduct extensive empirical studies on various non-convex neural network optimization problems, including multilayer perceptron, convolution neural networks and recurrent neural networks. The results indicate the block-normalized gradient can help accelerate the training of neural networks.  In particular,\n      we observe that the normalized gradient methods having constant step size with occasionally decay, such as SGD with momentum, have better performance in the deep convolution neural networks, while those with adaptive step sizes, such as Adam, perform better in recurrent neural networks. Besides, we also observe this line of methods can lead to solutions with better generalization properties, which is confirmed by the performance improvement over strong baselines. ", "This paper proposes a family of first-order stochastic optimization schemes based on (1)  normalizing (batches of) stochastic gradient descents and (2) choosing from a step size updating scheme. The authors argue that iterative first-order optimization algorithms can be interpreted as a choice of an update direction and a step size, so they suggest that one should always normalize the gradient when computing the direction and then choose a step size using the normalized gradient. \n\nThe presentation in the paper is clear, and the exposition is easy to follow. The authors also do a good job of presenting related work and putting their ideas in the proper context. The authors also test their proposed method on many datasets, which is appreciated.\n\nHowever, I didn't find the main idea of the paper to be particularly compelling. The proposed technique is reasonable on its own, but the empirical results do not come with any measure of statistical significance. The authors also do not analyze the sensitivity of the different optimization algorithms to hyperparameter choice, opting to only use the default. Moreover, some algorithms were used as benchmarks on some datasets but not others. For a primarily empirical paper, every state-of-the-art algorithm should be used as a point of comparison on every dataset considered. These factors altogether render the experiments uninformative in comparing the proposed suite of algorithms to state-of-the-art methods. The theoretical result in the convex setting is also not data-dependent, despite the fact that it is the normalized gradient version of AdaGrad, which does come with a data-dependent convergence guarantee.\n\nGiven the suite of optimization algorithms in the literature and in use today, any new optimization framework should either demonstrate improved (or at least matching) guarantees in some common (e.g. convex) settings or definitively outperform state-of-the-art methods on problems that are of widespread interest. Unfortunately, this paper does neither. \n\nBecause of these points, I do not feel the quality, originality, and significance of the work to be high enough to merit acceptance. \n\nSome specific comments:\np. 2: \"adaptive feature-dependent step size has attracted lots of attention\". When you apply feature-dependent step sizes, you are effectively changing the direction of the gradient, so your meta learning formulation, as posed, doesn't make as much sense.\np. 2: \"we hope the resulting methods can benefit from both techniques\". What reason do you have to hope for this? Why should they be complimentary? Existing optimization techniques are based on careful design and coupling of gradients or surrogate gradients, with specific learning rate schedules. Arbitrarily mixing the two doesn't seem to be theoretically well-motivated.\np. 2: \"numerical results shows that normalized gradient always helps to improve the performance of the original methods when the network structure is deep\". It would be great to provide some intuition for this.  \np. 2: \"we also provide a convergence proof under this framework when the problem is convex and the stepsize is adaptive\". The result that you prove guarantees a \\theta(\\sqrt{T}) convergence rate. On the other hand, the AdaGrad algorithm guarantees a data-dependent bound that is O(\\sqrt{T}) but can also be much smaller. This suggests that there is no theoretical motivation to use NGD with an adaptive step size over AdaGrad.\np. 2-3: \"NGD can find a \\eps-optimal solution....when the objective function is quasi-convex. ....extended NGD for upper semi-continuous quasi-convex objective functions...\". This seems like a typo. How are results that go from quasi-convex to upper semi-continuous quasi-convex an extension?\np. 3: There should be a reference for RMSProp.\np. 3: \"where each block of parameters x^i can be viewed as parameters associated to the ith layer in the network\". Why is layer parametrization (and later on normalization) a good way idea? There should be either a reference or an explanation.\np. 4: \"x=(x_1, x_2, \\ldots, x_B)\". Should these subscripts be superscripts?\np. 4: \"For all the algorithms, we use their default settings.\" This seems insufficient for an empirical paper, since most problems often involve some amount of hyperparameter tuning. How sensitive is each method to the choice of hyperparameters? What about the impact of initialization?\np. 4-8: None of the experimental results have error bars or any measure of statistical significance.\np. 5: \"NG... is a variant of the NG_{UNIT} method\". This method is never motivated.\np. 5-6: Why are SGD and Adam used for MNIST but not on CIFAR? \np. 5: \"we chose the best heyper-paerameter from the 56 layer residual network.\" Apart from the typos, are these parameters chosen from the training set or the test set? \np. 6: Why isn't Adam tested on ImageNet?\n\n  \nPOST AUTHOR RESPONSE: After reading the author response and taking into account the fact that the authors have spent the time to add more experiments and clarify their theoretical result, I have decided to upgrade my score from a 3 to a 4. However, I still do not feel that the paper is up to the standards of the conference. \n\n\n\n\n\n ", "This paper illustrates the benefits of using normalized gradients when training deep models.\nBeyond exploring the \"vanilla\" normalized gradient algorithm they also consider adaptive versions, i.e., methods that employ per block (adaptive) learning rates using ideas from AdaGrad and Adam.\nFinally, the authors provide a theoretical analysis of NG with adaptive step-size, showing convergence guarantees in the stochastic convex optimization setting.\n\nI find this paper both very interesting and important. \nThe normalized gradient method was previously shown to overcome some non-convex phenomena which are hurdles to SGD, yet there was still the gap of  combining NG with methods which automatically tune the learning rate.\n\nThe current paper addresses this gap by a very simple (yet clever) combination of NG with AdaGrad and Adam, and the authors do a great job by illustrating the benefits of their scheme by testing it over a very wide span of deep learning \nmodels. In light of their experiments it seems like AdamNG and NG should be adopted as the new state-of-the-art methods in deep-learning applications.\n\nAdditional comments:\n-In the experiments the authors use the same parameters as is used by Adam/AdaGrad, etc..\nDid the authors also try to fine tune the parameters of their NG versions? If so what is the benefit that they get by doing so?\n-It will be useful if the authors can provide some intuition about why is the learning rate  chosen per block for NG?\nDid the authors also try to choose a learning rate per weight vector rather than per block? If so, what is the behaviour that they see.\n-I find the theoretical analysis a bit incomplete. The authors should spell out the choice of the learning rate in Thm. 1 and compare to AdaGrad.\n", "This paper proposes a variation to the familiar AdaGrad/Adam/etc family of optimization algorithms based a gradient magnitude normalization. More precisely, the components of the gradient are split into blocks (one block per layer), and each block is normalized by its L2 norm. The concatenation of these normalized gradients are used in place of the standard gradient in AdaGrad/Adam/SGD. The authors find the resulting optimizer performs slightly better than its competitors on four tasks.\n\nI feel this paper would be much stronger focusing extensively on one or two small problems and models, providing insight into how normalization affects optimization, rather than chasing state-of-the-art numbers on a variety of datasets and models. I believe the significance and originality of this work to be lacking.\n\n## Pros ##\n\nThe paper is easy to follow. The algorithm and experiment setups are clearly explained, and the plots are easy to understand. I appreciate the variety in experimental setups. The authors provide a proof of convergence for the AdaGrad variant on convex functions.\n\n## Cons ##\n\nThe paper fails to provide new insights to the reader. It succeeds in asking a question (how do normalized gradients impact training of neural networks?), but fails to add theoretical or empirical knowledge that furthers the field. While effectively changing the geometry of the problem, no motivation (theoretical or intuitive) is given as to why this normalization scheme should be effective.\n\nFrom the empirical side, the authors compare the proposed optimizers on many datasets and models, but concerningly only using the baselines' default hyperparameters. Even ADAM, a supposedly \"hands-free\" optimizer, has been shown to vary greatly in performance when its hyperparameters are well chosen (https://arxiv.org/abs/1705.08292). This is simply unfair to the baselines, and conclusions cannot meaningfully be drawn from this alone. In addition, different tasks use different optimizers, which strikes me as odd, and no error bars are added to any plots.\n\nFrom the theoretical side, the authors show a convergence bound that is minimized when the number of blocks is one. This, however, is not what the authors use in experiments, and no reasoning about the choice of blocks == network layers is given.\n\n## Specific comments ##\n\np1: \"Gradient computation is expensive\" is not a good justification. All empirical risk minimization, convex or not, requires a full pass over the dataset. Many convex problems outside of ERM involve very expensive gradient computations.\n\np1: \"These two challenges indicate that for each iteration, stochastic gradient might be the best practical first order information we can get\". See loads of work in approximate second-order methods that show otherwise! Hessian-free Optimization, K-FAC, Learning to Learn Gradient Descent, ACKTR's use of Kronecker-factored Trust Region.\n\np2: You may want to reference Layer-Specific Adaptive Learning Rates for Deep Networks (https://arxiv.org/pdf/1510.04609.pdf), as it appears relevant to the layer-wise nature of your paper.\n\np2: \"Recently, provably correct algorithms...\" I'm fairly confident that Adam and RMSProp lack provable correctness. You may want to soften this statement.\n\np3: The expression being minimized is the sample risk, rather than the expected risk.\n\np5: The relationship between NG and NG_{UNIT} is confusing. I suggest keeping only the vanilla method analyzed in this paper, or that the second method be better motivated.", "We first thank the reviewer for the valuable feedback. The responses to your questions are as follows:\n\nQ: “The proposed technique is reasonable on its own, but the empirical results do not come with any measure of statistical significance.”\nA: Thank you for the suggestion! We have included the mean and variance in our experimental results in Section 4.2. Please see the revised version. We show that the normalized gradient method is indeed better than its unnormalized counterpart in many scenarios and it is not by chance. \n\nQ: “For a primarily empirical paper, EVERY state-of-the-art algorithm should be used as a point of comparison on EVERY dataset considered.”\nA: We believe this is a very harsh and unrealistic requirement and strikingly contradicts with AnonReviewer4’s suggestion. Nowadays, getting state-of-the-art performance on a dataset usually appeals to the combination of different efforts, including data preprocessing/augmentation, careful model designing and thorough parameter tuning, etc. However, it is not the main focus of this paper. Our paper aims to provide a simple alternative to train neural networks and it empirically works well on a number of tasks. In fact, in the CIFAR10/100 and ImageNet experiment, we largely adopted the parameter settings in [1], where the model championed the ImageNet 2015 challenge, except for the layer number, mini-batch size and GPU number (we don’t have that many GPUs). This should be considered a very strong baseline. Those parameters were well tuned by other researchers and we don’t see the necessity for re-tuning. Furthermore, we NEVER claim our method is a panacea and we believe none of the existing methods are either.\n\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep Residual Learning for Image Recognition. CVPR 2016. \n\nQ: “The theoretical result in the convex setting is also not data-dependent, despite the fact that it is the normalized gradient version of AdaGrad, which does come with a data-dependent convergence guarantee.”\nA: We are not sure what “data-dependent” means in the reviewer’s question. Since the reviewer said AdaGrad has a data-dependent convergence guarantee, we compare the convergence result of AdaGrad with our Theorem 1. We guess that the reviewer probably meant that the right-hand side of Adagrad’s convergence result (the inequalities in Theorem 5 by Duchi et al(2011)) has a summation of the norms of rows of historical gradients,, i.e., $\\sum_{i=1}^d \\|g_{1:T,i}\\|_2$, which makes their bounds data-dependent. If our understanding is correct, we think our convergence guarantee is in fact data-dependent in the exactly the same way as AdaGrad. Please take a look at our inequality (3) which contains two components: the first component depends on $x_t$ and the second component depends on $\\|g_t^i\\|$. The first component is still upper bounded as in (5) but the second component can be bounded just by the first inequality in (4). Then our convergence guarantee will contain  $\\sum_{i=1}^d \\|g_{1:T,i}\\|_2$ and will have the same data-dependency as AdaGrad. The reason this dependency did not appear in our Theorem 1 is because we further upper bounded the second component by 2\\sqrt{Td_i} as we showed in the second inequality in (4). In fact, the authors of AdaGrad also did the same thing in their Corollary 6 where the data-dependent term $\\sum_{i=1}^d \\|g_{1:T,i}\\|_2$ were also upper bounded by simpler terms. We thought the bound we reported in Theorem 1 was simpler. In the revision, we have included the data-dependent bound in Theorem 1.\n\nQ: “Any new optimization framework should either demonstrate improved (or at least matching) guarantees in some common (e.g. convex) settings or definitively outperform state-of-the-art methods on problems that are of widespread interest.”\nA: In terms of the performance of optimization under the convex setting, our result (Theorem 1) indeed matches AdaGrad in many ways. First, the optimality gaps ensured by both AgaGrad and our method convergence to zero in a rate of $1/\\sqrt{T}$. Second, according to our answer to the last question, the convergence guarantee of both AgaGrad and our method are data-dependent and contain the term $\\sum_{i=1}^d \\|g_{1:T,i}\\|_2$ in the same way. (We further upper bounded this term by a simpler quantity so this data-dependency might not be observed directly.) In addition, our method generalize AdaGrad by using block-wise adaptive subgradient. ", "Q: “Gradient computation is expensive” is not a good justification.\nA: By saying so, we want to emphasize that the full gradient computation for deep neural networks is unrealistic and we thus often use stochastic gradient. We made this point clearer in the revision. \n\nQ: On other optimization methods.\nA: We have softened our expression in the original text to avoid any confusion. Thanks for the pointing this out!\n\nQ: Additional references.\nA: Thanks for bringing this up, we will cite and discuss these two papers. In a nutshell, the updating rule in “Layer-Specific Adaptive Learning Rates for Deep Network” is different from ours in that they are essentially adding a term to the gradient rather than normalizing it, while the paper https://arxiv.org/abs/1705.08292 focuses more on the bad generalization cases of the adaptive step-size methods, which is orthogonal to our focus.\n\nQ: “I'm fairly confident that Adam and RMSProp lack provable correctness. You may want to soften this statement.”\nA: Yes we agree that there are some flaws in the proof of Adam, even for the convex case. That’s why our special case analysis can only be applied to Adagrad. We have softened this statement in the revision. \n\nQ: “The expression being minimized is the sample risk, rather than the expected risk.”\nA: In machine learning, the ultimate goal is to minimize expected risk, although in practice we can only work on the sample risk instead. Nevertheless, we think this is a minor point. In fact, the expression we wrote can present either sample risk or expected, depending on what is the distribution of \\xi in that expectation. If we consider the case where the distribution of \\xi in our minimization is simply the empirical discrete distribution corresponding to the finite sample, that expectation will just become the average of risk over samples. We prefer to use expectation instead of finite sum expression because the former is more general and our algorithms and theorem can be both applied to minimizing an expectation, no matter the corresponding distribution is continuous (expected risk) or discrete (sample risk). \n\nQ: “The relationship between NG and NG_{UNIT} is confusing.”\nA: We have clarified this in the revision and also rename the methods. The new method is a  variant when the normalization is relaxed to not be strictly 1. We empirically find that it helps improve the generalization performance in Sec 4.2.\n", "We first thank the reviewer for the valuable feedback. The responses to your questions are as follows:\n\nQ: “I feel this paper would be much stronger focusing extensively on one or two small problems and models, providing insight into how normalization affects optimization, rather than chasing state-of-the-art numbers on a variety of datasets and models.”\nA: Thanks for the suggestion! We indeed put more results (figures and tables) and analysis in Sec 4.2, by investigating the CIFAR10 and 100 datasets. Interestingly, this point of view strikingly contradicts with Reviewer 2’s, who requires “Every state-of-the-art algorithm should be used as a point of comparison on every dataset considered.”\n\nQ: “The paper fails to provide new insights to the reader. It succeeds in asking a question (how do normalized gradients impact training of neural networks?), but fails to add theoretical or empirical knowledge that furthers the field. While effectively changing the geometry of the problem, no motivation (theoretical or intuitive) is given as to why this normalization scheme should be effective.”\nA:  The intuition is that when the network is deep, the original gradient in the low layers will become very small or very large because of the multiplicative effect of the gradient of the upper layers, which is called gradient vanishing or explosion phenomenon. The layer-wise gradient normalization, which can also be interpreted as layer-wise learning rate, can counteract this negative effect automatically, maintaining the gradient magnitude as a constant, so that the information can still backprop to the bottom layers. We agree that our intuition is not strictly supported by theory, but this is also true of many of the effective approaches in deep learning, such as batch normalization, layer normalization, weight normalization and gradient clipping. Lacking theory does not prevent those method becoming prevalent.\n\nQ: “This is simply unfair to the baselines, and conclusions cannot meaningfully be drawn from this alone.”\nA: The goal of the experiments is to compare the performance between the existing algorithms and their gradient normalized counterpart. Hyperparameter tuning is orthogonal to our goal. We believe that as long as they are using the same parameter settings, the comparison is fair.  In fact, in the CIFAR10/100 and ImageNet experiment, we largely adopted the parameter settings in [1], where the model championed the ImageNet 2015 challenge, except for the layer number, mini-batch size and GPU number (we don’t have that many GPUs). This should be considered a very strong baseline. Those parameters were well tuned by other researchers and we don’t see the necessity for re-tuning. That said, we actually searched over the learning rate for Adam or other parameters, please see Sec 4.2 of the revision.\n\n[1] He et.al.. Deep Residual Learning for Image Recognition. CVPR 2016. \n\nQ: “In addition, different tasks use different optimizers.”\nA: In fact, it is commonly observed that for RNNs, the adaptive step-size method like Adam performs better, while for CNNs, SGD+momentum works much better. That’s why we selected the best baseline optimizers for the specific tasks and compare with their normalized gradient counterpart based on this. We have clarified this point in the revision. We have also added the Adam experiment on CNN with ImageNet data, and confirmed this common observation.\n\nQ: “no error bars are added to any plots”\nA: We have added the means and variances in the tables of  Section 4.2.\n\nQ: “the authors show a convergence bound that is minimized when the number of blocks is one.”\nA: According to what Theorem 1 stated, we agree that our convergence bound is minimized when the number of blocks is one. However, this is not the property of the algorithm. Instead, it is just because of our analysis. In the revision, we have derived the convergence bound in a tighter way so that the optimal number of blocks is not necessarily one. In fact, this is easy to derive. Instead of considering a constant M bounding the full gradient \\|F’\\|, we must consider a block-dependent constant M_i that upper bounds the corresponding block of gradient \\|F‘_i\\|. By simply replacing all M by M_i in the proof of Theorem 1, we obtain a convergence bound like O( [D^2\\sqrt{Bd}/\\eta+ \\eta(\\sum_i M_i^2)\\sqrt{d_i}] / sqrt{T} ). Then, consider a situation where some M_k is much larger than other M_i and some d_h is much larger than other d_i  but h is different from k. For instance, we can have M_k=O(M)>>1 and d_h=O(d) but d_i=M_i=O(1) for other i. Our new convergence bound becomes [D^2\\sqrt{Bd}/\\eta+ \\eta(M^2+B+\\sqrt{d})] / sqrt{T}. After optimizing eta, we obtain [D(Bd)^{1/4}\\sqrt{M^2+B+\\sqrt{d}}] / sqrt{T}. Compared this bound for B=1, which is [DM\\sqrt{d}] / sqrt{T}, our bound can be lower, for example, when B<M^2. We have add some discussions on when the new convergence bound when $B>1$ is better than when $B=1$ in the revision. \n", "Q: “When you apply feature-dependent step sizes, you are effectively changing the direction of the gradient, so your meta learning formulation, as posed, doesn't make as much sense.”\nA: We agree that we are indeed changing the direction of the real gradient. However, in this work we do demonstrate that this modification works well. We should also point out that a number of very successful and widely used approaches, such as batch normalization, layer normalization, weight normalization, gradient clipping, do dynamically change the data, or the weight, or the direction of the gradient. We believe our technique falls into the same category as those.\n\nQ: “What reason do you have to hope for this? Why should they be complimentary? Existing optimization techniques are based on careful design and coupling of gradients or surrogate gradients, with specific learning rate schedules. Arbitrarily mixing the two doesn't seem to be theoretically well-motivated.”\nA: Again, neither our starting point nor the goal of this paper is on theory, just like most of the prevalent techniques. Our intuition is supported by the thorough experiments, not by the proof. We also point out that none of the current optimization techniques can be proved to work under the general neural network setting, without unrealistic assumptions.\n\nQ:  “It would be great to provide some intuition for this”. \nA: The intuition is that when the network is deep, the original gradient in the low layers will become very small or very large because of the multiplicative effect of the gradient of the upper layers, which is called gradient vanishing or explosion phenomenon. The layer-wise gradient normalization can counteract this negative effect automatically, maintaining the gradient magnitude per layer as a constant, so that the information can still backprop to the bottom layers.\n\nQ: “This suggests that there is no theoretical motivation to use NGD with an adaptive step size over AdaGrad.”\nA: Yes, you are correct, our motivation is not from theory but from the practical observation. Please also see the response to the previous question.\n\nQ: “How are results that go from quasi-convex to upper semi-continuous quasi-convex an extension?”\nA: It is indeed a typo. We missed “differentiable”. We meant to say “NGD can find a \\eps-optimal solution....when the objective function is differentiable quasi-convex.” Kiwiel (Kiwiel, 2001) extended NGD for upper semi-continuous (not necessarily differentiable) quasi-convex objective functions.\n\nQ: “There should be a reference for RMSProp.”\nA: We will cite Geoff Hinton’s lecture note, as there is no formal publication on this method.\n\nQ: “Why is layer parametrization (and later on normalization) a good way idea?”\nA: We repeat the intuition here that when the network is deep, the original gradient in the low layers will become very small or very large because of the multiplicative effect of the gradient of the upper layers. The layer-wise gradient normalization can counteract this negative effect automatically, maintaining the gradient magnitude as a constant, so that the information can still backprop to the bottom layers.\n\nQ: “This seems insufficient for an empirical paper, since most problems often involve some amount of hyperparameter tuning. How sensitive is each method to the choice of hyperparameters? What about the impact of initialization?”\nA: The goal of the experiments is to compare the performance of the existing algorithms and their gradient normalized counterpart. We believe that as long as they are using the same parameter settings, the comparison is fair. Although hyperparameters tuning is orthogonal to the goal of our paper, we actually searched over the learning rate for Adam or other parameters, please see Sec 4.2 of the revision. Besides, we included the mean and variance of the performance over 5 runs for each method, each ResNet and each dataset with random initialization in Sec 4.2.\n\nQ: “NG_{unit} is never motivated.”\nA: Thanks for pointing this out! We have clarified this in the revision and also rename the method. The new method is a variant when the normalization is relaxed to not be strictly 1. We empirically find that it helps improve the generalization performance in Sec 4.2.\n\nQ: “Why are SGD and Adam used for MNIST but not on CIFAR?”\nA: Interestingly, even the Table 1 in the first submission exactly shows the SGD and Adam results on CIFAR10. We also added the result on CIFAR100 in the revision.\n\nQ: “are these parameters chosen from the training set or the test set?”\nA: They are chosen from validation set, which is clarified in the revision.\n\nQ: “Why isn't Adam tested on ImageNet?”\nA: We also included the Adam result on ImageNet in the revision. In fact, as a common wisdom, CNN is the basic model for ImageNet and SGD+momentum is usually better than Adam when using CNNs. That’s why we did not use Adam in the first version. We confirm this result in revision.\n", "We first thank the reviewer for the valuable feedback. The responses to your questions are as follows:\n\nQ: “In the experiments the authors use the same parameters as is used by Adam/AdaGrad, etc. Did the authors also try to fine tune the parameters of their NG versions? If so what is the benefit that they get by doing so?”\nA: We keep using the same parameters for both the normalized and original version, to make the comparisons fair. Otherwise, if we change the parameters in the normalized version, it is hard to tell whether the effect is due to the normalization or parameter tuning. \n\nQ: “It will be useful if the authors can provide some intuition about why is the learning rate  chosen per block for NG?\nA: “Block” in the neural network scenario means “layer”. So our method is a layer-wise normalization approach. The intuition is that when the network is deep, the original gradient in the low layers will become very small or very large because of the multiplicative effect of the gradient of the upper layers, known as gradient vanishing or explosion phenomenon. The layer-wise gradient normalization, which can also be interpreted as layer-wise learning rate, can counteract this negative effect automatically, maintaining the gradient magnitude as a constant, so that the information (error) can still backprop to the bottom layers.   \n\nQ: “Did the authors also try to choose a learning rate per weight vector rather than per block? If so, what is the behaviour that they see.”\nA: If we take all the variables of a neural network as a long vector, normalizing the gradient layer-wisely somehow has already changed the direction of this vector. And if we normalize by each weight vector, making the granularity of the normalization even finer, we are afraid the direction change will be more severe. Consider the extreme case of normalizing each dimension, which is equivalent to choose the sign of each coordinate of the gradient. We believe this would jeopardize the algorithm significantly. However, we feel it makes more sense to address the differences of the gradient magnitude between layers, rather than changing the relative values of weights within the same layer.\n\nQ: “The learning rate in Thm. 1”\nA: This learning rate is chosen to get through the proof under the convex setting. However, we should point out that in our experiments, where the objective function is no longer convex, it is unclear whether this learning rate would still provide convergence guarantee.  \n"], "review_score_variance": 8.666666666666666, "summary": "The paper proposes to study the impact of normalizing the gradient for each layer before applying existing techniques such as SG + momentum, Adam or AdaGrad. The study is done on a reasonable number of datasets and, after the reviewers' comments, confidence intervals have been added,  although Table 1 puts results in bold but many of these results are not statistically significant.\n\nThe paper, however, lacks a proper analysis of the results. Two main things could be improved:\n- Normalization does not always have the same effect but the reasons for it are not discussed. This needs not be done theoretically but a more thorough analysis would have been appreciated.\n- There is no hyperparameter tuning, which means that the results are heavily dependent on which hyperparameters were chosen. Thus, it is hard to draw any conclusion.\n\nRegarding the seemingly conflicting remarks of the two reviewers, it all depends on what the paper is trying to achieve. If it tries to show that is it state-of-the-art, then comparing to state-of-the-art algorithms on every dataset is crucial. If it tries to study the impact of one specific change, in this case layer normalization, on the optimization, then comparing to the vanilla version is fine. The paper seems to try to address the latter so it is OK if it is not compared to all the state-of-the-art algorithms. However, proper tuning of existing methods is still required.\n\nUltimately, a better understanding of layer normalization could be useful but the paper is not convincing enough to provide that understanding. There is no need to increase the number of datasets but it should rather focus on designing setups to test and validate hypotheses.", "paper_id": "iclr_2018_ry831QWAb", "label": "train", "paper_acceptance": "rejected-papers"}
{"source_documents": ["The importance weighted autoencoder (IWAE) (Burda et al., 2016) is a popular variational-inference method which achieves a tighter evidence bound (and hence a lower bias) than standard variational autoencoders by optimising a multi-sample objective, i.e. an objective that is expressible as an integral over K>1 Monte Carlo samples. Unfortunately, IWAE crucially relies on the availability of reparametrisations and even if these exist, the multi-sample objective leads to inference-network gradients which break down as K is increased (Rainforth et al., 2018). This breakdown can only be circumvented by removing high-variance score-function terms, either by heuristically ignoring them (which yields the 'sticking-the-landing' IWAE (IWAE-STL) gradient from Roeder et al. (2017)) or through an identity from Tucker et al. (2019) (which yields the 'doubly-reparametrised' IWAE (IWAE-DREG) gradient). In this work, we argue that directly optimising the proposal distribution in importance sampling as in the reweighted wake-sleep (RWS) algorithm from Bornschein & Bengio (2015) is preferable to optimising IWAE-type multi-sample objectives. To formalise this argument, we introduce an adaptive-importance sampling framework termed adaptive importance sampling for learning (AISLE) which slightly generalises the RWS algorithm. We then show that AISLE admits IWAE-STL and IWAE-DREG (i.e. the IWAE-gradients which avoid breakdown) as special cases.", "UPDATE: bumping up my score after the revisions\n\n---\n\n\nNice connections but novelty and practical takeaways unclear\n\nSUMMARY OF THE PAPER:\n\nThis paper views recent IWAE-based [1] methods (IWAE-STL [2], IWAE-DREG [3], RWS [4, 5]) for training generative models p and inference networks q under a common framework, AISLE.\nThis heavily relies on the \"double-reparameterization\" property by [2] and is restated in Lemma 1.\nThis framework makes it explicit that we're interested in\n1) maximizing the (log) marginal likelihood wrt p parameters, and\n2) minimizing some divergence between the posterior in the learned model to q.\n\nIn AISLE, IWAE-STL's q-gradient is viewed as a doubly-reparameterized self-normalized importance sampling (SNIS) estimate of KL(p || q).\nThis is in contrast to viewing it as a biased estimator of the IWAE's q-gradient.\nThis can potentially explain why it performs well when number of SNIS samples are increased.\nIt is also some evidence against the fact that having no unified objective is bad (because there isn't evidence of IWAE-STL diverging despite there being no unified objective).\n\nIWAE-DREG's q-gradient is viewed as a doubly-reparameterized SNIS estimate of X-divergence(p || q) (up to multiplicative constant of the number of SNIS samples).\nThis is in contrast to viewing it as an unbiased estimator of the IWAE's q-gradient.\n\nThe view on RWS is unchanged: the q-gradient is a SNIS estimate of KL(p || q).\n\nFor me, the main contribution is viewing IWAE-STL and IWAE-DREG q-gradient estimators as biased gradients of an explicit divergence rather than of the IWAE objective.\nI also found the observation that the signal-to-noise (SNR) decrease in IWAE's q-gradient can be proved by noting that it is a SNIS estimator of a zero vector nice.\n\nSTRUCTURE:\nThe article is well-written and easy to understand.\n\nNOVELTY:\nA different view on IWAE-STL and IWAE-DREG is interesting and novel (as mentioned above).\nThis means that IWAE-STL and IWAE-DREG are good not only because they reduce gradient variance (as previously understood) but also potentially because they directly target a divergence.\nViewing generalization of RWS as a main contribution (first bulletpoint of Section 1.2: \"...we show that AISLE admits RWS as a special case.\") is a bit of a stretch since this generalization is very straightforward from the way RWS is formulated.\nThe recommendation of using RWS-style algorithms over IWAE as given in the abstract (\"we argue that directly optimising the proposal distribution in importance sampling as in the RWS algorithm is preferable to optimising IWAE-type multi-sample objectives) is also not novel since this is also advocated by [5] (section 3.2: \"This makes RWS a preferable option to IWAE for learning inference networks because the phi updates in RWS directly target minimization of the expected KL divergences from the true to approximate posterior\").\nThe recommendation as a method for non-reparameterisable latent variables at the end of section 1.2 (\"as well as further algorithms which do not require reparameterisations\") is also given in [5].\nAre there different adaptive importance sampling algorithms that could be used within AISLE that would improve on IWAE-STL/IWAE-DREG/RWS?\n\nEXPERIMENTS:\nThere are no experiments in the main paper.\nHowever, experiments that would support/falsify the following points could be good:\n- RWS and IWAE-STL don't suffer from non-unified objectives because IWAE-STL has non-unified objectives but doesn't diverge,\n- [targeting direct divergence] is more useful than (or as useful as) [lower variance gradient estimators].\n\nCONCLUSION:\nWhile I really like the presentation and connections made in the paper, I'm not sure what the practical takeaways are (other than use IWAE-STL, IWAE-DREG, RWS over IWAE which is advocated by [2], [3], [5]).\nI'm giving this a weak accept due to the former.\nI'm willing to bump up my score if\n- the paper is modified to more accurately reflect the contributions or\n- there are experiments that provide additional support for the [targeting direct divergence] view in addition to [2, 3, 4, 5], or\n- there is a new practical algorithm that the AISLE generalization would suggest that is better than IWAE-STL, IWAE-DREG, RWS in some respects.\n\n[1] Importance Weighted Autoencoders. https://arxiv.org/abs/1509.00519\n[2] Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference. https://arxiv.org/abs/1703.09194\n[3] Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives. https://arxiv.org/abs/1810.04152\n[4] Reweighted Wake-Sleep. https://arxiv.org/abs/1406.2751\n[5] Revisiting Reweighted Wake-Sleep for Models with Stochastic Control Flow. https://arxiv.org/abs/1805.10469\n[6] Variational Inference via χ-Upper Bound Minimization. https://arxiv.org/abs/1611.00328\n[7] Tighter Variational Bounds are Not Necessarily Better. https://arxiv.org/abs/1802.04537", "Summary:\nThe authors review recent developments in gradient estimators for the IWAE bound and use them to develop a new theoretical justification for the Sticking the Landing (STL) estimator.\n\nUnfortunately, the sole novelty in this paper is a new justification for the STL estimator. The presentation, while thorough, is not novel or particularly clear. There are no experiments. These factors combine to lead me to suggest a reject.\n\nSpecific points:\n* The \"AISLE framework\" is simply used to point out that IWAE and RWS optimize KLs in different directions for the parameters of q. This is well-known in the literature and is discussed in several of the papers cited by the authors. A new framework is not needed to point this out.\n* There seems to be an overall misunderstanding of the difficulties associated with multi-sample objectives. The difficulties with IWAE do not come because it is multi-sample, but because the KL direction optimized for the approximate posterior (q) is from q to p. Thus to compute gradients of the KL we must take gradients back through latent variables sampled from q. RWS avoids this by optimizing the other KL direction, and thus does not need to take gradients through the sampling operation. Many of the issues with IWAE mentioned in the paper also appear with the standard ELBO, which is a single-sample bound. \n* Furthermore, IWAE does not necessarily require reparameterizations to deal with the high variance of its terms. Control variates can be used when latent variables are discrete, e.g. Mnih et. al 2016 \"Variational inference for Monte Carlo objectives\" and Tucker et al. 2017 \"REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models\". \n* RWS can still be thought of as a multi-sample objective in the sense that you use multiple samples of z to estimate the gradient for one data point x. The true difference is that the RWS gradient estimator is an asymptotically consistent estimator of the gradient of the marginal likelihood, while the IWAE gradient estimator is an unbiased estimator of the gradient of an objective (IWAE lower bound) that becomes the marginal likelihood in the limit of infinite samples.\n* As such, your claim to \"have shown that the adaptive-importance sampling paradigm of the reweighted wake-sleep is preferable to the multi-sample objective paradigm of importance weighted autoencoders\" is far too strong, especially considering the fact that experimental evidence in Tucker et al. 2018 shows there are situations where either one is preferable. Additionally, the DReGS estimator avoids 2 of the 3 issues you present in remark 1.\n* A smaller point: I found it hard to follow your derivations when compared with Tucker et al. 2018 because their identities use expectations over standard gaussian noise (epsilons) while expectations in your paper are all written with respect to q (e.g. the right-hand side of lemma 1). It would be helpful to go more in-depth about why that is.\n\nTo change my mind the authors would have to include experimental evaluation of some kind and demonstrate more novelty.", "COMMENT:\n\"As such, your claim to \"have shown that the adaptive-importance sampling paradigm of the reweighted wake-sleep is preferable to the multi-sample objective paradigm of importance weighted autoencoders\" is far too strong, especially considering the fact that experimental evidence in Tucker et al. 2018 shows there are situations where either one is preferable. Additionally, the DReGS estimator avoids 2 of the 3 issues you present in remark 1\"\nREPLY: \nIndeed, the IWAE-DREG estimator avoids the signal-to-noise ratio breakdown. This is precisely the point of our paper and the whole reason why we introduced the AISLE-framework in the first place. Specifically, our work makes it clear that in all known cases in which the IWAE $\\phi$-gradient can be modified to avoid the breakdown, the resulting estimators (AISLE-KL/IWAE-STL and AISLE-$\\chi^2$/IWAE-DREG) can be much more straightforwardly derived as a special case of the adaptive importance-sampling framework (without having to heuristically drop terms such as needed to obtain IWAE-STL from IWAE). Furthermore, the IWAE $\\phi$-gradient is consistently outperformed by AISLE-KL/IWAE-STL and AISLE-$\\chi^2$/IWAE-DREG in Tucker et al. (2019), i.e. by algorithms that can be derived without the need for the IWAE-paradigm (it is true that the \"plain\" RWS algorithm breaks down in one of their examples but we do not claim that this particular instance of AISLE is preferable).\n\nCOMMENT:\n\"A smaller point: I found it hard to follow your derivations when compared with Tucker et al. 2018 because their identities use expectations over standard gaussian noise (epsilons) while expectations in your paper are all written with respect to q (e.g. the right-hand side of lemma 1). It would be helpful to go more in-depth about why that is.\"\nREPLY:\nWe appreciate your concern. We believe that our notation and style of presentation is sensible because it is concise while also being general enough to accommodate arbitrary reparametrisations, i.e. these do not need to be based around standard Gaussian noise.\n", "\n\nCOMMENT:\n\"Many of the issues with IWAE mentioned in the paper also appear with the standard ELBO, which is a single-sample bound.\"\nREPLY:\nAs we stated in the abstract/introduction, the goal of our work is to analyse algorithms which use $K$ samples to reduce the bias of the estimated parameters $\\theta$ relative to the MLE (e.g. by obtaining a tighter lower evidence-lower bound than the standard VAE ELBO). We do not make any claims about whether or not seeking such bias-reductions is a sensible goal in the first place. All we are showing is that there is no \"free lunch\" in the sense that attempting to reduce this bias through the use of IWAE with $K>1$ samples\n\n(a) causes problems due to the nature of the IWAE multi-sample objective such as the signal-to-noise decay highlighted in Rainforth et al. (2018);\n\n(b) the best-case-scenario for IWAE which avoids the signal-to-noise ratio decay by using modified IWAE-gradients such as IWAE-STL or IWAE-DREG takes us back to the more classical adaptive importance-sampling setting (as proved in Propositions 1 and 2).\n\nCOMMENT:\n\"Furthermore, IWAE does not necessarily require reparameterizations to deal with the high variance of its terms. Control variates can be used when latent variables are discrete, e.g. Mnih et. al 2016 \"Variational inference for Monte Carlo objectives\" and Tucker et al. 2017 \"REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models\".\"\nREPLY:\nFair point. As mentioned in our reply to the other reviewers, we have now updated the manuscript to mention that control variates or continuous relaxations could alternatively be used to reduce the variance. Though as demonstrated in Le et al. (2019), these do not always lead to sufficient variance reductions or may not even be applicable.\n\nCOMMENT:\n\"RWS can still be thought of as a multi-sample objective in the sense that you use multiple samples of z to estimate the gradient for one data point x.\"\nREPLY: \nPerhaps we should have more precisely stated what we mean by \"multi-sample\" objective. We have now made it clear throughout the manuscript that we are referring specifically to IWAE's multi-sample objective. Loosely speaking, the IWAE multi-sample yields $\\phi$-gradients by /first/ approximating some idealised objective through importance-sampling and /then/ taking gradients of the resulting approxiation. This order of differentiation/approximation is responsible for the $\\phi$-gradient problems. In contrast, RWS and its generalisations /first/ take gradients of some idealised objective and /then/ approximate the resulting expression by some Monte-Carlo method. Whilst the approximated RWS/AISLE $\\phi$-gradient is then based on multiple (Monte Carlo) samples, we do not see how you could derive it by differentiating some Monte-Carlo approximation. The constructions given in Appendix 8.3 of Tucker et al. (2019) come closest to representing RWS/AISLE as multi-sample objective methods in the way you describe but this is only achieved by heuristically requiring that gradients are \"stopped\" w.r.t. certain random variables.\n\nCOMMENT\n\"The true difference [between RWS and IWAE] is that the RWS gradient estimator is an asymptotically consistent estimator of the gradient of the marginal likelihood, while the IWAE gradient estimator is an unbiased estimator of the gradient of an objective (IWAE lower bound) that becomes the marginal likelihood in the limit of infinite samples.\"\nREPLY: \nWe agree with this statement. Indeed, it highlights the two different philosophies/paradigmes at work here: \nunbiased gradient of biased objective (IWAE) vs biased gradient of unbiased objective (adaptive importance sampling). However, our work shows that the differences between these two paradigms is not /only/ philosophical because the former paradigm introduces practical problems (e.g. the signal-to-noise decay) while the latter paradigm does not. \n\n\n", "COMMENT:\n\"Unfortunately, the sole novelty in this paper is a new justification for the STL estimator. The presentation, while thorough, is not novel or particularly clear.\"\nREPLY:\nWe do not believe that this to be an accurate representation of our contributions. To our knowledge, the connections such as those formalised in Propositions 1 and 2 are indeed novel -- an assessment which does not appear to be contradicted by the other reviews. Taken together, these propositions make it clear that if one is interested in the bias-reduction potential offered by IWAEs over plain VAEs, then the adaptive importance-sampling framework is a more sensible starting point for designing new algorithms than the specific multi-sample objective used by IWAE. This is because the former retains all of the benefits of the latter without inheriting its drawbacks regarding the inference-network gradient (i.e. the fact that this gradient incurs high-variance term which must then be removed via reparametrisations and that it even then suffers from the signal-to-noise-ratio breakdown). \n\nGiven the large number of papers published on IWAEs and extensions (the seminal paper: Burda et al. (2016), which introduced IWAEs currently has 450 Google Scholar citations), we believe that this point is of interest to researchers working on IWAEs and related methods. \n\nWe would also be very happy to clarify the presentation if the reviewer could more concretely point out those places in which additional clarity is needed.\n\n\nCOMMENT:\n\"There are no experiments.\"\nREPLY: \nAs we mention in our replies to the other reviewers, we have now made it much more clear in the paper that some numerical illustrations are included in Appendix B and that extensive simulations for the main algorithms discussed in our work can be found in Le et al. (2019) and Tucker et al. (2019). Indeed, as pointed out by Reviewer 3, the argument -- supported by substantial numerical evidence -- that RWS's adaptive importance-sampling framework may be superior to IWAE's multi-sample objective framework is already can already be found in Le et al. (2019). Our work simply provides a more formal framework for this argument for which numerical results are not crucial. \n\nCOMMENT:\n\"The \"AISLE framework\" is simply used to point out that IWAE and RWS optimize KLs in different directions for the parameters of q. This is well-known in the literature and is discussed in several of the papers cited by the authors. A new framework is not needed to point this out.\"\nREPLY:\nThis is incorrect. What divergence (if any) these algorithms optimise depends on a number of factors including, crucially, the number of particles, $K$. In the trivial case $K=1$ IWAE reduces to a VAE and therefore does indeed minimise the \"exclusive\" KL-divergence $KL(q_\\phi\\|\\pi_\\theta)$. As discussed in more detail in our reply to the quoted comment immediately below, we can use the AISLE-framework to minimise the same divergence if we take the function $f$ in the $f$-divergence to be $f = - \\log$. \n\nCOMMENT:\n\"There seems to be an overall misunderstanding of the difficulties associated with multi-sample objectives. The difficulties with IWAE do not come because it is multi-sample, but because the KL direction optimized for the approximate posterior (q) is from q to p. Thus to compute gradients of the KL we must take gradients back through latent variables sampled from q. RWS avoids this by optimizing the other KL direction, and thus does not need to take gradients through the sampling operation.\"\nREPLY: \nThe difficulties with IWAE really /are/ a consequence of its specific multi-sample objective and not merely of the KL-direction. To see this, note that as we now discuss in Section 3.3.4 of the revised manuscript, we can use the AISLE-framework to optimise the \"exclusive\" KL-divergence $KL(q_\\phi||\\pi_\\theta)$ by taking the function $f$ in the $f$-divergence to be $f = - \\log$. If we do this, we obtain the analytical $\\phi$-gradient\n$$\n-\\nabla_\\phi KL(q_\\phi||\\pi_\\theta) \n = q_\\phi(\\log w_\\psi \\nabla_\\phi \\log q_\\phi) \n = q_\\phi(\\blacktriangledown_\\psi),\n$$\nwhere the last term assumes a reparametrisable proposal and where $\\blacktriangledown_\\psi$ is the \"black triangle\" defined on Page 5 of our manuscript. Either one of the last two expressions in the displayed equation can be trivially approximated by the vanilla Monte Carlo method with $K$ samples which immediately implies that the resulting $\\phi$-gradient estimators have a signal-to-noise ratio which does not degenerate with $K$. Note also that the second gradient expression, $q_\\phi(\\blacktriangledown_\\psi)$, in the above displayed equation indeed takes gradients back through latent variables sampled from $q_\\phi$. This shows that neither the direction of the KL-divergence being optimised nor the reparametrisation is the cause of the IWAE $\\phi$-gradient breakdown.", "COMMENT:\n\"I'm not sure what the practical takeaways are (other than use IWAE-STL, IWAE-DREG, RWS over IWAE which is advocated by [2], [3], [5])\".\nREPLY:\nWe agree that it is difficult to say which of the $\\phi$-gradients one should use in practice (except that, as you say, the standard IWAE-gradient should be avoided due to its signal-to-noise ratio breakdown). Most likely, the answer to this is highly dependent on the specific application and choice of tuning parameters, e.g. on the number of particles. In our view, the main \"practical\" takeaway from our work is the following: If one is interested in the bias-reduction potential offered by IWAEs over plain VAEs then the adaptive importance-sampling framework appears to be a better starting point for designing new algorithms than the specific multi-sample objective used by IWAE. This is because the former retains all of the benefits of the latter without inheriting its drawbacks.  \n\nCOMMENT:\n\"Viewing generalization of RWS as a main contribution (first bulletpoint of Section 1.2: \"...we show that AISLE admits RWS as a special case.\") is a bit of a stretch since this generalization is very straightforward from the way RWS is formulated.\"\nREPLY: \nAn absolutely fair point. We have now removed this sentence to more accurately reflect the fact that the connection between RWS and AISLE is obvious and was not meant to be viewed as a \"contribution\" in itself. \n\nCOMMENT:\n\"The recommendation of using RWS-style algorithms over IWAE as given in the abstract (\"we argue that directly optimising the proposal distribution in importance sampling as in the RWS algorithm is preferable to optimising IWAE-type multi-sample objectives) is also not novel since this is also advocated by [5] (section 3.2: \"This makes RWS a preferable option to IWAE for learning inference networks because the phi updates in RWS directly target minimization of the expected KL divergences from the true to approximate posterior\").\"\nREPLY:\nAgain a fair point. As you say, Le et al. (2019) [5] already demonstrated (based on extensive empirical studies) that RWS is often preferable to IWAE. Our work formalises this argument by showing that even the (heuristically) modified variants of IWAE (IWAE-DREG and IWAE-STL) which /do/ sometimes outperform \"plain\" RWS (e.g. in the numerical experiments in Tucker et al., 2019) can be derived in a principled manner from a suitably generalised version of RWS. Thus, our work is complementary to [5] (as a side node: [5] investigated only \"plain\" RWS -- as far as we know, our connection between the \"score-function free\" gradients AISLE-KL and AISLE-$\\chi^2$ and RWS is novel). To address your comment, we have taken the sentence in question out of the abstract and are now explicitly mentioning in the introduction (on Page 2) that [5] already reached the conclusion that seeking to directly optimise the proposal distribution is often preferable to IWAE, especially because the former does not rely on reparametrisations.\n\nCOMMENT:\n\"The recommendation as a method for non-reparameterisable latent variables at the end of section 1.2 (\"as well as further algorithms which do not require reparameterisations\") is also given in [5].\"\nREPLY:\nAlso a fair point. We have now updated the introduction to reflect the fact that the advantage of RWS for non-reparametrisable models is already stressed in [5]. \n\nCOMMENT:\n\"Are there different adaptive importance sampling algorithms that could be used within AISLE that would improve on IWAE-STL/IWAE-DREG/RWS?\"\nREPLY:\nAs mentioned in our reply to Reviewer 1, we have now also derived the $\\phi$-gradients in the case of a class of $\\alpha$-divergences. Furthermore, we have added a subsection discussing another special case which is obtained if we consider the \"reverse\" (i.e. \"exclusive\") KL-divergence $KL(q_\\phi\\|\\pi_\\theta)$. Here, the $\\phi$-gradient reduces to the VAE-STL gradient proposed in Roeder et al. (2017, Equation 8) (more precisely, it reduces to a simple average over $K$ independent replicates of such VAE-STL estimators). As we discuss in Appendix A, optimising the \"exclusive\" KL-divergence can lead to faster convergence for $\\phi$ than optimising the \"inclusive\" KL-divergence $KL(\\pi_\\theta\\|q_\\phi)$. However, care must be taken because minimising the \"exclusive\" KL-divergence does not can lead to poorly behaved or even ill-defined importance weights which can negatively affect learning of $\\theta$ (whose gradient is an importance-sampling approximation which makes use of those weights).", "COMMENT:\n\"It is not clear to me why the first bullet of Remark 1 is so crucial. IWAE can be applied in discrete settings (see, e.g., Mnih & Rezende, 2016) and, as you show, reparameterizations can be applied in the adaptive importance sampling type algorithms to potential improve the variance.\"\nREPLY:\nWhat we were trying to say here is that whilst reparametrisations /can/ be used for adaptive importance-sampling approaches, they are not crucial (this is in contrast to IWAE in which the $\\phi$-gradient incurs an additional high-variance term which is then removed through the reparametrisation). Following your suggestion, we have now clarified the first bullet point in Remark 1 to explain that control-variate approaches (Mnih & Rezende, 2016) can be used to reduce the variance of the IWAE $\\phi$-gradient in scenarios in which reparametrisations are not available (though, Le et al., 2019 demonstrate that control-variate constructions or continuous relaxations are not always applicable or sufficient).\n\nCOMMENT:\n\"The paper currently struggles with some organizational issues. It reads as if the paper was written as a 15 page paper and split in half to satisfy the length requirements. In particular, I would recommend shortening the introduction, cutting out as much of Sec 2 as possible, and moving experiments into the main draft. Derivations are fine left in the Appendix.\"\nREPLY: \nFollowing your suggestion, we have now significantly reduced the length of the introduction and of Section 2. As our contribution is to establish links between existing algorithms in a formal manner, numerical results are not crucial and do not contribute much additional insight. Therefore, we opt to leave the numerical illustrations in Appendix B and instead use the available space to add discussions on other interesting divergences as you suggested in the comment quoted immediately below.\n\nCOMMENT:\n\"I appreciate that it wasn't the primary aim of the paper to introduce new algorithms, but I think it could strengthen the contribution to consider at least a few. Are there any other interesting divergences to consider for the proposal distribution objective?\"\nREPLY: \nFollowing your suggestion, we have now also derived the $\\phi$-gradients in the case of a class of $\\alpha$-divergences. Furthermore, we have added a subsection discussing another special case which is obtained if we consider the \"reverse\" (i.e. \"exclusive\") KL-divergence $KL(q_\\phi\\|\\pi_\\theta)$. Here, the $\\phi$-gradient reduces to the VAE-STL gradient proposed in Roeder et al. (2017, Equation 8) (more precisely, it reduces to a simple average over $K$ independent replicates of such VAE-STL estimators). As we discuss in Appendix A, optimising the \"exclusive\" KL-divergence can lead to faster convergence for $\\phi$ than optimising the \"inclusive\" KL-divergence $KL(\\pi_\\theta\\|q_\\phi)$. However, care must be taken because minimising the \"exclusive\" KL-divergence can lead to poorly behaved or even ill-defined importance weights which can in turn negatively affect learning of $\\theta$ (whose gradient is an importance-sampling approximation which makes use of those weights).\n\nCOMMENT:\n\"The experiments are quite lacking. In tandem with the above point (consider novel algorithms), the ICLR community might rightfully expect some experiments on large scale models. I appreciate that there might not be much consistency in terms of which methods outperform others, but large scale experiments would at least present evidence of this point.\"\nREPLY:\nWe have now made it much more clear in the paper that some numerical illustrations are included in Appendix B and that extensive simulations for the main algorithms discussed in our work can be found in Le et al. (2019) and Tucker et al. (2019). Indeed, as pointed out by Reviewer 3, the argument -- supported by substantial numerical evidence -- that RWS's adaptive importance-sampling framework may be preferable to IWAE's multi-sample objective framework can already be found in Le et al. (2019). Our work simply provides a more formal framework for this argument for which numerical results are not crucial. ", "Summary: This paper presents a unifying framework through which much of the recent work on maximum likelihood learning in latent variable models (variational autoencoders) via multisample variational approaches and importance weighted approaches can be understood. It is a relatively clean framework that shows how many of the popular approaches can be described a distinct gradient-based approaches for a single underlying framework with two separate objectives for the generative model weights and the variational posterior weights.\n\nStrengths: \n- The framework is elegant and the derivations are simple. \n-  It clarifies the connection between distinct algorithms and shows the consistency of ones that were previously poorly understood (IWAE-STL).\n- It has the potential of generating new interesting algorithms.\n\nWeaknesses:\n- It is not clear to me why the first bullet of Remark 1 is so crucial. IWAE can be applied in discrete settings (see, e.g., Mnih & Rezende, 2016) and, as you show, reparameterizations can be applied in the adaptive importance sampling type algorithms to potential improve the variance.\n- The paper currently struggles with some organizational issues. It reads as if the paper was written as a 15 page paper and split in half to satisfy the length requirements. In particular, I would recommend shortening the introduction, cutting out as much of Sec 2 as possible, and moving experiments into the main draft. Derivations are fine left in the Appendix. \n- I appreciate that it wasn't the primary aim of the paper to introduce new algorithms, but I think it could strengthen the contribution to consider at least a few. Are there any other interesting divergences to consider for the proposal distribution objective?\n- The experiments are quite lacking. In tandem with the above point (consider novel algorithms), the ICLR community might rightfully expect some experiments on large scale models.  I appreciate that there might not be much consistency in terms of which methods outperform others, but large scale experiments would at least present evidence of this point.\n\nCitations:\nMnih & Rezende, 2016. https://arxiv.org/abs/1602.06725"], "review_score_variance": 4.222222222222222, "summary": "The authors argue that directly optimizing the IS proposal distribution as in RWS is preferable to optimizing the IWAE multi-sample objective. They formalize this with an adaptive IS framework, AISLE, that generalizes RWS, IWAE-STL and IWAE-DREG. \n\nGenerally reviewers found the paper to be well-written and the connections drawn in this paper interesting. However, all reviewers raised concerns about the lack of experiments (Reviewer 3 suggested several experiments that could be done to clarify remaining questions) and practical takeaways. \n\nThe authors responded by explaining that \"the main \"practical\" takeaway from our work is the following: If one is interested in the bias-reduction potential offered by IWAEs over plain VAEs then the adaptive importance-sampling framework appears to be a better starting point for designing new algorithms than the specific multi-sample objective used by IWAE. This is because the former retains all of the benefits of the latter without inheriting its drawbacks.\" I did not find this argument convincing as a primary advantage of variational approaches over WS is that the variational approach optimizes a unified objective. At least in principle, this is a serious drawback of the WS approaches. Experiments and/or a discussion of this is warranted.\n\nThis paper is borderline, and unfortunately, due to the high number of quality submissions this year, I have to recommend rejection at this point.\n", "paper_id": "iclr_2020_ryg7jhEtPB", "label": "val", "paper_acceptance": "reject"}
{"source_documents": ["Black-box optimization formulations for biological sequence design have drawn recent attention due to their promising potential impact on the pharmaceutical industry. In this work, we propose to unify two seemingly distinct worlds: likelihood-free inference and black-box optimization, under one probabilistic framework. In tandem, we provide a recipe for constructing various sequence design methods based on this framework. We show how previous optimization approaches can be \"reinvented\" in our framework, and further propose new probabilistic black-box optimization algorithms. Extensive experiments on sequence design application illustrate the benefits of the proposed methodology.", " I appreciate your comprehensive response to my review and addressing most of my comments. I have increased my rating to 'marginally above the acceptance threshold'. Unfortunately, I could not respond before the rebuttal deadline. \n\nI would appreciate if you could describe the differences or your proposed methods (in particular IR and iterative variants) and existing methods clearly *in the main text* (now in section C). \n\nI would also appreciate if could compare the performance *all methods* for different batch sizes for the UTR benchmark instead of TfBind8 since TfBind8 can be solved relatively easily. Methods are expected to be more sample efficient if the batch size is small. What is more important is to know how the relative ranking of methods changes for different batch sizes.\n\nI cannot follow your argument why you are reporting top-k scores instead of only the top-1 score. Why is it beneficial to propose the same (good) sequence multiple times or with few additional mutations that may not be correlated with the fitness function at all. What is more important is to know if methods keep on exploring other distinct optima after they found one optimum, which you did not examine.\n\nI suggest adding a conclusions section that briefly summarizes your experiments and highlights performance differences and when which method is expected to perform well. \n", "The paper relates likelihood free inference to methods for biological sequences design and proposes new sequence design methods based on this insight. # Major comments\n1) The paper is missing a related works section with an overview of existing methods for sequence design and likelihood free inference, and how they relate to the methods that were introduced in this paper.\n\n2) Section 3.1: FB-VAE and DbAs are both Estimation of Distribution Algorithms (EDA) based on a generative model. EDA is closely related to expectation maximization (https://arxiv.org/pdf/1905.10474.pdf). Please describe more clearly the differences and similarities of SNP, EDA, and EM.\n\n3) Section 3.1: Please describe the differences between FB-VAE and DbAs more clearly. Both approaches update a VAE iteratively by fitting it on the top scoring sequences. What does 'top' mean? Are there differences in the 'fitting' of the VAE?\n\n4) Section 3.2: IS is a discriminative approach for sequence design (or blackbox optimization) similar to (Bayesian) model-based optimization (MBO). Please describe the similarities between IS and MBO and differences (if there are any).\n\n5) Section 3.2: Please describe what 'construct q(m) using f(m)' means. What is f(m) and q(m) in your experiments? How were they trained and which hyper-parameters were optimized?\n\n6) Section 3.2: What are the differences between IS-A and IS-B?\n\n7) Section 3.3: IR seems like a minor variation of IS that uses a classifier instead of a regressor as surrogate models, which is not new. Are there any other differences?\n\n8) Section 3.3: Please describe which models you used for IR in your experiments and how sequences were generated.\n\n9) Section 3.4: Please describe more clearly how IPS and IPR relate to (and differ from) existing design methods that combine generative and discriminative models, e.g. RL, GANs, or optimizing a surrogate model using DbAs/CbAs, for example.\n\n10) Section 3.4: Please describe the differences between IPS-A and IPS-B more clearly, including 'differ in the detailed construction of the distribution q(m)'.\n\n11) Experiments: Since the performance of algorithms can be sensitive to the batch size, I would like to see experiments with a different batch size than 100, e.g. small (1), medium (100) and large (500).\n\n12) Experiments: Please compare to Bayesian Optimization (and RL if possible) using the same surrogate model as used for IPR/IPS and tuning hyper-parameters in the same way.\n\n13) Experiments: Please describe which hyper-parameters you tuned and how they were tuned.\n\n14) Experiments: Please describe what the boolean feature \\mathcal{E} is in all or your experiments.\n\n15) Experiments: Please describe the 'Evolution' baseline more clearly. \n\n16) Experiments: Please motivate why you report the average reward of the top-10/100 sequences instead of just the maximum reward (top-1). The average can be maximized by reporting identical or very similar sequences. More important for practical applications is that the optimizer finds a diverse set of high-reward sequences as explained in Angermueller et al, who used additional diversity metrics to quantify this.\n\n17) Experiments: How did you initialize the optimization? I would like to also see experiments that are initialized with a small set of labeled sequence (e.g. one or few parent sequences/homologs), which often exist in practice.\n\n\n# Minor comments\nSection 1, 'de novo biological sequence design': Describe which kind of sequence (DNA, RNA, protein, molecules represented as strings, ...?).\n\nSection 2, 2nd paragraph: Please cite reviews (e.g. http://arxiv.org/abs/2106.05466, http://www.nature.com/articles/s41592-019-0496-6) of existing ML design methods instead of single papers (Ahn, Gottipati, ...).\n\nDenoting 'm' as sequence and 's = f(m)' as the oracle function value is confusing since 's' is the first letter of *s*equence. I strongly suggest to use 's' to denote a sequence and, for example, 'y = f(s)' to denote the function value.\n\nYour benchmark problems seem to be similar to the benchmark problems introduced in Angermueller et al (except for Flu). If this is the case, please describe that you reused the benchmark problems from Angermueller et al and describe possible differences. By referencing Angermueller et al instead of describing each benchmark problem in detail, you can also shorten the Experimental section.\n\nExperiments (Flu). You hypothesize that backward modeling techniques perform better since sequences are long. Although sequences are long, there may be only a few variable positions while most positions are conserved. Since generative models can easily fit this conservation better they may perform well in this case since the optimization problem becomes trivial as only few variable positions are mutated. * The outlined relation between likelihood free inference and sequence design (blackbox optimization) is interesting; I am not aware of any existing papers with this insight. However, I am not sure about the impact of this insight on how sequences are designed in practice.\n* It is not described clearly enough how the proposed methods differ from existing methods for sequence design such as Bayesian model-based optimization, GANs, or RL.\n* Important details about the proposed methods and performed experiments are missing, which makes it hard to understand and assess.", " Dear Reviewer,\n\nWe are extremely appreciative of your endeavour for reviewing our paper, and as such, we try our best to offer a thorough rebuttal. As the deadline for the discussion session approaches and other reviewers have provided their thoughts, we humbly request your feedback on our detailed rebuttal response.", " Thank you very much for your feedback. We shall improve our work according to your suggestions.", " I have increased my rating to 'weak accept'. I appreciate all of the extra experiments you did and clarifications on related work. While I appreciate that the paper establishes a coherent framework that allows many different optimization approaches, I still wish there was more of a focus on a single method that the authors think is best. That said, I think the paper is above the bar for acceptance and that the ICLR community would benefit from it.", "The paper draws on connections between likelihood-free inference and black-box optimization to propose new black-box optimization methods. In general, the goal here is to not find the exact optimum of the black-box objective, but to sample from a set of sequences with high-quality objective. This is akin to the problem of collecting posterior samples in a likelihood-free inference problem. \n\nThe paper provides a number of proposed methods and compares them on some benchmark sequence optimization problems that have appeared in recent literature. Can you please comment on the relationship between your work and 'Derivative free optimization via repeated classification' https://arxiv.org/abs/1804.03761?\n\nThe paper builds up a variety of optimization methods, building on a line of work in the LFI literature. This leads to a lot of approaches to compare, and there is no clear indication as to what practitioners should use in practice. What do you actually suggest people should use?\n\nAlong these lines, the paper has far too few details about the actual optimization approaches. For example, IS-A and IS-B seem to be some of your strongest methods and these require MCMC to sample new proposed sequences. There is no discussion of how this MCMC is done. Further, there is no discussion of neural network architectures, optimization methods, etc. The paper would be stronger if it just focused on one method and provided sufficient details for practitioners to actually use it.\n\nI found this sentence in sec 4.2 very unsatisfying. Do you have any further insights about performance differences? \" This indicates that composite methods’ way of using parameterized models to replace computational procedures is not the optimal solution for small-scale tasks.\"\n\nI was surprised that the error bars were so small in all of your experiments, as I expect that the trajectory of an optimizer has high variance due, for example, to the initial set of sequences that are sampled. What do your error bars correspond to? Are they standard errors or standard deviations? Also, what sources of randomness are you accounting for when you generate multiple random trials?\n\nFor the methods that combine both a generative and discriminative models, there are two sources of approximation error. It would be very helpful if you isolated these by performing oracle experiments, where you assume that the forward model is exactly correct. This can be used to isolate the impact, for example, of using the amortized sampler q_\\phi in Alg 10. Just replace the forward model with the ground truth objective function. Can you run a quick experiment?\n The paper has some interesting methods, and the connection to LFI is helpful. However, it does not have a clear empirical recommendation for what algorithm readers should use going forward and the paper does not provide adequate details to understand how to go about actually using these methods in practice.", " We would appreciate it if you can let us know if our response has addressed your concern and thus improved your assessment of our work. We look forward to hearing from you!", " We would appreciate it if you can let us know if our response has addressed your concern and thus improved your assessment of our work. We look forward to hearing from you!", " Thank you for your insightful comments. Below, we provide responses to your comments. We hope we could address your concern such that we could receive a better score. For better demonstration, we group related questions together.\n\n**About the related work section.** We have a discussion in Section C about related work from sequence design and likelihood-free inference fields and their connection to our work. \n\n**About connection between CEM, EM and our probabilistic framework.** Thank you for pointing to this important reference [Brookes et al.]. Cross entropy method (CEM, [Rubinstein et al.]) might be another (more well-known) name for EDA — which refer to $\\max_{q} \\mathbb{E}_{q(m)}[f(m)]$. [Brookes et al.] shows that the optimization CEM could be interpreted as an EM algorithm. On the other hand, as we discussed in Section C, CEM could relate to the “backward modeling of the mechanism” approach under our framework. (SNP is the family of likelihood-free inference algorithm corresponding to this methodology.) Under our framework, the optimization target is $\\arg\\min_q KL(p(m|\\mathcal{E})||q(m)) = \\arg\\max\\int p(\\mathcal{E}|m)p(m)\\log q(m)dm$, which could be viewed as a more general form of the CEM target $\\arg\\max_q \\int f(m)\\log q(m)dm$ (derived in [Brookes et al.]) if a corresponding form of $\\mathcal{E}$ is defined such that $p(\\mathcal{E}|m) \\propto f(m) / p(m)$. As a result, both EM and EDA (CEM) could be interpreted in our probabilistic framework. We shall add related discussion into our Section C in the final version.\n\n**About \"model-based optimization\" (MBO).** We are not very certain about what MBO refers to here. (1) On one hand, generally speaking, \"model-based\"  means one has a model to imitate the behavior of the oracle $f(m)$. As we discussed in Section C, MBO is actually another expression of \"discriminative / forward modeling ideology\", and IS-A/B are just two of the concrete examples in this algorithm family. (2) On the other hand, MBO could mean a specific algorithm. In [Angermueller et al.], the authors use MBO to refer to a model-based RL algorithm (which utilizes the model to give reward) and latent-space MBO to refer to [Gómez-Bombarelli et al.] (which trains a GP regressor on a VAE-based latent space and do gradient ascent; this method would additionally require gradient information of the oracle). In this way, IS-A/B, MBO and latent-space MBO are different specific algorithms that follow the \"discriminative / forward modeling ideology\" in our Section 3.2.\n\n**About the IS algorithm.** As we have discussed in Section A.2, we use the notation  $\\tilde q(m)$ to denote our approximation of the posterior $p(m|\\mathcal{E})$. Notice that we want  $\\tilde q(m)\\propto p(m)p(E|m)$. If we choose Example A to serve as the definition of event $\\mathcal{E}$, then the samples of $\\tilde q(m)$ can be obtained in this way: (1) sample $m$ from prior $p(m)$ and (2) accept this sample if $\\hat f_\\phi(m)$ is larger than threshold $s$, or otherwise reject it. Alternatively, if we choose Example B, we have $\\tilde q(m)\\propto p(m) \\exp( \\hat f_\\phi(m)/\\tau )$. This is also how IS-A and IS-B differ.\n\n**About the IR algorithm.** There seems to be a misunderstanding, as IR is NOT a minor variant of IS. There is a fundamental difference between the contrastive modeling [Gutmann et al.] (such as GAN [Goodfellow et al.]) and directly learning a regressor. For IR, the probability ratio could be learned from the theoretical result of proposition 1 and be further used for the construction of the target posterior. The model architecture that IR adopts is the same as other algorithms (IS, IPS, IPR) in our draft, which is a Bi-LSTM as the binary classification logit model as we describe in Section B. As we described in Section 3.3 and Section A.3, the generation of the sequences is through an mcmc process whose target distribution is $r(m)\\cdot p(m)$.\n\n**About the composite algorithm family.**  Could you elaborate more on this question? We are not very sure why you claim RL and GAN are combining generative and discriminative models. For RL algorithm (we guess you are referring to [Angermueller et al. ICLR2020]), it is a policy that is being learned rather than a generative model. As for GAN [Goodfellow et al.], there is indeed both a generator and discriminator within, but in that case, these two models are playing a (possibly zero-sum) game until reaching equilibrium. In other words, the discriminator in GAN is a helper to achieve a good generator — it is an approach, not the purpose. However, both the generator and discriminator are important in our composite algorithms, and they are *collaborating* rather than *competing*. The difference between IPS-A and IPS-B also comes from the two definitions that we describe in Section 2.2 Section A.2 and Section A.4.", " **About different query set size.** Thank you for suggesting this ablation study. Due to the time limit, we only conduct experiments on IPR on TfBind (KLF11_R402Q_R1) following the same protocol in our draft. We report the average with fifty random seeds. We set query batch size to 25, 50, 100, and 200 and compare their results. Here the x-axis denotes the number of queries having been made, and the y-axis denotes the query batch size for each round. The following table shows that a smaller query batch size is more sample efficient, which makes sense because it would allow more exploration times. Here \"/\" means value not applicable. We shall extend this ablation study into the final version of our draft.\n\nKLF11_R402Q_R1 top-10\n\n|    |  300   | 400   | 500   | 600   | 700   | 800   | 900   | 1000  |\n|:-----------------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n| 25             |  0.438 | 0.443 | 0.445 | 0.449 | 0.455 | 0.458 | 0.461 | 0.464 |\n| 50             |  0.409 | 0.429 | 0.437 | 0.441 | 0.443 | 0.445 | 0.449 | 0.454 |\n| 100            |  0.408 | 0.430 | 0.442 | 0.444 | 0.444 | 0.445 | 0.446 | 0.447 |\n| 200            |   /     | 0.369 |   /    | 0.405 |   /    | 0.428 |   /    | 0.433 |\n\nKLF11_R402Q_R1 top-100\n\n|    |  300   | 400   | 500   | 600   | 700   | 800   | 900   | 1000  |\n|:-----------------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n| 25             | 0.346 | 0.370 | 0.382 | 0.388 | 0.392 | 0.397 | 0.399 | 0.403 |\n| 50             | 0.317 | 0.361 | 0.379 | 0.394 | 0.400 | 0.405 | 0.409 | 0.411 |\n|     100    |  0.262 | 0.333 | 0.364 | 0.379 | 0.392 | 0.401 | 0.405 | 0.408 |\n| 200            |  /  | 0.239 |    /   | 0.328 |  /     | 0.376 |  /     | 0.398 |\n\n\n**About top-k metric.** The usage of top-k score could be rationalized that top-1 score can be *unstable* and easy to be affected by outliers. We report top-k for better robustness and reasonable algorithm evaluation. For the diversity concern, we have a related discussion in Section B.  What's more, all of the metrics (top-1/10/100) are actually consistent with each other. We demonstrate this point by reporting the IPR's top-1 score performance for TfBind across three tasks (KLF11_R402Q_R1, PBX4_REF_R2, CRX_E80A_R1). We report the results from the 3rd round to the 10th round in order to keep consistent with Figure 1. We shall extend this ablation study into the final version of our draft.\n\nKLF11_R402Q_R1\n\n|       Round       |    3   |   4   |   5   |   6   |   7   |   8   | 9     |  10 |\n|:-----------------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|-------|\n| IPR-top1 | 0.437 | 0.439 | 0.453 | 0.457 | 0.457 | 0.457 | 0.458 | 0.458 |\n|     IPR-top10     |  0.408 | 0.430 | 0.442 | 0.444 | 0.444 | 0.445 | 0.446 | 0.447 |\n|     IPR-top100    |  0.262 | 0.333 | 0.364 | 0.379 | 0.392 | 0.401 | 0.405 | 0.408 |\n\nPBX4_REF_R2\n\n|       Round       |    3   |   4   |   5   |   6   |   7   |   8   | 9     |  10 |\n|:-----------------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n| IPR-top1 | 0.424 | 0.441 | 0.452 | 0.459 | 0.470 | 0.481 | 0.481 | 0.484 |\n|     IPR-top10     |  0.361 | 0.382 | 0.405 | 0.422 | 0.431 | 0.440 | 0.450 | 0.455 |\n|     IPR-top100    | 0.249 | 0.291 | 0.318 | 0.336 | 0.351 | 0.366 | 0.378 | 0.389 |\n\nCRX_E80A_R1\n\n|       Round       |    3   |   4   |   5   |   6   |   7   |   8   | 9     |  10 |\n|:-----------------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n|  IPR-top1  |   0.482 | 0.485 | 0.481 | 0.493 | 0.493 | 0.495 | 0.497 | 0.503 |\n|     IPR-top10     | 0.403 | 0.429 | 0.439 | 0.453 | 0.457 | 0.467 | 0.475 | 0.483 |\n|     IPR-top100    | 0.256 | 0.301 | 0.324 | 0.355 | 0.371 | 0.389 | 0.404 | 0.421 |\n\n**About the choice of $\\mathcal{E}$.** As we describe for each algorithm in Section 3 and Section A, we use Example A for IS-A, IR, IPS-A, and IPR, and Example B for IS-B and IPS-B. \n\n**About the Evolution baseline.** As we describe in Section B, can be seen as a substantial example of directed evolution [Chen & Arnold]. Like other model-based methods, Evolution also trains an LSTM regressor to predict the score of a sequence, which is further used to assist in the reproduce procedure. The Evolution algorithm maintains a generation list through the whole exploration process. In each round, the method mutates and reproduces the sequences to enlarge the generation list, and then utilizes the learned regressor to select top sequences for the next generation.", " **About the hyperparameters tuning.** As we describe in Section B, we use ZNF200_S265Y_R1 as the hold-out validation set for TfBind, and do not use a hold-out validation method for the other three tasks since we do not have access to one. For all the algorithms, we use the sweep feature of the weights and biases platform ([https://wandb.ai/site](https://wandb.ai/site)) to run fifty trials for each algorithm on each task, and pick the best hyperparameters. We tune the learning rate and whether to re-initialize the optimizer for each new round for all methods. We tune the threshold for DbAS, FB-VAE, and the methods that are with Example A. For the other choice of E, we tune the temperature. For evolution, we tune the number of offsprings for each sequence in the generation list, the probability of substitution, insertion, and deletion. \n\n**About the initialization.** The algorithms start with a batch sampled from the prior distribution as you stated. \n\n**About Bayesian optimization (BO).** As we discussed in Section C, BO belongs to the discriminative modeling under our probabilistic framework. The modeling of BO heavily relies on Gaussian process, which is known to perform poorly in modern machine learning because of the poor generalization ability of the kernel method in high dimension settings. On the other hand, the proposed IS algorithm could actually be seen as a deep neural network version of BO, replacing the Gaussian process regressor with a neural network. The acquisition function in BO could balance the exploration and exploitation and here in our case, the temperature coefficient could do the same job. \n\n**About the neural network architectures.** As we discussed in Section B, FB-VAE uses a VAE model which is described in Section B. All other methods utilize bi-directional long short-term memory (BiLSTM) [Hochreiter & Schmidhuber] models with a linear embedding layer. Both the embedding dimension and the hidden size of LSTM are set to 32. For composite methods that use two models, we use one-layer LSTM for each of them. For the other algorithms that only use one LSTM, we set its number of layers to be two. No Dropout is used in LSTM models. In this way, the number of parameters of the VAE is slightly larger than that of the two-layer BiLSTM, and all methods (except Random) share similar model parameter sizes.\n\n**About the minor comments.** (1) Generally speaking, all kinds of sequences are doable, but in this paper, we explore DNA and protein. (2) Thanks for referring to the important reviews, we shall modify the second paragraph as you suggest. (3) We are NOT using the suite from [Angermueller et al.].  In fact, as far as we know [Angermueller et al.] is not open-sourced, so we construct our own benchmark. For example, we train our own AMP oracle as we describe in the last paragraph of Section B. (4) Thank you for the analysis for Fluo. We shall take your analysis into consideration and elaborate more in the final version of this draft. \n\n**References**\n\nBrookes, David H., Akosua Busia, Clara Fannjiang, Kevin P. Murphy and Jennifer Listgarten. “A view of estimation of distribution algorithms through the lens of expectation-maximization.” *Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion* (2020): n. pag.\n\nRubinstein, Reuven Y.. “The Cross-Entropy Method for Combinatorial and Continuous Optimization.” *Methodology And Computing In Applied Probability* 1 (1999): 127-190.\n\nAngermueller, Christof, David Belanger, Andreea Gane, Zelda E. Mariet, David Dohan, Kevin Murphy, Lucy J. Colwell and D. Sculley. “Population-Based Black-Box Optimization for Biological Sequence Design.” *ICML* (2020).\n\nGómez-Bombarelli, Rafael, David Kristjanson Duvenaud, José Miguel Hernández-Lobato, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel, Ryan P. Adams and Alán Aspuru-Guzik. “Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules.” *ACS Central Science* 4 (2018): 268 - 276.\n\nGutmann, Michael U. and Aapo Hyvärinen. “Noise-contrastive estimation: A new estimation principle for unnormalized statistical models.” *AISTATS* (2010).\n\nGoodfellow, Ian J., Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville and Yoshua Bengio. “Generative Adversarial Nets.” *NIPS* (2014).\n\nAngermueller, Christof, David Dohan, David Belanger, Ramya Deshpande, Kevin Murphy and Lucy J. Colwell. “Model-based reinforcement learning for biological sequence design.” *ICLR* (2020).\n\nChen, Keqin and Frances H. Arnold. “Enzyme Engineering for Nonaqueous Solvents: Random Mutagenesis to Enhance Activity of Subtilisin E in Polar Organic Media.” *Bio/Technology* 9 (1991): 1073-1077.", " Thank you for your positive assessment and insightful comments. Besides, thank you for pointing out the typo. We have corrected them as \"... the general posterior $p(\\theta|x)$, which takes arbitrary $x$ as input and outputs a conditional distribution over $\\theta$\". We will modify our Section 3.1 in this way in our final version. \n\nWe are happy to answer any other questions you may have.", " Thank you for your insightful comments. There seem to be some misunderstanding as a result of our negligence in writing, and we hope our explanation below could help you address them.\n\n**About \"repeated classification\" [Hashimoto et al.] paper.** Thank you for suggesting this discussion. Among the six algorithms that our draft proposes, the algorithm in [Hashimoto et al.] is most close to the Iterative Ratio (IR) algorithm. With the definition of Example A, the $d_\\phi(m)$ in IR algorithm is solving a classification problem between high-score sequences and low-score sequences, which is similar to the behavior of $h(\\cdot)$ in algorithm 1 of [Hashimoto et al.]. Their differences emerge after the training of such a classifier: IR constructs the ratio between target posterior and prior distribution with learned $d_\\phi(m)$ which is justified by the proposition 1 in our draft, and further uses the ratio for the proposal of next round; on the other hand, inspired by classical cutting-plane algorithms, [Hashimoto et al.] uses the learned classifier to update the proposal with multiplicative weights algorithm [Arora et al.], making the proposal to have large probability where $h(\\cdot)$ is small. As of our Iterative Scoring (IS) algorithm, in IS a regressor instead of classifier is learned, and IS directly uses the regressor to propose new sequences, while  [Hashimoto et al.] uses the learned classifier to update proposal. We think it is also doable if one uses a regressor in the framework of multiplicative weights algorithm adopted by [Hashimoto et al.]. It is our negligence that we are not aware of this work when writing our draft, and we shall add more relevant discussion into our Section 3.3 and related work section in our final version.\n\n**Practitioner takeaway message.** In Section 4.2, the message we actually want to express is: for small-scale tasks, many proposed algorithms including composite algorithms perform approximately equally well and meanwhile, the composite methods are slightly more computational consuming than other simpler proposed methods. We do not claim that there exists any particular algorithm that would always outperform others under all settings, which is unreasonable due to the different properties of different tasks. On the other hand, we find composite methods could generally obtain SOTA results, especially in larger experiments which would be more realistic. We do not specifically distinguish within three composite methods either as we believe no one would consistently perform the best. In practical application, since we will face unknown tasks, we should still use composite methods as first trials because of their satisfying performance in numerical experiments. We shall rewrite relevant parts in the experiment section and avoid misleading arguments in our final version as you suggest.\n\n**References**\n\nHashimoto, Tatsunori B., Steve Yadlowsky and John C. Duchi. “Derivative Free Optimization Via Repeated Classification.” *AISTATS* (2018).\n\nArora, Sanjeev, Elad Hazan and Satyen Kale. “The Multiplicative Weights Update Method: a Meta-Algorithm and Applications.” *Theory Comput.* 8 (2012): 121-164.", " Thank you for your insightful comments. Below we provide responses to the questions. We first point out that, as explained in the footnote of Page 2, we slightly overuse the notation for the event $\\{m ∈ \\mathcal{E}\\}$ and the sequence set $\\mathcal{E}$. This may not be a good usage, and we shall improve this in our final version.\n\n1. It is true that $\\theta$ and $x$ can be seen as parameters and data which are distinctive from each other and $\\mathcal{E}$ is more closely related to $m$ in some sense. But here we provide another perspective. Notice that $x\\sim p(x|\\theta)$, meaning one can also see $x$ as a \"result\" or property of $\\theta$. Similarly, the event $\\{m ∈ \\mathcal{E}\\}$ can be seen as some property of sequence $m$ (for example, a desired specific chemical property).\n2. Sorry about our misunderstanding notation. What we actually mean is $p(m\\in\\mathcal{E}|m)$ rather than a distribution of some sequence sets. We will consider using different notation for $\\mathcal{E}$ and $\\{m\\in\\mathcal{E}\\}$ in our final version, as the reviewer suggests :)\n\nWe are happy to answer any other questions you may have.", " **Practice details.** Thank you for suggesting a more detailed experimental description. We put some relevant descriptions in Appendix but we agree more about implementation will definitely improve the quality of our draft. We shall make a more detailed one in the final version as you suggest. We also put relevant statement below. For neural network architectures, FB-VAE uses a VAE model. The encoder of the VAE first linearly transforms one-hot input into a hidden feature which is 64 dimension, and then separately linearly transforms to a 64-dimension mean output and 64-dimension variance output. The decoder contains a 64×64 linear layer and a linear layer that maps the hidden feature to categorical output. All other methods utilize bi-directional long short-term memory model (BiLSTM) [Hochreiter & Schmidhuber] with a linear embedding layer. Both the embedding dimension and the hidden size of LSTM is set to 32. For composite methods that use two models, we use one-layer LSTM for each of them. For the other algorithms that only use one LSTM, we set its number of layers to be two. No Dropout is used in LSTM models. In this way, the number of parameters of the VAE is slightly larger than that of the two-layer BiLSTM, and all methods (except Random) share similar model parameter sizes. For training methods, we use Adam [Kingma & Ba] optimizer. For sampling methods, we adopt the rejection sampling method which could be considered as a basic MCMC variant and we use the prior $p(m)$ as the proposal. As a side note, we want to claim that IS-A and IS-B are not the very strongest methods especially under larger / realistic settings. We will put a more specified version than the current statement in our final version. \n\n**About the error bars.** We have a description of random repeated runs in Section B. We perform fifty random trials and report the standard deviation. We sample an initial set of sequences from prior and stick to this fixed set. In our setting, the randomness source includes different initialization of neural networks, the sampling process from the proposal at each iteration, and stochastic gradients in the training. We shall add more relevant discussion in the final version. Actually, we think our randomness is consistent with the randomness scale in [Angermueller et al.], could you check the experimental part in that paper?\n\n**For impact isolation in composite methods.**\nWe perform this ablation experiment for IPR as you suggest. We replace the forward part with oracle and use \"IPR-oracle\" to refer to it. We test with TfBinding on KLF11_R402Q_R1, PBX4_REF_R2 and CRX_E80A_R1, and find IPR-oracle is consistently better than IPR as it exploits the extra information of oracle. We report the numbers in a way that the reviewer could have a comparison with the Figure 1 in our draft. Here are the results.\n\nKLF11_R402Q_R1\n\n|       Round       |    3   |   4   |   5   |   6   |   7   |   8   | 9     |  10 |\n|:-----------------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|-------|\n|     IPR-top10     |  0.408 | 0.430 | 0.442 | 0.444 | 0.444 | 0.445 | 0.446 | 0.447 |\n|  IPR-oracle-top10 |   0.432 | 0.434 | 0.439 | 0.449 | 0.454 | 0.462 | 0.473 | 0.487 |\n|     IPR-top100    |  0.262 | 0.333 | 0.364 | 0.379 | 0.392 | 0.401 | 0.405 | 0.408 |\n| IPR-oracle-top100 |  0.374 | 0.397 | 0.403 | 0.408 | 0.411 | 0.413 | 0.416 | 0.420 |\n\nPBX4_REF_R2\n\n|       Round       |    3   |   4   |   5   |   6   |   7   |   8   | 9     |  10 |\n|:-----------------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n|     IPR-top10     |  0.361 | 0.382 | 0.405 | 0.422 | 0.431 | 0.440 | 0.450 | 0.455 |\n|  IPR-oracle-top10 |  0.431 | 0.449 | 0.466 | 0.474 | 0.478 | 0.480 | 0.482 | 0.483 |\n|     IPR-top100    | 0.249 | 0.291 | 0.318 | 0.336 | 0.351 | 0.366 | 0.378 | 0.389 |\n| IPR-oracle-top100 |  0.331 | 0.367 | 0.398 | 0.420 | 0.433 | 0.441 | 0.447 | 0.451 |\n\nCRX_E80A_R1\n\n|       Round       |    3   |   4   |   5   |   6   |   7   |   8   | 9     |  10 |\n|:-----------------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n|     IPR-top10     | 0.403 | 0.429 | 0.439 | 0.453 | 0.457 | 0.467 | 0.475 | 0.483 |\n|  IPR-oracle-top10 | 0.459 | 0.488 | 0.495 | 0.496 | 0.496 | 0.496 | 0.496 | 0.497 |\n|     IPR-top100    | 0.256 | 0.301 | 0.324 | 0.355 | 0.371 | 0.389 | 0.404 | 0.421 |\n| IPR-oracle-top100 | 0.340 | 0.409 | 0.458 | 0.472 | 0.477 | 0.479 | 0.482 | 0.483 |\n\nWe shall add more ablation in this isolation comparison into the draft in the final version.\n\n**References**\n\nHochreiter, Sepp and Jürgen Schmidhuber. “Long Short-Term Memory.” *Neural Computation* 9 (1997): 1735-1780.\n\nKingma, Diederik P. and Jimmy Ba. “Adam: A Method for Stochastic Optimization.” ICLR (2015).\n\nAngermueller, Christof, David Dohan, David Belanger, Ramya Deshpande, Kevin Murphy and Lucy J. Colwell. “Model-based reinforcement learning for biological sequence design.” *ICLR* (2020).", "The authors describe a mapping from likelihood free inference to black-box sequence optimization, then use this mapping to link common algorithms in both fields. They go on to describe novel black-box sequence design algorithms induced by known LFI algorithms. Empirical results show their methods are competitive on standard datasets. The link described in this work is interesting and the novel algorithms proposed contain significant differences to existing sequence optimization techniques. Empirical results support claims that these novel algorithms are interesting and bear consideration for future design efforts. \n\nStrengths\n- Both Iterative Scoring and Iterative Ratio lead heavily on supervised learning (outputting low-dimensional predictions) compared to many existing design algorithms. Training regression models instead of likelihood models on protein sequence space could yield useful empirical advances, making this an interesting contribution.\n- The empirical evaluations are on well-known datasets and baselines appear to be used correctly. Results align with the conclusions of the paper.\n- Drawing a distinction between \"forward modeling\" and \"backward modeling\" could provide generally useful language for the community and enable communication of ideas.\n\nWeaknesses\nThe main weakness is in presentation of the link itself. In general the link seems to be \"correct\" in the sense that it is meaningful and consistently applied to link algorithms. However its exposition does give clear intuition for what is going on. I recognize this is not easy and try to provide some useful feedback below.\n- On the one hand, the quantities (theta, x) have clear distinctions as parameters and data. On the other hand, the quantities (E, m) are a set of sequence and a sequence and do not have such a clear distinction. It is quite hard to see how the set E pops out of the mapping T described beneath Table 1. \n- The notation p(E | m) = p(m \\in E | m) is very confusing. The function p(E | m) reads like a probability distribution over subsets of sequence space, not a distribution over sequence space restricted to the subset E. I recommend finding a better notation here. This comment is based on equation 3, which I could be misunderstanding.\n The contribution of algorithms which heavily rely on regression / classification to guide sequence design is interesting, especially since they are the product of a general mechanism for producing sequence optimization methods. The empirical results seem sound. The exposition of the model could use work to help readers have a crisper sense of how probabilistic modeling gets linked to a problem setting where the only randomness is in experimental noise.", "In this paper the authors draw direct parallels between likelihood-free inference (LFI) and black-box sequence design.  This allows that authors to draw parallels between existing methods from the LFI and black-box sequence design literatures.  In a few cases there is no direct analog in the black-box sequence design literature for a given LFI algorithm, and so the authors are able to immediately propose such an algorithm.  The authors also present a number of \"composite\" methods that combine ideas from a number of these approaches. I found the paper to be extremely clear and well written, providing an excellent review of both the LFI and black-box sequence design literatures and drawing clear, clean parallels between the two fields.  The proposed algorithms are all sensible and seem to work well on the empirical tasks, and the connection between the two fields seems like a fruitful area for further exploration (especially using the presented framework).  I have few comments below:\n\nTypos:\n- On p. 4 it is stated that \"...to model the general posterior $p(\\theta|\\mathbf{x})$, which takes arbitrary $\\theta$ and $\\mathbf{x}$ as two inputs and outputs a distribution\", but these models only take $\\mathbf{x}$ as input and output the conditional distribution over $\\theta$ (i.e., the posterior).\n- There are a number of typos in the supplementary materials (appendix).  The meaning was always clear, but that document could use some thorough copy editing. The paper is clear and well-written and provides a very useful conceptual framework tying two subfields together, with sufficient empirical evidence to show real gains from this approach."], "review_score_variance": 2.75, "summary": "The paper investigates various approaches, and a unifying framework, for sequence design. There were a variety of opinions about the paper. It was felt, after discussion, that the paper would benefit from a sharper focus, and somewhat suffers from being overwhelmed by various approaches, lacking a clear narrative. But overall all reviewers had a positive sentiment, and the paper makes a nice contribution to the growing body of work on protein design.", "paper_id": "iclr_2022_1HxTO6CTkz", "label": "train", "paper_acceptance": "Accept (Spotlight)"}
