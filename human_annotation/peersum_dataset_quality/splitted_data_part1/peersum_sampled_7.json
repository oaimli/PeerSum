{"source_documents": ["Generative deep learning has sparked a new wave of Super-Resolution (SR) algorithms that enhance single images with impressive aesthetic results, albeit with imaginary details. Multi-frame Super-Resolution (MFSR) offers a more grounded approach to the ill-posed problem, by conditioning on multiple low-resolution views. This is important for satellite monitoring of human impact on the planet -- from deforestation, to human rights violations -- that depend on reliable imagery. To this end, we present HighRes-net, the first deep learning approach to MFSR that learns its sub-tasks in an end-to-end fashion: (i) co-registration, (ii) fusion, (iii) up-sampling, and (iv) registration-at-the-loss. Co-registration of low-res views is learned implicitly through a reference-frame channel, with no explicit registration mechanism. We learn a global fusion operator that is applied recursively on an arbitrary number of low-res pairs. We introduce a registered loss, by learning to align the SR output to a ground-truth through ShiftNet. We show that by learning deep representations of multiple views, we can super-resolve low-resolution signals and enhance Earth observation data at scale. Our approach recently topped the European Space Agency's MFSR competition on real-world satellite imagery.", "1. > “This paper lacks many references. Recently, many works focus on MFSR containing video SR and stereo image SR via deep learning.”\n\nThank you for pointing out these references. We've included them in our revision.\nWe want to stress that our setting is different from video SR in several ways:\n\n- We learn to super-resolve sets and not sequences of low-res views. Video SR relies on motion estimation from a sequence of observations. Also, prediction at time t=T relies on predictions at t<T (autoregressive approach). Whereas in our case, we predict a single image from an unordered set of low-res inputs.\n\n- In our setting, the low-res views are multi-temporal (taken at different times) from different revisits. Please see Paragraph 1 in our comment to Reviewer 3: https://openreview.net/forum?id=HJxJ2h4tPr&noteId=B1l6sKmUoB\n\nOur work different from Stereo SR:\n- Multi-temporality (see above)\n- These images were taken at a nadir direction (top-down view) above different subpoints (coordinates below satellite). Whereas Stereo SR assumes that both views focus on the same point, but from different angles.\n\n---\n\n2. > “Thus, what is the advantage of recursive fusion compared to the above methods [Video SR papers: SPMC, FRVSR, FFCVSR, EDVR]? This paper should discuss the difference between recursive fusion and the above methods.”\n\nSPMC, FRVSR, FFCVSR and EDVR are all video SR algorithms. They all assume the input to be a temporal sequence of frames. Motion or optical flow can be estimated to super-resolve the sequences of frames. (see Part 1 of this comment above). In this work, we do not assume low-res inputs to be ordered in time. Our training input is a set of low-res views with unknown timestamps and our target output is a single image - not another sequence. More importantly, those Video SR methods were trained and tested on synthetically down-scaled data - we elaborate on this on Part 4 below.\n\nDeepSUM (Molini et al.) is another method that scored similarly to ours in the same ESA competition leaderboard with the same dataset. Their paper already showed competitive results compared to an architecture inspired by DUF (\"Deep video SR network using dynamic upsampling filters without explicit motion compensation.\" CVPR, 2018)\n\nThank for raising this important question. We've included this in the Related Work discussion.\n\n---\n\n3. > “It is better to test more datasets and compare with more state-of-the-art methods. This paper only tests in a satellite image dataset. Some datasets can be considered such as VID4 dataset in video super-resolution.”\n\nMany benchmarks exist for Super Resolution:\nVIDEO SR:\n- Vid4: C. Liu and D. Sun. A bayesian approach to adaptive video super resolution. In CVPR, pages 209–216. IEEE, 2011.\n- Vimeo 90K: Xue, Tianfan, et al. \"Video enhancement with task-oriented flow.\" International Journal of Computer Vision 127.8 (2019): 1106-1125.\n- Y10: Sajjadi, Mehdi SM, Raviteja Vemulapalli, and Matthew Brown. \"Frame-recurrent video super-resolution.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\n- REDS4 (NTIRE 2019): Nah, Seungjun, et al. \"Ntire 2019 challenge on video deblurring: Methods and results.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. 2019.\n\nSISR:\n- Set 5: M. Bevilacqua, A. Roumy, C. Guillemot, and M. L. AlberiMorel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding. 2012. \n- Set 14: R. Zeyde, M. Elad, and M. Protter. On single image scale-up using sparse-representations. In International Conference on Curves and Surfaces, pages 711–730. Springer, 2010.\n\nSTEREO:\n- FLICKR 1024: Wang, Yingqian, et al. \"Flickr1024: A Large-Scale Dataset for Stereo Image Super-Resolution.\" CVPR Workshops 2019\n\nHowever, in all of these datasets, the low-res views are artificially down-scaled, and prior work has shown such methods do not generalize to real world low-res imagery.\nSee also paragraph 4 of our comment to Reviewer3: https://openreview.net/forum?id=HJxJ2h4tPr&noteId=ryxbd5DuiH\n\n“The vast majority of prior work for this problem focus on how to increase the resolution of low-resolution images which are artificially generated by simple bilinear down-sampling (or in a few cases by blurring followed by down-sampling). We show that such methods fail to produce good results when applied to real-world low-resolution, low quality images” - https://arxiv.org/abs/1807.11458.\nSee also, Zero shot paper. Shocher, Assaf, Nadav Cohen, and Michal Irani. \"“zero-shot” super-resolution using deep internal learning.\" CVPR 2018.\n\nWe agree that benchmarking on more datasets and with more models would strengthen our paper. We are not aware of other publicly available datasets for MFSR with real low-res (not artificially down-sampled) images. By working on this dataset, we want to encourage the community to consider on real-world (not synthetic) images for super-resolution.\n", "Below we address various concerns and provide some clarifications\n\n1. > \"I am not convinced that pair-wise fusion can handle significant translational fusion as the filters have shared parameters\"\n\nIndeed such an approach would be problematic. However, this is not what we proposed. We have already addressed this concern in a previous comment. Please see paragraph 4 of this comment:\nhttps://openreview.net/forum?id=HJxJ2h4tPr&noteId=B1l6sKmUoB\n\"Please note that the recursive fusion stage accepts only the encoded low-res / reference pairs. So nothing in our co-registration scheme explicitly assumes that the difference in low-res images must be explained by translational motion only.\"\n\n---\n\n2. > \"How a single convolutional layer accomplishes a global encoding\"\n\nOur fusion block consists of 3 convolutional layers, not a single one. See section 3.1, paragraph \"Fuse\", and Table 3 in Appendix A1.\n\n---\n\n3. > \"Of course, such a problematic approach needs at least some kind of motion compensation\"\n\nWe hope our comments helped clarify our approach, and we thank the reviewer for bringing up these concerns.\n\n---\n\n4. > \"Even assuming the method only applies to satellite imagery\"\n\nWe focus on PROBA-V for a few reasons:\n- Prior SR work focuses on super-resolving low-res images that are artificially generated by simple bilinear down-sampling, see \"To learn image super-resolution, use a GAN to learn how to do image degradation first\", Bulat, et al. ECCV18.\nWe forgot to mention (and we'll include this) that PROBA-V has separate cameras onboard for capturing high-res / low-res pairs. We are not aware of other datasets with real low-res images (not synthetically down-scaled). As far as we know, the PROBA-V dataset is the first publicly available dataset for MFSR that contains naturally occurring low-res and high-res pairs . This is in contrast to most of the work in SR (SISR, MFSR, Video SR, Stereo SR) that synthetically down-sample high-res images. See also, image restoration track at CVPR19, where the vast majority of challenges are performed on synthetically downscaled / degraded images: REDS4:  \"Ntire 2019 challenge on video deblurring: Methods and results\", NTIRE Workshop at CVPR 2019, http://www.vision.ee.ethz.ch/ntire19\n- Methods that are trained on artificially downscaled datasets fail to produce good results when applied to real-world low-resolution, low quality images (Bulat, et al. ECCV18; Zero-shot SR using Deep Internal Learning, CVPR18). For this reason we experimented only on PROBA-V, a dataset that does not suffer from biases induced by artificial down-sampling.\n\n---\n\n5. > \"Characterization of satellite imagery noise models (Weibull, etc.) common in such imagery as a prior also completely disregarded.\"\n\nResults from DeepSUM and our HighRes-net (top methods in the ESA competition) and ESA's MISR paper (“Super-resolution of PROBA-V images using CNNs.” Astrodynamics, 2019) - all suggest that neural networks can learn the data-driven representations for MFSR, without the need to characterize noise models explicitly. For one, the form of the loss function itself implicitly assumes a noise model: a Gaussian for an MSE / L2-norm loss, a Laplace for a L1-norm loss. That said, we agree with you that using an explicit noise model in the architecture can be valuable depending on the application, e.g. Weibull for SAR satellite imagery (\"Modeling SAR images with a generalization of the Rayleigh distribution\", IEEE TIP 2004; \"Learning to detect roads in high-resolution aerial images\", ECCV 2010)\n\n---\n\n6. > \"it lacks mechanisms to compensate/distinguish cloud coverage\"\n\nClouds and other volatile objects (e.g. snow) can be accounted for with cloud masks - binary masks that indicate missing / occluded values in an image. Such masks can be added as an input channel, for every low-res view, as we did for the reference frame.\nWhen missing value masks are available, neural networks can learn which part of the input are anomalous, noisy, or missing from such binary masks. See e.g. \"Recurrent neural networks for multivariate time series with missing values\", Che et al., Scientific reports 8.1 (2018)\n\nThe ESA PROBA-V dataset provides clouds masks for every low-res view. We use them to bias our model towards sampling low-res views that are likely to have high \"clearance\", see appendix A.4.\nOne omission, is that we did not define \"clearance: Clearance is the fraction of occluded (clouded) values in an image, as per the cloud mask.\nWe will include this in our final version.\n\nIn satellite applications where clouds masks are not available, other segmentation methods are in order to infer such masks, e.g. \"U-net, Ronneberger, et al., MICCAI, 2015.\nMore generally, cloud detection can be considered a preliminary preprocessing step.\n\nWe have amended the discussion in Section 5.1\n\n---\n\n7. > \"atmospheric distortions\"\n\nPlease see paragraph 5 in this comment\nhttps://openreview.net/forum?id=HJxJ2h4tPr&noteId=B1l6sKmUoB", "Thank you for bringing points that were unclear to our attention.\n\n1. > \"It first estimates a reference image for the multiple input LR images by median filtering.\"\n\nOur reference image is not computed by median filtering. A median filter replaces each pixel with the median of its neighborhood (see e.g. Marion, \"An Introduction to Image Processing\", p 274.)\n\nOur reference frame is actually defined as the median across the LR set, please see section 3.1, equation 1.\nAs Reviewer 1 suggests\nreference_image($i, j$) $=$ median $( LR1(i, j), LR2(i, j), \\cdots)$\n\nThank you for raising this.\nWe have clarified this in equation 1, section 3.1.\n\n---\n\n2. > \"Then it pairwise encodes the reference image and each of the multiple images in a recursive fashion\"\n\nThe initial encoding itself is not recursive. Each reference-LR pair is encoded individually, see equation 2. The recursion happens later, during the fusion of all encodings, please see equations 3 and 4 and figure 4.", "> “What is the difference between ShiftNet and HomographyNet? This paper should add some details about ShiftNet and Lanczos interpolation.”\n\nThe architecture of ShiftNet is not novel and not the major contribution of this paper. ShiftNet and HomographyNet differ in the number of outputs predicted (2 shift parameters in our case vs. 8 homography parameters in the original paper). Translations are a special case of homographies. In this sense, ShiftNet is simply a special case of HomographyNet.\n\nMore importantly, ShiftNet differs from HomographyNet in that it is trained in a cooperative setting with HighResNet for MFSR (see section 4.1, Objective Function). The original HomographyNet paper trained in a supervised setting, on synthetically transformed data, using ground truth homography matrices for supervision.\n\nThank you for raising the need to clarify ShiftNet and Lanczos interpolation.\n\nWe have clarified the difference between ShiftNet and HomographyNet and Lanczos interpolation in Section 4.1. \nWe have also included details on the architecture of ShiftNet, in a new paragraph in Appendix A.3.\n\n\nPlease also see our comment to Reviewer1 on ShiftNet:\nhttps://openreview.net/forum?id=HJxJ2h4tPr&noteId=BkgWk9QjiH\nand our comment on Lanczos:\nhttps://openreview.net/forum?id=HJxJ2h4tPr&noteId=SylUYtXjiH", "3. > “Lanczos interpolation: can you please explain in a small sentence in what this interpolation differs from the others?”\n\nTo shift an image by a fractional amount, it must convolved with a filter that shifts and “interpolates”. Standard candidates filters in Image Processing are the sinc, bi-linear, bi-cubic, and Lanczos. See also. \"Filters for common resampling tasks.\" Turkowski, Graphics gems, 1990.\n\nThe sinc filter has an infinite support, so it produces ringing / ripple artifacts (aka Gibbs phenomenon) in practice. The bilinear filter over-smooths / over-attenuates high frequencies - the opposite of ringing but can alias results. The Lanczos filter - with finite support - approximates the sinc filter, and reduces the ringing artifacts. The bi-cubic filter performs similarly to Lanczos in practice but is more difficult to code from scratch.\n\nThank you for raising this. We have clarified the Lanczos interpolation filter in Section 4.1.\n", "4. > “ShiftNet: what is this network? We only know that it is adapted from HomographyNet, but we don't have any information about how it is composed. We just know from App. 1 that it has a very large number of parameters (34M). Why is that? Why a rigid registration needs such a large number of parameters?”\n\nShitNet has an architecture of 8 layers made of conv2D + BatchNorm2d + ReLU. Layer 2, 4 and 6 are followed by MaxPool2d. The final output is flattened to a vector x of size 32768. Then, we compute a vector of size 1024: x = ReLU( fc1( dropout( x ) ) ). The final shift prediction is fc2(x) of size 2.\n\nThe bulk of the parameters come from fc1, with 32768 * 1024 weights. These alone, account for 99% of ShiftNet's parameters. Adding a MaxPool2d on layer 3, 5, 7 or 8, halves the parameters of ShiftNet roughly by two.\nOur code for ShiftNet was adapted from: https://github.com/mazenmel/Deep-homography-estimation-Pytorch/blob/master/HomographyNet.py\n\nThank you for raising this question.\nWe have clarified the difference between ShiftNet and HomographyNet in Section 4.1: ShiftNet. We have also included details on the architecture of ShiftNet, in a new paragraph in Appendix A.3.\n\nPlease also see our comment to R2: https://openreview.net/forum?id=HJxJ2h4tPr&noteId=BylRdPcOjB\n", "5. > “median anchor image: this is one of the interesting points of the paper. Can you please just clarify that you take a naive median image? Such as: for pixel (i,j), med(i,j)= med( LR1(i,j), LR2(i,j) ,...).”\n\nYes, we simply take a median image the way you suggested. This is not median filtering as pointed by Reviewer3.\n\nWe have clarified this in equation 1, section 3.1.\n\n", "6. > “You are saying 'each imageset is padded to 32 views'. What do you mean? I thought your network was able to be used with different numbers of views. How did you pad your imageset?”\n\n- There is a typo in Algorithm 1, line 1: the right-hand-side should be K’, not K\n(LR1 ... LRK, α1 ... αK) = pad (LR1 ... LRK’), where the original input size K’ can be any number of low-res views.\n\nConsider an example with K = 16 low-res views. Once encoded, the 16 encoding tensors are grouped into pairs, and each pair is fused into one encoding. The pairing and fusing repeats until all pairs are fused into a single universal encoding. In this case, K is a power of 2 so no padding is needed. \n\nIf K’ is not a power of 2, then we pad the set of low-res views with dummy zero-valued views, such that the new size K is a power of 2. \n\nThe fact that we end up padding each imageset to 32 views is coincidental. An imageset in the PROBA-V dataset can contain up to 32 low-res views. So for PROVA-V, we typically have to pad the imageset up to 32 frames. Hence, even though HighRes-net can handle a variable number of low-res views, in practice for PROBA-V all imagesets end up with the same number of views prior to the forward pass. A power of 2 makes the recursive fusion more efficient, hence why we experimented with an upper limit of 1, 2, 4, 8, 16, 32 views in our ablation study (Appendix A.2).\n\nThank you for raising this ambiguity. To clarify this, we added a paragraph in Section 3.1, after equation 2.\n", "Thank you for your detailed assessment of our work\n\n1. > “it is quite strange to find that the results values in Table 1, in terms of public and private scores, differ from the actual leaderboards, even if the metric is still the same. Thus, I was not able to understand (i) if the authors really participate to the challenge (ii) what method they presented during the challenge as 3 or 4 are shown here (iii) what was their ranking in the real leaderboard. It is important to be precise. From what I see, they might be second on the private score (the one that matters), if so it can be clarified in the abstract where the word 'topped' was used.”\n\nTo clear any confusion, we did participate in the ESA challenge as team Rifat.\nBy “topped” we mean “achieved competitive results”. Our ensemble model, an ensemble of two HighRes-net models trained separately (discussed in a comment below) achieved\n- 0.947388637793901 (1st position) on the public leaderboard: https://kelvins.esa.int/proba-v-super-resolution/leaderboard/\n- 0.9477450367529225 (2nd position) on the final leaderboard (public + private): https://kelvins.esa.int/proba-v-super-resolution/results/\n\nThe source of the confusion was the rounding up of the scores to 4 decimals - reporting 0.9474 on the Public leaderboard and 0.9477 on the Public+Private.\nDeepSUM achieved the best score (0.9474466476281652) in the final leaderboard. \nTo get a sense of the scale, note that the 3rd place scored 0.9576339586408439.\n\nBeyond the cPSNR evaluation metric, our model requires an order of magnitude less training time - 10 hours for HighRes-net VS 1 week for DeepSUM. \nThis is because the DeepSUM architecture first learns to upscale each low-res view with a shared SISR-type (Single-Image SR) net.\nThen all the downstream tasks must to be learned on the size of a high-res image - x3 upscaling factor for PROBA-V, which means a x9 increase in memory demand.\n\nTo summarize, HighRes-net is competitive in terms of cPSNR, and also an order of magnitude faster to train.\n\nThank you for pointing out these sources of confusion. We’ll write the scores in their full precision in the revised manuscript.\n\n-----------------\n\n7. > “you list 'several baselines': for me, a baseline is something to compare to - usually, it is even something easy, such as the ESA baseline. But in your list, you are mixing baselines, other state-of-the-art methods (some of which you don't beat, so it's not a 'baseline'...), and your methods. It is very difficult to know which ones are yours. Please make different lists and/or identify yours in the table 1.”\n\nThank you for this suggestion. We have identified which lines in Table 1 correspond to our methods.\n\n------------------\n\n8. > “What is the Ensemble method? It appears that in the paper, you explain the 'HighRes-net + shiftNet' method, but actually, the 'Ensemble' is the better one, while not clearly described. I don't understand what outputs are averaged - I thought HighRes-net and shiftNet were performed simultaneously.\n\nWe can see how this is confusing. Our paper is about the HighRes-net + ShiftNet method indeed.\nFor our Ensemble method, we trained two HighRes-net+shiftNet models, one with K=16 and one with K=32 frames.\nWe averaged HighRes-Net_16 + HighRes-Net_32 outputs for our predictions on the final leaderboard.\nWe have included these details in our revised manuscript.\n", "10. > “Is it possible to have views of different sizes as inputs? Or views with missing parts?”\n\nThis question is orthogonal to a lot of the deep learning computer vision literature.\nOur model is trained on patches of size 64x64 and tested on images of size 128x128.\nA simplistic way to handle multiple inputs of different size is by padding the image dimensions to max(height, width), and indicate the padded parts with a binary (missing value) mask.\nReframed as a missing-value problem, then our model can handle views with missing parts (e.g. concealed pixels because of clouds or padding) with a missing-value channel.\nPlease see also Part 6 of our comment to R3: https://openreview.net/forum?id=HJxJ2h4tPr&noteId=ryxbd5DuiH\n\n----------------\n\n11. > “Usually, we have a high resolution image that is different in terms of type of acquisition (like a different type of satellite, but also true in medical images for ex.). Is your network ready for that? I mean: can we constrain the method even if it is not the same type of acquisition as ground truth? In that case, is it possible to have a super-resolution of the type of the input LR images?”\n\nYes we can. High-res and low-res images were obtained from different cameras onboard the PROBA-V satellite.\nPlease, see also Part 4 of our comment to R3:\nhttps://openreview.net/forum?id=HJxJ2h4tPr&noteId=ryxbd5DuiH\n\nIf “different” here means in spectral band, whether that is between input and output, or within inputs, then the general problem of fusion of hyperspectral / multispectral images for down-stream prediction tasks - but not MFSR - has been studied extensively in remote sensing.\nFor a starting point, see e.g. Helber, et al. (2018) in our references.\n\nAs far as we know, MFSR with inputs from different band is an open problem.\n\n----------------\n\n\"Typos\"\n\nTypos have been fixed.\n\nThank you for your detailed analysis of our paper. We have added all clarifications in our revised manuscript.", "9. > How did you select the hyperparameters of your model?\n\nFor the network design, we used design choices common in the SR literature that we have cited.\nFor upsampling, we used a stride = kernel = 3, which made sense for the upsampling factor of 3 needed for the competition, otherwise checkerboard artifacts become more frequent.\nThe learning rate and its decay schedule, were chosen by trial and error.\n", "2. > “cPSNR metric : (i) can you please explain the acronym; (ii) it was the competition metric, so it means it is not 'we use' but 'the challenge used' - which will also show that it was not a choice to satisfy your results, but a standard metric.”\n\nIndeed, cPSNR (clear corrected Peak to Signal Noise Ratio) is a standardized score used by the challenge as the competition metric.\n\nAlso see, https://kelvins.esa.int/proba-v-super-resolution/scoring/ for the justification of the score. \nWe also used it as our training objective with sub-pixel registration (see section on ShiftNet).\n\nThank you for raising this. We'd hate to give the impression that we custom-designed this metric to suit our performance. We have amended our paper accordingly.", "Our work would benefit from an expanded discussion on its novelty. Can you please elaborate on this statement?\n\n> \"Novelty-wise, there is very little as all modules have been commonly used for SR tasks.\"\n\nNovelty-wise, the use of neural networks as modules for SISR and MFSR tasks is common indeed these days (convolution, deconvolution, transpose convolution...).\n\nHowever, our work differs from SISR by conditioning the output on multiple inputs. Our work differs from video super resolution, in that we do not assume the frames or samples to be in a particular order. To our best knowledge, our fusion block is novel, offering an approach to combine information from multiple frames without requiring temporal order or motion models. Our full architecture could handle an arbitrarily large and unordered set of low-res inputs (unlike DeepSum e.g.).\n\nHere is an exhaustive list of novelties in our work:\n(a) implicit co-registration via a reference frame\n(b) recursive fusion\n(c) a differentiable registered loss in a deep MFSR setting\n(d) a deep MSFR architecture that accepts an arbitrarily large and unordered set (not just a sequence) of low-res views\n(e) sampling on the set of low-res views to prevent over-fitting\n(f) ablation experiments that demonstrate the efficacy of the above\n(g) through a public competition have we demonstrated SOTA performance on the new Proba-V dataset from the European Space Agency, for Earth Observation and Vegetation Growth monitoring - a unique dataset for MFSR in remote sensing.\n\n1. Besides the upsampling module, can you please specify the modules you referred to as non-novel, and which papers have used them for SR tasks?\n\n2. If any, can you please specify related methods we should therefore compare to?\n\n3. The biggest novelty is arguably the consolidation of all the above elements to achieve a deep MFSR in an end-to-end fully differentiable fashion. As far as we know, no other work has done this.", "Thank you for pointing out the limitations of this paper. Below we address these concerns and provide some clarifications:\n\n1. > \"The apparent pixel-wise motion due to the different topology of the earth's surface is what I referred to as the \"spatially varying motion\".\" (from comment https://openreview.net/forum?id=HJxJ2h4tPr&noteId=BJgRT5MMsr )\n\nThank you for your clarification on \"spatially varying motions\". The technical term for the effect you are describing is \"parallax\" - a core concept in Remote Sensing, Photogrammetry and Stereo SR, see for instance the papers that Reviewer2 cited:\n- Learning Parallax Attention in Stereo Super-Resolution, CVPR 2019\n- Enhancing the Spatial Resolution of Stereo Images using a Parallax Prior, CVPR 2018\n\nYou are correct in that if 32 low-res views were acquired during a single fly-over, the successive geolocations would indeed be significantly different and the parallax effect would be magnified. This is the case in aerial photography, for instance.\n\n---\n\n2. > \"This explicitly assumes the images are on a flat surface, which perhaps an acceptable assumption for high-orbit satellite imagery where the ground surface depth variances might be negligible. Still, this is a very critical limitation of the method.\"\n\nThe parallax p is inversely proportional to the distance d from the object (see e.g. Zeilik and Gregory, 1998):\np ∝ 1 / d.\n\nIf 32 low-res views were acquired during a single fly-over, the successive geolocations would indeed be significantly different and the parallax effect would be magnified. This is the case in aerial photography, for instance.\n\nIn our case, we are interested in detecting vegetation growth (PROBA-V), road networks (infrastructure), farms / ranches (agriculture), deforestation (Amazon) or human presence and buildings. In all these monitoring applications, the objects of interests are no more than, say, 50 m tall (for trees).\n\nThe parallax effect between low-res images does not inhibit the super-resolution of the previous objects. The lowest of LEO altitudes is 300 km (PROBA-V is about 800km), so the relative depth variation is at most 50m / 300,000m = 0.0033%. The parallax effect is imperceptible for 50m tall objects. Below are some calculations to support this claim: \nGiven a point A at height 50m (distance d_A = 300,000 - 50), and a point B at height 0 (distance d_B = 300,000m), their relative change in motion is: p_A / p_B = d_B / d_A = 300,000 / 299,950 = 30 / 29.995.\nThis means that if point A moved 30m, then point B moved 5 millimeters less than 30m due to parallax.\nIn the case of a fast LEO satellite like PROBA-V, its geolocation is accurate enough such that the translational shifts are mostly within a sub-pixel accuracy, and they almost never exceed 2 pixels. On the ground, 2 pixels amount to a baseline length of (2 px) * (300 m/px) = 600m.\nSo between two images where point A (50m altitude) moved 600m, and point B (0m altitude) has moved\n0.005 * 20 = 0.1 m.\nHence, the parallax effect is imperceptible for 50m tall objects. Even less so for the objects that we have underlined above, and our results support this.\n\nThere is indeed a limitation for objects (e.g. mountains, and towering clouds) whose depth variation (well beyond 50m) indeed leads to significant parallax effects. Thank you for pointing out this limitation of MFSR for remote sensing. We will include it in our discussion.\n\n ", "We address some of your comments below:\n\n1. “This is a (relatively fast) moving satellite. In other words, the low-resolution images are acquired at different geocentric coordinates.”\n\nThe orbital velocity is not the main source of variation of geolocation during the low-res acquisition.\n\nSatellites typically acquire only one low-res image per revisit of some coordinate (principal point). PROBA-V captures images at the nadir direction (top-down view) above a given coordinate (subpoint = principal point). So for a given coordinate (site) PROBA-V acquires one low-res view per 101 mins. The low-res views of a site are not taken in burst mode during a single flyover. In fact, in the ESA dataset the LR views are spaced days apart, see section 5.1.\n\nAny translational component in the motion between low-res views is due to noisy geolocation: satellites rely on noisy radio signals from ground stations to know their position, and therefore for knowing when to capture an image.\n\n--------\n\n2. \"The apparent pixel-wise motion due to the different topology of the earth's surface is what I referred to as the \"spatially varying motion\".\"\n\nThank you for your clarification on \"spatially varying motions\". The technical term for the effect you are describing is \"parallax\" - a core concept in Remote Sensing, Photogrammetry and Stereo SR, see for instance:\n- Learning Parallax Attention in Stereo Super-Resolution, CVPR 2019\n- Enhancing the Spatial Resolution of Stereo Images using a Parallax Prior, CVPR 2018\n\nYou are correct in that if 32 low-res views were acquired during a single fly-over, the successive geolocations would indeed be significantly different and the parallax effect would be magnified. This is the case in aerial photography, for instance.\n\n--------\n\n3. “The proposed method only estimates two translational coefficients per low-resolution image pair”\n\nThis is a misunderstanding: Our proposed method estimates two translational coefficients only for the super-resolved image (the final predicted output). Low-res co-registration is handled by the reference frame, together with our fusion block, which consists of three convolution layers (and not a single layer as reported in your original review).\n\n--------\n\n4. “thus completely disregards the effects of the non-flat earth surface, which changes the relative motion at every pixel depending on their distance from the camera imaging plane”\n\nPlease note that the recursive fusion stage accepts only the encoded low-res / reference pairs. So nothing in our co-registration scheme explicitly assumes that the difference in low-res images must be explained by translational motion only. The only stage where translation parameters are explicitly required, is to translate the SR output so as to accurately measure the image similarity in our loss function.  The final translational registration in the end does not directly affect the output, but indirectly by \"registering\" the loss towards a less biased estimate. Without registration-at-the-loss, the model would compensate for all possible motion between the SR prediction and the ground truth, which would result in a blurry output, see Table 5, Appendix A.3 for the ablation experiment.\n\nAs a crude analogy, we like to think of the registered-loss as a way to endow the model with corrective spectacles. This way, the error gradients carry a more meaningful feedback from the loss. Once fully trained (at test time), our method does not rely on ShiftNet or rigid translations.\n\n--------\n\n5. \"Also, the proposed method disregards the other common factors that cause pixel-wise observable motion between two images such as moving vehicles, waves, cloud movements, cast shadow changes due to earth rotation, atmospheric turbulence artifacts (heat-induced image warping), etc.\"\n\nAs we explain in paragraph 1 of this comment, the low-res images are spaced days apart. So short-lived motions like those of vehicles, waves, or clouds, would require require a video-like time-lapse of images across the time-scale of seconds, minutes, hours. This type of problem requires video super-resolution algorithms. These assume the input to be a sequence of frames, ordered by time. This way, motion or optical flow can be estimated and used to super resolve the sequences of frames. In our framework, we assume low-res observations as an unordered set.\n\nOther sources of motion, such as cast shadows, and atmospheric turbulence are difficult to model but we do not disregard them. Instead of hard-coding them as rules or domain-expert knowledge, we let our model learn to fuse images by implicit co-registration which is not limited to rigid transformation like pure translations (as we explain in paragraph 4 of this comment).\n\nWe hope that this clarifies our problem setting and the scope of our solution.", "As far as I am aware, there is no low orbit geostationary (fixed) satellite for imaging. The multiple low-resolution images used in the paper are obtained from the PROBA-V, which has 101 minutes orbit period. This is a (relatively fast) moving satellite. In other words, the low-resolution images are acquired at different geocentric coordinates. This means there is a baseline between any two low-resolution images (images are taken at different positions). Now, let's think about the translational motion. A translational only motion as used in the paper can model if only the motion of the camera is exactly parallel to the imaged planar surface. When the surface is not planar, the observed motion at each pixel would have a different motion even if the only observed motion is due to the satellite motion. For instance, a pixel corresponding to a mountain peak and another one corresponding to a valley floor will have very different translational motion coefficients (delta_x, telta_y) even every other thing on the surface is static. The fact is that the earth's surface is not flat. It is not a planar surface. The apparent pixel-wise motion due to the different topology of the earth's surface is what I referred to as the \"spatially varying motion\". The proposed method only estimates two translational coefficients per low-resolution image pair, thus completely disregards the effects of the non-flat earth surface, which changes the relative motion at every pixel depending on their distance from the camera imaging plane. Also, the proposed method disregards the other common factors that cause pixel-wise observable motion between two images such as moving vehicles, waves, cloud movements, cast shadow changes due to earth rotation, atmospheric turbulence artifacts (heat-induced image warping), etc. I hope this provides some clarification. Thanks.", "Thank you for reviewing our work.\n\nCan you please clarify what you mean by the following sentence?\n> \"The fusion strategy disregards the underlying spatially varying motion\"\n\nAre you suggesting that the fusion strategy disregards the relative translation between the low-res views?", "This paper proposes an end-to-end multi-frame super-resolution algorithm, that relies on a pair-wise co-registrations and fusing blocks (convolutional residual blocks), embedded in a encoder-decoder network 'HighRes-net' that estimates the super-resolution image. Because the ground truth SR image is typically misaligned with the estimation SR image, the authors proposed to learn the shift with a neural network 'ShiftNet' in a cooperative setting with HighRes-net. The experiments were performed on the ESA challenge on satellite images, showing good results.\n\nOverall, I found this paper interesting, and the method described is both clever and efficient. While some points need to be clarified, I am in favor of accepting this paper to ICLR.\n\nPositive aspects:\n- the paper is very clear and easy to read, with nice figures.\n- a sensitivity analysis on many different parameters or types of inputs are made, which makes this paper an interesting research paper. For example, the tests on the type of reference image to stack at the beginning are very interesting.\n- While I am not an expert on super-resolution, I do see a clever algorithm, that can be for example used with different number of input views. \n- The end-to-end framework is also quite interesting as it allows to be spread easily across the very large satellite images users, with the code aldready publicly available.\n- Lastly, the results are good wrt to the state-of-the-art, as the algorithm was proposed during a 2019 challenge and was in the top ones on the private leaderboard.\n\nRemarks and clarifications:\n- After looking at the challenge website, it is quite strange to find that the results values in Table 1, in terms of public and private scores, differ from the actual leaderboards, even if the metric is still the same. Thus, I was not able to understand (i) if the authors really participate to the challenge (ii) what method they presented during the challenge as 3 or 4 are shown here (iii) what was their ranking in the real leaderboard. It is important to be precise. From what I see, they might be second on the private score (the one that matters), if so it can be clarified also in the abstract where the word 'topped' was used.\n- cPSNR metric : (i) can you please explain the acronym; (ii) it was the competition metric, so it means it is not 'we use' but 'the challenge used' - which will also show that it was not a choice to satisfy your results, but a standard metric.\n- Lanczos interpolation: can you please explain in a small sentence in what this interpolation differs from the others?\n- ShiftNet: what is this network? We only know tha it is adapted from HomographyNet, but we don't have any information about how it is composed. We just know from App. 1 that it has a very large number of parameters (34M). Why is that? Why a rigid registration needs such a large number of parameters?\n- median anchor image: this is one of the interesting points of the paper. Can you please just clarify that you take a naive median image? Such as: for pixel (i,j), med(i,j)= med( LR1(i,j), LR2(i,j) ,...) .\n- You are saying 'each imageset is padded to 32 views'. What do you mean? I thought your network was able to be used with different numbers of views. How did you pad your imageset?\n- you list 'several baselines': for me, a baseline is something to compare to - usually, it is even something easy, such as the ESA baseline. But in your list, you are mixing baselines, other state-of-the-art methods (some of which you don't beat, so it's not a 'baseline'...), and your methods. It is very difficult to know which ones are yours. Please make different lists and/or identify yours in the table 1.\n- What is the Ensemble method? It appears that in the paper, you explain the 'HighRes-net + shiftNet' method, but actually, the 'Ensemble' is the better one, while not clearly described. I don't understand what outputs are averaged - I thought HighRes-net and shiftNet were performed simultaneously.\n- How did you select the hyperparameters of your model?\n\n\nScientific questions: \n- Is it possible to have views of different sizes as inputs? Or views with missing parts?\n- Usually, we have a high resolution image that is different in terms of type of acquisition (like a different type of satellite, but also true in medical images for ex.). Is your network ready for that? I mean: can we constrain the method even if it is not the same type of acquisition as ground truth? In that case, is that possible to have a super-resolution of the type of the input LR images?\n\nTypos: \n- is comprised of -> is composed of\n- in Table 7, the bold number should be the beta=infinity as it is the best one. It will be clearer, even if of course, a good train score does not mean a good method because of overfitting.\n", "The paper proposes a framework including recursive fusion to co-registration and registration loss to solve the problem that the super-resolution results and the high-resolution labels are not pixel aligned.  Besides, the method is able to achieve good performance in the Proba-V Kelvin dataset. However, I have some concerns about this paper:\n\n1) This paper lacks many references. Recently, many works focus on multi-frame super-resolution containing video super-resolution and stereo image super-resolution via deep learning.  They are using multiple low-resolution image to construct high-resolution image. For example:\n\nStereo super-resolution:\n\nJeon, Daniel S., et al. \"Enhancing the spatial resolution of stereo images using a parallax prior.\" *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 2018.\n\nWang, Longguang, et al. \"Learning parallax attention for stereo image super-resolution.\" *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 2019.\n\nVideo super-resolution:\n\nTao, Xin, et al. \"Detail-revealing deep video super-resolution.\" *Proceedings of the IEEE International Conference on Computer Vision*. 2017. \n\nFRVSR: Sajjadi, Mehdi SM, Raviteja Vemulapalli, and Matthew Brown. \"Frame-recurrent video super-resolution.\" *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 2018.\n\nFFCVSR: Yan, Bo, Chuming Lin, and Weimin Tan. \"Frame and Feature-Context Video Super-Resolution.\" *Proceedings of the AAAI Conference on Artificial Intelligence*. Vol. 33. 2019.\n\nEDVR: Wang, Xintao, et al. \"Edvr: Video restoration with enhanced deformable convolutional networks.\" *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops*. 2019.\n\n2) Recursive fusion is aimed to fuse multiple low-resolution image information. Recently, more and more work utilize different methods to fuse multiple low-resolution image. For example, Tao et al proposes SPMC (Sub-pixel Motion Compensation) to align image, FRVSR uses unsupervised flow network that predicts optical flow to warp image, FFCVSR directly concatenate low-resolution image as the input of 2D convolutional network to fuse the information, and EDVR fuses multiple image features via utilizing deformable convolution. Thus, what is the advantage of recursive fusion compared to the above methods? This paper should discuss the difference between recursive fusion and the above methods.\n\n3) Registration loss is important in this paper and it can solve the problem the output SR is not pixel-wise aligned to the HR ground truth. Registration loss utilizes ShiftNet that is adapted from HomographyNet. Thus, what is the difference between ShiftNet and HomographyNet? This paper should add some details about ShiftNet and Lanczos interpolation. \n\n4) It is better to test more datasets and compare with more state-of-the-art methods. This paper only tests in a satellite image dataset. Some datasets can be considered such as VID4 dataset in video super-resolution.", "This paper presents a multi-frame super-resolution method applied to satellite imagery. It first estimates a reference image for the multiple input LR images by median filtering. Then it pairwise encodes the reference image and each of the multiple images in a recursive fashion then fuses the corresponding feature maps with residual blocks and bottleneck layers until only one feature maps for the entire multiple images obtained. In other words, LR images are fused into a single global encoding. Then, it applies a standard upsampling network to obtain the super-resolved image this image is fed into a network that estimates only the translational shift, and the shifted image with the estimated translation parameters finally resampled. \n\nA major concern is the estimation of a single translational motion for the SR image at the end of the network after all multiple images are already fused. The fusion strategy disregards the underlying spatially varying motion. This explicitly assumes the images are on a flat surface, which perhaps an acceptable assumption for high-orbit satellite imagery where the ground surface depth variances might be negligible. Still, this is a very critical limitation of the method. Besides, I am not convinced that pair-wise fusion can handle significant translational fusion as the filters have shared parameters. How a single convolutional layer accomplishes a global encoding and compensates for any translation between any LR image pair is neither articulated nor convincing discussion and evaluations are provided. Of course, such a problematic approach needs at least some kind of motion compensation, which may explain the need for the ShiftNet layer at the end. Nevertheless, this seems quite problematic. \n\nEven assuming the method only applies to satellite imagery, it lacks mechanisms to compensate/distinguish cloud coverage and atmospheric distortions. Characterization of satellite imagery noise models (Weibull, etc.) common in such imagery as a prior also completely disregarded. For these reasons, the proposed method fails to be considered as a comprehensive approach for multi-image super-resolution of satellite imagery. \n\nNovelty-wise, there is very little as all modules have been commonly used for SR tasks. "], "review_score_variance": 8.666666666666666, "summary": "This paper proposes a multi-frame super-resolution method including recursive fusion for co-registration and registration loss to solve the problem where the super-resolution results and the high-resolution labels are not pixel-wise aligned. While reviewer #1 is positive about this paper, reviewer #2 and #3 rated weak reject and reject respectively. Both reviewer #2 and #3 have extensive experience in the topic of image super-resolution. The major concerns raised by the reviewers include the lack of many references, the comparison of recursive fusion with related work, limited test databases, using a single translational motion for the SR images, and limited novelty on the network modules.  The authors provided detailed response to the concerns, however they did not change the overall rating of the reviewers. While the ACs agree that this work has merits, given the various concerns raised by the reviewers, this paper can not be accepted at its current state.", "paper_id": "iclr_2020_HJxJ2h4tPr", "label": "train", "paper_acceptance": "reject"}
{"source_documents": ["We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper.", "Yes, that is an interesting point that ReLU networks are routinely used together with stochastic gradient VI. My concern would then apply to these methods as well, even though the discontinuity in the MH method is inherent to the problem where for NNs it can be resolved by replacing ReLUs with a continuously differentiable alternative.\n\nIt is true that sub-gradients exists, but I believe the interaction between sub-gradients and integration requires some careful consideration.\n\nI think the paper is good work and my comments are mostly me being curious if you had considered this problem (and whether it is a problem) and had any ideas how to resolve it. Thanks for the discussion!", "While it is true that for some value of \\xi, the integrand is not differentiable, it does admit a sub-gradient everywhere. This is sufficient for optimization ( https://en.wikipedia.org/wiki/Subgradient_method ). Also note that ReLU networks demonstrate this same characteristic (continuous function, with discontinuities in the first derivative), but are routinely trained in deep learning.\n\nThank you again for your interest and thoughtful reading of our work!", "I think the key issue here is establishing whether the integrand in Eq. (8) is an absolutely continuous function of \\theta for almost all \\xi. Then you can use e.g. Theorem 3 here http://planetmath.org/differentiationundertheintegralsign to validate the interchange. The easier to validate Theorem 2, which is sufficient for most cases in stochastic gradient-based VI, does not hold for your integrand because assumption 2 is not valid for the function A(|). This because the derivative of A(|) does not exist whenever the ratio of densities is exactly one in Eq. (3). But perhaps it is easy to show the absolute continuous property of A(|) wrt \\theta for almost all \\xi?\n\nI do agree that this is not an issue of any discrete random variables nor the function described by the numerical integration of an Hamiltonian flow. My concern is merely if the discontinuity in the gradient of A(|) wrt \\theta will be an issue.\n\nAlso, thanks for a very interesting read!", "Thank you for your question, we believe our derivations are correct and the gradients unbiased.\n\nIn Eq. (8), p(\\xi) and q(\\xi) are not functions of the parameters \\theta and the loss inside the expectation is an (almost everywhere) differentiable function of \\xi. This allows us, when differentiating w.r.t \\theta to easily exchange (under mild assumptions) derivative and integration.\n\nIt is important to note that when optimizing, we are NOT sampling through the accept/reject step. Given a state \\xi, we move it forward using our (differentiable) generalized Hamiltonian dynamics and use that new proposed state for the loss; explicitly marginalizing over the accept/reject decision. We thus do not need to back-propagate through a discrete decision variable, making our gradients unbiased. This is additionally detailed in Pasarica and Gelman 2010.\n\nWe hope this answers your question!", "The accept-reject step of the MCMC kernel introduces a discontinuity in the function A( ) that depends on both the random variable xi AND the parameter being optimized with respect to. This means that interchanging the order of expectation (integration) and differentiation in eq. (8) is invalid in general. Have the authors considered this in their derivations? Can you prove that the gradients used for learning in Alg. 1 are still unbiased, and thus will lead to convergence by standard stochastic approximation results? If the gradients are not unbiased (which I suspect is the case), have you studied the impact this has on the learning procedure?", "In this work, the authors propose a procedure for tuning the parameters of an HMC algorithm (I guess, if I have understood correctly).\n\nI think this paper has a good and strong point: this work points out the difficulties in choosing properly the parameters in a HMC method (such as the step and the number of iterations in the leapfrog integrator, for instance). In the literature, specially in machine learning, there is ``fever’’ about HMC, in my opinion, partially unjustified.\n\nIf I have understood, your method is an adaptive HMC algorithm  where the parameters are updated online; or is the training  done in advance? Please, remark and clarify this point.\n\nHowever, I have other additional comments:\n\n- Eqs. (4) and (5) are quite complicated; I think a running toy example can help the interested reader.\n\n- I suggest to compare the proposed method to other efficient methods that do not use the gradient information (in some cases as multimodal posteriors, the use of the gradient information can be counter-productive for sampling purposes), such as Multiple Try Metropolis (MTM) schemes\n\nL. Martino, J. Read, On the flexibility of the design of Multiple Try Metropolis schemes, Computational Statistics, Volume 28, Issue 6, Pages: 2797-2823, 2013, \n\nadaptive techniques, \n\nH. Haario, E. Saksman, and J. Tamminen. An adaptive Metropolis algorithm. Bernoulli, 7(2):223–242, April 2001,\n\nand component-wise strategies as Gibbs Sampling, \n\nW. R. Gilks and P. Wild, Adaptive rejection sampling for Gibbs sampling, Appl. Statist., vol. 41, no. 2, pp. 337–348, 199. \n\nAt least, add a brief paragraph in the introduction citing and discussing this possible alternatives.", "The paper proposed a generalized HMC by modifying the leapfrog integrator using neural networks to make the sampler to converge and mix quickly. Mixing is one of the most challenge problems for a MCMC sampler, particularly when there are many modes in a distribution. The derivations look correct to me. In the experiments, the proposed algorithm was compared to other methods, e.g., A-NICE-MC and HMC. It showed that the proposed method could mix between the modes in the posterior. Although the method could mix well when applied to those particular experiments, it lacks theoretical justifications why the method could mix well. ", "The paper introduces a non-volume-preserving generalization of HMC whose transitions are determined by a set of neural network functions. These functions are trained to maximize expected squared jump distance.\nThis works because each variable (of the state space) is modified in turn, so that the resulting update is invertible, with a tractable transformation inspired by Dinh et al 2016.\n\nOverall, I believe this paper is of good quality, clearly and carefully written, and potentially accelerates mixing in a state-of-the-art MCMC method, HMC, in many practical cases. A few downsides are commented on below.\n\nThe experimental section proves the usefulness of the method on a range of relevant test cases; in addition, an application to a latent variable model is provided sec5.2. \nFig 1a presents results in terms of numbers of gradient evaluations, but I couldn't find much in the way of computational cost of L2HMC in the paper. I can't see where the number \"124x\" in sec 5.1 stems from. As a user, I would be interested in the typical computational cost of both \"MCMC sampler training\" and MCMC sampler usage (inference?), compared to competing methods. This is admittedly hard to quantify objectively, but just an order of magnitude would be helpful for orientation. \nWould it be relevant, in sec5.1, to compare to other methods than just HMC, eg LAHMC?\n\nI am missing an intuition for several things: eq7, the time encoding defined in Appendix C\n\nAppendix Fig5, I cannot quite see how the caption claim is supported by the figure (just hardly for VAE, but not for HMC).\n\nThe number \"124x ESS\" in sec5.1 seems at odds with the number in the abstract, \"50x\".\n\n# Minor errors\n- sec1: \"The sampler is trained to minimize a variation\": should be maximize\n\"as well as on a the real-world\"\n- sec3.2 \"and 1/2 v^T v the kinetic\": \"energy\" missing\n- sec4: the acronym L2HMC is not expanded anywhere in the paper\nThe sentence \"We will denote the complete augmented...p(d)\" might be moved to after \"from a uniform distribution\" in the same paragraph. \nIn paragraph starting \"We now update x\":\n    - specify for clarity: \"the first update, which yields x' \"/ \"the second update, which yields x''  \"\n    - \"only affects $x_{\\bar{m}^t}$\": should be $x'_{\\bar{m}^t}$  (prime missing)\n    - the syntax using subscript m^t is confusing to read; wouldn't it be clearer to write this as a function, eg \"mask(x',m^t)\"?\n    - inside zeta_2 and zeta_3, do you not mean $m^t\" and $\\bar{m}^t$ ?\n- sec5: add reference for first mention of \"A NICE MC\"\n- Appendix A: \n    - \"Let's\" -> \"Let\"\n    - eq12 should be x''=...\n- Appendix C: space missing after \"Section 5.1\"\n- Appendix D1: \"In this section is presented\" : sounds odd\n- Appendix D3: presumably this should consist of the figure 5 ? Maybe specify.", "We believe we are the most flexible parameterization of a Markov kernel to date. However, there has been previous work that proposes general purpose kernels. Most relevant is Song et al, which trains a flexible class of volume-constrained Markov proposals using an adversarial objective. (We discuss and experimentally compare against this approach in our paper.) \n\nThanks for the question!", "Thanks for the response! As I understand it then, your method is the first in literature to be able to train expressive MCMC kernels? (as if I recall correctly, in the past, the focus has been more on tuning a very limited number of parameters associated with the proposal distribution, like the variance of a gaussian proposal for ex.)", "We thank the reviewers for their valuable time and comments.\n\nWe updated the paper with the following modifications:\n- Clarified some points and fixed typos pointed out by the reviewers.\n- Added a ``Future Work section as well as additional relevant references.\n- Added a comparison with Look-Ahead HMC (LAHMC; Sohl-Dickstein et al. 2014) in the Appendix.\n\nAdditionally, in the process of revisiting our experiments to compare against LAHMC, we empirically found that weighting the second term of our loss (the ‘burn-in’ term) could lead to even more improved auto-correlation and ESS on the diagnostic distributions. We therefore updated the paper and report the results obtained with slightly tuning that parameter (setting it to 0 or 1).", "Thank you for your question.\n\nYou are correct that in general our method’s proposals cannot be interpreted as (approximately) integrating the dynamics of any Hamiltonian. Ultimately our goal (and that of HMC) is to produce a proposal that mixes efficiently, not to simulate Hamiltonian dynamics accurately.\n\nThere are many other trainable proposals for which we could compute the Jacobian, but not all will mix efficiently. By choosing a parameterized family of proposals that can mimic the behavior of HMC (and initializing it to do so), we ensure that our learned proposal performs at least as well as HMC.\n\nThe momentum-resampling step is essential, since it is the only source of randomness in the proposal. Using gradient information (d_x U(x)) is essential for giving the proposal information about the local geometry of the target distribution.", "I had one question -- in Equations 4-6, you have functions Q, T to rescale and translate the momentum and position. However it seems that Q, T are vectors and thus you are learning arbitrary transformations to d_x U(x)? \n\nIf that is the case, then I'm unclear on how your leapfrog operator guarantees (approximate) integration of the Hamiltonian. And if it does not and your goal is simply to learn proposals for which you can compute the Jacobian, then what's the purpose of the momentum resampling step and/or having the d_x U(x) term in the update at all?\n\nIf you could shed some light on that, that would be great!", "We first and foremost want to thank you for your time and extremely valuable comments. We have uploaded a new version of the paper based on the feedback, and have addressed specific points below.\n\nClarification about 50x vs 124x:\nWe decided against advertising the 124x number as it is misleading considering that HMC completely failed on this task; the correct ratio was too large for us to experimentally measure. As such, we reported the one for the Strongly-Correlated Gaussian. We clarified this in the text and detail that L2HMC can succeed when HMC fails.\n\nIntuition on Eq 7.:\nWe define this reciprocal loss to encourage mixing across the entire state space. The second term corresponds exactly to Expected Square Jump Distance, which we want to maximize as a proxy for mixing. The first term discourages a particle from not-moving at all in a region of state space -- if d(x, x’) = 0, the first term would be infinite. We clarified that part in the text.\n\nTime encoding:\nOur operator L_\\theta consists of the composition of M augmented leapfrog steps. For each of those leapfrog, the timestep t is provided as input to the networks Q, S and T. Instead of providing it as a single scalar value, we provide it as a 2-d vector [cos(2 * pi * t / M), sin(2 * pi * t / M)].\n\nRegarding samples in Fig5:\nSample quality and sharpness are inherently hard things to evaluate. Our observation was that many digits generated by L2HMC-DGLM look very sharp (Line 1 Column 2, Line 2 Column 8, Line 5 Column 2, Line 7 Columns 3 and 7…). However, we will weaken the claim in the caption.\n\nComparison with LAHMC:\nWe compared our method to LAHMC on the evaluated energy functions. L2HMC significantly outperforms LAHMC on all tasks, for the same number of gradient evaluations. LAHMC is also unable to mix between modes in the MoG case. Results are reported in Appendix C.1. \n\nWe also note that L2HMC could be easily combined with LAHMC, by replacing the leapfrog integrator of LAHMC with the learned one of L2HMC.\n\nIn the process of revisiting our experiments to compare against LAHMC, we empirically found that weighting the second term of our loss (the ‘burn-in’ term) could lead to even more improved auto-correlation and ESS on the diagnostic distributions. We therefore updated the paper and report the results obtained with slightly tuning that parameter (setting it to 0 or 1).\n\nQuestion about computation:\nFor the 2d-SCG case, on CPU, the training of the sampler took 160 seconds. The L2HMC overhead for sampling, with a batch-size of 200, was about 36%. This is negligible compared to an 106x improved ESS.  We also should note that for the latent generative model case, we train the sampler online with the same computations used to train everything else; in that case L2HMC and HMC perform the exact same number of gradient evaluation of the energy and thus requires no training budget.\n\nThank you once again for your valuable feedback, we hope this helps answer your questions!", "Thank you very much for your review and comments. Guaranteeing mixing between modes is a fundamental (#P-Hard) problem. As such, we do not hope to solve it in the general case. Rather, we propose a method to greatly increase the flexibility and adaptability of a class of samplers which is already state of the art in many contexts. The relation between mixing time and expected square jump distance is thoroughly treated in [Pasarica & Gelman, 2010], and is the theoretical inspiration for our choice of training loss.\n\nWe further emphasize that, barring optimization issues, our method should always fare at least as well as HMC in terms of mixing.\n\nThank you once again, we have updated the text to more clearly discuss why our approach might be expected to lead to better mixing.\n\nAdditionally, in the process of revisiting our experiments to compare against LAHMC, we empirically found that weighting the second term of our loss (the ‘burn-in’ term) could lead to even more improved auto-correlation and ESS on the diagnostic distributions. We therefore updated the paper and report the results obtained with slightly tuning that parameter (setting it to 0 or 1).", "Thank you for your review and the pointer to references.\n\nWe wish to emphasize that our method is able, but not limited to, automatically tuning HMC parameters (which systems like Stan already have well-tested heuristics for). Our approach generalizes HMC, and is capable of learning proposal distributions that do not correspond to any tuned HMC proposal (but which can still be plugged into the Metropolis-Hastings algorithm to generate a valid MCMC algorithm). Indeed, in our experiments, we find that our approach significantly outperforms well-tuned HMC kernels.\n\nThe training is done during the burn-in phase, and the trained sampler is then frozen. This is a common approach to adapting transition-kernel hyperparameters in the MCMC literature. \n\nRegarding the references, we added those in the text. We also want to emphasize that all of these are complementary to and could be combined with our method. For example, we could incorporate the intuition behind MTM by having several parametric operators and training each one when used. \n\nAdditionally, in the process of revisiting our experiments to compare against LAHMC, we empirically found that weighting the second term of our loss (the ‘burn-in’ term) could lead to even more improved auto-correlation and ESS on the diagnostic distributions. We therefore updated the paper and report the results obtained with slightly tuning that parameter (setting it to 0 or 1)."], "review_score_variance": 0.6666666666666666, "summary": "This paper presents a learned inference architecture which generalizes HMC. It defines a parameterized family of MCMC transition operators which share the volume preserving structure of HMC updates, which allows the acceptance ratio to be computed efficiently. Experiments show that the learned operators are able to mix significantly faster on some simple toy examples, and evidence is presented that it can improve posterior inference for a deep latent variable model. This paper has not quite demonstrated usefulness of the method, but it is still a good proof of concept for adaptive extensions of HMC.\n\n", "paper_id": "iclr_2018_B1n8LexRZ", "label": "val", "paper_acceptance": "accepted-poster-papers"}
{"source_documents": ["Reinforcement Learning (RL) has demonstrated promising results across several sequential decision-making tasks. However, reinforcement learning struggles to learn efficiently, thus limiting its pervasive application to several challenging problems. A typical RL agent learns solely from its own trial-and-error experiences, requiring many experiences to learn a successful policy. To alleviate this problem, we propose collaborative inter-agent knowledge distillation (CIKD). CIKD is a learning framework that uses an ensemble of RL agents to execute different policies in the environment while sharing knowledge amongst agents in the ensemble. Our experiments demonstrate that CIKD improves upon state-of-the-art RL methods in sample efficiency and performance on several challenging MuJoCo benchmark tasks. Additionally, we present an in-depth investigation on how CIKD leads to performance improvements.\n      ", "[Update] Question/Comment #8 Response: \nWe have attached the new experimental results in the appendix.", "We would like to thank all the reviewers for their helpful comments. We have provided responses to all of the reviewers, and have updated our paper in response to the reviews. In particular, we have improve the presentation of the material, and have run additional experiments. These additional experiments have been added to the Appendix, Section A1. Unfortunately, we were unable to complete all of the reviewers’ experiments, and were only able to run SAC-CIKD for 1.5M timesteps on the additional domains, as opposed to 3M, despite the fact that our GPUs have been continuously running. These partial results are in Appendix A1. We will expand our set of experiments in the future.\n", "We would like to thank the reviewer for reading our paper and providing feedback on our work. We will address the reviewer’s various points here.\n\n- “Though extensive ablation studies have shown the effectiveness of each component of CIKD. It is still not clear why this approach can be effective. Intuitively, it is possible that the exploration from a set of agents would outperform a single agent. The measure of exploration efficiency could help in explaining the results.”\n\t\nThe purpose of the Ensemble-SAC baseline was to investigate how CIKD itself improves upon Ensemble-SAC, since Ensemble-SAC may naturally benefit from improved exploration upon a single SAC agent. In this way, we can decouple (to a certain degree), the benefits of an ensemble vs. the benefits of applying CIKD to an ensemble. In future work, it would be interesting to perform more experiments and analyses on the effect of improved exploration and the benefit of distillation (e.g., through plotting the state-visitation frequencies or distilling the knowledge from a separate hand-crafted dataset as opposed to agent data).\n\n- “Furthermore, better exploration not necessarily leads to better performance and sample-efficiency. Does knowledge distillation serve as a better alternative to exploit existing data?”\n\nIt is unclear how to compare various approaches to exploiting existing data, since there is no general framework for data exploitation. However, we would like to highlight two of our experiments that investigated this question. Off-policy RL offers an obvious way to exploit experiences. In Section 5.3 (Fig. 3a), we performed an experiment where we tuned an Ensemble-SAC agent to perform additional off-policy RL updates. We found that using CIKD with Ensemble-SAC outperforms Ensemble-SAC with additional RL updates. Our second experiment, the “hard-copy” experiment (Fig. 3b, Section 5.3), copies the best teacher into the students rather than performing distillation. We found that distillation performs better than strictly hard-copying the best agent. Interesting directions for future work include performing additional analyses on various data exploitation methods.\n\n- “Model/algorithm agnostic: The proposed method is more convenient to be applied with off-policy approach when the policy is in the form of softmax. Is it also applicable to other approaches? “\n\nOur method is certainly applicable to other approaches. In particular, our KL Loss can be applied to other policy gradient approaches as long as the policy outputs a distribution and is differentiable, as is the case with most modern policy representations. In principle, CIKD can be applied to value-based approaches as well by changing the distillation loss from a KL-Loss to another loss, such as mean-squared-error (MSE). In fact, in our paper, we distill our critics using an MSE loss.\n\n- “How do you determine when to stop the KD process? As mentioned in section 5.5, if we conduce KD fully, all students would be just imitating the teacher's behavior. It seems the key is to tune a good termination threshold for each task? Are there any guidelines to set up this threshold? Do you have some automatic way to terminate the KD procedure?”\n\nWe didn’t focus on optimizing the terminating threshold for distillation and found that CIKD worked quite well by randomly dividing the entire (bounded) replay buffer into several minibatches and performing distillation on all of these minibatches. If this process were to be repeated infinitely, this would amount to imitation learning. To verify that CIKD is not tantamount to pure imitation learning, we ran two key experiments. In one experiment (Section 5.5, Figure 5d), we tested an alteration of CIKD where we re-initialized the student networks prior to distillation. This amounts to pure imitation learning in that we have a randomly initialized student learning to directly imitate the teacher. We found that pure imitation learning fails to perform as well as CIKD. In Section 5.5 (Fig. 5c), we show that the student often outperforms the teacher after distillation. Note that outperforming the teacher is atypical in imitation learning, which further supports that CIKD does not reduce to imitation learning. Returning to the reviewer’s question, an interesting direction for future work is to investigate the tradeoff between pure imitation learning and a moderate amount of distillation. But in this work, we found that CIKD achieved good performance with straightforward distillation termination conditions and is in fact superior to distilling via pure imitation learning.\n\n- “Minor: L1, P5, ‘how to CIKD improves the sample efficiency’” \n\nWe have corrected this mistake in the paper.\n", "First, we would like to thank the reviewer for the time and effort given to the review, and for his/her valuable comments. We will address various points that the reviewer mentioned here.\n\n-“Interestingly they only use the most recently collected trajectories to update all policies, and despite storing the rollouts in a replay buffer, they seem to only use the stored transitions for the imitation part described below.”\n-“Some of their experimental results uses extra gradient steps, although it’s not clear if those gradient steps are also only on the last rollout collected, or on transitions sampled from the replay buffer as it is typical in off-policy RL methods”.\n\nIn fact, we indeed do what the reviewer notes that we should do, i.e., we use “transitions sampled from the replay buffer as it is typical in off-policy RL methods”, for all of our experiments, as the original SAC algorithm does. Perhaps this misunderstanding stems from our pseudocode (Algorithm 1), which was written to be general. Though we allude to the use of experience replay for policy training in Section 4.2, we realize that the pseudocode is misleading to make one think that our policy is only trained by the most recent rollouts. We have rewritten our pseudocode to accurately reflect our experiments and the SAC algorithm.\n\n\n-“I suspect that most of the benefit of their method comes from randomly perturbing the parameters of the policies in the ensemble. More thorough and careful experimentation needs to be carried out to investigate this direction.”\n\nFirst, we would like to clarify that our results demonstrate that selecting the best agent to be the teacher has some benefit over choosing a random teacher. Perhaps we misunderstood, but we interpreted the reviewer’s mention of the “random perturbation” to mean the change in parameters after performing distillation with a random teacher.  We agree that this question is worth investigating, and we will address these in subsequent experiments in the future.\n\nOur hypothesis (which we have updated in the draft), outlined in Section 5.4, is that the reason that distillation from a random teacher performs quite well is because reducing the KL divergence between policies in the ensemble makes each policy better at learning from off-policy data generated from other members of the ensemble, by reducing the extrapolation error that comes from off-policy data distributions [11]. Thus, the distillation improves the quality of the off-policy RL updates, leading to better-than-expected performance for the agents.\n\nWe intend to investigate this improvement by measuring the extrapolation error [11] (stemming from learning from off-policy data) before and after distillation to measure this effect. We can do so with the method used by Fujimoto et al. [11], where they measured the extrapolation error for DDPG, an off-policy actor-critic method applied to Mujoco tasks. We will also run additional ablations where we withhold some agents in the ensemble from distillation or distill from all members of the ensemble to a single agent. \n\nWe would like to re-emphasize that these additional investigations are not fundamental to our core claims and results in the paper. These experiments are interesting supplementary experiments that better explain the reasons behind our performance improvements upon Ensemble-SAC. However, they will not change our core result which is that Ensemble-SAC augmented with CIKD gives improves performance across several Mujoco tasks.\n\n-“specially realizing that the “HalfCheetah” experiments seem to not have all seeds run to convergence, please report the full results”\n-“Furthermore, the authors only run the environments for 1M steps, whereas in previous works some environments are shown to get higher return after more training steps.” \n\nWe intend to run experiments for longer training times and on all standard Mujoco tasks. Due to limited resources, we have prioritized (for the rebuttal) running experiments for longer training times. These experiments for longer training times are currently underway and we will post them to the rebuttal as soon as possible. We will also run experiments on more Mujoco tasks, though it is not likely that we will be able to complete them within the rebuttal period.\n", "Question/Comment #5 Response: \n\nWhether or not it is surprising that the KL update is necessary is quite subjective. However, given that this question (i.e. Point 5) is listed under the reviewer’s concerns about the paper, we would urge the reviewer to consider our contribution. When we have an empirical hypothesis, and we test it, and demonstrate that it is useful for improving performance, that is valuable to the community. We do not think that a hypothesis needs to be surprising for it to be a valuable contribution to the field (if that was a concern). \n\nRegarding the reviewer’s point about why selecting a random teacher provides some improvement, we agree with the reviewer, and appreciate the comment. We viewed our random teacher experiment as an auxiliary experiment to our core result of using the best teacher, and thus did not devote as much text to discussing this experiment. However, the reviewer’s comments are interesting and important, and we have updated the paper to reflect those comments.\n\nQuestion/Comment #6 Response: \n\nThe core idea of Osband’s method is to combine several value functions, which each induce a policy, and have these individual policies act in the environment and generate trajectories, which are then used to train all the value functions off-policy.  Our Ensemble-SAC similarly consists of several agents/policies, which act in the environment and generate trajectories, which are then used to train all the agents off-policy. \n\nWe did not presume to say that Osband’s method is the same as CIKD but without the KL update. Our wording was that it was “effectively equivalent to CIKD-RL without inter-agent knowledge distillation”, which we realize is strong wording. We have rephrased this in the paper to indicate that Ensemble-SAC is the natural analog to Osband’s method in this setting.\n\nQuestion/Comment #7 Response: \n\nActually, we did hyperparameter searches (on learning rate) for Ensemble-SAC (extra) and Vanilla-SAC (extra) and found out that the default hyperparameters are the best. Presumably, a smaller learning rate should be used with extra policy updates. However, it turned out that neither a smaller nor larger learning rate are better than the default hyperparameters reported in the original SAC paper.\n\nQuestion/Comment #8 Response: \n\nThe dominant agent experiment is meant to test whether a single agent is consistently better than other agents in the ensemble. For us it was not obvious that the KL update will necessarily cause the best agent to change so frequently within the ensemble. While the KL update brings the policies closer, it certainly doesn’t suggest that one agent should surpass another after a KL update. In particular, the KL update is unidirectional, so if an agent A is better than agent B, then we perform updates on agent B. While we expect agent B’s policy to grow more similar to A’s policy, we wouldn’t necessarily expect it to surpass A, especially considering that B is worse than A before the KL update. It would be useful to run an additional experiment in the absence of a KL update to see whether a single agent is consistently dominant. We will try to get these results before the end of the rebuttal period.\n\nQuestion/Comment #9 Response: \n\nIn an abstract sense, we are related to genetic algorithms in that we share knowledge between individuals, in our case an ensemble of RL agents, and in the case of genetic algorithms, a population of genotypes (candidate parameters to a certain problem in genetic algorithms). However, there are several differences in the details between CIKD and genetic algorithms. First, we use knowledge distillation for sharing knowledge, while typical genetic algorithms use crossover, which usually randomly exchanges the elements between two sequences. \n\nSecondly, we use RL and distillation to optimize each individual while typical genetic algorithms solely use mutation and crossover (i.e., randomly exchanging the elements between sequences). While distillation is somewhat related to crossover, typically crossover is a destructive process, either explicitly replacing the structure or parameters of the individual, unlike distillation, whose parameter changes are through gradient updates. Note that, as we showed in Figure 5d, the destruction of parameters before distillation harms the performance, which suggests the advantage of being able to preserve the learned knowledge.\n\n\nAgain, we would like to thank the reviewer again for providing valuable feedback and comments which we used to improve the paper. \n", "Question/Comment #1 Response: \n\nFirst, we would like to thank the reviewer for his/her detailed/thorough review of our paper. Regarding the biggest concern being that the paper is not motivated from a theory standpoint, it is unclear whether the reviewer is suggesting that we should have theoretical results or whether we should have theory motivating our method. We certainly think that rigorous, empirical contributions are extremely valuable contributions to the field, and there have been several empirical papers published at ICLR.\n\nRegarding 1a), it is true that our empirical results are on soft actor-critic, an actor-critic method, whereas Osband’s results are on value-function based methods. Is there a concern to be raised in 1a that we may address? \n\nRegarding 1b), we respectfully disagree that the KL distillation update is the only significant contribution of the paper. Our contribution is an empirical demonstration that combining the training of an ensemble of RL agents with periodic distillation between the members of the ensemble can significantly improve performance, and it is backed by several experiments. However, it is true that the distillation itself is primarily carried out through a combined KL update for actors and mean-squared error for the critics. However, we disagree with the characterization that the loss functions alone are the main/significant contribution, as it diminishes the importance of executing this gradient update in the context of training an ensemble with periodic distillation, which to our knowledge nobody has attempted. The point 1c seems to implicitly suggest (please correct us if we are wrong) that our paper’s aim is to justify diversity through randomization and/or imitation learning. Our paper builds off of ensemble RL, and our paper’s aim is to provide a method that improves upon a vanilla/standard form of ensemble RL. So our experimental results are not meant to further highlight the benefit of diversity beyond existing literature that justifies it. If we overemphasized the importance of diversity to the point where it mischaracterizes our contribution, we apologize. Our goal in motivating diversity was because we are considering settings where we are training an ensemble of RL agents. That is, our paper first motivates diversity since diversity is a precursor to the application of our actual method/contribution.\n\nQuestion/Comment #2 Response: \n\nWe apologize if we are unclear, and are happy to address any individual instances of unclear wording that the reviewer presents. In RL, exploration typically refers to trying actions randomly or randomly perturbing policy parameters to collect trajectories in the environment. To address the reviewer’s question, gathering more data through exploration can help improve the policy by adding some noise to the gradient, which is similar to the reviewer’s note. Our point was that for the agent to improve, it needs to be rewarded for “good” behavior. An RL agent typically explores policies that are close to its greedy policy (e.g., sampling from a gaussian policy or adding the noise to the greedy actions). If that greedy policy is poor, it can require quite a bit of exploration in order to acquire those good experiences. We understand the reviewer’s concern here, and have reworded those sentences in the paper.\n\nQuestion/Comment #3 Response: \n\nThank you for pointing this out. This is correct. It was our intention to suggest that we can take traditional on-policy updates and have them use off-policy data for learning by applying importance sampling. However, these are then off-policy algorithms, not on-policy algorithms. We have removed this from the paper.\n\nQuestion/Comment #4 Response: \n\nIn Section 4.3, we are speaking theoretically, whereas in Section 5.3, we are speaking empirically. That is, theoretically, off-policy methods can update their policies by experience generated by any policy (e.g., human experts, past experience, and the other agents’ policies). However, in Section 5.3, we are saying that SAC, in practice, cannot fully benefit from the past experience (similar to the reviewer’s comments in point 5). Recent work [11] which we cite in our paper, supports this claim, demonstrating that DDPG, an off-policy critic method failed to learn well from data that deviates too much from the agent’s current policy. We have updated the draft based on these comments.\n", "This paper introduces a method for using an ensemble of deep reinforcement learning policies, where members of the ensemble are periodically updated to imitate the most promising member of the ensemble. Thus learning proceeds by performing off policy reinforcement learning updates for each individual policy, as well as some supervised learning for inter-policy imitation learning.\n\nI start by what I view as the positive aspects about the paper:\n1- The algorithm is quite simple (to understand and to implement).\n2- Experimental results are performed on a variety of domains, and more importantly, each experiment is motivated by a question.\n\nThat said, I have some concerns about this paper which I list below:\n\n1- Perhaps my biggest concern is that the approach is not motivated from a theory stand point. There has been interesting results in Osband's work [Osband, 2016] (and references therein) for randomized value functions which can serve as a foundation for this work. That said, a) Osband's results, at least immediately, are related to value-function based methods, as opposed to policy gradient b) the KL update which one could argue is the main and only significant contribution of the paper, is not justified by Osband or any other prior work c) there is not anything that this paper adds to the literature to better justify diversity through randomization and/or imitation learning based on the best member of the ensemble.\n\n2- I have found various claims in the paper which are unclear, scientifically not true, or sometimes even contradicting. In Introduction, for example, the authors mention that the agent sometimes gets into a sub-optimal policy and may require a large number of interactions before escaping the sub optimal policy. How does gathering more data help to improve the policy? Either we are in a local maximum, which if we are doing gradient ascent, there is really not much we could do, or that we are in a saddle point, which we can escape by adding some noise to the gradient. [Jin,2017]\n\n3- In section 4.3 the authors talk about on-policy methods requiring importance sampling (IS) ratios. To the best of my knowledge, IS is only used for off-policy learning. Can the authors provide a link to an on-policy method that does IS?\n\n4- Again in section 4.3 authors claim and I quote \"Using off-policy methods, all the policies in the ensemble can easily be updated, since off-policy update methods can perform updates from any \\tau\". But later on in Section 5.3 authors claim that \"off-policy actor-critic methods (e.g. SAC) cannot fully utilize the other agent's or past experience.\" So which statement is true?\n\n5- Again, the KL update is interesting, but is it even surprising that the KL update is necessary for an ensemble of policies updates using policy gradients? In the absence of this KL update, which the authors characterize as the method that Osband proposed, the policies could generally be arbitrarily far from one another. This means that each policy needs to perform policy evaluation using trajectories that are coming from other policies who in principle can be radically different than the policy we want to update. This means that updates will be quite \"off-policy\" which we know can really degrade the quality of the estimated gradient. This is perhaps why even choosing a random policy to update towards is providing \"some\" improvement. I think this is the real insight, but it is not really discussed at all in the paper.\n\n6- On the same note, I do not think that one can say Osband's method is the same as CIKD but only without the KL update. Most notably, Osband's work was presented for value-function-based methods like DQN. These methods work fundamentally different than policy gradient methods, which rely on (near) on-policy updates to perform good policy improvements. In that sense, the presented results make sense, but I disagree with the framing of the results and how they are presented here.\n\n7- In section 5.3, when the authors utilize more policy updates to have a fair comparison, are they retuning hyper parameters? Surely they need to do that, at least for hyper-parameters that are known to be super important such as the step size.\n\n8- Overall I liked section 5.5 that is trying to dissect causes for improvement. However, it seems like that the \"dominant agent\" hypothesis has been rejected hastily, unless I misunderstood the experiment. The authors show that the notion of best is spread across different agents. But of course this will be the case in light of the KL update, since the policies are getting closer to one another. Can you redo the experiment in the absence of the KL update?\n\n9- Have the authors thought about any connection between this and genetic algorithms? In genetic algorithms, the idea is the next set of candidates are chosen based on the most promising candidates in the current iteration. CIKD seems like a soft implementation of this idea.\n\nIn light of the comments above, I am voting for weak rejection, though as I said before, I do see some interesting things in this paper. I encourage the authors to think about CIKD from a theoretical lens in the future.", "Summary:\nThis paper proposed an ensemble method (CIKD) that train multiple agents and\nuse knowledge distillation to transfer knowledge from the current best agent to\nsub-optimal agents periodically.  According to the reported results, CIKD is a\nsimple yet effective approach to improve sample-efficiency and final performance.  \nThe experimental results are sufficient, and the ablation studies are conducted thoroughly. It is shown that both selecting the best agent and using KD to\ntransfer knowledge are effective comparing to other naive alternatives. \n\n\nI recommend the acceptance of this paper. \n\nThe paper proposed a novel approach (CIKD) to improve the sample-efficiency of the state-of-the-art. The proposed ensemble approach is aligned with our intuition, and it is effective. The authors proposed to train several agents at the same time and randomly select one of\nthe agents as a behavior policy during each rollout. Then the collected trajectory is used to update the policy of all agents. Meanwhile,\nthey keep tracking the performance of each agent and use the current best agent to conduct knowledge distillation to other agents periodically. \n\nThis paper first conducts experiments to show when consolidating\nthe SAC with CIKD, both of the final performance and sample-efficiency can be improved. Then a set of ablation studies verified the best agent selection strategy, and the knowledge distillation\nstrategy is necessary for the ensemble method. \n\n\nInvestigation on the reasons for improvement:\nThough extensive ablation studies have shown the effectiveness\nof each component of CIKD. It is still not clear why this approach\ncan be effective. \nIntuitively, it is possible that the exploration from a set of agents would outperform\na single agent. The measure of exploration efficiency could help in explaining the results. Furthermore, better exploration not necessarily\nleads to better performance and sample-efficiency. Does knowledge distillation serve as a better alternative to exploit existing data? \n\nModel/algorithm agnostic\nThe proposed method is more convenient to be applied with off-policy approach when the policy is in the form of softmax. Is it also applicable\nto other approaches? \n\nExperiments:\nHow do you determine when to stop the KD process? As mentioned in section 5.5, if we conduce KD fully, all students would be just imitating\nthe teacher's behavior. It seems the key is to tune a good termination\nthreshold for each task? Are there any guidelines to set up this threshold?\nDo you have some automatic way to terminate the KD procedure?\n\n\nMinor:\nL1, P5, \"how to CIKD improves the sample efficiency\" \n\n", "This paper proposes an RL training procedure that maintains an ensemble of k policies and periodically pushes all the policies to be closer to the best performing one. The formulation, experiments and analysis are very clear and show a mild improvement over using the same underlying RL algorithm without the imitation part. The idea is close to many other proposed in the literature, but to my knowledge it is the first time this exact procedure is studied in detail.\n\nThe first piece of their approach is an off-policy RL algorithm. In their case, they use SAC. The second piece is adding an ensemble of policies (3 in their case), and randomly selecting one of them every time a rollout is collected, and using the collected rollout to update all the policies. This effectively implies 3 times more overall gradient updates compared to SAC. They call this ablation SAC-ensemble. Interestingly they only use the most recently collected trajectories to update all policies, and despite storing the rollouts in a replay buffer, they seem to only use the stored transitions for the imitation part described below. Some of their experimental results uses extra gradient steps, although it’s not clear if those gradient steps are also only on the last rollout collected, or on transitions sampled from the replay buffer as it is typical in off-policy RL methods. In general, I think the work could improve with more details about how much the policy training could improve by increasing the number of gradient steps on the full replay buffer.\n\nThe final piece of their method is selecting the best performing policy (or “teacher”) of the ensemble based on the recent experience, and update all other policies by executing some gradient steps on the KL divergence between them and the current “teacher”. They also try an experiment where the “teacher” is selected randomly, and it does surprisingly well in my opinion (specially realizing that the “HalfCheetah” experiments seem to not have all seeds run to convergence, please report the full results). I suspect that most of the benefit of their method comes from randomly perturbing the parameters of the policies in the ensemble. More thorough and careful experimentation needs to be carried out to investigate this direction. This is in fact not very surprising given the results of Evolutionary Strategy methods, or Population-based training (even if usually used for hyper-parameters adaptation).\n\nFurthermore, the authors only run the environments for 1M steps, whereas in previous works some environments are shown to get higher return after more training steps. I would also encourage the authors to report the results in all the standard MuJoCo benchmarks for the ablations (even if it’s in the appendix) to better asses their claims.\n\nOverall, this is a very well presented work, although it lacks some novelty and a few more thorough experiments to fully understand the improvements they show. I think this idea is worth sharing with the community, and I recommend a weak accept.", "Thank you for your interest in our work. Population-Based Training of Neural Networks (PBT) is similar to our work at a high level in that it similarly employs multiple agents for training. \n\nWe appreciate you for mentioning a related work. Population-Based Training of Neural Networks (PBT) is similar to our work in an abstract sense. PBT similarly employs multiple agents for training. However, our work differs from PBT in multiple ways.\n\nFirst, the goal of PBT is to optimize the hyperparameters online. However, our work aims to optimize an ensemble of policies given the same hyperparameters. Thus, PBT can be incorporated into CIKD, optimizing the hyperparameters of CIKD.\n\nSecondly, the core idea is different despite the similarity at an abstract level. Resembling evolutionary algorithms, PBT searches for the optimal set of hyperparameters via mutation, selection, and reproduction. Each set of hyperparameters is considered as an individual in the population. PBT iteratively mutates (i.e. randomly perturbs) the existing hyperparameters, then selects a group of top-ranked agents, and finally reproduces the population using the selected agents. Differing from PBT, our work does not require mutation and reproduction. We instead focus on improving the existing agents via distilling the knowledge of the selected best-performing agent. For a more detailed comparison, the reader can refer to Section 2 in our paper.\n", "Very interesting idea, how is this method compared to <Population Based Training of Neural Networks>"], "review_score_variance": 4.222222222222222, "summary": "The paper introduces an ensemble of RL agents that share knowledge amongst themselves. Because there are no theoretical results, the experiments have to carry the paper.  The reviewers had rather different views on the significance of these experiments and whether they are sufficient to convincingly validate the learning framework introduced. Overall, because of the high bar for ICLR acceptance, this paper falls just below the threshold. \n", "paper_id": "iclr_2020_BkeYSlrYwH", "label": "test", "paper_acceptance": "reject"}
{"source_documents": ["We propose several different techniques to improve contrastive divergence training of energy-based models (EBMs). We first show that a gradient term neglected in the popular contrastive divergence formulation is both tractable to estimate and is important to avoid training instabilities in previous models. We further highlight how data augmentation, multi-scale processing, and reservoir sampling can be used to improve model robustness and generation quality. Thirdly, we empirically evaluate stability of model architectures and show improved performance on a host of benchmarks and use cases, such as image generation, OOD detection, and compositional generation.", "This paper proposes several techniques to improve contrastive divergence training of energy-based models (EBMs). \nFirst, the paper proposes to estimate a gradient term, which is neglected in the standard contrastive divergence training method, and show that this correction avoids training instabilities in previous EBM training methods. \nOther techniques include: using data augmentation, defining the energy function as a sum of energies over multi-scales, and using reservoir sampling.\nEffects of each proposed techniques towards training EBMs are evaluated. The performance of the trained EBMs on image generation, OOD detection, and compositional generation are tested.\n\nIn generally, the paper is well written and addresses the important problem of improving EBM training. But I have some concerns.\n\n1. It is not easy for general readers to understand the upper part of Figure 2, which is said to illustrate the overall effects of the losses L_CD and L_KL. What are the meanings of the red balls (dark and light) in the curve?\n\n2. The paper overlooks a class of competitive training methods, which introduce auxiliary generators to train EBMs, including (Kim & Bengio, 2016; Kumar et al., 2019), [a] and so on.\nThe comment in Section 4 (related work), which describes these methods as utilizing pre-trained networks to approximate portions of energy training, is not correct (not capturing the core idea of these methods). Although learning EBMs without auxiliary generators is worthwhile exploring, the paper needs to give the readers an overall picture of the state-of-the-art of learning EBMs and does not give biased comments. \n\nAlthough the proposed method is somewhat new, the results are not strong, which weakens the contribution of this paper. [a] achieved much better results than the proposed method in CIFAR-10 (Table 1). Additionally, computational cost of the proposed method should be given and compared to previous methods.\n[a] Y. Song, Z. Ou. Learning Neural Random Fields with Inclusive Auxiliary Generators. arxiv 1806.00271, 2018.\n\nSNGAN performs much better than the proposed method in CIFAR-10, but much worse in CelebA-HQ and LSUN. This may confuse readers. The \"reimplementation of a SNGAN 128x128 model using the torch mimicry GAN library\" in CelebA-HQ and LSUN may not faithfully reflect the performance of SNGAN.\n\n3. Considering the above comment, the following claim in this paper needs revision.\n\"significantly outperforms past energy based approaches (with approximately the same number of parameters)\"\n\n4. The paper has sporadic writing problems.\n\nTypo in Eq.(3)\n\ndivergenceterm\n\nLSUN bedroom (?)\n\n5. In A.4 (COMPARISON OF CD/KL GRADIENT MAGNITUDES), it is said that \"the gradient of the KL objective is non-negligible\". But it is this non-negligible gradient term that stabilize the EBM training. Need more analysis here.\n\nHow \"Influences and relative magnitude of both loss terms\" are calculated?\n\n--------update after reading the response-----------\n\nThanks for the authors' response, but some non-trivial concerns are still not adequately addressed.\n1) The inconsistent comparison results between SNGAN and the proposed method over CIFAR-10 and LSUN Bedroom datasets.\n2) I can see the benefit such as compositionality from the proposed method of training EBMs. But the paper still seems to overlook the importance of giving the readers an overall picture of the state-of-the-art of learning EBMs. Table 1 should be expanded to include more state-of-the-art results from EBMs, whether using auxiliary generators or not.", "Dear Reviewers,\n\nThank you very much for your thorough and insightful review. We have revised our discussion of related work significantly and have also added clarifications and additional experiments on the effects of the KL loss towards performance. We believe our work shows very strong performance on interesting downstream tasks such as out-of-distribution detection and compositional generation (that we believe are the best that has been reported). \n\nWe spent a large amount of work answering the questions initially requested. We would appreciate it if you could take a look at the revised version and re-evaluate our work.\n\nThanks,\nPaper Authors\n\n", "Thanks for catching that. We unintentionally uploaded an old version of our manuscript. Typos and references should now be fixed.", "Thank you for your comments. We agree the mentioned papers are related and we have added them  in the paper (applications in the Introduction section and more method based papers in the Related Work section)\n\nQ1) The contribution of the paper is quite limited. Even though this paper tried to estimate the missing term in the CD learning, it lacks a comprehensive analysis of the benefit and advantages of doing so. For example, (1) what is the cost to add such a term? (2) Can you validate theoretically such a missing term can be helpful for MCMC mixing as you claimed in the paper?\n\nWe have added analysis to the cost of adding the KL term (training is roughly 1x slower) in section 3.2 (stability/KL loss). The overall cost comes primarily from loading the 1000 nearest neighbors for entropy calculation.  \n\nWe can theoretically see that such a missing term is helpful for MCMC mixing is the objective function is directly the KL divergence between the MCMC distribution and the model distribution, which by definition is minimized when MCMC mixing is the entire model distribution. \n\n\nQ2) About motivation. Even though the motivation of the current paper is clear, which is to improve the CD learning. However, the CD learning (in equation 2) is biased compared with MLE (in equation 1). The original motivation for CD learning is to make EBM learning more efficient. Since currently there has been EBM training method without MCMC or with amortized sampling, I am not sure if the current method is still useful for the community.\n\nWhile there are approaches towards training EBM without either MCMC or amortized sampling, MCMC based training of EBMs enables generation through MCMC. This enables considerable flexibility in generation, such as the ability to compositionally generate images from multiple models (section 3.3), of which we show the best compositional generation results that of an model that we are aware of. Furthermore, this enables EBMs to be robust and performance well at out-of-distribution detection (which we also outperform any other method we are aware of).\n\nQ3) Related Work\nWe have revised both our narrative and missing references. We have changed our narrative to discuss each of the papers you have cited.\n\nQ4) About synthesis quality: the synthesized images generated by the proposed images are not impressive. Artifacts can be obviously observed in Figure 12.\n\nOur generated image quality is the best we have seen for a sampling method based only on implicit generation on the energy landscape and seems to outperform images seen in [1,2,3,4]. We note that out of the presented images (including un-curated ones in Figure 4) ,only the ones in Figure 12 have obvious artifacts.\n\n[1] Implict Generation and Generalization with Energy Based Models. NeurIPS 2019.\n \n[2] Learning generative ConvNets via multigrid modeling and sampling. (CVPR 2018) \n\n[3] Cooperative Learning of Energy-Based Model and Latent Variable Model via MCMC Teaching (AAAI 2018) \n\n[4]  Theory of Generative ConvNet (ICML 2016)\n", "Thanks for the update.   I found some references are corrupted in the introduction part, for examples: \"3D shapes synthesis ()\" and \"video generation ()\".  \n\nAlso, some typos in  the revised related work, e.g.,  \"where a energy function\" => \"where an energy function\". Please double check if the uploaded revised paper is the right version.   ", "Q1) how well is the entropy estimation? We all know that estimating the entropy of data distribution from a high-dimenstional space is very difficult. Does this form of nearest neighbor applicable in other areas? It would also be great if the authors could provide some theoretical analysis here.\n\nFirst, please note that the benefit of our nearest neighbor entropy estimator is not so much in helping generate diverse chains (Langevin dynamics with data augmentation aims to do that), but in calculating L_KL term and preventing training destabilization of energy network if sample chains collapse (i.e. there is still a non-zero gradient on energy network weights pushing the samples away from the buffer of what was previously generated if samples collapse). That said, our entropy estimation term can be directly used in other data distributions in high dimensional space, by doing a similar nearest neighbor query in other high-dimensional spaces. Analysis for the nearest neighbor entropy estimator is given at [1], which we have referenced. To reiterate:\n\n1) Our entropy estimator exhibits a sqrt(n) convergence (where n is the number of nearest neighbor) when modeling distributions with exponential tails. \n2) The estimator is mean square consistent to the empirical entropy estimate\n\nIt is difficult to compare with recent approaches towards estimating entropy based on learned networks as there is limited theoretical analysis on their convergence properties. However, our estimator is unbiased compared to the neural network estimators but is likely less sample efficient (since neural networks can represent points of high likelihood for data) .\n\n[1] Jan Beirlant, E.J. Dudewicz, L. Gyor, E.C. van der Meulen Nonparametric Entropy Estimation: An Overview\n\n \n\nQ2) while the main contribution of the paper seems to be the KL term added into the objective, there are a few other techniques tagging along. It is not clear what role each of these techniques plays in the experiments. I recommend the authors to show an ablation study.\n\n\n\nWe had ablation experiments in the appendix in our original submission and we have now moved them from the appendix to the main paper (Table 2). Overall, we find that the KL term improves the overall generation of samples. Furthermore, we found that only with the addition of KL term could we reliably train a multiscale model as train models with data augmentation for a long time.\n\nQ3) Figure 9 would need to compare against other methods. It is not clear to me how the arithmetic results are stronger than the other published results.\n\nWe note that past works in addition with generative models have been limited to low resolution images[1], while we are the first work to show high resolution compositionality using the generative models. We provide comparisons with [1] in Figure 16. Our generated results are significantly higher resolution. We believe that the high resolution compositional generation is one of the most promising applications of our approach and note that [3] also quantitatively shows that energy based models significantly outperforms [1].  \n\n\nQ4) An important argument in the paper is that the added KL term enhances the mode coverage. Could the authors provide some more evidence on this point?\n\nThe KL term serves as a regularizer to prevent model weights from entering an area in weight space where sampling has poor mode coverage -- not necessarily that mode coverage is better compared to another healthily trained model.  Without the KL term, after a long period of training, sampling always collapses in EBMs [2], which we illustrate in Appendix A.5. The KL term prevents such a possible sampling collapse.\n\n\n[1] Ramakrishna Vedantam, Ian Fischer, Jonathan Huang, Kevin Murphy Generative Models of Visually Grounded Imagination. ICLR 2018 \n[2] Grawthawl et. al. Your Classifier is Secretly an Energy Based Model. ICLR 2020\n[3] Yilun Du, Shuang Li, Igor Mordatch. Compositional Visual Generation with Energy Based Models\n\n", "We thank the reviewer for their thorough and insightful review. We agree the mentioned papers are related to our work and we now discuss the mentioned papers in the related work of the rebuttal version.\n\n\nQ1) The advantage of adding the KL term is not quite obvious given the current experiments. The only experiment that isolates the effect of the KL term is Figure 8 (stability of training), which can be accomplished by simply adding spectral normalization. For all the other improvements, I tend to believe they are due to the techniques of (2)(3)(4).\n\nWe had ablation experiments in the appendix in our original submission and we have now moved them to the main paper (Table 2). We find that the KL loss separately improves generation performance. Furthermore, both techniques (2) and (3) can only be used in conjunction with the KL loss -- otherwise the training process is unstable and neither techniques (2) or (3) contribute to the final results. All results in Figure 5 and 7 are based  on the presence of the KL loss. The addition of spectral normalization does not enable techniques (2) or (3).\n\nQ2) For the first term of L_KL, it is not entirely correct to take gradient only over the last step of Langevin sampling. Need more justifications. For the second term, it requires computing on 1000 samples per update, where the efficiency should be discussed.\n\nWe have added comparisons to test the efficiency of computing the nearest 1000 samples in the revised paper in section 3.2 (stability/KL loss). The overall computational cost of adding the KL loss is roughly the same as generating a negative sample using 60 steps of Langevin (slowing training down by a factor of 1). However the KL loss enables us to use arbitrary architectural components inside the EBM network. We further add a section in appendix A.4 comparing the effect of backpropogating through all steps of Langevin compared to 1. We notice no quantitative impact when backpropogating through all steps of Langevin on MNIST compared to the last step, but note that full backpropogation is significantly more expensive.\n\nQ3) The multi-scale processing of EBMs has been explored in [1], which should be discussed and compared. Besides, [2][3][4] are relevant references of training EBMs that should be discussed.\n\nThank you for this feedback. We agree the mentioned papers are related to our work and we have added and discussed them in the related work section.\n\nQ4) Qualitative speaking, long-run chains in figure 7 still have a trend of degradation from realistic images. Quantitative analysis (e.g., German-Rubin statistics) would be helpful for evaluating the long-run chains clearly.\n\nThank you for this insightful feedback. We have added a plot showing the Inception scores of generated samples over time from an EBM trained with the KL+data augmentation and without using the KL+data augmentation in Figure 7. We find that an EBM trained with  KL+data augmentation exhibits a slower decay in Inception score of generations. An overall decay of Inception score over the number of steps of sampling is to be expected, as we are generating low temperature samples from our model, which thus have lower diversity. However, in comparison with our model without KL/data augmentation, our low temperature samples more consistently capture the underlying shape of objects. We unfortunately could not find a reference for German-Rubin statistics, but we are happy to include them if the reviewer could provide a reference.\n", "We thank the reviewer for their thorough and insightful review. We address major concerns below and have also correspondingly updated the text.\n\nQ1) It is not easy for general readers to understand the upper part of Figure 2, which is said to illustrate the overall effects of the losses L_CD and L_KL. What are the meanings of the red balls (dark and light) in the curve?\n\nWe have redesigned Figure 2 (see updated paper). The intent of the red balls is to show that the L_KL encourages generated samples (dark red balls) to have low energy and high diversity (shown now by blue balls).\n\nQ2) The paper overlooks a class of competitive training methods, which introduce auxiliary generators to train EBMs, including (Kim & Bengio, 2016; Kumar et al., 2019), [a] and so on. The comment in Section 4 (related work), which describes these methods as utilizing pre-trained networks to approximate portions of energy training, is not correct (not capturing the core idea of these methods). Although learning EBMs without auxiliary generators is worthwhile exploring, the paper needs to give the readers an overall picture of the state-of-the-art of learning EBMs and does not give biased comments.\n\nWe have revised our related work with comments about auxiliary generators. We agree the mentioned papers are related to our work and we have added them in the related work section.  \n\nQ3) Although the proposed method is somewhat new, the results are not strong, which weakens the contribution of this paper. [a] achieved much better results than the proposed method in CIFAR-10 (Table 1). Additionally, computational cost of the proposed method should be given and compared to previous methods. [a] Y. Song, Z. Ou. Learning Neural Random Fields with Inclusive Auxiliary Generators. arxiv 1806.00271, 2018.\n\n\nWe believe our approach gets the best generative performance out of approaches that do not use any auxiliary generator networks. Our implicit generation, compared to generation with auxiliary generators has unique benefits such as compositionality, which we show in section 3.3. Regarding performance, we have compared the effect of adding a KL loss in section 3.2. Our approach is roughly two times slower than past approaches based on implicit generation.\n\nHow \"Influences and relative magnitude of both loss terms\" are calculated?\nWe set the overall magnitude of both CD and KL loss terms to be 1 to 1, which the ratio indicated based off the derivation.\n\nWe have revised our statement of “our approach significantly outperforming other past energy based approaches”, to “our approach significantly outperforms other implicit sampling based EBM approaches”. Such an approach has many desirable properties, such as compositionality and out of distribution robustness. We have also fixed typos mentioned. \n", "Review: This paper studies how to improve contrastive divergence (CD) training of energy-based models (EBMs) by revisiting the gradient term neglected in the traditional CD learning. This paper also introduces some useful techniques, such as data augmentation, multi-scale energy design, and reservoir sampling to improve the training of energy-based model. Empirical studies are performed to validate the proposed learning strategy on the task of image generation, OOD detection, and compositional generation.\n\nStrength: \n+ The idea of dealing with the missing term in the traditional CD learning is important and relevant. \n+ The paper is well written. Specifically, the figure illustration and the organization of the paper make me feel quite easy to follow the paper.  \n+ The motivation of the method is clear, and the experimentation looks OK.  \n\nConcerns: \n+ The contribution of the paper is quite limited. Even though this paper tried to estimate the missing term in the CD learning, it lacks a comprehensive analysis of the benefit and advantages of doing so. For example, (1) what is the cost to add such a term? (2) Can you validate theoretically such a missing term can be helpful for MCMC mixing as you claimed in the paper? \n\n+ About motivation. Even though the motivation of the current paper is clear, which is to improve the CD learning. However, the CD learning (in equation 2) is biased compared with MLE (in equation 1). The original motivation for CD learning is to make EBM learning more efficient. Since currently there has been EBM training method without MCMC or with amortized sampling, I am not sure if the current method is still useful for the community.    \n\n+ About synthesis quality: the synthesized images generated by the proposed images are not impressive. Artifacts can be obviously observed in Figure 12.    \n\n+ Missing important references in related works. The current paper missed to cite the pioneering paper about MLE training of ConvNet-EBM [1]. Those EBM papers you have cited from 2019 is based on [1] or its variant. \n\n+ Incomplete narrative of the development of EBMs in the introduction.  Even though the narrative of the development of EBMs is quite comprehensive, it is not complete and even a little bit misleading. For example, since 2016, the EBMs have been applied to realistic image generation (2016-2019)[1, 3, 4, 6, 7], video generation (2017)[2, 4], and 3D generation (2018)[5] in the community of computer vision. Therefore, the current research direction seems not to be novel, given the fact that authors might miss a lot important developments about EBM made by other fields. CD learning is also studied and discussed in [1] for deep EBM. The current papers only discussed and connected EBM development happened recently in ML community (2019, 2020).     \n\n+ typo: in Section 2.1 line 16: KL divergenceterm => KL divergence term\n\nSome references:\n+ [1] A Theory of Generative ConvNet (ICML 2016)\n+ [2] Synthesizing Dynamic Pattern by Spatial-Temporal Generative ConvNet (CVPR 2017)\n+ [3] Cooperative Learning of Energy-Based Model and Latent Variable Model via MCMC Teaching (AAAI 2018)\n+ [4] Cooperative learning of descriptor and generator networks. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI 2018).\n+ [5] Learning Descriptor Networks for 3D Shape Synthesis and Analysis. (CVPR 2018) \n+ [6] Learning generative ConvNets via multigrid modeling and sampling. (CVPR 2018)  \n+ [7] Divergence triangle for joint training of generator model, energy-based model, and inference model. (CVPR 2019)   \n", "The paper proposes a series of new techniques to enhance the training of an energy-based model, and the proposed techniques include: adding the often neglected KL term to the training scope/data augmentation + multi-scale energy function/an experience replay buffer for training.\n\nThe experiments demonstrate the proposed method could generate high-quality images, compositional tasks and perform out-of-distribution detection.\n\nThe main idea and motivation are well and clearly conveyed by the writing.\n\nThe paper would be stronger if the authors could provide the following pieces:\n- how well is the entropy estimation? We all know that estimating the entropy of data distribution from a high-dimenstional space is very difficult. Does this form of nearest neighbor applicable in other areas? It would also be great if the authors could provide some theoretical analysis here.\n- while the main contribution of the paper seems to be the KL term added into the objective, there are a few other tenichques tagging along. It is not clear what role each of these techniques plays in the experiments. I recommend the authors to show an ablation study.\n- Figure 9 would need to compare against other methods. It is not clear to me how the arithmetic results are stronger than the other published results.\n- An important argument in the paper is that the added KL term enhances the mode coverage. Could the authors provide some more evidence on this point?\n\n", "This paper proposed an improved version of contrastive divergence learning of energy-based models by combining a bag of techniques: (1) add back a KL term that is neglected by previous methods (2) data augmentation (3) multi-scale processing (4) reservoir sampling. Experiments demonstrate the effectiveness of the improvements. \n\nPro:\nThe paper is well-written and easy to follow. Various experiments are performed to demonstrate the efficacy of the improved method. \n\nCons:\n1. The advantage of adding the KL term is not quite obvious given the current experiments. The only experiment that isolates the effect of the KL term is Figure 8 (stability of training), which can be accomplished by simply adding spectral normalization. For all the other improvements, I tend to believe they are due to the techniques of (2)(3)(4).\n\n2. For the first term of L_KL, it is not entirely correct to take gradient only over the last step of Langevin sampling. Need more justifications. For the second term, it requires computing on 1000 samples per update, where the efficiency should be discussed. \n\n3. The multi-scale processing of EBMs has been explored in [1], which should be discussed and compared. Besides, [2][3][4] are relevant references of training EBMs that should be discussed. \n\n4. Qualitative speaking, long-run chains in figure 7 still have a trend of degradation from realistic images. Quantitative analysis (e.g., German-Rubin statistics) would be helpful for evaluating the long-run chains clearly. \n\nOverall, the paper proposes effective improvements on contrastive divergence of EBMs, and performs various experiments to demonstrate the efficacy. However, I am concerned about the correctness and the necessity of adding the gradient term (L_KL), which is one of the major contributions that the authors claim. Please address my concern as listed above. \n\n[1] Learning Energy-Based Models as Generative ConvNets via Multi-grid Modeling and Sampling, Gao et al.\n[2] A Theory of Generative ConvNet, Xie et al. \n[3] Flow Contrastive Estimation of Energy-Based Models, Gao et al. \n[4] Learning the Stein Discrepancy for Training and Evaluating Energy-Based Models without Sampling, Grathwohl et al. "], "review_score_variance": 0.1875, "summary": "This paper introduces a bag of techniques to improve contrastive divergence training of energy-based models (EBMs), particularly a KL divergence term, data augmentation, multi-scale energy functions, and reservoir sampling. The overall paper is well written and clearly presented. \n\nIn response to the major concerns from reviewers, the AC recognizes the authors' effort in expanding related work and adding ablation on the effects of the KL loss. However, reviewers remain unconvinced by the significance of the current results. In particular, the quality improvement by adding the KL terms is subtle compared to using reservoir sampling (as evidenced in the contrast of the last two rows in Table 2). Moreover, the authors are also encouraged to compare additionally with recent development in EBM, as pointed out by R2 & R4.\n\nThe AC does find the results on downstream tasks such as out-of-distribution quite promising and interesting. Perhaps it's worth expanding the discussion with formal reasoning on why KL loss helps in this case. \n\nAll four knowledgeable reviewers are leaning towards rejection, the AC respects and agrees with the decision. \n", "paper_id": "iclr_2021_daLIpc7vQ2q", "label": "train", "paper_acceptance": "withdrawn-rejected-submissions"}
{"source_documents": ["Graph Neural Networks (GNNs) have achieved great success in various tasks, but their performance highly relies on a large number of labeled nodes, which typically requires considerable human effort. GNN-based Active Learning (AL) methods are proposed to improve the labeling efficiency by selecting the most valuable nodes to label. Existing methods assume an oracle can correctly categorize all the selected nodes and thus just focus on the node selection. However, such an exact labeling task is costly, especially when the categorization is out of the domain of individual expert (oracle). The paper goes further, presenting a soft-label approach to AL on GNNs. Our key innovations are: i) relaxed queries where a domain expert (oracle) only judges the correctness of the predicted labels (a binary question) rather than identifying the exact class (a multi-class question), and ii) new criteria of maximizing information gain propagation for active learner with relaxed queries and soft labels. Empirical studies on public datasets demonstrate that our method significantly outperforms the state-of-the-art GNN-based AL methods in terms of both accuracy and labeling cost. ", " We thank the reviewer for the continuous comments.\n\n### 1. Technical errors\nWe have corrected the notation errors pointed out by the reviewer in Equation (4) and Equation (8) as follows: \nEquation(4)\n\n\\begin{equation}\n\\small\n    \\begin{aligned}\n    a_t\\^{\\prime}=\n    \\begin{cases}\n    \\mathbb{1}(t = l) \\& v_i\\~\\text{belongs to class $l$}\\\\\\\\\n    \\frac{a_t\\cdot \\mathbb{1}(t = l)}{\\sum_{j \\neq l}a_j} \\& v_i~\\text{does not belong to class $l$}\n    \\end{cases} \n    \\end{aligned}\n\\end{equation}\n\nEquation(8)\n\n\\begin{equation}\n\\begin{aligned}\n    IGP(v_j,v_i,k)= P(v_i+)IGP(v_j,v_i,k,v_i+) + P(v_i-)IGP(v_j,v_i,k,v_i-)\n\\end{aligned}\n\\end{equation}\n\nFor Eq.(7), we compute the marginal gain of entropy reduction on a specific unlabeled node $v_j$ if we select a node $v_i$ to query from the oracle. In particular, for a specific node $v_j$, each labeled node $v_m$ $\\in V_l$ would have a different label-smoothing effect on node $v_j$, captured by the influence magnitude function $I_f(v_j,v_m, k)$ in Eq.(6). So the aggregated smoothed label for node $v_j$ is given by $\\sum_{v_m\\in \\mathcal{V}_l}I_f(v_j,v_m,k)\\hat{\\boldsymbol{y}}_m^{\\prime}$. Notice here $\\hat{\\boldsymbol{y}}_m^{\\prime}$  is the normalized soft label defined in Eq.(4), which is a probability distribution over the possible classes. Therefore, the entropy in Eq.(7) actually computes the entropy of aggregated smoothed label for node $v_j$. The probabilities here indicate which class the node $v_j$ belongs to based on the label-smoothing effect in GNNs. \n\n### 2. Relaxed queries\nWe clarify here that we are not arguing that active learning becomes more efficient with relaxed queries than those using a perfect oracle. Instead, we argue that it is too strong in traditional AL to assume that oracles may always behave perfectly, requiring all oracles to label instances that may be out of their domain knowledge.\n\nTherefore, this paper handles a different yet more challenging AL setting where the assumption of a perfect oracle might not be met. We allow an oracle to admit that he/she is incapable of labeling some query instances and simply answer “this data does not belong to my familiar domain (label)”. The effect of such relaxation on traditional AL algorithms is that they cannot leverage queries that fail to obtain a hard label from the non-perfect oracle. By contrast, our method could still leverage these queries based on the soft label. In particular, we exclude the label disagreed by the oracle from the model prediction and generate a soft-max (soft label) taken over the remaining classes to provide supervision with our defined objective function in Equation (10). Therefore, compared to traditional AL algorithms, we improve the utilization of relaxed queries under non-perfect Oracle.\n\n\n\n### 3. Sample complexity.\nThank you for bringing sample complexity to our attention. With a non-perfect oracle, our method could reduce sample complexity compared with other AL strategies because we improve the utilization of queries and samples. As we explained earlier, traditional AL algorithms have to discard the data samples of queries that have not obtained hard labels from non-perfect Oracle. By contrast, we could still leverage these data samples in a soft-label manner, thus reducing the error quickly than other AL algorithms.\n\nAlthough some AL algorithms enjoy theoretical guarantees on the traditional linear machine learning models (e.g., linear regression), analyzing the sample complexity of AL for deep learning is still an open research challenge due to the non-linear nature of neural networks. Most sample complexity is analyzed under the passive learning for neural networks in the literature. To the best of our knowledge, only another submission [1] in ICLR 2022 studies the sample complexity problem for training neural networks, but the analysis is limited to the simplest one-hidden layer neural networks. In addition to the non-linear nature of neural networks, the graph neural networks (GNNs) are even more challenging as graph data is no longer independent and identically distributed (iid). We have added the above discussion in Appendix.K in the revised manuscript.\n\n[1] Zhao Song, Baocheng Sun, Danyang Zhuo.  [*Sample Complexity of Deep Active Learning.*](https://openreview.net/forum?id=PU3VGS93gxD) Under review at ICLR 2022.", " Thanks for the your continuous comments. To clarify how much accuracy would drop when we filter the small-degree nodes, we increase the filtering degree from 0 to 15 and then report the corresponding mean test accuracy of 10 runs with GCN in Table 7. Note that the threshold we used in the previous experiments is **15 in Reddit**.\n\n\n**Table 7: The test accuracy (%) with different filtering degrees on the Reddit dataset.**\n\n| Filtering Degree|0|5|10|15|\n|:-:|:-:|:-:|:-:|:-:|\n|Test Accuracy (%)|93.6(±0.3)| 93.5(±0.3)| 93.4(±0.2)| 93.4(±0.2)|\n\nAs shown in Table 7, the test accuracy only drops slightly by 0.2\\% when we increase the threshold from 0 to 15, meaning that our efficiency optimization will not introduce a large negative influence on the prediction ability of the GCN model.\n\nWe have added the above analysis in Appendix. L in the revised manuscript.\n\nWe are very glad to respond if you have any new questions.\n\nRespectfully,\n\nPaper607 Authors", " Thank you for adding new comparisons with the recently proposed method (SEAL). The results are more convincing and encouraging. ", " Thanks for adding running time results for AL queries. IGP appears to be efficient on sparse datasets where the number of edges has a similar magnitude of the number of nodes. When you filter out small-degree nodes on Reddit, could you please clarify how much accuracy would drop as a result?", " ### 2. Comparison to SEAL\n\nThanks for pointing out this related work. We have added the comparison with SEAL, with the result shown in Table 1 and Table 2. SEAL devises a novel AL query strategy for node classification in an adversarial way: the divergence score generated by the discriminator serves as the informativeness measure to select the most informative node to be labeled by an oracle. From the Table 2, the experimental results show that SEAL could outperform AL algorithms using the general uncertainty measure, such as AGE, ANRMAB, and GPA.\n\nHowever, SEAL still falls into the routine of perfect Oracle that is capable of providing hard label for each query node. By contrast, IGP is a new AL query strategy with relaxed query (e.g., Oracle with knowledge blind spot), its novelty lies in explicitly exploiting the (1) soft labels for extra supervision, and (2) influence propagation over graph structure to enhance semi-supervision. As a result, we find that IGP outperforms SEAL.\n\n**Tabel 1: The test accuracy (\\%) on different datasets with the same labeling budget.**\n\n| Methods |      Cora       |     Citeseer      |      PubMed       |     Reddit      |     ogbn-arxiv      |\n|:-------:|:--------------:|:--------------:|:--------------:|:--------------:|:--------------:|\nRandom|78.8(±0.8)|70.8(±0.9)|78.9(±0.6)|91.1(±0.5)|68.2(±0.4)|\nAGE|82.5(±0.6)|71.4(±0.6)|79.4(±0.4)|91.6(±0.3)|68.9(±0.3)|\nANRMAB|82.4(±0.5)|70.6(±0.6)|78.2(±0.3)|91.5(±0.3)|68.7(±0.2)|\n| GPA     | 82.8(±0.4)    | 71.6(±0.4)     | 79.9(±0.5)     | 91.8(±0.2)     | 69.2(±0.3)     |\n| SEAL    | 83.2(±0.5)    | 72.1(±0.4)     | 80.3(±0.4)     | 92.1(±0.3)     | 69.5(±0.1)     |\n| ALG     | 83.6(±0.6)    | 73.6(±0.5)     | 80.9(±0.3)     | 92.4(±0.3)     | 70.1(±0.2)     |\n| GRAIN   | 84.2(±0.3)    | 74.2(±0.3)     | 81.8(±0.2)     | 92.5(±0.1)     | 70.3(±0.2)     |\n| **IGP** | **86.4±0.6)** | **75.8(±0.3)** | **83.6(±0.5)** | **93.4(±0.2)** | **70.9(±0.3)** |\n\n**Table 2: Test accuracy of different models on PubMed**\n\n| Methods |      SGC       |     APPNP      |      GCN       |     MVGRL      |\n|:-------:|:--------------:|:--------------:|:--------------:|:--------------:|\n| Random  |   77.6(±0.8)   |   79.2(±0.6)   |   78.9(±0.6)   |   79.3(±0.5)   |\n|   AGE   |   78.8(±0.5)   |   79.9(±0.5)   |   79.4(±0.4)   |   79.9(±0.4)   |\n| ANRMAB  |   77.8(±0.4)   |   78.7(±0.5)   |   78.2(±0.3)   |   78.9(±0.3)   |\n|   GPA   |   79.5(±0.6)   |   80.2(±0.4)   |   79.9(±0.5)   |   80.4(±0.3)   |\n|  SEAL   |   79.8(±0.5)   |   80.5(±0.5)   |   80.3(±0.4)   |   80.7(±0.3)   |\n|   ALG   |   80.5(±0.4)   |   81.2(±0.5)   |   80.9(±0.3)   |   81.3(±0.2)   |\n|  GRAIN  |   81.1(±0.3)   |   82.0(±0.4)   |   81.8(±0.2)   |   82.1(±0.1)   |\n| **IGP** | **83.2(±0.6)** | **83.7(±0.5)** | **83.6(±0.5)** | **83.9(±0.4)** |\n\n\n### 3. Related work\nThanks for your helpful suggestion. we combine Section 2.3 and Section 4.4 into one related work section. \n\n### 4. Typo\nThanks for pointing us this typo. We have corrected it in the revision.", " We have updated the standard deviation in all the tables (Table 1,2,3,5,6). Specifically, we show Table 1 as follows and others in the revised manuscript.\n\n**Table 1: The test accuracy (%) on different datasets with the same labeling budget.**\n\n| Methods |      Cora       |     Citeseer      |      PubMed       |     Reddit      |     ogbn-arxiv      |\n|:-------:|:--------------:|:--------------:|:--------------:|:--------------:|:--------------:|\nRandom|78.8(±0.8)|70.8(±0.9)|78.9(±0.6)|91.1(±0.5)|68.2(±0.4)|\nAGE|82.5(±0.6)|71.4(±0.6)|79.4(±0.4)|91.6(±0.3)|68.9(±0.3)|\nANRMAB|82.4(±0.5)|70.6(±0.6)|78.2(±0.3)|91.5(±0.3)|68.7(±0.2)|\n| GPA     | 82.8(±0.4)    | 71.6(±0.4)     | 79.9(±0.5)     | 91.8(±0.2)     | 69.2(±0.3)     |\n| SEAL    | 83.2(±0.5)    | 72.1(±0.4)     | 80.3(±0.4)     | 92.1(±0.3)     | 69.5(±0.1)     |\n| ALG     | 83.6(±0.6)    | 73.6(±0.5)     | 80.9(±0.3)     | 92.4(±0.3)     | 70.1(±0.2)     |\n| GRAIN   | 84.2(±0.3)    | 74.2(±0.3)     | 81.8(±0.2)     | 92.5(±0.1)     | 70.3(±0.2)     |\n| **IGP** | **86.4±0.6)** | **75.8(±0.3)** | **83.6(±0.5)** | **93.4(±0.2)** | **70.9(±0.3)** |", " Thanks for your detailed response. Your feedback addresses my concerns.", " Thank you for the running time results which are encouraging, and the parameter sensitivity experiment as well. As for the accuracy difference, 2.2% is convincing if the standard deviation is showing it is a significant difference. Since you didnt report that so far it is hard to judge.", " Sample complexity: In my review I was referring to a missing classic sample complexity analysis for active learning. Namely, how many samples are needed to be queried by IGP to reduce the error by a \\delta. The authors instead provide in their response a running time complexity, which was also missing in their paper but is not related to sample complexity.\n\nTechnical errors: you still didnt clarify regarding my item 6 \n\nRelaxed queries: does active learning become more efficient with relaxed queries, i.e., does the number of queries lower when compared with using a perfect oracle. what is really the effect of relaxing the problem? none of these is thoroughly discussed.", " ### 6. Experimental validation\nWe add the standard deviation for Table 1, Table 2, and Table 3. It is worth to point our method improves SOTA by up to 2.2%, which is a significant accuracy improvement. \n\n### 7. parameter tuning & running time\nWe add the experiment on the runtime of different methods for each batch selection in Table 5, showing that our method achieve highest accuracy and good efficiency. Here we measure the cost by the relative selection time to that of GRAIN approach.\n\n**Table 5: Performance along with running time on the Reddit dataset.**\n\n|        Methods        |  ANRMAB  |   GPA    |   AGE    |   SEAL   |   ALG    |   IGP    |    IG    |  GRAIN   |\n|:---------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n| Relative Running Time |  340.00  |  331.25  |   8.25   |   4.00   |   1.63   |   1.38   |   1.25   |   1.00   |\n|   Test Accuracy(\\%)    | 91.5±(0.3) | 91.8±(0.2) | 91.6±(0.3) | 92.1±(0.3) | 92.4±(0.3) | **93.4±(0.2)** | 91.7±(0.3) | 92.5±(0.1) |\n\n\nWe also add the discussion of parameter tuning. Our proposed IGP has only one hyperparameter $\\alpha$, which controls the importance of a weak label. To investigate its impact on IGP, we set $\\alpha$ to different values and then report the corresponding test accuracy with different base GNN models on different datasets. The experimental results in Table 6 show that removing the weak label will lead to large performance degradation in different models, which verifies the effectiveness of our proposed weak label. Besides, our method IGP is robust to the choice of $\\alpha$ since its performance is stable on different models when we increase $\\alpha$ from 0.5 to 2.\n\n**Tabel 6: The test accuracy (\\%) on different hyperparameters on the PubMed dataset.**\n\n| parameters | $\\alpha$=0 | $\\alpha$=0.5 | $\\alpha$=1 | $\\alpha$=2 |\n|:----------:|:----------:|:------------:|:----------:|:----------:|\n|    SGC     | 79.6(±0.7) |  82.8(±0.6)  | 83.2(±0.6) | 82.9(±0.5) |\n|   APPNP    | 80.4(±0.6) |  83.4(±0.4)  | 83.7(±0.5) | 83.6(±0.5) |\n|    GCN     | 80.7(±0.4) |  83.5(±0.5)  | 83.6(±0.5) | 83.4(±0.6) |\n|   MVGRL    | 80.5(±0.6) |  83.2(±0.5)  | 83.9(±0.4) | 83.5(±0.4) |", " Thank you for your review and valuable feedback.\n\n### 1. Motivation: advantages of IGP\n\n**(1)Relaxed query**\nThe reviewer raises a good question of “why relaxed queries are important and where could this make a difference” in real life. Our motivation here is that it is too strong to require oracles to label instances that may be out of their domain knowledge. The importance of our new form of queries lies in the relaxation of the traditional assumption, extending AL to the applications where oracles are not perfect and have knowledge blind spot. The difference we made here is that we allow an oracle to admit that he is incapable of labeling some query instances and simply answer “this data does not belong to the domain (label) I am familiar with.”\n\n**(2)Query criteria**\nAnother important contribution of this paper is the new query criteria itself under the relaxed query. Its advantages relative to prior works lie in the following two points. First, we propose the new IG criteria that further exploits soft labels in relaxed queries and their supervision in training. Second, we extend IG to IGP that explicitly employs the influence propagation of GNNs to achieve much higher performance. This is the reason why our method is a good tool to use for AL on GNNs.\n\n**(3)Discussion around state of the art**\nSection 2.3 and Section 4.4 discuss the related AL methods. The key difference here is that existing SOTA methods still fall into the routine of perfect Oracle that is capable of providing a hard label for each query. By contrast, IGP proposes a new AL research problem and strategy under relaxed query (e.g., Oracle with knowledge blind spot). \n\n\n\n### 2. Paper organization\nThe reviewer is right that both Sec 2.3 and Sec. 4.4 are related work, which focuses on traditional AL methods for general models and GNN models, respectively. As suggested by the other reviewer, we combine them together.\n\n### 3. Notation \nWe thank the reviewer for pointing out the possible ambiguity of using the same indicator notation $i$ in Equation (4), we replace $i$ with another indicator notation $t$ to avoid such ambiguity. Besides, the last component in Equation (8) is $v_i-$.In Equation (7), $I_f(v_j,v_m,k)$ can be computed by Equation (6) and $\\hat{\\boldsymbol{y}}$ is the model predicted probabilities produced by the GNN models.\n\n### 4. Theoretical foundations\n**(1)complexity analysis** \nThank you for bringing this to our attention. We add the complexity analysis of IGP in Appendix. E. Given a $K$-layer GNN, the propagation matrix ($N\\times N$, with $M$ nonzero values) and label matrix ($N \\times c$) should be multiplicated $K$ times in IGP. Therefore, in the stage of node selection (line 3-5 in Algorithm 1), selecting the node that maximizes the propagation of information gain $F(\\mathcal{V}_l \\cup \\{v\\})$ from unlabeled nodes requires $\\mathcal{O}(KMc)$, where $K$ is the number of layers of GNNs, $M$ is the number of edges, and $c$ is the number of classes. In all, the cost of selecting a batch of b nodes is $\\mathcal{O}(bKMc)$. \n\n\n\n**(2)The importance of theorem**\nWe would like to highlight the importance of the theorem, which points out that the general uncertain measure (entropy) under traditional AL query is not optimal under our new AL setting with relaxed query. This provides a theoretical interpretation for the requirement of new IG criteria.\n\n### 5. Clarity\nWe clarify the sentence as the influence of a node on its different neighbors can be various. For example, compared with the distant neighbors, a labeled node is more likely to have larger influence on its adjacent neighborhood nodes.”", " Thanks for your positive review of our submission and valuable feedback.\n### 1. Class-imbalance Problem\nThank you for bringing this to our attention. We agree with the reviewer that the class-imbalance problem has not been considered here since it is not the main focus of this paper. However, this is a promising direction to move our approach forward through explicitly introducing class-balancing constraints in the query process to reduce the imbalance of the labeled subset. For example, as a means of balancing, priority is given to nodes whose predicted classes are underrepresented among those already labeled. We will add the discussion in the paper.\n\n### 2. Over-smoothness problem\nOur method focuses on the query process to provide labels for GNNs, which are orthogonal to and compatible with the over-smoothing combatting technique. For example, as insightfully pointed out by the reviewer, our method is applicable to many GNN variants (as shown in Table). Thus, one could select those that prevent over-smoothing, such as the APPNP model.\n\n### 3. Paper Relation\nThanks for pointing out to us the related Two-Dimensional Active Learning (2DAL) work. 2DAL is a two-dimensional active learning strategy, which selects the most “informative” sample-label pairs to reduce the uncertainty along the dimensionalities of both sample and label. The key difference here is that 2DAL leverages the label correlation under a multi-label setting. By contrast, we leverage the labels correlation of the connected samples given graph structure, which is still along the sample dimension under a single-label setting. We have added the discussion of 2DAL in the paper, which inspires us further to adapt AL to graphs under a multi-label setting.", " Thanks for your insightful feedback. We appreciate your assessment about this paper being \"well motivated for AL and it seems to be applicable in real-life scenarios\". The answers to your concerns are as follows.\n\n### 1. Computational Complexity Analysis\n\nThank you for bringing this to our attention. We add the complexity analysis of IGP in Appendix. E. \nGiven a $K$-layer GNN, the propagation matrix ($N\\times N$, with $M$ nonzero values) and label matrix ($N \\times c$) should be multiplicated $K$ times in IGP.\nTherefore, in the stage of node selection (line 3-5 in Algorithm 1), selecting the node that maximizes the propagation of information gain $F(\\mathcal{V}_l \\cup \\{v\\})$ from unlabeled nodes requires $\\mathcal{O}(KMc)$, where $K$ is the number of layers of GNNs, $M$ is the number of edges and $c$ is the number of classes. \nFor efficiency optimization, we only take $n$ nodes with degree larger than a threshold as candidates, and get the corresponding propagation matrix ($n\\times N$, with $m$ nonzero values), we can reduce the overall time complexity to $\\mathcal{O}(bKmc)$. \nTo effectively mitigate the cost, we could use the degree to identify and dismiss uninfluential nodes. Table 5 shows the cost of IGP selection for each batch, where we filter out the nodes with degrees smaller than 15 in Reddit. Here we measure the cost by the relative selection time to that of GRAIN approach. To measure the extra cost introduced by the propagation, we also add a baseline that only use IG without propagation. From the table we see that our method IGP can get comparable efficiency as GRAIN, while achieving the accuracy improvement of 0.9\\%. Compared to IG, introducing propagation IGP can improve 1.7\\% accuracy while only increasing the cost slightly. \n\n**Table 5: Performance along with running time on the Reddit dataset.**\n\n|        Methods        |  ANRMAB  |   GPA    |   AGE    |   SEAL   |   ALG    |   IGP    |    IG    |  GRAIN   |\n|:---------------------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n| Relative Running Time |  340.00  |  331.25  |   8.25   |   4.00   |   1.63   |   1.38   |   1.25   |   1.00   |\n|   Test Accuracy(\\%)    | 91.5±(0.3) | 91.8±(0.2) | 91.6±(0.3) | 92.1±(0.3) | 92.4±(0.3) | **93.4±(0.2)** | 91.7±(0.3) | 92.5±(0.1) |", " We appreciate your assessment about this paper being \"a novel adaptation of AL to graphs \". Thanks for your constructive feedback! We believe that addressing this feedback will make our paper stronger.\n\n### 1. Motivation & Example\n\nThank you for bringing this to our attention. We clarify that we are not arguing that a node may belong to more than one category. Instead, we argue that it is too strong in traditional AL to assume that an oracle is capable of providing labeling information for each query instance. In our example, the task in ogbn-papers100M is to leverage the citation graph to infer the labels of the arXiv papers into 172 arXiv subject areas (a single-label, 172-class classification problem). In this example, a specialist/expert in the subject areas of machine learning is incapable of labeling query instances with subject areas of finance (such as mathematical finance or computational finance), which is out of his domain knowledge. \n \nTo relax the traditional assumption, we define a new AL query strategy for Oracle with knowledge blind spot (KBS), which allows an oracle to admit that he/she is incapable of labeling some query instances and simply answer “this data does not belong to my familiar domain (label)”. Under this new strategy, we define new IG criteria to obtain the largest uncertain reduction even when the unlabeled instance belongs to the oracle’s KBS. The key insight here is that each query process is useful from the perspective of information gain, and answering “this data does not belong to the certain class” also brings entropy reduction, hence supervision. Inspired by this insight, we propose the new IG criteria considering that the oracle will \"agree/disagree\" with the pseudo label. If the Oracle disagrees with the model, the label is discarded, and a soft-max (soft label) is taken over the remaining classes. The updated soft label still provides weak supervision with our defined objective function in Equation (10).\n \nBy contrast, in the traditional AL approach requiring a hard label, if the oracle doesn't agree with the model, the label has to be discarded. This AL query will not provide any label information, hence a waste of labeling cost. In fact, the occurrences of such disagreement queries can be common in that: (1) the model is inaccurate in the initial AL process due to low label rate, (2) and traditional AL selects most uncertainty node to label. Both make the model cannot accurately assess the likelihood of selected unlabeled instances belonging to the oracle’s KBS.\n \nIn summary, the effectiveness of our design stems from the consideration of soft labels in relaxed queries and model training.\nWe also appreciate your assessment about “The idea of soft-labeling is intriguing!”\n\n### 2. Influence of the number of classes\n\nThe reviewer raises a very good point. Based on the reviewer’s comment, we examine the model accuracies as a function of the number of classes on the ogbn-arxiv dataset. Specifically, we gradually increase the number of classes and the corresponding number of nodes in the ogbn-arxiv dataset, and report the mean test accuracy of 10 runs with and without the soft-label. As shown in the newly added Figure 7(a) in Appendix.J, although more classes increase the complexity of classification problem (thus a decreasing model accuracy), we see that the performance gain of adding soft label decreases slightly as the number of classes increases. \n\nThe reason is that although the soft-max is taken over the remaining classes, the distribution of the output of soft-max (i.e., soft label) is expected to be highly skewed, which can help to alleviate the effect of increased class number on the quality of the soft label. To demonstrate this, we add a new Figure 7(b) showing the percentage of the node whose top-K accuracy is larger than 80\\%. As the percentages of top K class increases (i.e., K/total class number), more than 90\\% nodes only need the predicted top 20\\% classes to guarantee the top-K accuracy larger than 80\\%, which means few classes predominate the softmax distribution.", "The paper proposes an active learning method for GNNs that is based on an information gain maximization where ethe information gain is obtained by querying a data point and looking at the influence of the queried node on the neighborhood relative to their previous information.  They also claim the setting of relaxing the oracle answer to be a binary confirmation of the most probable label. Experiments are presented where some advantage is shown for the method. Strong points:\n1.\tThe paper provides a relative new framework for GNNs where the information gain propagation is maximized. \n2.\tThe setting of relaxed queries seems to be facilitative in this case.\n3.\tResults show some advantage over other methods\nWeak points:\n1.\tMotivation: except for the motivation for the relaxed queries, I do not see much motivation and discussion in the paper, especially a discussion around the state of the art in this area and how it related to the proposed method. Based on section 4.4 is the only advantage of IGP over others is the use of relaxed queries? What about the query criteria itself? What are its advantages relative to prior works.\nThere are many other mssign motivation throught the paper for example after the IG definition why is this a good a good tool to use?\nWhy relaxed queries are important, where in real life could this make a difference?\n2.\tPaper organization: is section 2.3 meant to be related work? How is it related to section 4.4 on prior work?\n3.\tTechnical errors:\na.\tequation (8) isn’t that supposed to be v_i – on the last component?\nb.\tEquation (4) the indicator is on I – an index of a node equal l – a class? That is incorrect use of the two different symbols in the indicator function\nc.\tWith respect to what is the entropy in eq (7) computed? What are the probabilities?\n4.\tTheoretical foundations: there is no sample complexity analysis or any optimality analysis for the given criterion. The only theorem provided is a rather trivial one showing that entropy doesn’t coincide with IG selection criterion.\n5.\tClarity:\na.\tUnclear sentences like: “Since the influence magnitude is diverse to the influenced nodes…” what does it mean?\n6.\tExperimental validation: I am left unimpressed by the rate of improvement shown by the proposed algorithm. Moreover, the analysis lacks error bars and standard deviation for the 10 executions.\n Strong points:\n1.\tThe paper provides a relative new framework for GNNs where the information gain propagation is maximized. \n2.\tThe setting of relaxed queries seems to be facilitative in this case.\n3.\tResults show some advantage over other methods\nWeak points:\n1.\tMotivation: except for the motivation for the relaxed queries, I do not see much motivation and discussion in the paper, especially a discussion around the state of the art in this area and how it related to the proposed method. Based on section 4.4 is the only advantage of IGP over others is the use of relaxed queries? What about the query criteria itself? What are its advantages relative to prior works.\nThere is lacking motivation throughoutt the paper for example after the IG definition why is this a good a good tool to use?\nWhy relaxed queries are important, where in real life could this make a difference?\n2.\tPaper organization: is section 2.3 meant to be related work? How is it related to section 4.4 on prior work?\n3.\tTechnical errors:\na.\tequation (8) isn’t that supposed to be v_i – on the last component?\nb.\tEquation (4) the indicator is on I – an index of a node equal l – a class? That is incorrect use of the two different symbols in the indicator function\nc.\tWith respect to what is the entropy in eq (7) computed? What are the probabilities?\n4.\tTheoretical foundations: there is no sample complexity analysis or any optimality analysis for the given criterion. The only theorem provided is a rather trivial one showing that entropy doesn’t coincide with IG selection criterion.\n5.\tClarity:\na.\tUnclear sentences like: “Since the influence magnitude is diverse to the influenced nodes…” what does it mean?\n6.\tExperimental validation: I am left unimpressed by the rate of improvement shown by the proposed algorithm. Moreover, the analysis lacks error bars and standard deviation for the 10 executions.\n7.\tThere is not discussion of parameter tuning nor running time of the proposed method.\n", "The paper proposes a new method active learning (AL) on graphs. Unlike other AL approaches, the proposed approach provides soft labels via *relaxed queries* to the *domain experts*. \n\n\n\nMain Contributions: \n\n1). The paper proposes a new innovative approach for graph active learning with soft labels. The key idea is to ask a human regarding whether the model prediction is correct or not (a binary classification task) as opposed to asking them the correct \"hard label\" of the node. The incorrect model predictions are also not \"thrown away\" and used as indirect supervision by performing a soft-max over the remaining classes. This leads to a new criteria for active learning, called \"maximizing information gain propagation\" as opposed to maximizing entropy as done by standard AL.\n\n\n2). Results are shown on a variety of real-world datasets which show the superior performance of the proposed method in achieving higher test accuracy with a certain labeling budget. Originality & Quality: The paper is well written and puts itself nicely in context of previous work. The overall presentation of the paper is good except a couple typos. The proposed approach is a novel adaptation of AL to graphs (to the best of my knowledge).\n\n\n\n1). The idea of soft-labeling is intriguing! Though, the example that the paper provides in Section 1 regarding arXiv subject categories is misleading. The paper argues that a node may belong to more than one categories, e.g., mathematical finance and computational finance and that is hard for a human labeler to classify. However, it is unclear how the proposed approach solves that problem since the approach still relies on a model to make the label prediction, e.g., mathematical finance in the case above and then the human accepts it. So, we still have the same problem that the example was provided to get rid of. This is so because the example of arXiv classification is more of a multi-label classification problem and hence not a good example to motivate this paper in my opinion. \n\n\n\n2). It will be interesting to see the model accuracies as a function of the number of classes. If the humans doesn't agree with the model, the label is discarded and a soft-max is taken over the remaining classes. So it will be interesting to see how this weak labeling by the remaining classes performs when the number of classes is large and hence the prediction problem is hard. \n\n\n\nSignificance: The paper addresses an important problem of active learning on graphs using soft-labels. The paper provides an intriguing idea of soft-labeling to improve active learning (AL) on graphs. It seems more efficient than the conventional AL approaches on graphs and is more cost efficient. The results are mostly strong, except it is unclear what is the impact of the number of classes on the output performance.", "This paper proposes a new GNN-based active learning (AL) method for graph data, under a relaxed query setting where the oracle can only judge the correctness of the predicted labels. A new AL query criterion is proposed to select the nodes that can maximise the information gain propagation (IGP) in local neighbourhood. \n Strengths:\n1. The relaxed query setting is well motivated for AL and it seems to be applicable in real-life scenarios.\n2. The proposed AL criterion takes both information gain and influence magnitude into consideration, which can explicitly maximise the propagation of information gain. \n3. The source code of the paper is released to ensure the reproductivity of results.\n4. Overall, this paper is well-written, though there are some typos in equations and text.\n \nWeakness:\n1. No complexity analysis on the AL query criterion of IGP. As IGP needs to maximise the propagation of information gain in local neighbourhood, it is expected to incur extra computational overhead. \n2. Missing discussion and comparison with recently published GNN-based AL methods, e.g.,\nLi et al. SEAL: Semi-supervised adversarial active learning on attributed graphs, IEEE TNNLS, 32 (7), 3136-3147, 2021. \n\n3. The discussions on related work are respectively given in Section 2.3 and Section 4.4. Would it be better to combine the two sections?\n4. Some symbols used in Eq.(8) are incorrect. e.g., would it be v_i - in the last term? \n This paper considers a new AL setting with relaxed queries. Under this setting, a new AL query criterion is proposed to incorporate soft labels and information gain propagation. The setting and idea of this paper are interesting, but there are some concerns, e.g,, the lack of computational analysis of the AL query criterion and comparison to some state-of-the-art GNN-based AL methods. \n\n", "The submissiom presents a GNN-based AL method using soft-label technique with 2 innovations different from existing works\n1. using relaxed queries and 2. building a new criteria of maximizing IGP for active learner with relaxed queries and soft labels. 3.outperforming SOTAs in performance. 1.  The strengths:\n1.1. present using the relaxied queries to replace commonly-used exact labeling strategy for AL, i.e., only judging the correctness of the predicted labels (a binary question) rather than identifying the exact class (a multi-class question), doing so is relatively easier and to be my best knowledge, it seems hardly to be done before.\n1.2. provide a new criteria for active learner with the help of such relaxed queries and soft labels via maximizing defined the IGP. \n1.3. obtain significant performance boost comparing with SOTAs.\nOne the whole, the proposed method can have more flexibility, e.g., being applicable to a large variety of GNN variants.\n2. The  weaknesses:\n2.1 Class-imbalance problem;\n2.2. Due to GNN characteristics, IGP inherits possible oversmoothness problem.\nIn addition, please concern possible relation with \"Two-dimensional active learning for image classification\" availavle at internet. In this submission, in my opinion, the authors consider using relaxied queries to replace exact labeling for active learner and define a new criterion with IGP  for learning. Using the relaxied queries indeed provides the oracle a relatively easier labeling andthus more reliable. While Theorem 3.1 gives some insight. The authors provide their codes for REPRODUCIBILITY and the experiments are also relatively sufficiently conducted and compared, the results are convincing! "], "review_score_variance": 6.5, "summary": "This paper proposes a new approach to graph-based active learning, using the query whether the predictions made by the current model are correct or not.\nAlthough the theoretical underpinnings of the proposed approach are a bit weak, the problem formulation that is newly proposed in this paper makes sense from a practical point of view, and the paper makes a simple and interesting proposal that would be worth sharing with the community.", "paper_id": "iclr_2022_USC0-nvGPK", "label": "val", "paper_acceptance": "Accept (Poster)"}
{"source_documents": ["Recently, many stochastic gradient descent algorithms with variance reduction have been proposed. Moreover, their proximal variants such as Prox-SVRG can effectively solve non-smooth problems, which makes that they are widely applied in many machine learning problems. However, the introduction of proximal operator will result in the error of the optimal value. In order to address this issue, we introduce the idea of extragradient and propose a novel accelerated variance reduced stochastic extragradient descent (AVR-SExtraGD) algorithm, which inherits the advantages of Prox-SVRG and momentum acceleration techniques. Moreover, our  theoretical analysis shows that AVR-SExtraGD enjoys the best-known convergence rates and oracle complexities of stochastic first-order algorithms such as Katyusha for both strongly convex and non-strongly convex problems. Finally, our experimental results show that for ERM problems and robust face recognition via sparse representation, our AVR-SExtraGD can yield the improved performance compared with Prox-SVRG and Katyusha. The asynchronous variant of AVR-SExtraGD outperforms KroMagnon and ASAGA, which are the asynchronous variants of SVRG and SAGA, respectively.", "Question 1: It would be good and well understood to explain intuitively the steps of the proposed algorithm.\n\nResponse: Thank you for your valuable suggestion. To address your concern, we have added some explanations for the proposed algorithm in Section 3.1 in the revised manuscript and made it easier to understand.\n\nQuestion 2: How should “$K$” be chosen in practice?\n\nResponse: In our experiments, we used the once extragradient update every 25 inner-iterations, which have also been explained in Section 5 in the manuscript. In practical problems, the frequency of using the extragradient update step can be adjusted according to specific problems and specific requirements. In this way, $K$ can be determined. In addition, we have added more experimental results for choosing the parameter $K$, in the revised manuscript, and also provided a discussion in choosing the parameter. ", "Thank you for your valuable comments. Through your comments, we get a lot of inspiration and directions to improve the manuscript better.\n\nQuestion 1: This paper claims that extragradient reduces the gap of the optimal value, which is confusing. \nResponse: To address your concern, we have revised the description about the above claim according to the obtained results. We find that the reason why the extragradient can reduce the gap of the optimal value can be explained as follows:\nAccording to [R1], we know that EEG [R1] can make use of the curvature information of the objective function. Although we change EEG into a stochastic version, the advantage of EEG is still retained to some degree. Thus, we find that the extragradient method is not to directly reduce the gap of the optimal value, but to improve the bad result brought by the gap, which can be verified by our new experiments in the revised manuscript. We have added the above discussion in Section 3 in the revised manuscript.\n\nQuestion 2: Besides, how this claim is reflected in the convergence result is not discussed. Section 1.2: the claim extragradient examines the geometry and curvature of the problem is confusing. \nResponse: We know that the original EEG method is a deterministic algorithm, while our algorithm is a stochastic algorithm. Therefore, the framework and process of theoretical analysis in these two kinds of algorithms are very different.\nIn our analysis, due to the introduction of extragradient, there are several issues that are not easy to address. For instance, most of the previous related algorithms only update once in each inner iteration, and thus there is only the relationship between $x_{k-1}$ and $x_k$. In this way, we can use the existing theories and ideas of some algorithms for analysis. But in our algorithm, we update twice in each inner iteration, and thus there are the relationship between $x_{k-1}$ and $x_{k-1/2}$ and the relationship between $x_{k-1/2}$ and $x_k$. Therefore, we need to establish the intermediate connection so that we can get the relationship between $x_{k-1}$ and $x_k$, and finally get the convergence result of the algorithm.\n[R1] uses a linear search method, while it is not applicable for stochastic algorithms, because the direction used in each update of stochastic algorithms is not necessarily the descent direction, and thus the function value may not be reduced after linear search. \n\nQuestion 3: How does extragradient affect the complexity and choice of hyperparameters such as $K$, $\\eta_1$, $\\eta_2$, and $\\beta$?\nResponse: We note that the hyperparameters related to the extragradient are mainly two step sizes, i.e., $\\eta_1$ and $\\eta_2$. They need to satisfy the conditions given in the manuscript, which are based on EEG step sizes given in [R1]. As for $K$, it is only used to control the frequency of using extragradient, and thus it can be adjusted manually according to specific problems and requirements. For the acceleration hyperparameter $\\beta$, it is mainly related to momentum. And its setting can refer to the settings of related algorithms (such as MiG [R2]). In addition, it can also be adjusted manually according to the experimental results. All the discussions have been added in the revised manuscript.\n\nQuestion 4: A more careful experimental design is required to better demonstrate the performance:\n1.\tThe choice of inner iterations.\n2.\tThe comparisons on number of iterations.\n3.\tThe comparison with MiG [R2].\n4.\tAt least solve two different optimization programs (e.g. logistic regression, neural network).\nResponse: We note that Katyusha has two options for $y_{k+1}$, as shown in [R3] . When we choose the first option, we need to set $m\\!=\\!n$ in our paper, but when we choose the second option, $m\\!=\\!2n$ is appropriate. Besides, in order to make a more comprehensive comparison, we have added more new experiments in the revised manuscript, including the comparison based on iteration, the comparison with more compared algorithms, and the performance comparison when solving logistic regression with $\\ell_1$ norm. We also give some reasonable explanations for the experimental results.\n\nQuestion 5: The presentation and structure of this paper need to be improved. \nResponse: We have carefully revised the manuscript according to your suggestions. We have moved all the lemmas to Appendix, fixed typos and incorrect grammar, and added the definitions of some undefined symbols.\n\n[R1] T. Nguyen et al., Extragradient method in optimization: Convergence and complexity, 2017.\n[R2] Zhou et al., A simple stochastic variance reduced algorithm with fast convergence rates, 2018.\n[R3] Allen-Zhu Zeyuan. Katyusha: the first direct acceleration of stochastic gradient methods. 2017.", "Response for “Detailed comments”:\nThank you for checking the manuscript so carefully. According to your “Detailed comments”, we have revised and improved our paper. Besides, here are the explanations of some main questions:\nResponse for “1.”: Our algorithm is applicable to both sparse and dense data sets. But in practical problems, the data is usually large-scale and sparse, and thus our main purpose of this algorithm is to solve the problem of large-scale sparse data sets. Besides, our sparse asynchronous variant can take advantage of the sparsity of data, and thus we have moved the asynchronous algorithm to the main body of the paper according to your suggestion.\nResponse for “6”: Here “the process” refers to the whole process of solving the problem to be solved. In the optimization problem, it refers to the process of solving the optimal value of the objective function. And “it” refers to the idea of extragradient.\nResponse for “10”: Here “the function” means the objective function to be optimized. We have also revised it in the revised manuscript.\nResponse for “11”: We have given the citations where “APG” and “Acc-Prox-SVRG” appear in the revised manuscript. Due to the limited space, we do not give the citations where they reappear.\nResponse for “14”: The equations of Section 3.2 solve the minimum point of the function, and thus adding and reducing a constant term which is independent of the variable in the function will not change the final minimum point. In addition, the purpose of this equation is to explain that the proximal operator will introduce a gap of the optimal value. In particular, we can add the statement in the revised manuscript.\n\nAdditional question:\n[R4] update the extragradient step by sampling a new stochastic gradient while in this paper the same sample is used twice. How you compare these two approaches in terms of their performance and convergence?\nResponse: We know that the algorithm without extragradient, such as Prox-SVRG, will sample one stochastic gradient to update in each inner iteration. Therefore, sampling a new stochastic sample to update the extragradient is equivalent to using the algorithm without extragradient to update twice. In this way, we cannot take advantage of the advantage of extragradient. However, we update twice on the same sample point in our algorithm. It is different from the algorithms without extragradient, and can get better results, which can be seen from the experimental comparison between VR-SExtraGD and Prox-SVRG.\n\n[R4] R. Rockafella, Convex Analysis, 1970.", "Question 1: How does it compare to SVRG++ and Proximal Proximal Gradient? \n\nResponse: Thank you for the suggestion. To address your concern, we have added some experimental results to compare SVRG++ with the proposed algorithm. All the results show that SVRG++ slightly outperforms Prox-SVRG. However, our AVR-SExtraGD has better performance than SVRG++, due to extragradient and momentum acceleration. As for the Proximal-Proximal-Gradient (PPG) method, when solving the same problem as ours, it is actually proximal gradient method, which means it is a more general algorithm of proximal gradient method. Thus, the comparison with PPG is meaningless.\n\nQuestion 2: Combining momentum with an existing algorithm is not extremely novel.\n\nResponse: In this paper, we first introduced the idea of extragradient descent into Prox-SVRG and propose a new algorithm, called VR-SExtraGD. In particular, we also proposed a new momentum accelerated VR-SExtraGD algorithm, called AVR-SExtraGD. Thus, we not only combine momentum acceleration with the proposed VR-SExtraGD algorithm, but also introduce an innovative idea into the algorithm. Another main contribution of this paper is the convergence results of the proposed algorithms including VR-SExtraGD and AVR-SExtraGD. Due to the introduction of extragtadient descent, our convergence analysis for the proposed algorithms needs more improvement and innovation, which is also our main novelty and overcoming technical difficulty.", "We thank all the reviewers for their invaluable comments. We answered all the reviewers' concerns and questions, respectively, and uploaded a revised version of our paper.", "The paper proposes an optimization method for solving unconstrained convex optimization problems where the objective function consists of a sum of several smooth components f_i and a (not necessarily smooth) convex function R. The proposed method AVR-SExtraGD is a stochastic descent method building on the previous algorithms Prox-SVRG (Lin 2014) and Katyusha (Zeyuan 2017). The previous Prox-SVRG method using a proximal operator is explained to converge fast but leads to inaccurate final solutions, while the Katyusha method is an algorithm based on  momentum acceleration. The current paper builds on these two approaches and applies the momentum acceleration technique in a stochastic extragradient descent framework to achieve fast convergence.\n\nI am not working in the field of optimization, therefore, unfortunately I am not in a position to give detailed technical comments for the authors. However, as far as I could follow the paper, it seemed sound and well-written to me in general. I hope the following minor comments may be useful for improving the paper:\n\n- The paper gives detailed explanations about previous work. However, the proposed AVR-SExtraGD algorithm is only presented in the form of a pseudocode in Algorithm 1 and it is not explained in much detail. It would be good to explain and discuss intuitively the steps of the proposed algorithm in the main body of the paper as well, so that it is well understood.\n\n- Algorithm 1 has a set K as input, according to which the solution is updated. How should this set be chosen in practice?", "This paper proposed a new stochastic algorithm: AVR-ExtraGD. AVR-ExtraGD combines the extended extragradient method proposed in [3] and the accelerated SVRG method in [1][2]. In their experiments, AVR-ExtraGD outperforms [2] in running time for sparse linear regression.\n\nThis paper presents their convergence analysis using results in [1][2]. They showed that the proposed algorithm can achieve O(sqrt{kappa n} log (1 / \\epsilon)) complexity for strongly convex problem and O(1/sqrt{epsilon}) for convex problem, which are the best results for both cases.\n\nThe idea of an accelerated version of variance reduced stochastic extragradient method is novel. However, there are some issues that the authors should address in order for the paper to match the quality of ICLR.\n\nEach step of extragradient approximates the proximal operator x_{k+1} = argmin_x P(x) + 1/(2 eta_k)\\|x – x_k\\|_2^2, therefore we would expect a faster and more stable convergence from this method. This paper claims that extragradient reduces the gap between the obtained optimal value and the real optimal value, which is confusing. The update of extragradient is actually biased towards x_k. The claim is then discussed in section 3.1 and 3.2 but is not clearly explained. Besides, how this claim is reflected in the convergence result is not discussed. I encourage the authors to clearly elaborate this claim and make relevant remarks after the main theorems.\n\nTo better understand the convergence result, it is important to know how extragradient affects the complexity and choice of hyperparameters such as K, eta_1, eta_2, and beta. Such discussion is not in this paper. I suggest the authors to make these aspects clear.\n\nThe experiments compare the proposed algorithm with other algorithms by their running time for lasso and elastic-net. The comparisons show the efficiency of the proposed algorithm. However, a more careful experimental design is required to better demonstrate the performance:\n1.\tFor the choice of inner iterations, choosing m=2n for Katyusha actually requires calculating 5n stochastic gradient because each iteration of Katyusha does gradient updates twice.\n2.\tThis paper only presents comparisons of running time. I encourage the authors to also plots comparisons on number of iterations, which will help revealing where the speed up of AVR-ExtraG comes from.\n3.\tIt is also preferable that the author compare with MiG [1], since the proposed algorithm is an extragradient version of [1].\n4.\tPlease at least solve two different optimization programs (e.g. logistic regression, neural network) so any conclusions are not specific to the oddities of a particular program.\n\nThe presentation and structure of this paper need to be improved. Here are some suggestions:\n1.\t In Section 1, only provide a high-level literature review and then motivate the work. A comprehensive review can come after the introduction.\n2.\tIn Section 4, put all the lemmas into the appendix while giving more intuitions and remarks. \n3.\tIssues including notions without pre-definition or reference, typos, and incorrect gramma need to be fixed.\n\n\nDetailed comments:\n1.\tFrom the title, the main application of this work is sparse learning problem. However, how the proposed algorithm benefits sparsity is not discussed. Besides, I suggest the authors to move the asynchronous algorithm in the appendix to the main paper.\n2.\tThe paragraph before section 1.1: lasso and elastic-net are used without citation.\n3.\tSection 1.1: PGD and SGD are used without citation.\n4.\tBeginning of page 2: “And” should be “Besides”\n5.\t“Besides, for accelerating the algorithm and …”: “for accelerating” should be “to accelerate”\n6.\tSection 1.2: “Nguyen et al. (2017) proposed the idea of extragradient which can be seen as a guide during the process, and introduced it into the optimization problems.” What does “the process” and “it” refers to is unclear.\n7.\tSection 1.2: the claim extragradient examines the geometry and curvature of the problem is confusing. The geometry of the problem is inspected through a line search step in [3]. However, line search is not discussed in this paper.\n8.\tSection 1.2: “reduce the gap between the optimal value we get and the real optimal value”, these two kinds optimal values are important notions of this paper but they are not defined.\n9.\tIn Assumption 2, you can refer to Part 2, Section 7 of [5] for the definition of semi-continuity.\n10.\t“dw is the gradient of the function at w”, what does “the function” refers to?\n11.\t“APG and Acc-Prox-SVRG” needs citation.\n12.\t“was proposed to simply the structure of Katyusha”, “simply” should be “simplify”\n13.\tSection 3.1: “updated with the update rules of MiG”: “with” should be “by”\n14.\tIn the equations of Section 3.2, the equivalent of gradient norm square and function f is incorrect, and the purpose of this equation is unclear.\n15.\tSection 4.1 Theorem 1: The inequality in theorem 1 is not intuitively related to the convergence rate. I suggest the author to simplify the inequality (For example, Theorem 2.1 in [2]).\n16.\tThe references are not in a uniform format. Conference/Journal names are missing for some references.\n17.\tOne useful reference for this paper is [4], it discussed extragradient for online convex learning.\n\nAdditional question:\n[5] update the extragradient step by sampling a new stochastic gradient while in this paper the same sample is used twice. How you compare these two approaches in terms of their performance and convergence?\n\n[1] A simple stochastic variance reduced algorithm with fast convergence rates, Zhou et al., 2018.\n[2] Katyusha: the first direct acceleration of stochastic gradient methods, Z. Allen-Zhu, 2017\n[3] Extragradient method in optimization: Convergence and complexity, T. Nguyen et al., 2017\n[4] Online Optimization with Gradual Variations, Chiang et al., 2012\n[5] Convex Analysis, R. Rockafella, 1970\n[6] Reducing Noise in GAN Training with Variance Reduced Extragradient, Chavdarova et al.  2019\n", "This is an optimization algorithm paper, using the idea of \"extragradient\" and proposing to combine acceleration with proximal gradient descent-type algorithms (Prox-SVRG). Their proposed algorithm, i.e., accelerated variance reduced stochastic extra gradient descent, combines the advantages of Prox-SVRG and momentum acceleration techniques. The authors prove the convergence rate and oracle complexity of their algorithm for strongly convex and non-strongly convex problems. Their experiments on face recognition show improvement on top of Prox-SVRG as well Katyusha. They also propose an asynchronous variant of their algorithm and show that it outperforms other asynchronous baselines.\n\n- technically sound, seems like a nice addition to the variance reduced gradient-type methods. Combines the nice properties of proximal methods with variance reduced gradient-descent.\n- Nice summary of recent progress in this research area. \n- How does it compare to SVRG++? How about the Proximal Proximal Gradient? \n- algorithm suitable for non-smooth optimization problems\n- their experimental results look convincing.\n\nHaving said that, it seems to me that combining momentum with an existing algorithm is not extremely novel -- I would defer to reviewers who are experts in the optimization area to fully assess the novelty and technical difficulty of the proposed solution.\n\n "], "review_score_variance": 8.666666666666666, "summary": "This paper proposes a stochastic variance reduced extragradient algorithm. The reviewers had a number of concerns which I feel have been adequately addressed by the authors.\n\nThat being said, the field of optimizers is crowded and I could not be convinced that the proposed method would be used. In particular, (almost) hyperparameter-free methods are usually preferred (see Adam), which is not the case here.\n\nTo be honest, this work is borderline and could have gone either way but was rated lower than other borderline submissions.", "paper_id": "iclr_2020_BklDO1HYPS", "label": "train", "paper_acceptance": "reject"}
