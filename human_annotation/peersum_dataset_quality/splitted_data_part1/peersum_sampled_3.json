{"source_documents": ["Despite its potential to improve sample complexity versus model-free approaches, model-based reinforcement learning can fail catastrophically if the model is inaccurate. An algorithm should ideally be able to trust an imperfect model over a reasonably long planning horizon, and only rely on model-free updates when the model errors get infeasibly large. In this paper, we investigate techniques for choosing the planning horizon on a state-dependent basis, where a state's planning horizon is determined by the maximum cumulative model error around that state. We demonstrate that these state-dependent model errors can be learned with Temporal Difference methods, based on a novel approach of temporally decomposing the cumulative model errors. Experimental results show that the proposed method can successfully adapt the planning horizon to account for state-dependent model accuracy,  significantly improving the efficiency of policy learning compared to model-based and model-free baselines.\n      ", "#rebuttal responses\n \nThe authors' reply does not convince me, and I still think the paper has some problems:\n(1) I do not believe that the cumulative model-error can not be learned efficiently;\n(2) Experimental results are weak as some baselines do not converge! \n\nThus I keep my rating as reject.\n\n#review \nThis paper proposes a new adaptive model-based value-expansion method, AdaMVE, that decides the planning horizon of the learned model by learning the model-error. The model-error is learned by temporal difference methods.\nExperimental results show that AdaMVE beats MVE, STEVE, and DDPG in several environments.  \n\nOverall the paper is well written.  The paper proposes an interesting question: how to adaptively change the planning horizon based on the state-dependent model-error? Firstly, The authors upper bound the cumulative target error by the cumulative model-error.  Then the cumulative model-error is learned by the temporal difference method. With the learned cumulative model-error function over different rollout steps, the planning horizon is decided by a softmax policy.  \n\nHowever, I do not think that learning an upper bound of the target error helps to determine the value of H, as there is no justification that the gap between the target error and the model error is small theoretically.  I also doubt that the cumulative model-error can be learned without large loss, as there are no plots of W in this paper. \n\nThe authors claim that it is expensive to retrain the model error for the current policy at every step, thus they use some reference policy. I think it is ok, but I want to see the results of AdaMVE using the updated current policy, or updating the Q function before improving the policy. Adding a figure showing the change of H in the training helps to motivate this paper.\n\nFinally, AdaMVE is only compared in two MuJoCo environments. Baselines in other environments are only trained in 1e5 steps, thus the experimental results are not convincing.\n\nI am happy to change my opinion on this paper if authors give better motivation and the detail of the learning.", "\n# Rebutal Respons:\n\nPlanning Horizon:\n- I agree small horizons can speed up learning. However, if we want to drastically reduce the sample complexity, we need longer planning horizons. Therefore, we should be looking further in this direction but this is out of scope for this paper.\n\nModel Learning:\n- Good idea to update the conclusion and treat the online model learning as future work.\n\nFigure 3:\nYou could plot axes a - f in a single row and remove the colorbar for each individual plot. Just do one colorbar for all plots. This would also improve the comparability of the different methods. Currently, the color-coding is different for each axis, which is bad practice. Axes g - i can be reshaped in a separate figure. Furthermore, figure 2 is not necessary as the 4 room environment is depicted in Fig. 3 a - f and and one could reference these axes. \n\n=> I keep my rating as weak accept.\n\n# Review:\nSummary:  \nThe paper proposes an adaptive scheme to adapt the horizon of the value function update. Therefore, the sample efficiency should be increased and the value function should be learned faster. \n\nConclusion: \nThe problem of learning good policies from partially correct models is very interesting and important. The proposed approach is technically sound and reasonable. The experiments highlight the qualitative as well as quantitative performance. Furthermore, the quantitative performance is compared to state-of-the-art methods. I cannot comment on the related work as I am not familiar with the baselines MVE & STEVE. \n\nMy Main Concerns are:\n\n- The roll-out horizon is 3-5 timesteps. This horizon is really short especially for problems with strong non-linearities and high sampling frequencies. For such systems 5 timesteps would only correspond to 0.5s (10Hz) or 0.05s (100Hz) and I am uncertain whether these short horizons really help for such problems. \n\n- The specialized online model learning algorithm seems quite hacky. It feels like it was introduced last minute to make it work. The overall question of how should we learn a model optimally is super important and should be addressed within a separate paper (and more thoroughly). I would even propose to remove the online learning section from this paper as it is just too hacky and without relation to prior work or context. \n\n- Could the authors please update figure 3 as the figure has too much whitespace. This whitespace could be used to enlarge the individual axis when the axis are rearranged.  \n", "“However, I do not think that learning an upper bound ... the model error is small theoretically.”\n\n-- Dealing with model compounding error is an important issue in model-based RL when the model is inaccurate. However, the model compounding error cannot be computed directly, which motivates our work to derive a learnable upper bound (the cumulative model error) on it (Thm 1). \n\nOnce the cumulative model error is learned, the remaining question is to decide h given upper bound estimates of the target error for each h. The softmax policy (Eq 8)  aims at selecting h where the cumulative model error is small while keeping as many such h (with small error) as possible to utilize longer horizons.\n\nThe reviewer worries that selecting h according to the upper bound might be problematic when the upper bound is loose. However, if the upper bound is loose (the model error is high while target error is small), our adaptive strategy (Eq 9) becomes conservative and will not bring any negative effect on policy learning. This is because in Eq 9 we also consider h=0 that has zero model error, which makes any h with large error get a small weight, and the expanded value reduced to the model-free target. However, the fact that our method outperforms model-free baselines and that the selected h is not close to 0 (see Fig 3 & Fig 7 in A.5) is clear evidence to support that our upper bound is *not* loose. \n\n\n\n“I also doubt that the cumulative model-error can be learned without large loss.”\n\n-- The cumulative model-error can be learned by minimizing the TD error (Eq 7). In our scenario, W can be directly computed for any (s,a,s’) using the l2 norm (see Sec 2.2 and 3.1). By this formulation, learning the cumulative model-error is just another value learning problem in MDP and is no harder than learning the Q values. (And probably easier, as the cumulative model-error is of finite horizon.) Like in Q-learning (DQN, DDPG), it is hard to tell whether the Q-values are well learned or not by simply plotting the training loss (what values are *large* and what values are *small*). It is more important that the learned values are good enough for its purpose: In Q-learning whether the learned values induce a good policy, in our case whether the learned model error is effective in deciding planning horizons that improve the performance. (E.g., in DQN and DDPG, people have observed that there is always a gap between learned and real Q values, but the learned values can be good enough to induce a good policy.)\n \n\n\n“I want to see the results of AdaMVE using the updated current policy or updating the Q function before improving the policy.”\n\n-- One of our reference policies, the *greedy* reference policy is the current policy. (see Sec 3.1 & Fig3 (g)-(i)). \n\n\n“Adding a figure showing the change of H in training helps to motivate this paper.“\n\n-- We include two supporting figures showing the change of the weighted average horizon during training in mujoco navigation using a pre-trained model. (See Fig 7 in Appendix A.5 in our updated version). We can see the weighted average horizon over different states has a high variance, which is a sign of successful adaptation. \n\n\n“Finally, AdaMVE is only compared in two MuJoCo environments.” \n\n\n-- We agree that testing on more tasks will certainly make our results stronger. However, our main contribution is to propose a novel approach to leverage an inaccurate model in model-based RL, rather than pursuing state-of-the-art performance on hard tasks. To demonstrate the effectiveness of our method, we choose a diverse set of tasks:  simple gridworlds, Mujoco navigation, Mujoco locomotion. The advantage of selecting this set of tasks instead of a set of harder but similar ones (e.g. a full set of Mujoco locomotion tasks) is that some of these tasks allow more interpretable demonstrations. E.g., the gridworld allows us to visualize the adaptive horizon over all states. In Mujoco navigation, the flexibility in customizing the walls allows us to pretrain a partially correct model with a specific meaning: by removing the walls the pretrained model is correct with locomotion but inaccurate with interaction to the wall. This sets a clear expectation that a successful adaptive strategy should be able to utilize this pretrained model. With these explanations we hope we can convince the reviewer that we put a sufficient amount of effort in demonstrating the effectiveness of our method in various ways so that evaluation on more Mujoco locomotion tasks would be a bonus rather than a necessity. \n\n\n“Baselines in other environments are only trained in 1e5 steps”\n\n-- On these tasks, the best performing algorithm has converged to near optimal performance given the training budgets (2e5 steps). We also want to note that the 2e5 steps is a reasonable choice in Mujoco navigation tasks and it is also adopted in previous works [1]. \n\n[1] Wu, Y., et al. The laplacian in rl: Learning representations with efficient approximations. \n", "We thank the reviewer for the valuable feedback. We are glad the reviewer found the problem studied to be interesting and important. Responses to the reviewer’s comments are addressed below. We are also happy to discuss further if the reviewer has additional concerns.\n\n\n“One thing that concerns me is that the maximum horizon H looks very small. A max horizon of 3, 5 or even 10 is still much smaller than … Or maybe that scale of horizon is not common in Steve and thus not tried in the experimental section? “\n\n-- As suggested in the review, the reason we only consider small scale of maximum horizon is that a large rollout horizon is not common in MVE based methods, e.g. STEVE. A short rollout horizon can be sufficient to utilize the benefit of a model when used in certain model-based deep RL algorithms. In previous works that combine value based RL methods with multi-step value expansion, they found that a small rollout horizon can be beneficial. For example, when applying DDPG+MVE in Mujoco continuous control tasks [1, 2, 3], a small rollout horizon can be effective in providing better performance than purely model-free algorithms. For DQN on discrete action domains (e.g. Atari), [4] tunes the number of steps used to get a multi-step return as a hyper-parameter. They found that among {1, 3, 5}, 3 works the best as the number of rollout steps. Although the maximum horizons in these domains (Mujoco, Atari) are usually quite large (>~500), the observations in these previous works support that a small rollout horizon (1-5) can be helpful on these domains. \n\nA larger max-horizon may be needed for other model-based RL algorithms such as MPC based methods, e.g. PETS. As we mentioned in our conclusion section, we would leave combining our adaptive planning horizon method with other model-based RL approaches as future work. We note that our method considers the problem of learning accumulative model errors as policy evaluation in finite-horizon MDP, and it is valid for any large maximum planning horizon. For model-based algorithms that require large planning horizons, our method for estimating model errors and adapting planning horizon can be directly applied in those cases. \n\n\n“For the high dimensional control tasks, only Half-Cheetah and Swimmer was tried. It will generalize the conclusion if the results of other benchmarking environments are tested, including complex tasks such as Humanoid, and easier tasks such as Cart-pole.”\n\n-- We agree that testing on more tasks will certainly make our results stronger. In the online model learning setting as considered in our experiments, it is very difficult to learn a partially accurate model in complex domains (such as Humanoid) by simply fitting a neural network with one minibatch per environment step.  For a poorly learned model where the model error is large almost everywhere, our adaptive horizon selection strategy (equation 8) assigns negligible weights to horizons above zero, which makes our algorithm behaves similarly as the model-free baseline. To get a good model, we may need a much larger neural network with many more gradient steps per environment step (as in [3]), which requires a significant amount of computation resource.  \n\n\n\n“The used baselines in the paper are generally not considered to be state-of-the-art. I guess it is sort-of fair comparison since you add the proposed component on top of the baselines. But extending the current state-of-the-art such as TD3 / PETS”\n\nWe apply DQN and DDPG as the model-free baselines in our experiments, as in most of our test domains, these algorithms already have reasonably good performance (e.g. TD3 will only give a marginal improvement over DDPG on those tasks). Also, as suggested in the review, we believe this choice of the baseline is fair since we add the proposed component on the top of the baselines. Furthermore, we do extend the current state-of-the-art version in domains where DDPG performs not well. For example, in Swimmer we use the double-Q-function trick proposed by TD3 in DDPG to get a good model-free baseline performance. This is mentioned in Appendix A.4.  \n\n\n\"I also suggest adding this benchmarking paper into the related work, which empirically studies the compounding error in different state-of-the-art model-free and model-based reinforcement algorithms.\"\n\nThanks for your suggestion. We have included this paper in the related work in the updated version. \n\n\n[1] Feinberg, V., Wan, A., Stoica, I., Jordan, M.I., Gonzalez, J.E. and Levine, S., 2018. Model-based value estimation for efficient model-free reinforcement learning. \n\n[2] Buckman, J., Hafner, D., Tucker, G., Brevdo, E. and Lee, H., 2018. Sample-efficient reinforcement learning with stochastic ensemble value expansion. \n\n[3] Janner, M., Fu, J., Zhang, M. and Levine, S., 2019. When to Trust Your Model: Model-Based Policy Optimization. \n\n[4] Hessel, Matteo, et al. Rainbow: Combining improvements in deep reinforcement learning.\n", "We thank the reviewer for the valuable feedback. We are glad the reviewer found the problem studied to be interesting and important. Responses to the reviewer’s comments are addressed below. We are also happy to discuss further if the reviewer has additional concerns.\n\n\n“The roll-out horizon is 3-5 timesteps... I am uncertain whether these short horizons really help for such problems.”\n\n-- A short rollout horizon can be sufficient to utilize the benefit of a model when used in certain model-based deep RL algorithms, as shown in previous works. In our experiments, the learning algorithms are value-based deep RL algorithms (DQN, DDPG) combined with multistep model-based value expansion (MVE).  In previous works that combine value-based RL methods with multi-step value expansion, they found that a small roll-out horizon can be beneficial. For example, when applying DDPG+MVE in Mujoco continuous control tasks [1, 2, 3], a short rollout horizon can be effective in providing better performance than purely model-free algorithms. For DQN on discrete action domains (e.g., Atari), [4] tunes the number of steps used to get a multi-step return as a hyper-parameter. They found that among {1, 3, 5}, 3 steps of rollout works the best. Although the maximum horizons in these domains (Mujoco, Atari) are usually quite large (>=500), the observations in these previous works support that a short rollout horizon (1-5) can be helpful on these domains. \n \nIndeed a larger max-horizon may be needed for other model-based RL algorithms. We note that our method considers the problem of learning accumulative model errors as policy evaluation in the finite-horizon MDP, and it is valid for any large maximum planning horizon. For model-based algorithms that require large planning horizons, our method for estimating model errors and adapting the planning horizon can be directly applied in those cases. \n\n\n\n“The specialized online model learning algorithm seems quite hacky... The overall question of how should we learn a model optimally is super important and should be addressed within a separate paper (and more thoroughly)” \n\n-- We agree that how to learn a (maybe partially correct) model online is an important problem that is worth a separate and thorough study. We present these results because of the following. (i) We (and potential readers, according to some feedback we got) are curious about whether our method can work with an online-trained model. (ii) We found that a better online model-learning method does help with performance. These results can serve as supporting evidence for that simply fitting a neural network with minibatch and training L2 losses is not good enough and a better model learning method is needed. Our proposed selective model learning method is a preliminary attempt in this direction.  We have updated the conclusion section of our paper to clearly state that our online model learning method is a preliminary attempt and it is an important problem that needs future study. \n\n\n\n“Figure 3”\n\n-- We have not figured out how to rearrange the space nicely. We will try to do it in the final version. We would also appreciate any suggestions from the reviewer.\n\n\n\n[1] Feinberg, V., Wan, A., Stoica, I., Jordan, M.I., Gonzalez, J.E. and Levine, S., 2018. Model-based value estimation for efficient model-free reinforcement learning.\n\n[2] Buckman, J., Hafner, D., Tucker, G., Brevdo, E. and Lee, H., 2018. Sample-efficient reinforcement learning with stochastic ensemble value expansion. In Advances in Neural Information Processing Systems.\n\n[3] Janner, M., Fu, J., Zhang, M. and Levine, S., 2019. When to Trust Your Model: Model-Based Policy Optimization. \n\n[4] Hessel, Matteo, et al. \"Rainbow: Combining improvements in deep reinforcement learning.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018.\n", "Learning to Combat Compounding-Error in Model-Based Reinforcement Learning\n\nThe authors study the problem of choosing the appropriate planning horizon in model-based reinforcement learning. We want to have a balance between the efficiency of planning into the future and while avoiding the compounding error brought by the learnt model.\nAnd the authors propose to learn an error estimation model that is a function of current state, policy and horizon by estimating the compounding error as a new reward using policy evaluation.\nPros -\nI think the novelty in this paper is most appealing for me to vote for a clear acceptance. The growth of model-based reinforcement learning is in a big need of a framework to work with the planning horizon.\nThe proposed method is a very elegant way of estimating the errors and choose the appropriate horizon.\nStill, I think there is a lot of room for improvement, as summarized below.\n\nCons\n- One thing that concerns me is that the maximum horizon H looks very small. A max horizon of 3, 5 or even 10 is still much smaller than the value we usually use in planning based reinforcement learning (MPC in your case). A common choice can be up to 30 or 50 for PETS based algorithms [1].\n\nDoes that indicate that the proposed framework is not stable or robust when the horizon is big? Or maybe that scale of horizon is not common in Steve and thus not tried in the experimental section?\n\n- Experiments\nFor the high dimensional control tasks, only Half-Cheetah and Swimmer was tried. It will generalize the conclusion if the results of other benchmarking environments are tested, including complex tasks such as Humanoid, and easier tasks such as Cart-pole.\n\n- Algorithms\nThe used baselines in the paper are generally not considered to be state-of-the-art. I guess it is sort-of fair comparison since you add the proposed component on top of the baselines. But extending the current state-of-the-art such as TD3 [2] / PETS\n\nI also suggest adding this benchmarking paper into the related work, which empirically studies the compounding error in different state-of-the-art model-free and model-based reinforcement algorithms.\n\n[1] Chua, K., Calandra, R., McAllister, R., & Levine, S. (2018). Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems (pp. 4754-4765).\n[2] Fujimoto, Scott, Herke van Hoof, and David Meger. \"Addressing function approximation error in actor-critic methods.\" arXiv preprint arXiv:1802.09477 (2018).\n[3] Wang, Tingwu, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel and Jimmy Ba. “Benchmarking Model-Based Reinforcement Learning.” ArXiv abs/1907.02057 (2019): n. pag."], "review_score_variance": 8.666666666666666, "summary": "The paper received mixed reviews: R (R3), WA (R2), A (R1). AC has read the reviews, rebuttal and paper. AC is concerned about the short planning horizon, which seems like a major issue: (i) as R1 notes, most MPC algorithms use much longer horizons as they find it helps performance and (ii) the claim of the approach to be able to pick the planning horizon is moot if its dynamic range is small.  Overall, the paper is very borderline. The idea is interesting but without addressing longer horizons, the contribution is limited. Under guidance from the PCs, the AC feels that the paper just falls below the acceptance threshold and thus cannot be accepted unfortunately. The work is definitely interesting however and should be revised for a future submission. \n\n", "paper_id": "iclr_2020_S1g_S0VYvr", "label": "train", "paper_acceptance": "reject"}
{"source_documents": ["As the decisions made or influenced by machine learning models increasingly impact our lives, it is crucial to detect, understand, and mitigate unfairness. But even simply determining what ``unfairness'' should mean in a given context is non-trivial: there are many competing definitions, and choosing between them often requires a deep understanding of the underlying task. It is thus tempting to use model explainability to gain insights into model fairness, however existing explainability tools do not reliably indicate whether a model is indeed fair. In this work we present a new approach to explaining fairness in machine learning, based on the Shapley value paradigm. Our fairness explanations attribute a model's overall unfairness to individual input features, even in cases where the model does not operate on sensitive attributes directly. Moreover, motivated by the linearity of Shapley explainability, we propose a meta algorithm for applying existing training-time fairness interventions, wherein one trains a perturbation to the original model, rather than a new model entirely. By explaining the original model, the perturbation, and the fair-corrected model, we gain insight into the accuracy-fairness trade-off that is being made by the intervention. We further show that this meta algorithm enjoys both flexibility and stability benefits with no loss in performance.", "The goal of the paper is to design mechanisms to explain the unfairness in the outcomes of a ML model and propose methods to mitigate unfairness. The paper uses the Shapley value framework. The main idea is to alter the prediction function so that instead of providing the classification score, an \"unfairness\" score is returned. An out of the box application of the Shapley value framework on this unfairness score now returns the \"unfairness\" feature attribution. These feature attributions can be used to explain the unfairness of the model. The paper then proposes to learn a linear perturbation, which when combined with the additive property of Shapley framework results in updated \"unfairness\" attributions.\n\nWhile the paper tackles an interesting and timely problem, it lacks clarity at several important points. Additionally, some of the claims seem to be a little overreaching. The experimental evaluation can also use some more thoughtful analysis. Please see detailed comments and suggestions for improvement below:\n\n1- First of all, I would highly recommend setting aside a separate (sub)section to describe the setup. Right now, the details about whether f(x) is a real number of a probability, whether y is {0,1} or {-1,1}, or what \"a\" is are scattered across the text, reducing the readability of the paper.\n\n2- Going from Eq. 2 to Eq. 5, the paper somehow switches from prediction probabilities to accuracies. Shouldn't there be a threshold function to convert the probabilities into predictions? Or is the paper only focusing on randomized classifiers? If only the randomized classifiers are the focus, if/how does it limit the extensibility of the proposed technique?\n\n3- Does it make any difference for the explanations when the probabilities are high non-calibrated (https://arxiv.org/pdf/1706.04599.pdf)?\n\n4- What was the accuracy/generalizability of this $\\delta$ in the experiments?\n\n5- After reading Section 2.2, a reader would think: Why even train the linear perturbation? Why not simply train a separate fair model (from the same model class) and get the Shapley predictions of that second model? I think the main idea here is that the paper tries to leverage the Shapley additive property here. How precisely does the additive property help?\n\n6- The results in Figure 1 are interesting, but not very surprising. Specifically, given that one achieves roughly the same accuracy on Adult data regardless of whether the sensitive feature is used or not, it would have been more interesting (and insightful) to show the Shapley plots for the cases when the sensitive feature is indeed not used for classification.\n\n7- The claim made in the paper that fairness is achieved at \"no loss of accuracy\" is perhaps a bit too bold given that there are well-known results about fairness-accuracy tradeoffs (e.g., https://arxiv.org/abs/1701.08230 and https://arxiv.org/abs/1609.05807).\n\n--------------------\nPost rebuttal comments:\n\nThanks to the authors for the helpful comments -- they indeed help clarify some of the confusions. As a result, I have upgraded my score. However, I am still leaning towards reject because I feel there are still open questions that may hinder the adaptability of the proposed method in the real world. Specifically, given the response to question 3 above, it would help to know what are the real world situations where one uses a randomized classifier and is still interested in model interpretability (the two seem to be at odds with each other as randomness inherently seems a bit arbitrary). Another concern that I have is about Eq. 3 in the paper: Why is the sum function chosen to compute global explanations from local ones? There seem to be multiple ways to do this (e.g., median, sum of absolute values) and it would help to know what are the (dis)advantages of not using other aggregation functions.", "We thank all of our reviewers for careful consideration of our paper. A common theme to all reviews was a lack of clarity in certain key parts of the paper. We have made a number of improvements, most notably the addition of a background / notation subsection at the start of section 2. We additionally have clarified assumptions or wording in a number of places in the text, and improved the readability of one of the main figures. Though the content is ultimately unchanged, we believe the clarity is far improved. We hope the reviewers agree, and thank them for specific examples pointed out in their reviews, all of which we have addressed.", "We thank the reviewer for a careful reading of our paper and the many helpful comments and questions.\n\nAs discussed in our responses to the other reviewers and the general comment, we have made a number of edits to the paper to improve clarity in key sections, including the addition of a section with setup and notation. We hope this addresses a number of the points of confusion.\n\nWe now address specific points made by the reviewer. Any points we do not address directly should be assumed to be remedied by the clarity improvements.\n\n**Introduction + Methods**\n* Your interpretation is correct, we have made this explicit in the text\n* We have also clarified this. The goal is to explain the predicted probability of the ground truth label (so for different data points we might explain a different component of the output of $f$). Therefore in both cases $y$ was being used to mean the same thing.\n* “Splicing” is commonly used terminology in this context, but we have reworded the description to be more explicit.\n* We have also reworded this description. The first term in equation (5) (now (4)) is the average probability the model assigns to the correct label. This is therefore equivalent to the proportion of correct predictions you would expect if you sampled labels from the predicted probabilities. In other words it represents the accuracy of a randomised classifier.\n* We have clarified the notation and role of a in the definition of $g$. The factor of $p(a)$ is needed to counteract the effect of different protected group sizes when aggregating to get global Shapley values.\n* Delta is a function, and equation (10) (now (9)) refers to addition of functions, rather than modification of the internal weights / parameters of the model $f$. We have updated the notation to make this clearer. This also answers the reviewers next question about $\\delta$ being written as a function in the following equation.\n\n**Experiments**\n* We have made the labels larger and added tick labels to each axis as suggested.\n* In section 3.2 we seek to show that fairness Shapley values cannot be manipulated to hide unfairness in the same way that regular Shapley values can [1]. The key observation is that while it is still possible to manipulate individual Shapley values, collectively the Shapley values are constrained to sum to the demographic parity difference, and hence the only way to hide unfairness from the fairness Shapley values is to eliminate it. We additionally observe that the attribution of unfairness to individual features allows us to understand what is happening in the manipulated model: namely the model has shifted focus instead to close proxies of sex, and as a result should not be considered to be fair. We have clarified some of the confusing exposition in this section.\n\n**General Question + Comments:**\n* Adversarial fooling of explanation methods. The effect of the adversarial attacks described in [2] would likely be similar to our findings with the suppression technique of [1]. As noted above, the Shapley values must sum to the unfairness, so at best any attack could manipulate individual Shapley values, but would not be able to give the illusion of a fair model without actually making the model fair. As a side note, the attack described in [2] exploits the fact that Shapley explanations rely on model evaluations on data that lies off the data manifold / out of distribution. The fix to this is to use an on-manifold approximation of the value function as described in [3] and mentioned in footnote 1 of our paper. This is not the focus of our paper, and the changes required are simple so we did not include the details in our write-up.\n\nWe hope these answers clear up any outstanding questions, and believe that the revisions we have made address the stated clarity concerns.\n\n[1]: Dimanov, Botty, et al. \"You Shouldn't Trust Me: Learning Models Which Conceal Unfairness From Multiple Explanation Methods.\" SafeAI@ AAAI. 2020.\n\n[2]: Slack, Dylan, et al. \"Fooling lime and shap: Adversarial attacks on post hoc explanation methods.\" Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. 2020.\n\n[3]: Frye, Christopher, et al. \"Shapley-based explainability on the data manifold.\" arXiv preprint arXiv:2006.01272 (2020).\n", "We thank the reviewer for a thoughtful summary of our work and positive comments.\n\nThe reviewer cited clarity of section 2 as a concern. As noted in our general response and response to the other reviewers we have made a number of edits to the paper to address some of the issues. In particular we have made explicit the supports of $a$, $y$ and $f(x)$ as requested.\n\nWe appreciate the comment that we should be careful to claim that the explanations presented in our work do not suggest any particular intervention. This is very much aligned with our view, and in fact we are sceptical that selection of a fairness metric could or even should be automated. Instead we view selection of a fairness metric or intervention to be a choice that is heavily dependent on a good understanding of the context. We believe that fairness-specific explanations can help with understanding the context, and so are a valuable component in choosing the right notion of fairness and an intervention. By themselves we would not consider them sufficient to make such decisions responsibly. It is worth considering the counterfactual, without these explanations interventions have to be selected on the basis of high-level metrics only. Attributing these metrics to individual features is relevant information that can help with downstream decision making.\n\nWe believe the revisions fully address the reviewer's primary concern of a lack of clarity, and hope that they agree.", "We appreciate the reviewer’s careful summary of our paper and thoughtful comments.\n\nThe reviewer’s primary concern was the lack of clarity. This was shared with the other reviewers, so we have made a number of edits to the paper in order to address these concerns. See the general comment for more details. We will focus here on addressing the reviewer’s specific comments. We would particularly like to bring attention to the reviewer's point 7. Here there was concern we were overreaching with our claims, in fact the claim we were making is weaker but was not stated clearly. We have amended the text and explained the true claim below.\n\n1 - We thank the reviewer for this suggestion and have added an explicit subsection for notation and setup.\n\n2 - We have made it clearer in the text that we are referring to the accuracy of a randomised classifier. This does not limit the extensibility of our work to non-randomised classifiers in our view. Firstly, the randomised accuracy of a non-randomised classifier is still a valid quantity of interest, blending accuracy and model confidence in a single metric. Secondly the value function can be adjusted to use model predictions instead of model predicted probabilities. In this case the Shapley values sum to accuracy and demographic parity respectively. These value functions are non-linear, which compromises the linearity axiom, hence we opted for the value functions described in the paper.\n\n3 - The Shapley values sum to randomised accuracy which depends both on accuracy and the model confidence. Accuracy and confidence of course interact via calibration. Consider two 100% accurate models, one which predicts with 100% confidence (and hence is perfectly calibrated) and another which predicts with 51% confidence always. Though these models have the same accuracy, the latter will have a randomised accuracy of only 51%. So calibration will have an effect, but it is better to think of randomised accuracy via accuracy and confidence.\n\n4 - Tables 1 and 2 show the accuracy of perturbed models on two test datasets, with comparisons to several baselines. The tables show that the perturbed models generalised well to unseen data.\n\n5 - Leveraging the additive property is indeed the point. It allows us to construct three related models on the data with consistent explanations, that together help us understand unfairness from multiple perspectives: sources of unfairness for a model trained without intervention, the change that was required to make the model fair, and fairness within the corrected model.\n\n6 - This is an interesting question. We did experiment with models trained without access to the protected attribute. As one might expect, the original model is a little fairer, and the correction is a little less effective. Ultimately we decided to use the current experiment in the main body of the paper. However since the same experiment without the protected attribute is of interest we have included a figure and discussion in the supplementary material.\n\n7 - This is not a claim we intended to make. When we say there is “no loss in accuracy” we mean that applying training-time fairness algorithms to an additive correction performs as well as applying the same training-time fairness algorithm to a new, unconstrained model. We have clarified this in the text to avoid future confusion.\n\nWe believe the submitted improvements address the primary concern of a lack of clarity, and hope that the reviewer agrees.", "Quality\n- This paper has defined a well-scoped problem: explaining the unfairness of an ML model in terms of the features used, as well as explaining an additive perturbation that will make the model more “fair”.\n- This paper acknowledges that definitions of fairness can be fraught and should be applied with care and domain knowledge. Therefore it has proposed an “explanation” procedure that works with multiple statistical definitions of fairness. This is a strength of the work.\n\n\nClarity\n- Clarity of the “section 2” could be improved. What is the support of $a$, $y$ and $f(x)$?\n\nOriginality\n- There has not been a lot of work on fairness and explainability. As one of the first forays into these questions showing a positive result (as claimed in introduction, and also to my best knowledge), I think the paper is sufficiently original. Even though it is adapting a well-known construct (Shapley value), its originality also lies in connecting explanations to work on post-hoc fairness corrections (section 2.2).\n\nSignificance\n- As stated above, I think the paper makes a significant contribution to research on fairness and explainability by developing some basic tools to help with “explaining” group fairness issues with ML models. \n- That being said, I think one must be careful to claim that the explanations offered by shapley value suggest any particular intervention. For example, it is not true that the features that contribute most to unfairness should be always be removed. I think a careful discussion on how to use the insights from the shapley values in practice, or some open questions regarding the interpretation of the shapley values would improve this paper.", "This paper presents a method for feature attribution for fairness of the classifier.  They also demonstrate a feature augmentation technique to mitigate unfairness.  They connect their attribution method to to the augmentation technique and demonstrate that their method can attribute the necessary changes to achieve fairness. They evaluate their approach on a few tabular data sets.\n\nIntroduction + Methods\n\n- I’m slightly confused about equation (2), is the correct interpretation that y is the label we’re explaining? So, say, if we’re explaining the -1 class and f(x) is 0.25, then f_y(x) yields 0.75? \n- I think some clarity on the notation would be useful in general.  Is y the ground truth label from the data? That seems to be described this way in section 1.  However, in equation (2) is seems to refer to the label we’re selecting to explain, which feels slightly different.  I think some of this notation could be cleared up a bit.\n- “Splicing disjoint sets of features” is this common terminology? I think what this is trying to say is that this step is where we substitute values from the data into a point to create a perturbation.  Let me know if I am correct.\n- For the description for equation (5): “…the expected accuracy for a model which samples a predicted label according to the predicted probability” what does this mean? The notation in equation (5) looks like it’s the expected prediction of the model, where is the relation to accuracy here? I’m not quite seeing this.\n-  I think that equation (5) could be clarified in general.  It would be good to explicitly state this significance of this property.  (It’s stated clearly for equation 9, and stating it explicitly here would help readers)\n- Is g a value function? If so, it’d be useful to state this leading up to the equation. Also, we’ve introduced the term “a” at this point which refers to the protected attribute (going back to the introduction).  What values can a take on? Are we assuming $a \\in \\{ 0, 1\\}? It would be good to clarify this. Last what is p(a) meant to refer to? Is this the overall proportion of individuals with a certain protected attribute? If so, why do we need this term?  \n- “The linearity axiom of the Shapley values guarantees that the fairness Shapley values of a linear ensemble of models are the corresponding linear combination of Shapley values of the underlying models” — it would be good to clarify this sentence as it’s slightly confusing right now.\n- For equation (10) assuming that f Is a linear model, is $\\delta_\\theta$ meant to be a vector of values added to each of it’s coefficients?\n- Is is slightly contradictory that in equation (10) $\\delta_\\theta$ is a perturbation (and can be explicitly added to f) but then becomes a function in equation (11)? I get what is being said but perhaps the notation could be improved here.\n\nExperiments\n- Figure 1 is hard to read, can you make the labels bigger for each graph. Also, it might be good to place the y axis values on each graph so that its easier to see the scale.  I see what the graph is saying, but this would make it much easier to figure out.\n- I think figure 1 is a nice visualization overall and gets the point across well.\n- I’m not fully understanding the point being made by section 3.2.. Is the idea that we know this suppressed model doesn’t rely on sex at all, so the fairness Shapley values shouldn’t say it does? \n\nGeneral Questions + Comments:\n- I liked the ideas presented by this paper overall.  The idea of using Shapley values to provide feature attributions for fairness is interesting — particularly the applications for explaining the differences between two models, one of which has been corrected for fairness.\n- I do feel however that the paper can be improved in a number of places in order to strengthen the work as mentioned in my comments.  It would be very useful if the authors provided answers to some of my questions and took some of the comments into consideration for a revision. Particularly, I'm somewhat confused about the contribution of section 3.2 and would appreciate clarification.\n- One related question is that the authors demonstrate their method on the suppression techniques from Dimanov et al.  Would their techniques help at all with the related methods (more so phrased as attacks) from https://arxiv.org/abs/1911.02508?\n- Overall, my sentiments right now are borderline, leading towards reject. There’s a number of points which could warrant further clarification right now in the paper.  "], "review_score_variance": 0.22222222222222224, "summary": "All the reviewers found interesting the use of Shapley values to provide feature attributions for fairness, however, the reviewers brought up a number of issues, particularly in terms of presentation and clarity. While the authors' responses did clarify some of these concerns, this was not enough for the reviewers to broadly support acceptance.", "paper_id": "iclr_2021_cFpWC6ZMtmj", "label": "train", "paper_acceptance": "withdrawn-rejected-submissions"}
{"source_documents": ["Deep autoregressive models are one of the most powerful models that exist today which achieve state-of-the-art bits per dim. However, they lie at a strict disadvantage when it comes to controlled sample generation compared to latent variable models. Latent variable models such as VAEs and normalizing flows allow meaningful semantic manipulations in latent space, which autoregressive models do not have. In this paper, we propose using Fisher scores as a method to extract embeddings from an autoregressive model to use for interpolation and show that our method provides more meaningful sample manipulation compared to alternate embeddings such as network activations.", "Motivated by the observation that powerful deep autoregressive models such as PixelCNNs lack the ability to produce semantically meaningful latent embeddings and generate visually appealing interpolated images by latent representation manipulations, this paper proposes using Fisher scores projected to a reasonably low-dimensional space as latent embeddings for image manipulations. A decoder based on a CNN, a Conditional RealNVP, or a Conditional Pyramid PixelCNN is used to decode high-dimensional images from these projected Fisher score.  Experiments with different autoregressive and decoder architectures are conducted on MNIST and CelebA datasets are conducted. \n\nPros:\n\nThis paper is well-written overall and the method is clearly presented.\n\n\nCons:\n\n1) It is well-known that the latent activations of deep autoregressive models don’t contain much semantically meaningful information. It is very obvious that either a CNN decoder, a conditional RealNVP decoder, or a conditional Pyramid PixelCNN decoder conditioned on projected Fisher scores will produce better images because the Fisher scores simply contain much more information about the images than the latent activations. When the $\\alpha$ is small, the learned decoder will function similarly to the original pixelCNN, therefore, latent activations produce smaller FID scores than projected Fisher scores for small $\\alpha$’s. These results are not surprising. Detailed explanations should be added here.\n\n2) The comparisons to baselines are unfair. As mentioned in 1), it’s obvious that Fisher scores contain more information than latent activations for deep autoregressive models and are better suited for manipulations. Fair comparisons should be performed against other latent variable models such as flow models and VAEs with more interesting tasks, which will make the paper much stronger.\n\n3) In Figure 3, how is the reconstruction error calculated? It’s squared error per pixel per image?\n\n4) On pp. 8, for semantic manipulations, some quantitative evaluations will strengthen this part.\n\nIn summary, this paper proposes a novel method based on projected Fisher scores for performing semantically meaningful image manipulations under the framework of deep autoregressive models. However, the experiments are not well-designed and the results are unconvincing. I like the idea proposed in the paper and strongly encourage the authors to seriously address the raised questions regarding experiments and comparisons.\n\n------------------\nAfter Rebuttal:\n\nI took back what I said. It's not that obvious that the \"latent activations of deep autoregressive models don’t contain much semantically meaningful information\". But the latent activations are indeed a weak baseline considering that PixelCNN is so powerful a generator. If the autoregressive generator is powerful enough, the latent activations can theoretically encode nothing.  I have spent a lot of time reviewing this paper and related papers, the technical explanation about the hidden activation calculation of PixelCNN  used in this paper is unclear and lacking (please use equations not just words). \n\nRelated paper:  The PixelVAE paper ( https://openreview.net/pdf?id=BJKYvt5lg ) explains that PixelCNN doesn't learn a good hidden representation for downstream tasks\n\nAnother paper combining VAE and PixelCNN also mentions this point:\n\nECML 2018: http://www.ecmlpkdd2018.org/wp-content/uploads/2018/09/455.pdf\n\nPlease also check the related arguments about PixcelCNN (and the \"Unconditional Decoder\" results) in Variational Lossy Autoencoder (https://arxiv.org/pdf/1611.02731.pdf )\n\nAs I mentioned in the response to the authors' rebuttal, training a separate powerful conditional generative model from some useful condition information (Fisher scores) is feasible to capture the global information in the condition, which is obvious to me. This separate powerful decoder has nothing to do with PixelCNN, which is the major reason that I vote reject.\n", "\"> The paper uses the activation of the last layers of the PixelCNN as a baseline, which I consider to be a very weak baseline:   Our paper focuses on the question: is it possible to perform image interpolation using autoregressive models? Our experiments were designed to show that project Fisher scores provide better and more meaningful image manipulations than any other alternative latent spaces inherent to an autoregressive model, such as layer activations and noise space. In particular, the goal of our work is not to develop a new representation learning method; it is to develop an image interpolation method using autoregressive models. We believe that this is an interesting avenue of research because autoregressive models are not clearly suited for image interpolation, unlike VAEs and flows which have a clearly usable latent space.\"\n\nThe response from the authors to Review #4 is also highly related to my question. My major concern still remains. Since PixelCNN can have impressive performance as a generative model, it is really unsurprising that it can capture global semantic information considering its parameters as a whole and the associated Fisher scores. This paper just wants to prove this unsurprising point using a separate powerful decoder.\n\nThe authors also argue that \"Our paper does not focus on solving unsupervised learning using Fisher scores. Rather, it focuses on a novel method using Fisher scores to perform image manipulation using autoregressive models. We believe that a metric designed to evaluate interpolations is a more direct quantitative measurement of image manipulation quality, compared to standard representation learning evaluation methods.\"\n\nI still feel that this argument is not enough to convince me that showing the parameters of PixelCNN as a whole can capture global semantic information is interesting. \n\nTo summarize, from a reviewer's perspective, showing A can do B is not interesting (it's unsurprising); many other C, D, E, F, G can do B too; if you want to convince me, it's better to show that A can do B much better than C, D, E, F, G, which might seem to be controversial. But indeed, training a powerful conditional generative model from some useful condition information is feasible to capture the global information in the condition, which is obvious to me. ", "Dear reviewers,\n\nThank you very much for your efforts in reviewing this paper.\n\nThe authors have provided their rebuttal. It would be great if you take a look at them, and see whether it changes your opinion in anyway. If there is still any unclear point or a serious disagreement, please bring it up. Also if you are hoping to see a specific change or clarification in the paper before you update your score, please mention it.\n\nThe authors have only until November 15th to reply back.\n\nI also encourage you to take a look at each others’ reviews. There might be a remark in other reviews that changes your opinion.\n\nThank you,\nArea Chair", "Thank you for your review. \n\n> 1) It is well-known that the latent activations of deep autoregressive models don’t contain much semantically meaningful information.\n\nTo the best of our knowledge, we do not know of any existing work that shows this property in deep autoregressive models. We would appreciate if you could provide references to such existing work.\n\nWe would like to emphasize that our paper targets the problem of image manipulation with autoregressive models, and is not aiming to solve a representation learning problem. Therefore, we designed our experiments to show that Fisher scores are a better latent space than alternative latents in autoregressive models. Among possible latent spaces that could be extracted from autoregressive models, we found layer activations to be the strongest candidate. We believe such a choice is justified, as layer activations are also commonly used in many prior self-supervised learning methods [3] [4].\n\n> 2) It’s obvious that Fisher scores contain more information than latent activations for deep autoregressive models and are better suited for manipulations\n\nWe agree that Fisher scores do contain more information than latent activations. However, we believe that it is not obvious that projected Fisher scores provides a latent space which entails more meaningful semantic manipulations for global attributes such as pose, hair color, and lighting. Experiments in [2] showed that smaller receptive fields attain competitive log-likelihood scores, suggesting that PixelCNNs model low-level statistics about the data. However, our experiments with using Fisher scores as an embedding space to interpolate show otherwise to common knowledge.\n\n> comparisons should be performed against other latent variable models\n\nOur paper aims to show that image manipulation is possible using autoregressive models, despite the lack of easily accessible latents such as those in VAEs or flow models. We believe that it is more meaningful to compare our method against existing and alternative methods that perform image manipulation using an autoregressive model, which motivates our decision as using layer activations for our baseline method.\n\n> 3) how is reconstruction error calculated\n\nThe decoder was trained to model discrete pixels values, so the reconstruction error is negative log-likelihood (nats per dim).\n\n> 4) for semantic manipulations, some quantitative evaluations will strengthen this part\n\nTo the best of our knowledge, we do not know of a method to effectively quantitatively evaluate the quality of semantic manipulations. We experimented with a few different evaluation metrics, such as using binary classifiers in a method similar to [1], however, none showed meaningful results as an evaluation metric. Training binary classifiers on CelebA attributes proved too easy, and provided no discernable difference between semantic manipulations using different embedding spaces even when there was a clear visual difference. We were also unable to use the same FID evaluating metric as interpolations, since semantic manipulations by design conditionally generate images out of distribution, whereas FID compares datasets of the same distribution.\n\n[1] Ravuri, Suman, and Oriol Vinyals. \"Classification Accuracy Score for Conditional Generative Models.\" arXiv preprint arXiv:1905.10887 (2019).\n[2] Salimans, Tim, et al. \"Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications.\" arXiv preprint arXiv:1701.05517 (2017).\n[3] Noroozi, Mehdi, and Paolo Favaro. \"Unsupervised learning of visual representations by solving jigsaw puzzles.\" European Conference on Computer Vision. Springer, Cham, 2016.\n[4] Gidaris, Spyros, Praveer Singh, and Nikos Komodakis. \"Unsupervised representation learning by predicting image rotations.\" arXiv preprint arXiv:1803.07728 (2018).\n", "Thank you for your review. We would like to emphasize that the novelty of our work is showing the Fisher scores of PixelCNNs do indeed contain high-level semantic information about the data, which we believe is interesting given the common knowledge that PixelCNNs tend to model only low-level statistics of the data. Prior experiments such as in [2] showed that PixelCNNs with small receptive fields maintain log-likelihood performance close to PixelCNNs with large receptive fields, suggesting that PixelCNNs focus on modeling low level statistics about the data, rather than global semantically meaningful information.\n\nWe show that contrary to this common knowledge, PixelCNNs actually do contain global semantically meaningful information — although to access this information one must be careful to extract it in the correct way.  Our experiments show that naively interpolating using PixelCNN layer activations is not the correct way, as it results in non-meaningful interpolations. Surprisingly, however, we show that interpolating in Fisher score space does result in semantically meaningful interpolations. We believe that this demonstrates that PixelCNNs do capture high-level information about the data, contrary to the common knowledge that they only capture low-level statistics.\n\n> The paper uses the activation of the last layers of the PixelCNN as a baseline, which I consider to be a very weak baseline\n\nOur paper focuses on the question: is it possible to perform image interpolation using autoregressive models? Our experiments were designed to show that project Fisher scores provide better and more meaningful image manipulations than any other alternative latent spaces inherent to an autoregressive model, such as layer activations and noise space. In particular, the goal of our work is not to develop a new representation learning method; it is to develop an image interpolation method using autoregressive models. We believe that this is an interesting avenue of research because autoregressive models are not clearly suited for image interpolation, unlike VAEs and flows which have a clearly usable latent space.\n\n> For the evaluation, the paper only considers the FID score on the interpolated images and reconstructions. There are much better ways to compare the quality of unsupervised representations …\n\nWe designed our evaluation metric with the intention to quantitatively show that interpolations using Fisher scores are better than those of using alternative embedding spaces of an autoregressive model. Empirically, using FID on interpolated datasets showed a strong correlation between FID scores and interpolation quality. \n\nOur paper does not focus on solving unsupervised learning using Fisher scores. Rather, it focuses on a novel method using Fisher scores to perform image manipulation using autoregressive models. We believe that a metric designed to evaluate interpolations is a more direct quantitative measurement of image manipulation quality, compared to standard representation learning evaluation methods.\n", "This paper focuses on the problem of interpolating between data points using neural autoregressive models. The core idea is that it is possible to use (a smaller-dimensional projection of) the Fisher score of the density function defined by the autoregressive model to represent data points in embedding space, and a neural decoder for mapping them back to input space. Experiments on both MNIST and Celeb suggest that this is a sensible method, and leads to smoother interpolations rather than just relying on the embeddings resulting from the network activations.\n\nMinor: the FID acronym on pg. 2 was not introduced beforehand.\n", "This work proposes to learn a latent space for the PixelCNN by first computing the Fisher score of the PixelCNN model and then projecting it onto a lower-dimensional space using a sparse random matrix.\n\nMy first concern about this work is its novelty. Defining a feature space using the Fisher kernel of a generative model is a very well-known idea, and there is a large body of work around that. As the paper points out, the problem with the Fisher score for the recent deep generative model architectures is that Fisher score operates in the parameter space and the deep models have a very large number of parameters. The paper proposes to get around this problem by projecting the Fisher score onto a lower-dimensional space using random matrices. But I am not convinced that this random projection can learn useful representations, which brings me to my second concern about the evaluation metric. The paper uses the activation of the last layers of the PixelCNN as a baseline, which I consider to be a very weak baseline. Each activation at spatial position (i,j) only depends on the previous pixels and I believe they are not in general good high-level representations. For the evaluation, the paper only considers the FID score on the interpolated images and reconstructions. There are much better ways to compare the quality of unsupervised representations such as their performance on classifying images with a linear classifier as done in [1]. The paper would improve by comparing the quality of its latent representations with the recent unsupervised/self-supervised learning methods such as [1,2].\n\n[1] Data-Efficient Image Recognition with Contrastive Predictive Coding\n[2] Learning deep representations by mutual information estimation and maximization"], "review_score_variance": 8.666666666666666, "summary": "The paper proposes learning a latent embedding for image manipulation for PixelCNN by using Fisher scores projected to a low-dimensional space.\nThe reviewers have several concerns about this paper:\n* Novelty\n* Random projection doesn’t learn useful representation\n* Weak evaluations\nSince two expert reviewers are negative about this paper, I cannot recommend acceptance at this stage.\n", "paper_id": "iclr_2020_Bye8hREtvB", "label": "train", "paper_acceptance": "reject"}
{"source_documents": ["There has long been debates on how we could interpret neural networks and understand the decisions our models make. Specifically, why deep neural networks tend to be error-prone when dealing with samples that output low softmax scores. We present an efficient approach to measure the confidence of decision-making steps by statistically investigating each unit's contribution to that decision. Instead of focusing on how the models react on datasets, we study the datasets themselves given a pre-trained model. Our approach is capable of assigning a score to each sample within a dataset that measures the frequency of occurrence of that sample's chain of activation. We demonstrate with experiments that our method could select useful samples to improve deep neural networks in a semi-supervised leaning setting.", "The idea of calculating a score to indicate the usefulness of a sample for training deep networks by analyzing the neural activations in semi-supervised learning is interesting.\n\nHowever, the effectiveness of the proposed method is not validated. In the cifar-10 semi-supervised image classification experiment, other semi-supervised learning methods are not compared. In my experiments, simply applying the trained model in the labeled data to obtain pseudo labels on the unlabeled data can obtain significant improvements.\n\nTheoretically, the proposed scoring method uses the pre-trained model to obtain correct activations that has two problems: (1) The evolving power that may be produced by the unlabeled data is constrained. (2) If there are a few numbers of labeled examples, it is very hard to learn a network; thus, the correct activation is not reliable. ", "This paper proposes image score, to use the amount of strong activations in each single image compared with the average activation on the entire dataset as a metric of how well the image is interpreted by a particular deep model. I am not sure whether that intuition makes sense, and there seem to be a lot of ways the simplistic equation can be broken. Section 2.3 shows some intuition of a higher score corresponding to higher testing accuracy, but somehow is done only on 1 class in CIFAR and I don't think that is generalizable to other classes and especially other classification problems.\n\nThe paper claims interpretability but I don't see any experiments verifying interpretability. The experiments are done on a semi-supervised learning task, where the \"image score\" is used to select some unlabeled examples as labeled by trusting a partially trained classifier. Hence we would have to evaluate it as a semi-supervised deep learning paper. Note that the amount of labeled examples in this paper is significantly higher than most semi-supervised approaches, which leaves the question that if extremely few supervised examples have been used, whether this approach will already fail. Although the model showed some improvements over the baseline, there has been no comparison at all with any existing semi-supervised deep learning approaches. Any semi-supervised learning approach usually outperforms the supervised baseline (used in this paper) by a bit, so I don't quite seem to believe that the results reported in this paper is significant enough. One can refer to the following paper for a few relatively new semi-supervised learning approaches:\n\nhttps://arxiv.org/pdf/1804.09170.pdf\n\nTable 4 is more baffling and less convincing. Because deep networks are volatile, it is hard to show this kind of result and hope people will be convinced. I would rather the author has trained both approaches to completion and then compare the end result. Also we still need comparisons with state-of-the-art semi-supervised learning approaches.", "This paper proposes a new metric called the image score that compares the similarity of activation between a given image with a pool of groundtruth images. The paper finds it useful for semi-supervised learning with self-teaching, where the network picks the most confident sample and use the network prediction as the label. It finds that the proposed method is better than 1) not using the unlabeled data and 2) using softmax as an indicator for model prediction certainty.\n\nMotivation: The introduction begins by motivating the interpretability story of deep learning, but I don’t see gaining any more interpretability by reading the rest of the paper. The paper proposes to improve interpretability by assigning a score to each individual example, but then the obtained scores are not properly analyzed in the paper, and only final classification accuracy is evaluated. What are the training samples that makes the model make certain decision at test time? How to measure the correlation between the usefulness of training samples and the proposed image score? These questions left unanswered in the paper. Figure 1 helps a little bit, but then the top row is not necessarily the bad images, but maybe hard examples that needs extra attention to learn. Therefore, I think the end results presented in the experiments do not align with the motivation. Rather than shooting for interpretability, this is just another semi-supervised learning paper.\n\nModels: The major issue of this paper is the model formulation that is not well motivated. The intuition of how the authors come up with the equation for computing the image score is not well explained. Hence the formulation seems very ad-hoc, and it is unclear why this is the selected method.\n\nExperiments: As a semi-supervised learning paper, a common setting for CIFAR-10 is to use 4k labeled images. Here, the method uses 30k, which is 7.5x the size of the usual setting. It also does not compare to prior semi-supervised learning work (e.g. one of the recent one is: https://arxiv.org/abs/1711.00258). The only two baselines discussed here are weak. Also the improvement from the baselines by using the proposed method is not very significant.\n\nComparison: Figure 2-4 shows some positive correlation between the accuracy and score, which is fair, but it doesn’t compare to any baselines--the only one we have is softmax baseline and it is not shown in the figure.\n\nIn conclusion, I couldn’t see how the paper improves interpretability as claimed in the introduction. The proposed method seems ad-hoc, without any justification. Being considered as a semi-supervised learning paper, it lack significant amount of comparison to prior work and adopting a common semi-supervised benchmark. Due to the above reasons, I recommend reject.\n\n---\nMinor points:\n“...almost all of the existed works investigate only the models and ignore the relationship between models and samples”. This is over-exaggerated. I believe most of the visualization techniques are dependent on the actual input samples. It is true to say about “training samples” not “samples” in general.\n\n“all correctly classified images should have similar chain of activation, while incorrectly classified images should have very different activations both within themselves and with correctly classified images”. This claim seems not backed up. How do you know it is the case for “all” correctly classified images? What defines similar/different?"], "review_score_variance": 0.22222222222222224, "summary": "Reviewers are in full agreement for rejection.", "paper_id": "iclr_2019_HJeNIjA5Y7", "label": "val", "paper_acceptance": "rejected-papers"}
{"source_documents": ["Most unsupervised neural networks training methods concern generative models, deep clustering, pretraining or some form of representation learning. We rather deal in this work with unsupervised training of the final classification stage of a standard deep learning stack, with a focus on two types of methods: unsupervised-supervised risk approximations and one-class models. We derive a new analytical solution for the former and identify and analyze its similarity with the latter.\n      We apply and validate the proposed approach on multiple experimental conditions, in particular on four challenging recent Natural Language Processing tasks as well as on an anomaly detection task, where it improves over state-of-the-art models.", "UPDATE:\nI acknowledge that I’ve read the author responses as well as other reviews. \nAfter reading, I would keep my rating at 3 (Weak Reject), since the key reasons for my rating still hold.\n\n####################\n\nThis work makes a connection between recently introduced one-class neural networks [8, 4] and the unsupervised approximation of the binary classifier risk under the hinge loss [1]. An explicit expression of this risk approximation is derived for the case that the prior class probabilities are known and that the class-conditional distributions of classification scores are Gaussian. This solution is then used to formulate an end-to-end differentiable loss for unsupervised binary classification which is combined with a posterior class probability regularizer to avoid trivial solutions. Finally, the paper presents an experimental evaluation on synthetic data, the Wisconsin Breast Cancer dataset, four NLP tasks, as well as on the anomaly detection task on MNIST where the proposed method slightly outperforms the two existing one-class networks [8, 4].\n\nI think this paper makes an interesting, original connection between unsupervised-supervised risk estimation and one-class neural networks which provides a principled motivation for existing methods [8, 4] and also hints to potential flaws in their formulations, namely that OC-NN [4] and soft-boundary Deep SVDD [8] make no use of positive samples during learning (as illustrated in Figure 2). The paper is not yet ready for acceptance in my opinion, however, due to the following key reasons: \n(i) The experimental evaluation is not convincing and not sufficient to assess the significance of results; \n(ii) Though making this connection is interesting, the theoretical derivations presented in the paper are rather straightforward.\n\n(i) I think the experimental evaluation is the weakest part of the paper at the moment which I find not convincing due to the lack of competitors, the use of rather simple datasets, and missing experimental details. The synthetic experiment only serves as a sanity check not giving any additional insights. As the proposed unsupervised method approximates the risk of a supervised binary classifier, I agree that it makes sense to compare to the supervised “gold standard” on binary classification tasks (Wisconsin and NLP sentiment tasks) to infer the unsupervised-supervised performance gap. However, there is no comparison to other unsupervised competitors (OC-SVM, GMM, Deep SVDD, OC-NN, etc.) to put the performance of the proposed method in these experiments into perspective (only K-Means does not establish a strong baseline). Those classification tasks further are not really a convincing use case in my mind since labels here are usually available. In contrast, I find anomaly detection to be an important application of this method, but an evaluation solely on MNIST that also lacks recent deep competitors [6] is not sufficient to assess the significance of the presented results. Moreover, from the text it seems that only the hyperparameters of the proposed method are tuned on some validation set that includes positive as well as negative samples which would be an unfair advantage and might explain the slight edge in performance. Finally, many experimental details are not reported: (ia) Are the networks used randomly initialized or pretrained? (ib) How are prior class probabilities set? (ic) What are the batch sizes (relevant for quantile estimation) (id) What score are the hyperparameters tuned on? (ie) Are negative samples in the validation set from all the anomaly classes?\n\n(ii) The technical derivations in the paper (and the appendix) are correct but rather straightforward. The theoretical heavy-lifting is from Balasubramanian et al. [1] and this paper presents an explicit solution for the risk approximation under the assumption that the prior class probabilities are known and the class-conditional score distributions are Gaussian. I do not want to discount that making this connection and loss derivation may lead to significant results (which is left to be demonstrated experimentally), but I find the current theoretical contribution on its own not sufficient. Parts of the theoretical Section 3 could also be greatly cut in my opinion (no need to define a quantile or sample mean and variance etc.). Finally, one key property of the loss expressed in the paper is its differentiability and use with autograd, but this does not hold after adding the posterior regularization term which is based on the empirical p0-quantile, correct? (the gradient of the quantile is zero almost everywhere due to the argmin)\n\nApart from the two key points above, the presentation of the paper is unpolished (nested lists in the main text, etc.) and major deep anomaly detection related work [10, 6, 5, 7, 3] is missing.\n\n\n####################\n*Additional Feedback*\n\n*Positive Highlights*\n1. The paper makes an interesting connection between unsupervised-supervised risk approximation [1] and recently introduced one-class neural networks [8, 4] that provides a principled motivation and points to potential flaws in existing formulations.\n2. I think the question how to learn neural classifiers in an unsupervised manner is original and interesting.\n3. The technical derivations in the paper are rigorous and correct.\n\n*Ideas for Improvement*\n4. Make a comparison to state-of-the-art unsupervised deep anomaly detection (AD) methods.\n5. Run AD experiments on more complex datasets like Fashion-MNIST, CIFAR-10, and the recently introduced MVTec [2].\n6. Include major deep AD works [10, 6, 5, 7, 3] into the related work.\n7. Compress Sections 1–3 (fewer lists; no need to give definitions of a quantile, sample mean and variance; etc.)\n8. Motivate the non-AD experiments. Currently these appear rather constructed artificially.\n9. In the NLP tasks, make a comparison to text-specific one-class classifiers [9].\n10. Add a sensitivity analysis w.r.t. the prior class probability p0 to infer robustness to this parameter that seems crucial.\n11. Provide guidance how to select p0 in a particular application.\n12. Consistently report performance metrics with standard deviations in your tables to allow to infer statistical\nsignificance.\n13. What to do if there are no negative, but only normal samples as in fully unsupervised AD? Nevertheless make an assumption on the class prior?\n\n*Minor comments*\n14. Unordered lists should be used sparsely in a main text, stylistically speaking. Avoid nesting as in the introduction.\n15. Section 2.1, first sentence (and elsewhere): “Let be given ...” is grammatically wrong. Correct would be either “Let f be a binary linear classifier ...” or “Given a binary linear classifier f ...”.\n16. In Section 2.1, index the classifier $f$ with parameter $\\theta$, i.e. $f_\\theta$. Otherwise the risk optimization parameter $\\theta$ does not even appear on the right hand side of the risk Eq. (1).\n17. Combine Eqs. (1) and (2) into one equation.\n18. Consistently enumerate equations throughout the paper or do not enumerate at all.\n19. In Eq. (3) index $i$ misses in the sum.\n20. A subsection title following a section title directly is bad style. A new major section should at least be introduced with a few sentences on what this section is about.\n21. Mention that $erf$ is the Gaussian error function.\n22. Center the equation in Section 3.2.\n23. Plots is Figure 3 are rather poorly formatted: use thicker lines and more distinctive colors; place the legend legibly.\n24. Show the average with confidence intervals over the 10 runs in Figure 3.\n25. Put results, as in Section 4.2 on the Wisconsin Breast Cancer dataset rather in a table.\n\n\n####################\n*References*\n[1] K. Balasubramanian, P. Donmez, and G. Lebanon. Unsupervised supervised learning ii: Margin-based classification without labels. Journal of Machine Learning Research, 12(Nov):3119–3145, 2011.\n[2] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger. Mvtec ad–a comprehensive real-world dataset for unsupervised anomaly detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9592–9600, 2019.\n[3] R. Chalapathy and S. Chawla. Deep learning for anomaly detection: A survey. arXiv preprint arXiv:1901.03407, 2019.\n[4] R. Chalapathy, A. K. Menon, and S. Chawla. Anomaly detection using one-class neural networks. arXiv preprint arXiv:1802.06360, 2018.\n[5] H. Choi, E. Jang, and A. A. Alemi. Waic, but why? generative ensembles for robust anomaly detection. arXiv preprint arXiv:1810.01392, 2018.\n[6] I. Golan and R. El-Yaniv. Deep anomaly detection using geometric transformations. In NIPS, 2018.\n[7] D. Hendrycks, M. Mazeika, and T. G. Dietterich. Deep anomaly detection with outlier exposure. In ICLR, 2019.\n[8] L. Ruff, R. A. Vandermeulen, N. Görnitz, L. Deecke, S. A. Siddiqui, A. Binder, E. Müller, and M. Kloft. Deep one-class classification. In International Conference on Machine Learning, pages 4393–4402, 2018.\n[9] L. Ruff, Y. Zemlyanskiy, R. Vandermeulen, T. Schnake, and M. Kloft. Self-attentive, multi-context one-class classification for unsupervised anomaly detection on text. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4061–4071, 2019.\n[10] T. Schlegl, P. Seeböck, S. M. Waldstein, U. Schmidt-Erfurth, and G. Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In Proceedings International Conference on Information Processing in Medical Imaging, pages 146–157. Springer, 2017.", "Thank you very much for your detailed review !\n- I agree the derivation is quite simple. But it's also the best and fastest implementation of Balasubramanian et al. paper published so far: it gives an immediate and exact value of the risk, an equation that can be studied, approximated and used in deep learning toolkits, and it enables to exhibit a connection with OC-models.\n- I agree to remove the synthetic experiments, and reduce Section 3 to make more room for detailed experimental setup and new experiments.\n- I agree anomaly detection is the most interesting application here, and I compare my work to Ruff et al. (ICML'2018), which is quite recent. Golan et al. (NIPS'2018) report better results, but with a method dedicated to images, while this work is generic.\n- The proposed approach does not assume that the training set only contains positive samples (which is difficult to guarantee in real conditions without labels), so I rather consider this as an advantage. But I agree that in that sense, it may be more related to time-series outlier-detection methods rather than image one-classification, according to (Chalapathy et al., 2019). That's why I've tested it on both types of data.\n- The gradient of the posterior regularization term is never zero, except when the median is exactly on the hyper-plane. But in general, this term represents the distance of the median to the hyperplane, and its gradient will tend to make the median closer to the hyperplane. When the median sample changes, this indeed corresponds to a discontinuity of the regularization term, but at every epoch, the local gradient exists and is in general non-null.\n", "Thank you for your time and review !\n- The main contribution is providing an exact loss for the unsupervised-supervised risk, which leads to an approximated form that strongly resembles the training objective of one-class models, hence establishing a connection between these two areas of research, although they were originally developed from very different fundamental hypotheses. I think such connections are always valuable in the long term, because they give researchers different points of view to analyze and interpret the behavior of methods from these families.\n- Thank you for the PR AUC reference, however in these experiments, I had to use the same evaluation metric than the state-of-the-art papers to enable comparison with them.\n- I didn't reproduce OC-NN results because they gave nearly the same results as deep-SVDD, except for 2 digits out of 10. But I agree this is another interesting comparison to do.\n- For Table 1, I've not found any unsupervised approach in the literature to compare to on SentEval. I could of course adapt other unsupervised methods to these corpora, but this is far less convincing, as when these methods give worse results than mine, one may think it is due to not-so-finely-tuned hyper-parameters, while the supervised code and model for this specific corpora were published by their authors.\n", "Thank you very much for your time and recommendations.\n- I agree the algorithm is simple, but one of the main contribution is connecting two previously unrelated research domains: unsupervised risk and one-class models.\n- I will improve experimental setup description and compare with generative models.\n- You're right: the gradient of the loss is easy to derive, and I will provide it in appendices.\n- p_0 is assumed to be known in the SentEval experiments: I agree it is a strong assumption, but that is reasonable in a number of practical situations. In the MNIST experiments, p_0 is set to the same value as the \"nu\" parameter of the deep-SVDD model.\n- Thank you very much for the references.\n- The Gaussianity assumption is verified in nearly all of the experiments I have realized, on other corpora and on SentEval and MNIST: I have actually plotted this distribution multiple times and it was always bi-modal. I didn't put such a plot here because of space constraints, but I agree it is important to add at least in annex.\n", "The anonymous authors consider the problem of training of classifiers in an unsupervised way. They propose an extension to a one-class based approach that can do anomaly detection in an unsupervised fashion.\n\nThe main contribution is a modification of the target function for the training of one-class NN. The experiments are not convincing and the modification doesn't seem to provide much inside into representation learning and anomaly detection area. \n\n1. Figure 3: no axis labels\n2. ROC AUC is not the best quality to measure the quality of imbalanced classification problems or anomaly detection, PR AUC (average precision) is better\n3. In Table 2 and other experiments, there is a comparison to only one existed method e.g. authors don't reproduce results for OC-NN\n4. Table 1: why compare a supervised method to an unsupervised one and don't compare to other methods?\n", "Paper summary:\n\nThis paper proposes an algorithm to train a binary classifier without supervision, simply relying on (i) class prior, (ii) the hypothesis that class conditional classifier scores are Gaussian distributed. Experiments over sentiment classification and anomaly detection highlight the effectiveness of the approach.\n\nReview Summary\n\nThe paper reads well and is technically correct. It proposes a simple algorithm for unsupervised training of binary classifier. Experiments are appropriate but lack baseline comparison with generative models and a thorough description of the unsupervised validation procedure. Overall, the approach is simple and it would be a good paper with the addition of baselines (mixture) and the clarification of the validation procedure.\n\nDetailed review:\n\nThe paper is divided into two parts: a closed form solution to the problem introduced by Balasubramanian et al 2011 and an algorithm leveraging class prior for training. Both parts are clear. Since the algorithm only requires the derivative of 5 wrt model parameters, could you write this derivative (it should yield a simpler expression without erf, no?).\n\nThe experimental study could discuss the robustness wrt to the choice of p_0 and report a grid of experiments over a training set with varying true and assumed p_0. In unsupervised classification, the problem of parameter validation always occurs, e.g. for senteval, how did you select p_0 and the other parameter of the model? If you used a labeled validation set, could you report its size? Could you report the performance of a supervised system trained on a set of that size? Could you report the performance of your method when fine tuned with the label of that validation set?\n\nFor the senteval experiments, it would be interesting to report the number of labels for which the supervised and unsupervised accuracy are equal. It would be more informative than simply reporting accuracy with 100 labels. When reporting the number of labels, you need to report the size of the training and validation set combined.\n\nAs far as baseline are concerned, it would be necessary to consider generative models such as mixture models and possibly mixture model with constraints on the mixing weights (Chauveau 2013).  Adding one class SVM (RBF, polynomial kernel) and one class neural nets (Ruff, Chalapati) for SentEval would be necessary to show the advantage of your approach. A curve number of training points accuracy for all models is a must have.\n\nYou could also cite related work in adaptation to new class prior, class prior estimation, see below:\n\nDidier Chauveau, David Hunter. ECM and MM algorithms for normal mixtures with constrained parameters. 2013. ffhal-00625285v2f\n\nAdjusting the Outputs of a Classifier to New a Priori Probabilities: A Simple Procedure\nM Saerens, P Latinne, C Decaestecker - Neural computation, 2002 - MIT Press\n\nSemi-Supervised Learning of Class Balance under Class-Prior Change by Distribution Matching\nMarthinus Christoffel du Plessis, Masashi Sugiyama, ICML12\n\nClass proportion estimation with application to multiclass anomaly rejection\nT Sanderson, C Scott - Artificial Intelligence and Statistics, 2014\n\nFinally, I feel that the assumption that model scores are Gaussian distributed would be strongly justified if you could plot the distribution/run a statistical test on whether it is the case for a supervised model with the same architecture as yours."], "review_score_variance": 2.0, "summary": "This paper makes a connection between one-class neural networks and the unsupervised approximation of the binary classifier risk under the hinge loss. An important contribution of the paper is the algorithm to train a binary classifier without supervision by using class prior and the hypothesis that class conditional classifier scores have normal distribution. The technical contribution of the paper is novel and brings an increased understanding into one-class neural networks. The equations and the modeling present in the paper are sound and the paper is well-written.\n\nHowever, in its current form, as pointed out by the reviewers, the experimental section is rather weak and can be substantially improved by adding extra experiments as suggested by reviewers #1, #2. Since its submission the paper has not yet been updated to incorporate these comments. Thus, for now, I recommend rejection of this paper, however on improvements I'm sure it can be a good contribution in other conferences.", "paper_id": "iclr_2020_S1x522NFvS", "label": "train", "paper_acceptance": "reject"}
{"source_documents": ["Incorporating prior knowledge in reinforcement learning algorithms is mainly an open question. Even when insights about the environment dynamics are available, reinforcement learning is traditionally used in a \\emph{tabula rasa} setting and must explore and learn everything from scratch. In this paper, we consider the problem of exploiting priors about action sequence equivalence: that is, when different sequences of actions produce the same effect. We propose a new local exploration strategy calibrated to minimize collisions and maximize new state visitations. We show that this strategy can be computed at little cost, by solving a convex optimization problem. By replacing the usual $\\epsilon$-greedy strategy in a DQN, we demonstrate its potential in several environments with various dynamic structures.", "This paper considers a property of MDPs where multiple sequences of states end up at the same action. The main contributions of this paper are (1) a formalization of the action sequence phenomenon, (2) an algorithm to build a local dynamics graph, (3) a simple exploration procedure based on the local dynamics graph.\n Strengths:\n\n-The paper formalizes an interesting phenomenon of equivalent action sequences that exists in some MDPs. \n\n-The method to construct a sampling policy that maximizes entropy is interesting. It uses a local dynamics graph built using action sequence equivalence knowledge  + convex optimization to  accomplish this.\n\n\nWeaknesses:\n\n-The formulation of equivalent action sequences could be simplified -- although nice, I’m not sure all of the formality is necessary here.\n\n-The method is designed for deterministic MDPs, in which case it is believable that the transitions are known. The authors also discuss an extension to the stochastic case, where equivalences are determined based on next state distributions. This seems nearly impossible in the model-free RL setting. Even in a dynamic programming setup where the stochastic transitions are fully known, it can be difficult to verify equality of distributions. Moreover, even if it were possible to verify, I cannot think of any problem where we’d expect equality of next state distributions between action sequences.\n\n-All experiments are performed on simple domains. It is unclear how applicable this method is in real problems.\n Overall, I am not convinced that the method is truly useful in real settings beyond the deterministic + spatial problems that the paper focuses on. The formalization of equivalent action sequences satisfied my curiosity but is perhaps slightly overkill and unnecessary. \n", " Thank you to the authors for providing the response. The new experiment on sticky actions partially addresses the concern on stochastic problems, but for general problems this seems to still be very challenging. In any case, I'd like to increase my score slightly. \n\nI believe the idea of the paper is interesting but am still not sure that the results are generalizable to real problems. To significantly improve the paper (esp. for a conference like ICLR), I believe some of formal setup could be replaced by more extensive experimental work to really prove the idea works.", " Thank you for the response and a revision of the paper. Now I understand the computation of the exploration distribution clearly. \n\nI still maintain that this is a nice paper that fills a gap, but I find it is not significant enough for ICLR. The number of domains where there are known equivalences seems limited and there is limited future work that could further improve on these results.", " We would like to thank all the reviewers for the time spent on our paper, and for the constructive comments provided. We have revised our submission in the ways detailed in our answers below. To make way for additions to the paper we removed some of the detail from section 4.1. We hope the updated version addresses your concerns.\n", " We thank the reviewer for the positive feedback on the proposed method. We provide additional remarks and clarifications in what follows.\n\nQ: I think that the paper would greatly benefit from a quick summary of what it does. In the introduction, the last paragraph gives a bit of guidance, but already a bit too technical in my opinion (\"Then, we show that priors on this type of structure can easily be exploited during offline exploration by solving a convex optimization problem\") for instance. I would put, either in the introduction or in a dedicated \"Overview\" section, an intuition of the steps that the algorithm takes.\n\nA: Thank you for your suggestion. We have added an overview at the end of the introduction.\n\nQ: Something else that could be made clearer is when the state-space has to be discretized. It seems that the DAG nodes don't consider states, but it is not that clear to me, and I have the feeling that explicitly mentioning what happens to the states in the Freeway example may help.\n\nA: It is perfectly right that DAG nodes are an abstract and discretized representation of the environment states. In the case of Freeway, which we modified during the rebuttal to add stochasticity in the dynamics using sticky actions [2], nodes would represent state distributions. But such (possibly complex) distributions fortunately do not need to be made explicit in order to apply EASEE: only the collisions between them are taken into account.\n\n[2]: Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents, Machado et al., IJCAI 2018.\n", " We thank the reviewer for their comments and additional references. We provide additional remarks and clarifications in what follows.\n\nQ: I see only a single extension, where the exploration strategies are learned: however this is already studied in prior work [1].\n\nA: Thank you for the additional reference that we added in the paper (Related Work: Improved Exploration). Although the objective of improving exploration is shared between [1] and our work, we feel the approaches differ significantly in spirit. Meta-training and prior knowledge assume very different access to resources and understanding regarding the environment. In many cases it may not be practical or feasible to develop the suite of similar tasks necessary for meta-learning. Moreover, [1] focuses on policy-based methods while our work is more readily used with value-based methods.\n\nQ: Additionally, the applicability seems to be limited to grid-like MDPs, where it is easy to hand-craft the action equivalences. For more general environments the improvement seems marginal (Figure 7).\n\nA: We complexified the environment Freeway by adding “sticky actions” (setting from [2]): with probability 0.25, the agent executes the previous action instead of the current desired action, which makes the dynamics stochastic. Our method worked out-of-the-box without the need of modifying any hyperparameters.\nThis shows that EASEE can perform well in complex environments. Thus, the poor performances in Fig.7 are likely due to poor prior knowledge, rather than the inherent complexity of these environments.\n\nQ: \"Ideally we would like each t-step state distribution to be uniform. However, depending on the exact local-dynamics graph this may or may not be possible.\" Why? Can you give an example of when this would not be possible?\n\nA: An example is provided in Figure 1: if the 2-step distribution is uniform there is a ⅙ chance of sampling each of the states 7, 8, and 9. Since states 7,8, and 9 are only accessible via state 3, this means that there must be a ½ chance of sampling state 3. But if the 1-step distribution is uniform then there is a ⅓ chance of sampling state 3. \n\nQ: I believe it should be straightforward to use uniform strategy in the abstracted MDP and translate it back to the original problem. Since this is much simpler than solving the optimization problem, it would be preferable, and a comparison to the max-entropy solution should be made.\n\nA: In cases where a uniform strategy exists in the abstract MDP, it could indeed be computed easily with a level-wise inductive approach, and it would lead to the same solution that the one computed with entropy-maximization. However, as shown in the previous answer, it is not necessarily the case. Thus, finding a better policy requires oversampling some actions and undersampling others in a way that is related to the local dynamics structure.\n\nQ: Action Space Structure: there is no explanation how the last 3 references relate to the work.\n\nA: These three papers make use of the structure of the action space to speed-up the learning procedure (one with curriculum, one with demonstrations, one with a self-supervised approach). We made the link with our method clearer in the paper. \n\n[2]: Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents, Machado et al., IJCAI 2018.\n", " We thank the reviewer for their comments. We provide additional remarks and clarifications in what follows.\n\nQ: The authors also discuss an extension to the stochastic case, where equivalences are determined based on next state distributions. This seems nearly impossible in the model-free RL setting.\n\nA: In a very general stochastic setting, it might be hard to have good priors on action sequence equivalences. Nevertheless, our method can still be used when priors are available. To showcase this, we modified the setting of Freeway (Sec. 5.4) to add “sticky actions” (setting from[2]): with probability 0.25, the next action is identical to the previous action. The environment is thus stochastic, although our previous priors remain approximately true. Training with the exact same hyperparameters as in the deterministic setting, we obtain similar results. We updated Fig 4.c using this new setting.\n\nQ: All experiments are performed on simple domains. It is unclear how applicable this method is in real problems.\n\nA: Our framework is explicitly built for discrete control problems, and we believe that a lot of complex problems can be formalized in this setting (e.g. the Atari suite). Freeway for example is a quite complex problem, due to the large observation space, and cars arriving randomly (and now stochastic dynamics due to sticky actions).\n\n[2]: Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents, Machado et al., IJCAI 2018.\n", "In the context of reinforcement learning, authors propose an exploration strategy based on environment-specific prior knowledge of action equivalence. An example of such equivalence is rotating 180° twice in a grid world, as the agent comes back to the same original state: the action sequence forms an identity in this case. The proposed exploration, instead of picking a random action with probability (\\epsilon)/(number of actions), builds an abstracted lookahead tree of specified depth that merges equivalent action sequences and adjusts the distribution of \\epsilon over them. Authors demonstrate this i) increases the number of unique state visitations in grid-world examples, and ii) DQN with their exploration strategy achieves a higher reward faster than the baseline.  The paper is written very clearly and the contributions are easy to follow. The idea of using explicit equivalences is fairly simple and I found it surprising this has not been done before (or at least, I couldn't find such a reference). I was thinking about how this work could be meaningfully built upon to improve a general zero-knowledge RL algorithm. I see only a single extension, where the exploration strategies are learned: however this is already studied in prior work [1]. The paper therefore seems to fill a \"void\" in the hand-crafted case with known priors. I don't find this sufficiently significant as its own contribution. Additionally, the applicability seems to be limited to grid-like MDPs, where it is easy to hand-craft the action-equivalences. For more general environments the improvement seems marginal (Figure 7). \nA suggestion how to make the work and empirical results more interesting: perhaps the problem can be cast as a Bayesian update using the hand-crafted priors with meta-learning [1].\n\nOther notes:\n\n- \"Ideally we would like each t-step state distribution to be uniform. However, depending on the exact local-dynamics graph this may or may not be possible.\" Why? Can you give an example when this would not be possible? I believe it should be straightforward to use uniform strategy in the abstracted MDP and translate it back to the original problem. Since this is much simpler than solving the optimization problem, it would be preferrable, and a comparison to the max-entropy solution should be made.\n- Action Space Structure: there is no explanation how the last 3 references relate to the work.\n\n\n[1] Meta-Reinforcement Learning of Structured Exploration Strategies, Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, Sergey Levine\n I recommend 5) marginally below the acceptance threshold. There is existing literature that tackles the problem of exploration in a more general way without handcrafting prior knowledge into the problem.", "The paper proposes a method that, from a simple encoding of sequences of actions that have equivalent outcome in an MDP, allows to compute a local policy for local high-quality exploration (it replaces the random action of $\\varepsilon$-greedy with an action that maximizes the entropy of future visited states). Full algorithmic details about how to do that are given in the paper, and experiments on a few environments show that the method is promising. The Freeway experiment is particularly interesting, as it shows that a (small) benefit can be obtained from the method even on an Atari game, with minimal domain knowledge (only a few equivalent action sequences are encoded), and using a slightly modified DQN algorithm. The idea proposed in the paper is quite elegant, albeit its realization is quite complex. The paper does a great job at explaining all the steps that go from equivalent sequences of actions, to an actual local policy that can be queried for an exploratory action. Overall, the paper is well-written, even if its complexity is sometimes a bit difficult for the reader.\n\nI think that the paper would greatly benefit from a quick summary of what it does. In the introduction, the last paragraph gives a bit of guidance, but already a bit too technical in my opinion (\"Then, we show that priors on this type of structure can easily be exploited during offline exploration by solving a convex optimization problem\") for instance. I would put, either in the introduction or in a dedicated \"Overview\" section, an intuition of the steps that the algorithm takes:\n\n- Assume that we have sets of equivalent actions sequences for the environment. Equivalent action sequences are sequences that lead to the same state (please correct me if I'm wrong)\n- These sequences are used, from a current state $s_t$, to build a DAG, that sorts of models where the agent will end up after any sequence of actions of length $d$. Because some sequences are equivalent, several parent nodes may share a child node. This is the information that we want to extract.\n- In a Q-Learning-like algorithm, every time-step, construct the DAG above for $s_t$, then, with probability $\\varepsilon$, execute an exploratory action $a_t$ that maximizes the entropy of the future visited states.\n\nA further intuition about the fact that the resulting exploratory policy will, basically, assign low probabilities to actions that share a same future outcome, may also help the reader understand why the proposed algorithm is beneficial to exploration.\n\nSomething else that could be made clearer is when the state-space has to be discretized. It seems that the DAG nodes don't consider states, but it is not that clear to me, and I have the feeling that explicitly mentioning what happens to the states in the Freeway example may help.\n\nOther than that, I think that the paper is highly novel and proposes a complicated (but justified) algorithm. As such, I recommend accepting it. The paper is relatively well-written, and proposes a sound algorithm that leads to good empirical results (even if the amount of empirical results is quite low). The algorithm is general enough to be applicable to an Atari game, with minimal engineering effort. The approach is quite novel, and multi-disciplinary (RL, information theory, convex optimization). As such, I think that the paper will be very interesting to people attending ICLR, and recommend accepting it."], "review_score_variance": 2.0, "summary": "This well-written paper introduces an improved exploration strategy by exploiting knowledge about sequences of actions that lead to the same state. The idea is straightforward and easy to understand and apply, which makes it potentially interesting. An important downside is the limited applicability of the method, as there mainly seems to be an advantage in (mostly deterministic) grid-like MDPs. In addition, priors about action-sequence equivalences have to be available. Overall, the contribution of the paper is not deemed significant enough for publication at a top-tier conference like ICLR by the majority of the reviewers as well as myself. For these reasons, I recommend rejection.", "paper_id": "iclr_2022_NeRrtif_hfa", "label": "train", "paper_acceptance": "Reject"}
