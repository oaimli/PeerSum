{"source_documents": ["As reinforcement learning agents become increasingly integrated into complex, real-world environments, designing for safety becomes a critical consideration. We specifically focus on researching scenarios where agents can cause undesired side effects while executing a policy on a primary task. Since one can define multiple tasks for a given environment dynamics, there are two important challenges. First, we need to abstract the concept of safety that applies broadly to that environment independent of the specific task being executed. Second, we need a mechanism for the abstracted notion of safety to modulate the actions of agents executing different policies to minimize their side-effects. In this work, we propose Safety Aware Reinforcement Learning (SARL) - a framework where a virtual safe agent modulates the actions of a main reward-based agent to minimize side effects. The safe agent learns a task-independent notion of safety for a given environment. The main agent is then trained with a regularization loss given by the distance between the native action probabilities of the two agents. Since the safe agent effectively abstracts a task-independent notion of safety via its action probabilities, it can be ported to modulate multiple policies solving different tasks within the given environment without further training. We contrast this with solutions that rely on task-specific regularization metrics and test our framework on the SafeLife Suite, based on Conway's Game of Life, comprising a number of complex tasks in dynamic environments. We show that our solution is able to match the performance of solutions that rely on task-specific side-effect penalties on both the primary and safety objectives while additionally providing the benefit of generalizability and portability. ", "**Update after authors' response**\nI want to thank the authors for their responses. My responses to the authors' comments are in the respective threads.\n---\n\n**Summary**\nThe paper addresses the timely and important problem of how to train RL agents such that they solve desired tasks while not engaging in undesired behavior that is not explicitly specified via the reward function. In particular, the paper focuses on training agents that learn to avoid unnecessary side effects, that is (irreversible) alterations to the environment which are not necessary to solve the task at hand. Experiments are performed on SafeLife, which provides a suite of tasks in an environment (potentially with rich intrinsic dynamics), along with a quantitative measure of the strength of undesired side effects. The main idea of the paper is to co-train an RL policy on this side effect measure with the aim of minimizing side effects. This policy is used for regularizing the reward-optimizing agent during training, such that the trained agent learns to bias its actions towards avoiding side effects when the task allows for multiple viable actions. The paper compares against a strong, previously reported baseline, both in static and dynamic SafeLife environments/tasks. Additionally, the generalization of the side-effect-avoiding policy is tested, by using it for training a reward-optimizing agent on task-versions that the side-effect-avoiding policy has not been trained on.\n\n---\n**Contributions, Novelty, Impact**\n\n1) Incorporation of the two (sometimes conflicting) objectives of maximizing reward and avoiding side-effects into a single training objective, where purely reward maximizing actions are regularized by action-distributions from a side-effect-minimizing policy. This is an interesting idea that turns trading off avoiding side effects against reward maximization into a learning problem. I think this is a promising way forward. What I’d like to see in the paper for even greater impact is a clear discussion of the requirements (the objective of avoiding side-effects must be specified as a trajectory-dependent, quantitative function, similar to a reward function), and the current limitations (unclear how to assess “how much” of the task-relevant state-space is well covered by the side-effect-avoiding policy, particularly in the zero-shot setting).\n\n2) Experimental evaluation of the proposed method on *the* state-of-the-art benchmark suite, and comparison against a strong, previously proposed baseline. The results are promising, though it’s hard to distill a very clear message in favor of the method from the results shown. I personally think that’s fine (and to some degree expected when discussing solutions that solve a particular trade-off in a different fashion), but I’d like to see even more of a multi-faceted evaluation and discussion in the paper.\n\n3) The idea of learning a side-effect-avoiding regularizer that generalizes well, e.g. to different tasks under the same environment dynamics. This is very interesting and a promising step towards tackling the side effects problem at scale. It is very nice to see the zero-shot results. To make the paper even stronger and more impactful it would be nice to evaluate the generalization of the trained side-effect-avoiding police in more detail.\n\n---\n**Score and reasons for score**\nI am (currently) in favor of accepting the paper, though I think that some additional work could improve the strength and potential impact of the work. The topic addressed is timely and very important, and the approach taken is interesting and sensible. Results look promising, and the paper does a great job at presenting the work. To further strengthen the paper it would be nice to discuss results in more detail and potentially perform additional experiments to highlight certain aspects that are “buried” in the current results. Additionally it would be good to say something more substantial about the generalization properties of the side-effect-avoiding policy. While the latter two issues are probably beyond what’s easily possible in the rebuttal phase, I want to strongly encourage the authors to add a short paragraph that clearly states the assumptions/requirements (the strongest assumption is perhaps the presence of a quantitative side effect measure which can be used directly as a reinforcement signal), and current limitations. I am looking forward to the other reviews and authors’ response, and will update my final verdict accordingly.\n\n---\n**Strengths**\n1) Empirically promising results on a timely and important problem, including the comparison against a strong baseline method.\n\n2) Evaluation of proposed method by: (i) multiple runs to assess statistical significance, (ii) ablation studies regarding the “distance” metric used by the method, (iii) control-experiments regarding the (zero-shot) generalization performance of the side-effect-avoiding policy.\n\n3) Well written paper, with good introduction to the problem and discussion of related literature (given the limited space of a conference-format publication).\n\n---\n**Weaknesses**\n1) The experimental results shown are interesting and promising, but it’s hard to distill a clear message from the results other than: “the proposed method seems to work on par with a previously proposed method but often makes the trade-offs (between high reward and low side-effects) differently, which makes comparison more difficult”. Drilling down on some of the findings and trying to control for more factors to get a clearer picture would strengthen the results.\n\n2) The generalization of the side-effect-avoiding policy is a very interesting aspect of the work, however the current analysis of how well that generalization behaves is a bit crude. It is unclear to which degree the previously trained side-effect-avoiding policy in the zero-shot regime covers the state-space encountered when solving a particular task. It is also unclear whether the side-effect-avoiding policy in the generalization setting “behaves mostly well overall” or whether it has some severe and potentially even systematic shortcomings (leading to undesired policies) in particular situations of the generalization regime. Addressing this in full generality is of course beyond the scope of this paper, but some more analysis into this issue would be very nice to see (e.g. comparing the zero-shot vs the trained side-effect-policies in isolation, and potentially drilling in on some of the differences encountered).\n\n---\n**Correctness**\nThe construction of the algorithm and training scheme presented in the paper seems correct to me.\n\n---\n**Clarity**\nThe paper is mostly well written, and the method is clearly described. Perhaps two things to improve: (i) the discussion of results could be expanded a bit more, there’s a lot going on in the plot and unfortunately there’s no intuitive message that one can easily take away visually. (ii) To facilitate the flow of the manuscript to readers unfamiliar with SafeLife it would be nice to include a short section describing the side-effect penalty.\n\n---\n**Improvements / major issues**\n1) The results currently shown are interesting but it’s hard to distill a clear message (which is understandable to some degree, as the paper also points out, because different solutions to a multi-objective optimization cannot be easily compared). It might be worthwhile though to expand the discussion (and perhaps even presentation) of the results a bit more. \n\n2) One of the most interesting aspects of the work is the potential to train a task-agnostic side-effects-avoiding policy that generalizes to a broad range of tasks. The paper demonstrates that this works by applying said policy in a zero-shot setting and analyzing the resulting policy. It would be nice to also do some more comparison of the side-effects-avoiding policies directly (e.g. what is the side effect score when directly comparing a zero-shot Z vs a Z trained on the current task/environment - are there any systematic deviations between the two, do certain biases get baked into the zero-shot Z that can be explained by the tasks/environment-variants it’s been trained on).\n\n3) A clear discussion of the requirements (the objective of avoiding side-effects must be specified as a trajectory-dependent, quantitative function, similar to a reward function), and the current limitations (unclear how to assess “how much” of the task-relevant state-space is well covered by the side-effect-avoiding policy, particularly in the zero-shot setting).\n\n4) Please clarify: why are there separate zero-shot agents shown in prune-still and append-still - shouldn’t they be the same SARL JS/WD since the zero-shot agents have been trained on these two tasks respectively?\n\n5) Please clarify and potentially discuss in the paper: perhaps the main requirement for the method is having a side-effect-strength signal s. This signal must be suitable for a reinforcement learning algorithm to train a side-effects-minimizing policy Z. But if such a signal is available, why not simply combine it with the task-specific reward function r to create a “safe reward function” to train a reward-optimizing agent that avoids side-effects? Would the solution obtained this way be qualitatively different (in some aspects) compared to the solution obtained by the proposed scheme? It’s fine to simply comment on this - the strongest version would include actual control experiments (but I understand that this might not be easily doable).\n\n6) Please comment and potentially discuss in the paper: What is the advantage of co-training Z with A (lines 11-15 of Algorithm 1)? Why not train Z first (e.g. would that improve training stability)?\n\n\n---\n**Minor comments**\n\nA) Please give a few details for the side effect metric that’s used by the experiment (fine to refer to the SafeLife paper for full details, but the rough idea should be in the paper to improve readability).\n\nB) How exactly is it ensured that Z sees the same parts of the state-space that A does (i.e. how is it ensured that Z “explores” similarly to A, which is solving some tasks)? I assume that the actions actually taken (which lead to a certain state on which A and Z are evaluated in line 5 and 6 in Algo 1) are driven by A?\n\nC) P4: “In this formulation, policy characteristics are converted to distributions in a latent space of behavioral embeddings on which the Wasserstein Distance is then computed.”. I have a hard time following this sentence, please consider unpacking it a bit.", "Thank you for your comment. \n\nThe primary goal of our work is, in fact, to abstract a notion of safety independent of a given task. A key motivation of this goal comes from human notions of safety - which are often pre-formed and often task agnostic. \n\nFor example, we understand that colliding with a fragile object is generally unsafe. Thus, if we need to learn how to navigate in a complex environment that we have never encountered before, we would generally learn to do so in a way that avoids collisions with fragile objects - even if we have never seen those specific objects before. While we will still need to find an optimal policy to solve the primary task, we will generally not need to retrain our understanding of safety. Rather, our pre-formed notions of safety would modulate our own actions as we explore and train ourselves to discover performant strategies. \n\nThe safe agent in our work embodies that notion of task agnostic safety. Similar to the human scenario above, the safe agent modulates the policy of the primary agent as it trains on new tasks. \n\nWe investigate the performance of such \"zero-shot\" safe agents where the safe agents are ported zero shot to tasks that they have never seen before, which we compare with safe agents that are trained from scratch for those same tasks - this configuration would be the equivalent of an agent that is trained specifically for a given task. \n\nOur results in Fig 3 show that the zero shot performance is similarly performant as the setting where the safe agent and main agent are jointly trained on the same task. You are correct that there is some tuning required - as we describe in Section 4. However, this formulation is relatively simple - with only the regularization parameter (beta) needing any significant tuning, as is common in many regularization formulations. ", "You response clarified the definition of side effect metric, which is very helpful. However, I am still concerned about the fact that the safety agent does not care about the task, or the task agent does not affect the update of the safety agent. This makes the training less robust and may need excessive tuning to make it work. ", "Thank you for your response. We wanted to reply to some of the questions you raised:\n- Precise Goal: \nThe goal for this paper is a framework that allows us to abstract the notion of side-effects into a safe actor that can then be ported across several tasks and environment settings that share the same underlying dynamics but different primary objectives. In the zero-shot settings, we show that this is possible within the SafeLife suite where a safe agent trained on one task can modulate the behavior of a primary agent on a second task.\n- Behavior of the Agent:\nIn terms of what behavior the agent is trying to achieve, that is a significantly more difficult question because it highly depends on the goal of the practitioner. The ideal policy, of course, would solve the task perfectly without leaving any side effects but that is not possible to achieve with current methods, and often not possible even in principle. As we mention in Section 6, we believe that applying ideas from multi-objective optimization to obtain a Pareto Frontier is a promising idea to further quantify such behaviors to enable practitioners to make better decisions about task and safety performance given their design settings (reward function, side effect metric, algorithm, etc). Given the large scope of such a study, we believe that is a great extension for future work.\n- Why is this solution concept preferred?\nThis solution concept, SARL, is preferred because it is flexible to be trained with different side-effect metrics that can induce different behaviors. As discussed in Section 2,  the role of side-effects of RL policies is an open problem and we anticipate that new formulations of side-effects will be invented in the future and wanted to design a framework that will be flexible to those innovations.\n- Co-Training\nWe agree with your statement that using co-training can be misleading. We will use better terminology in the final paper.\n", "Thanks for the responses and the updated paper. \n\nI'm still unclear about the \"precise\" goal that this algorithm is trying to achieve. While it's clearer what the side effects are now, I still do not understand what kind of behaviors we wish the agent to achieve, given such a side effect definition. Is there a threshold on how much side effect that the agent can induce? Is the side effect treated as penalty on the rewards? Should the learner reach Pareto front? Why is that solution concept preferred in the application here?  I think the overall issue of this paper is that the desired solution concept is unclear, as I mentioned in the previous comment. The authors propose a method but it is unclear what problem precisely it's trying to solve. I think a more in-depth comparison with relevant solution concepts and literature is mandatory to improve the paper's clarity. \n\nLastly, I think it's misleading to say that the task agent and the safe agent are co-training, because the task agent never affects the update the safe agent in Algorithm 1. ", "Happy to see the interactive part of the reviewing process working well via OpenReview, even happier to see the additional writing (1. and 3.) and the new experimental results (2.).\n\nI like the reasoning in 5. Perhaps worth adding (some of) it to the paper.", "Thank you for your response. Let’s go through your revised points in more detail:\n1. Thank you for the suggestion. We have added a section to the appendix that describes the figures in the results section in more detail. We plan to add this description, along with the section on limitations, in the final version of the paper.\n2. Thank you for the clarification. You are right in your understanding that the figures only show the results for the task actor A and not the safety actor Z. We will re-run our experiments to include logging  for Z_psi in the test environments in addition to A_phi, and add those figures into the appendix. We hope to be able to include them in the next update of the paper if the experiments finish in time. We will let you know if/when a revision with those figures is uploaded.\n3. We have added a section in the appendix to discuss limitations and assumptions more thoroughly. We plan to move this section to the final 9-page version of the paper if accepted. We agree that SARL in its current form relies heavily on the side effect metric, just like any RL agent is dependent on the reward function used for its training. Moreover, the SafeLife metric while a good fit for the impact penalty method might not be ideal for SARL, which is why we believe that a more thorough study of training with different safety metrics, including unsupervised metrics, would be a valuable extension of this work.\n4. You’re welcome.\n5. Learning a separate safe policy can be much more efficient than learning a combined policy when the safety metric is itself expensive to compute. This would be the case, for example, when safety is learned via human feedback. If the safety policy itself is relatively straightforward, or if the safety policy can be applied to many different tasks, then learning the safety policy by itself can be much more sample efficient in safety metric calculations than an agent that learns via a combination of the safety metric and its standard reward. Moreover, a separate safety policy, once learnt, should be able to distinguish between side effects that are within the agent's control and those that are not. In contrast, the safety metric by itself may introduce considerable noise into the training procedure of the primary task if it includes side effects that are not caused by the agent and are instead part of the inherent environment dynamics. This is particularly important in the dynamic environments within SafeLife. We also want to highlight that the SARL framework allows for more flexibility in how to compute the distance between policies, which gives more options on how to act in complex spaces.\n6. You’re welcome.\n", "Let me clarify some of the issues raised.\n\n1. Yes, I understand the broader goal. What I meant was to expand the actual description/discussion of the results in the paper to literally walk the reader through the most important parts to pay attention to in Fig. 3. The figure has a lot of information that cannot easily be grasped. A bit more guidance for the reader would be nice to have.\n\n2. That's not quite what I had in mind (I think). What I meant was to directly report the side-effect score (r_{side-effect}) of Z (without any involvement of A). As far as I understand Sec. 5 shows results for evaluating r_{side-effect} for an agent A regularized via Z (either a co-trained Z or a Z trained on some other task). The reason for asking for such a comparison is that it might give some insight into the generalization performance (and gaps!) of Z in the most direct way possible (not indirect evaluation by studying A trained via various Z). Please let me know if these results are actually shown in Fig. 3 (which panel, which lines?) if I'm still misunderstanding Fig 3.\n\n3. What I meant here was adding a 'limitations' section to the paper, where the assumptions/requirements and limitations are clearly and concisely stated. To me, the biggest limitation of applying the work outside of SafeLife is the requirement of having a rich side-effect metric (rich enough to train an agent on). I would even argue that having such a function in the first place is the main difficulty. This might be obvious when talking about SafeLife, but should be clearly discussed as a problem w.r.t. the applicability of the method in the paper.\n\n4. Thanks!\n\n5. Thanks for clarifying the effort of searching for a good scaling factor to strengthen the baseline. Besides the empirical evaluation my comment was also aimed at the conceptual level: if a rich safety-metric signal for an environment (not only a specific task in that environment) is available, why not use that signal directly as a regularizer-term in the reward function? What are the (theoretical?) advantages of using it to train a safe policy (which is then used as a regularizer)?\n\n6. Thanks for clarifying!", "Thank you for your review and comments. We would like to address the concerns you identified directly:\n1. We have updated our description of the side effect metric in Section 2 of our new draft. There we describe the difference between the frame-by-frame metric used for training the SARL agent and the episodic metric we used to report our results in Section 5.\n2. The training and testing sets refer to different environment configurations within a given task. Let’s take prune-still as an example: Within prune-still there is a large set of different environment configurations that lead to different maps in SafeLife for which the agent will attempt to solve the prune-still task. Our process takes out a test set of configurations that are not used during training and tracks the champion policy (of the primary task agent) within them purely to report test performance. The safety agent never interacts with the test environments in any of the training runs, and the generalization claim pertains to transferring the safety agent across different environments, meaning that a safety agent trained on prune-still will be applied in append-dynamic, etc. We hope this clarifies the process.\n3. Your intuition that the safety agent will converge towards a “no action” policy is what we often observed. We also believe this a major reason why finding a good \\beta is difficult to achieve. The “no action” policy, however, does not apply to all states that the task actor visits due to the complexity of the SafeLife environment. One potential mitigation for this problem is to modify the reward signal the safety agent is trained on, such as incentivizing it to reach only the level exit as safely as possible. We would also like to add that since task performance and safety often come at a trade-off, it is difficult to define an optimal solution. As we discuss in Section 6 and in our general comments, we plan to extend this work in the future to cast it as a multi-objective RL problem where such trade-offs can be better analyzed.\n4. We have added more detailed descriptions about the different tasks. We have currently placed them in the appendix due to space concerns. \n5. Thanks for pointing out this typo.\n", "Thank you for your review and comments. We would like to address the weaknesses you identified:\n1. It is true that we primarily focused our work on the SafeLife suite and cannot make any statements beyond the environments in SafeLife. We believe, however, that SafeLife provides a rich set of different settings that allow us to demonstrate the different notions we discuss in the paper. We believe that a thorough theoretical framework for generalization outside of the SafeLife suite is beyond the scope of the paper. A theoretical framework for generalization of RL agents, as well generalization neural networks overall, are broader fields of research and open problems in the deep learning community.\n2. Thank you for this feedback. We have updated our notation and further descriptions in our new draft.\n3. Our current framework assumes that the task agent employs a loss function given from an established RL algorithm. Based on your feedback, we will add a deeper discussion on further development of different loss functions that involve probabilistic distances. We believe conducting this study would not be feasible within the scope of this paper, but we agree that a more thorough theoretical discussion is important.\n", "Thank you for your very thorough review, we hope that we can clarify some of your concerns through these discussions. In this comment, we would like to focus on the Improvements you outlined and address them directly:\n1. We agree that a clearer discussion on the key takeaways would be useful. Our primary goal was to train a portable safe-agent that could generalize across multiple tasks that share common dynamics. Our results on zero shot transfer show that the transferred safe agents performed similarly compared to training SARL agents from scratch on the same environment. The primary distinctions from prior work are:\na. the use of a safe-agent to abstract the concept of safety.\nb. the portability of the safe agent across multiple tasks.\nc. using probabilistic distances to capture differences between policies with different objectives on the same environment\nWe did not cast this problem as a multi-objective optimization problem in this paper - however, that is a natural next step for this line of work in the future. As mentioned in our general comment, proper reframing of the framework as a multi-objective setup requires significant future work, which we believe this is out of scope for this paper. \n2. Based on our understanding of your comment, we already performed those experiments. We call them “from scratch training” and show them in Section 5 in the lighter shade of color. Here, the task agent and safety are trained concurrently on the same environment, and we observe similar behavior to using safe policies that are zero-shot generalized from other environments. Does this address the scenario you are suggesting?\n3. We have updated the details pertaining to the side effect metric in Section 2 of our new draft. We take the side effect metric from SafeLife as is and use it to train our safety agent. We would like to highlight that we designed SARL to be agnostic to the safety metric used, as the distance function formulation allows for that flexibility. \n4. In the prune-still environment we take the safety agent trained on the append-dynamic environment, and in the append-still case we take the safety agent trained on the prune-dynamic environment. As we discuss in Section 5, we wanted to take the safety agent trained on the environment that is most dissimilar to the task environment since we believe that would be the most difficult to generalize from. It is not strictly necessary to do that, as we could have one zero-shot agent that is generalized to all environments except for the one it is trained on.\n5. We already train a baseline policy on a shaped reward that combines the primary objective and side-effect penalty. Since this was the method applied in the original SafeLife paper, we use this method as the “baseline” in our experiments. This method takes the frame-by-frame side effect from SafeLife and subtracts that from the frame-by-frame reward (with a scaling factor). We searched for a good scaling factor as a hyperparameter in our experiments to support the side effect baseline. \n6. Thank you for this suggestion. Co-training the task and safety agents on the same environment was primarily a practical choice, but we will add experiments that work with a safety agent that is previously trained. Intuitively, we do not expect major changes and agree with you that it should improve training stability. \n\nRegarding the Minor Comments:\n1. We have added more details on the side effect metric in our new draft.\n2. The current environment includes a fully visible state space; we can therefore pass the entire state to A and Z when we train A. In the current setup (using an on policy method like PPO), it is difficult to ensure exploration of the same states. If we were to use an off-policy algorithm, such as DQN, we can use the replay buffer to ensure that A and Z are drawing from the same distribution of states during their training. \n3. This sentence refers to how the Wasserstein distance is computed using the method described in Pacciano et al. They construct a space of test functions in a latent behavioral space and compute in that space. Essentially what happens is: 1. A trajectory (defined by the user) is given 2. A function transforms that trajectory to latent embedding space 3. The WD distance is computed in that space iteratively via test functions. The original paper provides a lot more detail on this process, which essentially allows one to take any definition of a trajectory and compute a distance on it.\nPlease let us know if we addressed your points and if you want to continue to discuss more.\n", "Thank you for your review. We would like to address your main concerns directly: \n\n1. \n\na. We agree that the description of side-effects was not completely clear in the paper and have updated the description in our new draft as mentioned in our response to all reviewers. For SARL training, we assume that the environment provides a definition of the side effect. As we describe in Section, SafeLife calculates a side effect signal by taking the deviation between a baseline state and the current state. This is the frame-by-frame metric we use to train our agents. We also use the episodic side-effect to report our results where this difference is calculated between a distribution of states at the beginning and end of an episode to account for environment dynamics. \nb. Thank you for pointing out the inconsistent notations. We have updated the paper, including Algorithm 1, to correct this instance as well as a few others we noticed. In the current version, s refers solely to the state of the environment. In the old notation, the ‘s’ in Algorithm 1, line 1-2, ‘s’ referred to the state of the environment and in Algorithm 1, line 12-14 we referred to the side effect metric as ‘s’ in green color.\nc. You are correct that the multi-objective MDP formulation is very applicable here. As we discuss in Section 6, this is a future direction that we are highly interested in since we believe it would be a more effective way to understand the trade-offs between task performance and safe behavior. Our primary goal in this paper was to encapsulate the concept of safety into a safe actor - that could then be used to modulate the behavior of a primary agent on different tasks in the same dynamic environment. This mitigates the need to define or learn a penalty factor for every task that shares similar dynamics.\n2. In the zero-shot experiment, the safety agent is trained on a different environment and then transferred over to a new environment without re-training. In the case of online co-training, the safety agent is trained in conjunction with the regular task agent on the same environment.\n3. In this paper, the side-effect metric is defined by the environment, as we previously mentioned in Point #1. Side-effect metric and safety metric are used interchangeably - we have updated the manuscript to keep the terminology consistent. Our discussion in Section 1 and Section 6 focused on the general difficulty in developing an effective side effect metric which continues to be an active area of research. \n4. S[\\pi_theta] in Equation 3 refers to the entropy of the policy as described in the original PPO paper.\n\nPlease let us know if we addressed your points and if you want to continue to discuss more.\n", "We thank all reviewers for their comments, suggestions and feedback. As we read through all the reviews, we noticed some common threads that we think are relevant to all reviewers and have motivated us to make updates to our current paper draft, which has been uploaded to replace the previous draft. \n1. The first change we made is to standardize the notation used in our method (Section 3) to clear up inconsistencies and to convey the important parts of the SARL framework in a clearer manner. \n2. We have also updated and added more detail to the description of the side effect metric used for training the SARL agents in Section 2. These metrics are identical to the ones in the original SafeLife paper, and we agree with the reviewers that more detail on this metric was needed. We hope our changes provide a better description of these metrics and how we applied them in the SARL method.\n3. We have also made some changes to clarify the setup of our zero-shot generalization experiments. In each of those experiments we load a safety agent trained on a different environment and apply it to the SARL agent without any updates, only performing lines 1-10 in Algorithm 1. In this set-up, since the safety agent is trained on a different environment than the task agent and does not perform gradient updates, we labeled these experiments as zero-shot generalization.\n4. We wanted to also address the comments regarding multi-objective formulation. Proper reframing of the framework as a multi-objective setup requires significant future work, including: 1. choosing preferences functions for the two objectives to cast a proper MOMDP; 2. Develop an algorithm to train an agent on this MOMDP; 3. Develop a method for creating and visualizing Pareto fronts that describe the trade-off between the different objectives for the given method; 4. A study of how flexible the method is to changes in preferences during training and testing. Given the extensive nature of the work required, we believe this is out of scope for this paper.\nWe have also prepared individual responses to all reviewers and hope to continue this discussion to address any questions that come up throughout this process.\n", "This paper proposes a safety-aware reinforcement learning algorithm that learns to perform tasks with minimal side-effects. The key idea is that a safety policy is learned independent of the task reward. When learning the task, this safety policy is incorporated by minimizing the distance between the task agent and the safety agent. In this way, the paper claims that the safety agent can be generalized to different tasks. The method is tested on SafeLife Suite, and its performance can match task-specific safe learning baselines.\n\nSafe reinforcement learning is an extremely important research area, when we need to apply reinforcement learning to real-world applications, such as robotics, recommendation system, power grid, etc. This paper works in this direction and addresses the key challenges, including how to learn generalizable safety agents. While I think that the paper is promising, I have the following three major concerns:\n\n1) The \"side effect metric\" is not clear to me. The description in Section 2.1 is high-level and vague. More rigorous mathematical definition is preferred here. For a safe learning paper, it is extremely important to clearly define what safety means. Is the \"side effect metric\" the same as the \"safety metric\" in Line 12 of Algorithm 1? Reading from the text, it seems that the side effect metric is calculated per episode, while the safety metric is per step.  \n\n2) Section 3.4 seems to leak the testing set into training. One claim of this paper is that the learned safety agent is generalizable: Z(\\psi) can be taken zero-shot from previous trained environments. However, during training, by tracking the Champion policy , decisions are made based on the performance on the testing environments, by retaining the policy that performs the best in the testing environments. If my understanding is right, this makes any claim about generalization less convincing because the training directly optimizes the policy in the testing environments.\n\n3) Intuitively, I do not understand how Algorithm 1 could work. According to Algorithm 1, the training of the safety agent Z(\\psi) is totally independent of the task, whose only objective is to be safe. If it is the case, the learned safety agent would not move or take action at all. The action distribution P_Z_\\psi would be concentrated on the zero action. This would make optimizing A(\\theta) using the loss (eq. (1)) extremely difficult. This might explain why the paper observes that the hyperparameter \\beta is difficult to tune.\n\nHere are some minor suggestions about writing:\n1) A brief description of the 4 tasks is needed (prune-still, prune-dynamic, append-still, append-dynamic) to make this paper more self-contained. If the page-limit is a concern, this description could be added to the Appendix. Otherwise, it is difficult for readers to understand the difficulties and the usefulness of these tasks.\n\n2) \"Line 13-17 from Algorithm 1\": The pseudo-code ends at Line 16.\n\nFor the above reasons, I would not recommend acceptance at this time.", "The paper aims to reduce the unwanted side effects of the actions of a reward-maximizing reinforcement-learning (RL) agent. The authors study a framework in which the environment issues a metric that measures the total side effects of the agent's actions at the end of each episode. The work's proposed solution trains an agent who focuses on the total reward and another agent who minimizes the total side effects. The authors then empirically investigate the effectiveness of combining the two agents via a distance measure between the policies that unifies them into one.\n\nI find the following items strong points in the submission:\n* The posed problem is relevant in the context of safety in AI.\n* The chosen testbed for the experiments matches the goals and premises of the posed problem.\n* The empirical results suggest that the proposed method is effective.\n* The discussion regarding the choice of the distance measure is thorough and makes sense.\n\nOn the other hand, I find the following issues as weaknesses in the submitted manuscript:\n* There are no theoretical developments to demonstrate whether the reported results generalize beyond the adopted environment settings and the value assigned to the parameter beta or not.\n* The paper dives right into introducing the loss function in Section 3 without establishing the required notation and preliminary materials. A brief summary of the task agent and the virtual safe agent descriptions is currently provided in the caption of Figure 2. In my opinion, Section 3 would read better if the authors append a preliminaries section wherein they establish the notations and the descriptions of the task agent and the virtual safe agent.\n* The loss function adopted in equation (3) provides little room for theoretical developments. The original paper that introduces the PPO algorithm (Schulman et al., 2017) offers multiple loss function choices. In my opinion, the combination of the Jenson-Shannon distance with the loss function that incorporates the KL divergences enables the authors to study their proposed algorithm beyond empirical results.\n\nI find the posed problem relevant in the context of safety in AI and the suggested method well-motivated and intuitive. I believe the submission is far from theoretically solving the posed problem; however, the methodology alongside the promising empirical results that the manuscript offers may be of interest.", "This paper aims to address the issue of mitigating side effects in policy learning. The authors propose an algorithm SARL, which uses a safe policy to define a regularization term for penalizing the agent's actions deviating from the safe agent in policy learning. In the experiments, four variations for SARL are shown and compared with a baseline method based on reward penalty. The proposed algorithm is competitive across the experiments presented in the paper. \n\nI think side effects and safety in reinforcement learning is an important issue. However, this paper does a bad job in describing the problem it wishes to address and, therefore, it's unclear whether the proposed algorithm really achieves that goal. \n\n1. The main motivation of this paper is to mitigate the side effects in learning. However, the definition of side effects were never given. It's only until Algorithm 1 is presented where the paper mentions a safety metric that the safe agent aims to optimize (is this the same s appearing in A(s|theta) and Z(s|psi) in Sec 3.2?), which however is not defined. Therefore, I do not fully understand what the objective of this learning algorithm wants to achieve. From the paper's vague description, it seems like the goal is that the learner should have high performance in the original reward while not causing high side effects. This is a multi-objective MDP problem or at least can be framed as a constrained MDP. However, the proposed algorithm, based on simple regularization with a constant weight, can address neither of these two criteria. I am wondering if the authors consider to more explicitly outline the solution concept they wish to obtain. Current hand-wavy description makes me difficult to judge whether the proposed algorithm actually solves the problem they wish to solve.\n\n2. In Algorithm 1, since the safe agent Z is updated independently of the progress of the learner agent A, when there's only a single environment, there is no point of distinguishing the so-called \"zero-shot\" and the online version, as in high level this dependency allows us to pretrain the safety agent alone beforehand and get the same results. Or do the authors mean zero-shot in the sense that the safe agent is trained on a different set of environments and the online version means they're trained on the same environment? \n\n3. In the paper, the authors write multiple times that a difficulty in this problem setup is that the side effects are difficulty to define. But it seems that the proposed algorithm assumes some safety metric. How are the two related precisely? And what is that used in the experiments?\n\n4. What is S[\\pi_theta] in (3)?\n\nOverall, I think the paper is rather incomplete and therefore I do not recommend acceptance.\n"], "review_score_variance": 1.6875, "summary": "Based on the paper, reviewers' comments and discussions, and the responses, the meta-reviewer would like to suggest the authors to improve the paper and resubmit.", "paper_id": "iclr_2021_RDpTZpubOh7", "label": "train", "paper_acceptance": "withdrawn-rejected-submissions"}
{"source_documents": ["Continuously trainable models should be able to learn from a stream of data over an undefined period of time. This becomes even more difficult in a strictly incremental context, where data access to previously seen categories is not possible. To that end, we propose making use of a conditional generative adversarial model where the generator is used as a memory module through neural masking to emulate neural plasticity in the human brain. This memory module is further associated with a dynamic capacity expansion mechanism. Taken together, this method facilitates a resource efficient capacity adaption to accommodate new tasks, while retaining previously attained knowledge. The proposed approach outperforms state-of-the-art algorithms on publicly available datasets, overcoming catastrophic forgetting.", "We updated the paper with the CIFAR results as well as cite the mentioned papers on capacity growth. \n\nConsidering the comparison to Progressive Networks:\nSimilarly to Progressive Neural Networks [1] and its evolution [2] our method addresses the challenge of knowledge transfer by ensuring the reusability of parameters across the tasks. Our method does it naturally since it only keeps a single network for long and short-term memory with different neurons assigned to different memory types. Using binary masking allows keeping both memory types in a single network without forgetting. DGM neither require keeping a pool of networks (columns) used for previous tasks (as in [1]) nor utilizing separate long and short-term memory networks (as in [2]). \n\nOverall, we thank the reviewer again for the constructive feedback, which we will consider in our future work.\n\n[1] Progressive Neural Networks, Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, Raia Hadsell, https://arxiv.org/abs/1606.04671\n[2] Progress & Compress: A scalable framework for continual learning, Jonathan Schwarz, Jelena Luketina, Wojciech M. Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, Raia Hadsell, https://arxiv.org/abs/1805.06370\n", "We thank the reviewer again for his/her extensive response.\n\nWe believe, the just storing real samples of previous classes does not comply with the fundamental vision of how a continually trainable system should work (e.g. compared to natural intelligence). Further, the challenge of scalability in continual learning cannot be addressed by simply storing real samples (at least not in large-scale context). “Strictness” is an increasingly important issue in the literature and has been addressed by other works such as [ 1,2,3 ]. We, therefore, stick to “strictness” requirement and prohibit storing real samples which naturally leads to using the generative memory. \n\nAs opposed to DGR [1] based approaches, DGM replays a 'complete' learned representation of previous tasks - meaning no information is lost due to continuous retraining of the G on samples generated by the previous generator.\n\n\n[1] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In Advances in Neural Information Processing Systems, pp. 2990–2999, 2017.\n[2] C. Wu, L. Herranz, X. Liu, Y. Wang, J. van de Weijer, and B. Raducanu. Memory Replay GANs: learning to generate images from new categories without forgetting. In Advances In Neural Information Processing Systems, 2018.\n[3] Seff, Ari, et al. \"Continual learning in generative adversarial nets.\" arXiv preprint arXiv:1705.08395(2017).\n", "1. We first want to point out the main contributions of the paper.\nFirst, we address the catastrophic forgetting problem in continual learning.  Thereby we introduce Dynamic Generative Memory (DGM) - an adversarially trainable generative network endowed with neuronal plasticity through efficient learning of sparse attention mask for layer activations. Hereby we extend the idea of [2] to generative networks. We highlight the differences to DGR [3] in the Sec. 2 of our work. \n\n2. Equation (5) and (6) are taken from [2] one to one. Equations (3) and (4) are adopted from [2]: equation (3) describes the annealing of the parameter s, we anneal it globally over the course of epochs, whereas [2] anneal it for each epoch over the number of batches; equation (4) is a simplified version of the one used by [2].\n\n3. To avoid confusion of the proposed method to utilize techniques of DGR[3] in order to prevent forgetting in the G, we kindly ask the reviewer to refer to our response (2) to the Reviewer 1.\n\nIn the proposed work we adopt the generative replay not in order to avoid storing previous samples, but in order to prevent forgetting in the discriminator (which is used as a final classification model). Data synthesized by the generator is replayed for to the discriminator during the training of the subsequent tasks. There is no replay applied to the generator network. In order to avoid storing previous data, we utilize parameter level attention mechanism similar to HAT [2].\n\nConcerning the time comparison, there is no reason why our approach should be less time efficient then DGR based approaches [1, 3] as our method does not require retraining the generator from scratch at each time step.\n\n4. Why our method does not outperform joint training on SVHN?\nUsing generated samples accommodates for better performance then joint training is the case of tasks of relatively low complexity such as MNIST. Indeed, such a result has been shown in other works, e.g. [1]. As explained in Sec. 5.2, this can be attributed to a potentially higher diversity with a steady quality of the generated samples. Clearly, the performance of the classifier trained on the generated samples highly depends on the complexity of the task and quality of the generated samples. Thus, this effect can not be observed neither in the SVHN not the CIFAR10 benchmarks.\n\n5. Grammar mistakes and typos.\nThis will be fixed in the updated version of the paper.\n\n6. No guarantee to work for any task or scenario.\nAs pointed out by the reviewer and is true for many machine learning method, there is no guarantee that the proposed method will work for any task or scenario. \n\n[1] C. Wu, L. Herranz, X. Liu, Y. Wang, J. van de Weijer, and B. Raducanu. Memory Replay GANs: learning to generate images from new categories without forgetting. In Advances In Neural Information Processing Systems, 2018.\n[2] J. Serrà, D. Surís, M. Miron, and A. Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. CoRR, abs/1801.01423, 2018.  URL http://arxiv.org/abs/1801.01423.\n[3] H. Shin, J. K. Lee, J. Kim, and J. Kim.  Continual learning with deep generative replay.  In\nAdvances in Neural Information Processing Systems, pages 2990–2999, 2017.\n\n", "\n Thanks again for the responses.\n\n> Furthermore, using generated samples accommodates for better performance than simply storing instances \n> only in case of tasks of relatively low complexity such as MNIST. \n\n  Sure, but 1) this makes it not surprising like it's presented in the paper (there are a large number of papers that essentially use generative models as data augmentation, and you could do this for your joint training methods as well) and 2) I was saying that I'm surprised that methods such as iCARL or simply replaying a small number of examples wouldn't do well on these tasks.\n\n> The CIFAR results will be provided in the Tab. 1 alongside with other datasets in the next version. \n\n  If you have these available, can you post them on openreview?\n\n  In terms of experimental methodology, I don't believe growing the generator is fair, or at least it brings in other competitors that do the same. Specifically, replay methods that use real samples typically reduce the number of samples per task as the number of tasks grow, specifically to keep the memory constant. Here, while you are not growing the discriminator, you are still growing the amount of memory you use. Again, a simple baseline would be to take the same amount of memory your method uses (including expansion) and replay those examples during training. In all of these comparisons, a table is needed that shows exactly how much memory is used for all of the baselines/competitors that use replay, and how much memory your method uses. Note the other reviewer asked for the same, and included time complexity as well.\n\n  Further, if you are going to use capacity expansion there are a number of methods that aren't cited in your work, including progressive networks [1,2] the latter of which uses distillation as a mechanism to avoid large-scale growth in the networks. \n\n   This paper and results does have promise, but given that it essentially uses HAT for the generative model, this is largely an empirical paper. As such, there should be precise experiments that make it much easier to discern the advantage of the method over both state of art as well as much simpler baselines.  \n\n[1] Progressive Neural Networks, Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, Raia Hadsell, https://arxiv.org/abs/1606.04671\n\n[2] Progress & Compress: A scalable framework for continual learning, Jonathan Schwarz, Jelena Luketina, Wojciech M. Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, Raia Hadsell, https://arxiv.org/abs/1805.06370", "\n I appreciate the authors' detailed response. \n\n  In terms of contribution, it would be great to make this much more clear in the revision. However, you seem to agree that essentially it is to extend HAT to the generative network in addition to adding (a very simple) capacity expansion, neither of which adds a great amount of novelty or advances our understanding of continual learning. For example, as far as I can tell most of the technical description in section 4 is for HAT.\n\n  I also still do not understand why the method is claimed to be in such stark contrast to the previous work. When you say that your method only loses information through \"natural forgetting\" what does this mean precisely? What does \"'complete' learned representation\" mean? These are vague terms and should be precisely defined. To me both of these are again just achieved by using HAT. \n\n  Finally, I am still not convinced why a \"strict\" incremental setting rules out the storage of real samples. If storing a number of examples equivalent to the amount of memory used by your method achieves better performance (of course, equalizing for the capacity growth you have), why is that an issue? The only reason I agree with might be privacy, but that can be addressed through other privacy methods (e.g. random perturbation). \n\n  Overall, I think generative models are great and can be useful to addressing catastrophic forgetting, but such methods have have other limitations (complexity of training, etc.) and should be compared to simpler replay baselines. Further, just the idea of using HAT for a generative model plus capacity expansion seems to me not a significant enough contribution. Given the additional concerns about methodology (see next comment), I do not believe these comments sufficiently address all of the concerns to warrant acceptance. ", "\n4. Our approach has 2 important hyperparameters: scaling parameter s used for calculating binary mask from the embedding matrix as well as  λ_RU, that controls the size accuracy trade-off (see Sec. 4.1 “joint training”).  We add a table analyzing the sensitivity of the parameter λ_RU observing the expected behavior: higher values of λ_RU lead to a smaller model size, however, reduced G size is positively correlated with the final classification performance of D (smaller G -> lower accuracy of D).\n+---------+---------+-------+\n| λ_RU  | Acc.5 | Size |\n+---------+---------+-------+\n| 2E-06 | 98.16 | 660  |\n+---------+--------+--------+\n| 0.002 | 98.22 | 638  |\n+---------+--------+--------+\n| 0.2     | 98.02 | 598  |\n+---------+--------+--------+\n| 0.75   | 97.36 | 577  |\n+---------+--------+--------+\n| 2        | 86.82 | 522  |\n+---------+--------+--------+\n\n5. We use the baseline presented by [1], that tackles identical scenario. To our knowledge [1] provides the state of the art performance in \"strict\" class incremental setup without using real samples.\n\n We consider a joint training (JT, classical training) of the discriminator as the upper performance bound. Joint training features a setup in which the discriminator is trained on ALL real samples of the previous tasks. The reviewer proposes to simulate information loss and use a random subset of real samples to train the upper bound model. However, this would certainly give a worse performance than when using all real samples. We, therefore, think that used JT upper bound is appropriate.\n\nFurthermore, using generated samples accommodates for better performance than simply storing instances only in case of tasks of relatively low complexity such as MNIST. Indeed, such a result has been shown in other works, e.g. [1]. As explained in Sec. 5.2, this can be attributed to a potentially higher diversity with steady quality of the generated samples. Clearly, the performance of the classifier trained on the generated samples highly depends on the complexity of the task and quality of the generated samples. Thus, this effect can be observed neither in the SVHN nor the CIFAR10 benchmarks.\n\n6. The CIFAR results will be provided in the Tab. 1 alongside with other datasets in the next version. \n\nTo ensure a fair comparison with the benchmark methods that do not use any network expansion strategy for the generator (e.g. [1,6]), we initialize our G to be approximately 50% of the size of the G used in the benchmarks. Also a study on network growth dynamics is provided in Fig. 5 (Sec. 5.3), showcasing a lower network capacity than the worst case scenario. Growing the generator is an essential part of our method that addresses the scalability problem in continual learning, e.g. with always growing amount of data model’s capacity will be exhausted at a certain point. Noteworthy, the discriminator is not affected by the proposed dynamic network expansion mechanism and features the same architecture as in the benchmark methods.\n\nWe believe the comparison to the joint training is fair because DGM only grows the capacity of the generator. In the discriminator, only the last classification layer is expanded with the growing model’s output space as new classes are added. Thus, for k-th task we compare the accuracy of a discriminator with identical architecture trained on real samples of all k tasks (JT) with one trained on DGM-synthesized samples of k-1 tasks+reals of k-th tasks. Thus DGM’s discriminator has no advantages over the joint training generator.\n\n8. Finally, we will address typos, writing and presentation issues in the updated version of the paper.\n\n\n[1] C. Wu, L. Herranz, X. Liu, Y. Wang, J. van de Weijer, and B. Raducanu. Memory Replay GANs: learning to generate images from new categories without forgetting. In Advances In Neural Information Processing Systems, 2018.\n\n[2] J. Serrà, D. Surís, M. Miron, and A. Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. CoRR, abs/1801.01423, 2018.  URL http://arxiv.org/abs/1801.01423.\n\n[3] H. Shin, J. K. Lee, J. Kim, and J. Kim.  Continual learning with deep generative replay.  In\nAdvances in Neural Information Processing Systems, pages 2990–2999, 2017.\n\n[4] S. Rebuffi, A. Kolesnikov, and C. H. Lampert. icarl: Incremental classifier and representation\nlearning.CoRR, abs/1611.07725, 2016. URL http://arxiv.org/abs/1611.07725.\n\n[5] A. Chaudhry, P. K. Dokania, T. Ajanthan, and P. H. S. Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. CoRR, abs/1801.10112, 2018. URL http://arxiv.org/abs/1801.10112.\n\n[6] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In Advances in Neural Information Processing Systems, pp. 2990–2999, 2017.\n", "We thank the reviewer for their constructive comments. We address them as follows.\n\n1. We first would like to point out the contributions of our work.\n\nFirst, we address the catastrophic forgetting problem in continual learning.  Thereby we introduce Dynamic Generative Memory (DGM) - an adversarially trainable generative network endowed with neuronal plasticity through efficient learning of sparse attention mask for layer activations. Hereby we extend the idea of HAT[2] to generative networks. \n\nSecondly, we address the scalability problem in continual learning. To ensure sufficient model capacity to accommodate for new tasks, we propose an adaptive network expansion mechanism in which newly added capacity is derived from the learnable neuron masks.\n\n\n2. We further we would like to clarify a possible confusion of the proposed method to be a combination of Deep Generative Replay (DGR)[6] and HAT[2].\n\nAs pointed out in the Sec. 2 of our work, Deep Generative Replay (DGR) tries to prevent forgetting in the generator by retraining it from scratch every time a new data chunk becomes available. Thus, in DGR the generator would lose information at each replay step since the quality of generated samples highly depends on the quality of samples generated by the prior generator causing \"semantic drift\". This contrasts our method, which effectively retains the knowledge in the generator using HAT like neuron masking and only loses information through “natural” forgetting.  This allows us to use “complete” learned representation during learning and inference of the subsequent tasks as well as speed up the training (no replay of G is involved).\n\n3. We are not simply shifting the forgetting problem into G. \n\nOur work tackles the problem of class incremental learning.  As opposed to task-incremental setup and shown in previous work, e.g. [3,4,5], models in class incremental setup (with single-head architecture) require a replay of previously seen categories when learning new ones. The reason for using G is not having access to samples of previous classes in the “strict” incremental setup and using generated samples instead. As pointed out in our work, restricting storage of real samples represents a more realistic setup, since in real-world applications such an “episodic memory” with real samples is often impossible due to memory and privacy restrictions. ", "\nThe proposed method tackles class-incremental continual learning, where new categories are incrementally exposed to the network but a classifier across all categories must be learned. The proposed method seems to be essentially a combination of generative replay (e.g. Deep Generative Replay) with AC-GAN as the model and attention (HAT), along with a growing mechanism to support saturating capacity. Quantitative results are shown on MNIST and SVHN while some analysis is provided on CIFAR.\n\nPros\n\n + The method combines the existing works in a way that makes sense, specifically AC-GAN to support a single generator network with attention-based methods to prevent forgetting in the generator.\n\n + The method results in good performance, although see caveats below. \n\n + Analysis of the evolution of mask values over time is interesting.\n\nCons\n \n - The method is very confusingly presented and requires both knowledge of HAT as well as more than one reading to understand. The fact that HAT-like masks are used for a generative replay approach is clear, but the actual mechanism of \"growing capacity\" is not made clear at all especially in the beginning of the paper. Further the contributions are not clear at all, since a large part of the detailed approach/equations relate to the masking which was taken from previous work. The authors should on the claimed contributions. Is it a combination of DGR and HAT with some capacity expansion?\n\n - It is not clear whether pushing the catastrophic forgetting problem into the generator is the best approach. Clearly, replaying data accurately from all tasks will work well, but why is it harder to guard against the generative forgetting problem than the discriminative one?\n\n - The approach also seems to add a lot of complexity and heuristics/hyper-parameters. It also adds capacity and it is not at all made clear whether the comparison is fair since no analysis on number of parameters are shown.\n\n - Relatedly, better baselines should be used; for example, if the memory used by the generative model is merely put to storing randomly chosen instances from the tasks, how will the results compare? Clearly storing instances bypasses the forgetting problem completely (as memory size approaches the dataset size it turns into the joint problem) and it's not clear how many instances are really needed per task, especially for these simpler problems. As such, I find it surprising that simply storing instances would do as poorly as stated in this paper which says cannot provide enough diversity.\n\n It also seems strange to say that storing instances \"violates the strictly incremental setup\" while generative models do not. Obviously there is a tradeoff in terms of memory usage, privacy, performance, etc. but since none of these methods currently achieve the best across all of these there is no reason to rule out any of the methods. Otherwise you are just defining the problem in a way that excludes other simple approaches which work.\n\n - There are several methodological issues: Why are CIFAR results not shown in a table as is done for the other dataset? How many times were the experiments run and what were the variances? How many parameters are used (since capacity can increase?) It is for example not clear that the comparison to joint training is fair, when stating: \"Interestingly, DGM outperforms joint training on the MNIST dataset using the same architecture. This suggests that the strictly incremental training methodology indeed forced the network to learn better generalizations compared to what it would learn given all the data.\" Doesn't DGM grow the capacity, and therefore this isn't that surprising? This is true throughout; as stated before it is not clear how many parameters and how much memory these methods need, which makes it impossible to compare.\n\n Some other minor issues in the writing includes: \n   1) The introduction makes it seem the generative replay is new, without citing approaches such as DGR (which are cited in the related work). The initial narrative mixes prior works' contributions and this paper's contributions; the contributions of the paper itself should be made clear, \n\n   2) Using the word \"task\" in describing \"joint training\" of the generative, discriminative, and classification networks is very confusing (since \"task\" is used for the continual learning description too, \n\n   3) There is no legend for CIFAR; what do the colors represent?\n\n   4) There are several typos/grammar issues e.g. \"believed to occurs\", \"important parameters sections\", \"capacity that if efficiently allocated\", etc.).\n\n In summary, the paper presents what seems like an effective strategy for continual learning, by combining some existing methods together, but does not make it precise what the contributions are and the methodology/analysis make it hard to determine if the comparisons are fair or not. More rigorous experiments and analysis is needed to make this a good ICLR paper. ", "This paper attempts to mitigate catastrophic problem in continual learning. Different from the previous works where episodic memory is used, this work adopts the generative replay strategy and improve the work in (Serra et al., 2018) by extending the output neurons of generative network when facing the significant domain shift between tasks.\n \nHere are my detailed comments:\nCatastrophic problem is the most severe problem in continual learning since when learning more and more new tasks, the classifier will forget what they learned before, which will be no longer an effective continual learning model. Considering that episodic memory will cost too much space, this work adopts the generative replay strategy where old representative data are generated by a generative model. Thus, at every time step, the model will receive data from every task so that its performance on old tasks will retain. However, if the differences between tasks are significant, the generator cannot reserve vacant neurons for new tasks or in other words, the generator will forget the old information from old tasks when overwritten by information from new tasks. Therefore, this work tries to tackle this problem by extending the output neurons of the generator to keep vacant neurons to retain receive new information. As far as I am concerned, this is the main contribution of this work.\n \nNevertheless, I think there are some deficiencies in this work.\n \nFirst, this paper is not easy to follow. The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. For example, in Section 4.1, I am not sure the equation (3), (4), (5), (6) are the contributions of this paper or not since a large number of citations appear.\n \nSecond, the authors mention that to avoid storing previous data, they adopt generative replay and continuously enlarge the generator to tackle the significant domain shift between tasks. However, in this way, when more and more tasks come, the generator will become larger and larger. The storing problem still exists. Generative replay also brings the time complexity problem since it is time consuming to generate previous data. Thus, I suggest the authors could show the space and time comparisons with the baseline methods to show effectiveness of the proposed method.\n \nThird, the datasets used in this paper are rather limited. Three datasets cannot make the experiments convincing. In addition, I observe that in Table 1, the proposed method does not outperform the Joint Training in SVHN with A_10. I hope the author could explain this phenomenon. Furthermore, I do not see legend in Figure 3 and thus I cannot figure out what the curves represent.\n \nFourth, there are some grammar mistakes and typos. For example, there are two \"the\" in the end of the third paragraph in Related Work. In the last paragraph in Related Work, \"provide\" should be \"provides\". In page 8, the double quotation marks of \"short-term\" are not correct.\n \nFinally yet importantly, though a large number of works have been proposed to try to solve this problem especially the catastrophic forgetting, most of these works are heuristic and lack mathematical proof, and thus have no guarantee on new tasks or scenarios. The proposed method is also heuristic and lacks promising guarantee.", "As a paper on how to prioritize the use of neurons in a memory this is an excellent paper with important results. \n\nI am confused by the second part of the paper an attached GAN of unlimited size. It may start out small but there is nothing to limit its size over increased learning. It seems to me in the end it becomes the dominate structure. You start the abstract with \"able to learn from a stream of data over an undefined period of time\". I think it would be an improvement if you can move this from an undefined time/memory size to a limited size for the GAN and then see how far that takes you. "], "review_score_variance": 4.666666666666667, "summary": "The authors propose to tackle the problem of catastrophic forgetting in continual learning by adopting the generative replay strategy with the generator network as an extendable memory module. \n\nWhile acknowledging that the proposed model is potentially useful, the reviewers raised several important concerns that were viewed by AC as critical issues:\n(1) poor presentation clarity of the manuscript and incremental technical contribution in light of prior work by Serra et al. (2018); (2) rigorous experiments and in-depth analysis of the baseline models in terms of accuracy, number of parameters, memory demand and model complexity would significantly strengthen the evaluation – see R1’s and R3’s suggestions how to improve; (3) simple strategies such as storing a number of examples and memory replay should not be neglected and evaluated to assess the scope of the contribution. \nAdditionally R1 raised a concern that preventing the generator from forgetting should be supported by an ablation study on both, the discriminator and the generator, abilities to remember and to forget.\n\nR1 and R3 provided very detailed and constructive reviews, as acknowledged by the authors. R2 expressed similar concerns about time/memory comparison of different methods, but his/her brief review did not have a substantial impact on the decision.\n\nAC suggests in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper.\n", "paper_id": "iclr_2019_H1lIzhC9FX", "label": "train", "paper_acceptance": "rejected-papers"}
{"source_documents": ["In this paper, we solve the arms exponential exploding issues in multivariate Multi-Armed Bandit (Multivariate-MAB) problem when the arm dimension hierarchy is considered. We propose a framework called path planning (TS-PP) which utilizes decision graph/trees to model arm reward success rate with m-way dimension interaction, and adopts Thompson sampling (TS) for heuristic search of arm selection. Naturally, it is quite straightforward to combat the curse of dimensionality using a serial processes that operates sequentially by focusing on one dimension per each process.  For our best acknowledge, we are the first to solve Multivariate-MAB problem using graph path planning strategy and deploying alike Monte-Carlo tree search ideas. Our proposed method utilizing tree models has advantages comparing with traditional models such as general linear regression. Simulation studies validate our claim by achieving faster convergence speed, better efficient optimal arm allocation and lower cumulative regret.", "In this paper, the authors address the curse of dimensionality in Multivariate Multi-Armed Bandits by framing the problem as a sequential planning problem.\nTheir contributions are the following:\n1. They propose to solve the Multivariate MAB sequentially across each dimension, which I think is a reasonable and not very common approach in the literature compared to e.g. structured bandits, even though the authors themselves call this approach \"natural\" and \"quite straightforward\".\n2. They propose a framework based on a Monte-Carlo Tree Search procedure to solve this sequential decision problem.\n3. They introduce several approximations and heuristics to avoid traversing the entire tree and trade-off performance for computational complexity, which they evaluate empirically.\n\nThe obtained results seem to support their original claim: the tree-based approaches (e.g. FPF) perform better than the considered baselines. However, this claim is at the same time contradicted by the fact that a depth-one planning procedure with heuristic leaf evaluation (PPF2) reaches similar performances, which indicates that the underlying problem can be solved greedily and does not involve long-term (m-way) interations and limits the interest of the main idea in this paper. This is probably due to the design of experiments which only model pairwise interactions, but then this questions the relevance of the proposed empirical evaluation with respect to the claim.\nGenerally, though several good ideas were presented, I felt that they were not properly motivated and that the paper generally lacks rigor and clarity. The many variants and modeling decisions proposed seem quite arbitrary, and in the end fail to provide any valuable insight. \n\nSeveral aspects of the proposed approach and algorithms were not clear or justified. I will list some of them, grouped by the corresponding contribution:\n\n1. Multivariate MAB framed as a planning problem.\nNo intuition is provided regarding why it is expected to perform any better than classical N^D-MAB. In this absence of assumption regarding additional structure to the problem considered (such as, say, the independence assumption in D-MABS), is there any reason to think so? (for instance, all the N^D arms are present at the leaves of the tree) The authors only mention in their conclusion that they leverage a \"hierarchy dimension structure\", but this claimed structure was is never defined nor formalized in the paper. I can only guess that such a structure would involve a particular ordering of dimensions such that shallow ones have more influence on the payoff than the deeper ones? But then, it would be unknown in practice, and the choice of the particular ordering used in TS-PP is expected to have a crucial influence on its performance. Yet, the sensitivity of their algorithms to the ordering is never discussed by the authors -they choose it arbitrarily-  nor evaluated in their experiments.\n\n2. The proposed TS-PP framework\n- The authors propose using MCTS with TS instead of UCB as a sampling rule, which they claim is quite novel: \"Applying TS as node selection policy is not well investigated from our best knowledge\". However, the authors do not mention important reference (Bai et al, 2014) until the very end of the paper in their conclusion: \"We notice some related work dealing with continuous rewards in this area\". Yet, Bai et al. also consider Bernoulli rewards in their experiments and use conjugate Beta priors, exactly like the authors of the present work do. Moreover, the choice of TS rather than UCB is not properly motivated, and the UCT algorithm is not considered as a baseline in the experiments. Hence, there is no basis to assess whether TS is particularly relevant or not for this problem.\n- They also propose to repeat S times the candidate searching procedure in Algorithm 1 in order not to be \"[stuck] in sub-optimal arms\". This property is normally ensured by optimistic sampling rules, it is not clear why it is required here. More importantly, the sensitivity of the Algorithm 1 to S is not evaluated.\n- I did not fully understand the Candidates Construction Stage in Algorithm 1. For instance, in the case of FPF, for each search c=1..S (l3) there is a call (l4) to Full Path Finding() to generate a candidate A^c. From what I understand, a random ordering is chosen (Algorithm 2, l2), and a single rollout is performed (l2, the dimensions 1->D are iterated from root to leaf) to return a candidate. When we move to the next search c -> c+1, since a new ordering d_1:D is chosen then the previous tree storing the models for p(fd_i | fd_1...fd_i-1) is not relevant anymore and must be discarded. Does that mean that the priors at the nodes are recomputed for each search/ordering/tree given the entire history, instead of being maintained and updated? The authors do state that \"FFP utilizes hidden states from the same decision tree\", but I do not see how this is feasible when the dimensions ordering changes at each iteration. \n- This confusion is related to the fact that the update of the Beta posterior does not appear anywhere in Algorithm 1 and 2. This is unfortunate, especially considering that contrary to TS in a normal MAB setting, the observations for an internal node are not i.i.d. since they depend on the underlying planning procedure, which induces a distributional drift as more sample are collected and probably make the derived posterior invalid. Has this issue been considered by the authors?\n\n3. The variants of FPF\nThe idea behind Partial-PF-m is common in the tree-search literature: to stop at early depths and use a heuristic to estimate the value at the leaf. Here, the suggestion in the particular context of Multivariate-MAB to use TS with an independence assumption (D-MABs) is interesting and sound. However, It requires the knowledge the degree m of interactions, whereas the idea of MCTS is precisely to avoid using such heuristics for leaf evaluation by replacing them without random rollouts. Other methods such as DS start at a leaf and move locally which rather resembles Hill-climbing more closely than actual Tree-Search, and I do not really see how they fit in the TS-PP framework?\n\nFinally, I found the manuscript difficult to read due to many language mistakes and typos.\n\nMinor comments:\n- The citation style is incorrect.\n- What is the point of introducing contextual MABs in the problem formulation when the context is not going to play any part whatsoever in the presented work? This only brings unnecessary complexity and impedes clarity.\n- In the experiments, H=100 replications are performed, which is appreciated, but unfortunately the corresponding confidence intervals or standard deviations are not shown, only the mean performance.\n- I found the section 3.1 unclear. Is B_A^T \\mu a dot product? Where do the coefficients of B_A or its length M appear in (1)? How is M chosen, how is B_A obtained?\n- Figure 1.a is also unclear, the graph seems to be used to define permutations/orderings of dimensions 1..D. But all paths are not valid (eg. d1 -> d2 -> d1), ad random orderings are used in Algorithm 2, so where is this graph used exactly?\n\n\n=============\nAfter Rebuttal:\nOn one hand, some of the points mentioned are now clearer (e.g. 1 & 2.2, 2.3).\nOn the other hand, most of my concerns remain valid, and some were not addressed.\nTypically, the dependency to an unknown (and undefined...) optimal ordering and the underlying optimization procedure (random search) are not properly discussed, whereas they seem central to justify the tree-based approach.\nOther remaining concerns are more generally the lack of justification and clarity of the proposed framework and implemented variants, and the inadequacy of the experiments with respect to the claim (using m=2 basically means only the first depth of the tree is useful). Therefore, I will maintain my initial score.", "This paper proposes to use path planning on the tree structure for multivariate MAB problem. The idea is to model the hierarchical structure in the decision space, thus tree search idea can be applied to improve the efficiency of learning.\n\nIn general the trend of the paper is easy to follow. However, the proposed method is a bit ad-hoc and I don’t think the paper provides enough justification for it. \n1. FPF is not really practical in that it needs to main D! decision trees, each one with N^D leaves. On the other hand, other approximation methods like PPF, relies on some independence assumption. However, this paper provides not analysis about such independence even in simulations. For example, when these independence assumptions are strongly violated, what the performances would be.\n2. There is no experiments on real data. The simulation in the this paper is very specific and I am not sure how representative it could be. The simulation results don’t provide a clear message too.\n\nOther comment\n1. On page 4, why would P(f_{d1:d_D}) be propotional to P(f_{d1:d_i})? \n\n\n=========================\nAfter Rebuttal\nI don't think the rebuttal addresses my concern: the proposed method is not well justified and its performance is not really analyzed nor with convincing evidence.", "Thank you for your comments. \n\n1, about your first concern that sensitive to the ordering. In our algorithm, we do not just rely on one particular ordering of dimensions, instead, we randomly pick multiple different dimensions orders from all the permutation of the dimensions orders. We repeated S times to go through different dimension ordering to overcome the sensitivity of just using one dimensions order.\n\n2.1, Regarding Bai etal 2014: In Bai’s paper, it utilize MAB algorithm UCT to solve Markov Decision Process. However as we said in the beginning of our paper, we are interested in solving MAB problem inspired by Monte Carlo Tree Search. especially, in online experiment, Thompson Sampling (TS) algorithm attracts a lot of attention due to its simplicity at implementation and resistance in batch updating.\n\n2.2, about the repeat S times the path planning procedure in Algorithm 1, this part is very necessary. after each run of path planning procedure, we only find one sub-optimal arm based on the heuristic dimension ordering. we will have S different candidates arms and then select the best one within the S arms to roll out using Thompson sampling.\n\n2.3, about the question on the \"node\" priors updating. Here the notation Node(fdL|fd1:dL−1) is used to load the node fdL’s (with prePath fd1:dL−1) hidden states in to memory, which corresponds the joint density P(fd1:dL). It worth to note that any relative order of [fd1:dL] represents the same joint distribution (with same hidden states).  So, in practice, we do not need to recompute them every time but just maintain all the joint density P(fd1:dL). And then, the Node(fdL|fd1:dL−1) will only requires O(1) computation complexity without the need of re-computation. \n\n2.4, we use back propagation to update Beta posterior,  which is trivial for our setting. The posterior of an internal node does not only depend on itself but also the previous planning procedure, that’s why we use Node(f_dL | f_d1 : f_d(L-1)) to difference them.\n\n3, for the variants of path planning methods, actually we considered different assumptions, the degree m of interactions is important knowledge/assumptions when using BOOSTED DESTINATION SHIFT method, the independence assumption is for D-MAB, and the conditional independence assumption is for PPF. for the DS, it comes from hill climbing idea, however, we can also treat it as bottom-up tree-search.\n", "Thank you for your comments. \n\n1. We understand that FPF needs to maintain D! trees. But comparing with maintaining N^D arms, it is not that bad. We further provide other approximation methods like PPF by assuming up to m-way interactions, which is normal in our practice of fitting regression models.\n\n2. Our simulation covariates are randomly generated and we also control its relavent weights by changing alpha and beta.", "This paper approaches the problem of exploding arms in multivariate multi-armed bandits. To solve this problem, the authors suggest an approach that uses Thompson sampling for arm selection and decision trees and graphs (inspired from Monte-Carlo tree search) to model reward success rates. They propose to versions of this path planning procedure and 2 version inspired from Hill-climbing methods. They validate their claims on a simulation showing better performance and faster convergence, and providing an extensive analysis of the results. \n\nThe idea seems novel. The paper is well structured, but the writing can be improved, and some parts are hard to read and follow (See minor comments below). \n\nHere are few questions:\n- The authors claim that the approach can be extended to categorical/numeric rewards. Can they give more details on how? \n- The experiments are done only with D=3 and N=10. How easy would it be to scale to higher dimensions?\n- In the curves of Figure 3, N^D-MAB and DS seem not converged yet contrarily to the other methods. Have the authors tried to let them run for longer to see to which values they converge? \n\nMinor comments (non-exhaustive examples): \n- Punctuation issues: \n    *Multi-Armed Bandit (MAB) problem, is widely ...\n    * Generally, RT is on the order O( T) under linear payoff settings Dani et al. (2008)Chu et al. (2011)Agrawal & Goyal (2013). Although the optimal regret of non-contextual Multivariate MAB is on the order O(logT ) ...\n\n- Imprecise statements:  ... for each combination of C (assuming not too many), ...\n\n- Sentences to rewrite: \n     * The posterior sampling distribution of reward is its likelihood integrates with some fixed prior distribution of weights μ.\n     * ..., and would call joint distribution of (A, C) and (B, C) are independent. \n     * ..., which means the algorithm converges selection to single arm (convergence) as well as best arm. \n\n- Typos: Jointly distribution/relationship -> joint ,It worth to note -> it is worth to note, extensively exam -> extensively examine, a serial processes -> process, would been -> be ...\n "], "review_score_variance": 4.222222222222222, "summary": "This paper tackles the multivariate bandit problem (akin to a factorial experiment) where the player faces a sequence of decisions (that can be viewed as a tree) before obtaining a reward. The authors introduce a framework combining Thompson Sampling with path planning in trees/graphs. More specifically, they consider four path planning strategies, leading to four approaches. The resulting approaches are empirically evaluated on synthetic settings.\n\nUnfortunately, the proposed approaches lack theoretical justification and the current experiments are not strong enough to support the claims made in the paper. Given that most of reviewer's concerns remained valid after rebuttal, I recommend to reject this paper.", "paper_id": "iclr_2020_H1lWzpNKvr", "label": "val", "paper_acceptance": "reject"}
