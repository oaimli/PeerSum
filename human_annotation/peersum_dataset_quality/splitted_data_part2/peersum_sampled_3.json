{"source_documents": ["Jiun Tian Hoe, Kam Woh Ng, Tianyu Zhang, Chee Seng Chan, Yi-Zhe Song, Tao Xiang", " $\\textbf{Larger loss with short code.}$ A very short code indeed challenges most hashing methods. However, in the Supplementary Material, Section D.3 (Page 6), we have shown in Table 4 that explicitly maximizing inter-class hamming distance (maxHD) or Hadamard matrix style for target code generation performs better than Bernoulli with short code (e.g. 16 bits in ImageNet100). We plan to examine more carefully the impact of  (extremely-)short code (e.g. $<8$ bits) as part of our future work.\n\n$\\textbf{Evaluation Protocol.}$ All of our evaluation protocols are strictly followed all previous reported works [1,2,3,4,5,6,7,8] in order to have a fair comparison as detailed in our Supplementary Material, Section C.3 - Evaluation Detail. We will make it clearer.\n\nMore specifically, following the settings of prior works on category-level retrieval,  we used mAP@1000 for ImageNet100, mAP@5K for NUSWIDE, and MSCOCO. To make it more consistent,  we now have conducted new evaluations on mAP@[10,100,1000,10000,all] of 64-bits ImageNet100, NUSWIDE \\& MSCOCO of DTSH (perform well for multi-labeled), CSQ (perform well for single-labeled) and our method (OrthoCos+BN) for category-level retrieval. The results are recorded below. It is observed that our method still outperforms SOTAs under these mAP metrics. Although our method is only on par with CSQ and DTSH in mAP@all, it is worth noting that in mAP@all, the order of all retrieved images are being considered altogether, in contrast for mAP@1000, only the order of first 1000 retrieved are being considered. Hence, the protocol (i.e. mAP@1K for ImageNet100, mAP@5k for NUS-WISE and MSCOCO) set by SOTAs [1,2,3,4,5,6,7,8] are reasonable in this case.\n\n| 64-bits ImageNet100 | mAP@10 | mAP@100 | mAP@1000 | mAP@10000 | mAP@all |\n| ------------------- |:------:| -------:| -------- | --------- | ------- |\n| DTSH                | 0.667  | 0.631   | 0.585    | 0.503     | 0.480   |\n| CSQ                 | 0.649  | 0.706   | 0.696    | 0.655     | 0.608   |\n| OrthoCos+BN         | 0.720  | 0.721   | 0.711    | 0.664     | 0.608   |\n\n| 64-bits NUSWIDE | mAP@10 | mAP@100 | mAP@1000 | mAP@10000 | mAP@all |\n| ------------------- |:------:| -------:| -------- | --------- | ------- |\n| DTSH                | 0.884 | 0.865   | 0.856    | 0.841     | 0.740   |\n| CSQ                 | 0.884  | 0.864   | 0.852    | 0.827     | 0.714   |\n| OrthoCos+BN         | 0.892  | 0.874   | 0.864   | 0.839     | 0.712   |\n\n| 64-bits MSCOCO | mAP@10 | mAP@100 | mAP@1000 | mAP@10000 | mAP@all |\n| -------------- |:------:| -------:| -------- | --------- | ------- |\n| DTSH           | 0.915  | 0.824   | 0.786    | 0.736     | 0.666   |\n| CSQ            | 0.934  | 0.859   | 0.821    | 0.760     | 0.644   |\n| OrthoCos+BN    | 0.943  | 0.871   | 0.830    | 0.763     | 0.636   |\n\nFor the evaluation metrics employed in the instance-level retrieval, we also strictly followed all previous reported works [9,10]. This task is more difficult, e.g., the number of images per instance have lesser than 100 images in GLDv2. For GLDv2, we retrieve 100 images from the database which contain 762K images, and note that there will be about 100K number of unique landmarks, hence measuring mAP@100 seem reasonable. Since ROxf/RPar contain only 5000 to 6000 images (with about 11 unique landmarks), mAP@all seem reasonable too. Note that, we did not include ROxf/RPar into training;  we only used GLDv2-trained model run query for ROxF/RPar.\n\n$\\textbf{Only ordinary improvements are achieved.}$ Please refer to the answer from \"(4) Results explanation in Table 1\" in the reply to Reviewer 7uiu.\n\n$\\textbf{Scalability to very large dataset.}$ Thanks. At the moment,  all of our experiments in category-level is around 130K images and for instance-level retrieval (i.e., GLDv2), the database images are about 762K. These are reasonable scale and used by most of the reported work too. We agree that our method may shine even more with larger scale benchmarks. However,   due to the limited computation resources at academia and limited time, we can only leave it to future work.\n\n$\\textbf{Reference:}$\n1. Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Philip S. Yu. Hashnet: Deep learning to hash by continuation. ICCV 2017.\n2. Lixin Fan, Kam Woh Ng, Ce Ju, Tianyu Zhang, and Chee Seng Chan. Deep polarized network for supervised learning of accurate binary hashing codes. IJCAI 2020.\n3. Shupeng Su, Chao Zhang, Kai Han, and Yonghong Tian. Greedy hash: Towards fast optimization for accurate hash coding in cnn. NeurIPS 2018.\n4. Haomiao Liu, Ruiping Wang, Shiguang Shan, and Xilin Chen. Deep supervised hashing for fast image retrieval. CVPR 2016.\n5. Yuming Shen, Jie Qin, Jiaxin Chen, Li Liu, Fan Zhu, and Ziyi Shen. Embarrassingly simple binary representation learning. ICCV Workshop 2019.\n6. Rongkai Xia, Yan Pan, Hanjiang Lai, Cong Liu, and Shuicheng Yan. Supervised hashing for image retrieval via image representation learning. AAAI 2014.\n7. Hanjiang Lai, Yan Pan, Ye Liu, and Shuicheng Yan. Simultaneous feature learning and hash coding with deep neural networks. CVPR 2015.\n8. Li Yuan, Tao Wang, Xiaopeng Zhang, Francis EH Tay, Zequn Jie, Wei Liu, and Jiashi Feng. Central similarity quantization for efficient image and video retrieval. CVPR 2020.\n9. Bingyi Cao, AndrÃ© Araujo, and Jack Sim. Unifying deep local and global features for image search. ECCV 2020.\n10. Filip Radenovic, Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. Revisiting oxford and paris: Large-scale image retrieval benchmarking. CVPR 2018.", " We would first like to thank the reviewer for the constructive suggestions.\n\n$\\textbf{(1) Inconsistent with sentence and Table 1 in abstract.}$ We would like to point out that Table 1 compares category-level retrieval results, while the statement in abstract is on our method's superior performance on instance-level retrieval benchmarks. The results in Table 2 do show that our single-loss method clearly beat SOTA multi-loss alternatives on instance-level retrieval. We will further clarify.\n\n$\\textbf{(2a) Baseline without change.}$ Thank you but that's exactly what we did. Specifically, all our baseline experiments in Table 1 and Table 2 were conducted with the baseline methods without any change, with the only exceptions being SDH-C and CE (Table 1). SDH [5] was first proposed in 2015 and they were using their own designed architecture (only 3 layers as backbone, yet using GIST descriptor as input), however, we think that the multiple loss functions (e.g., quantization, orthogonality and bit balancing) proposed in SDH are worth to compare, hence we modified the method such that it is using AlexNet as backbone and using classification objective instead of pairwise objective.\nAs for CE, we also included CE+BN and CE+BiHalf to make it more competitive. \nFinally, for a fair comparison (as opposed to previous works which often have inconsistent hyperparameters setups), instead of taking the original results from the original paper, we ran all methods with AlexNet as backbone, Adam as optimizer and learning rate of 0.0001. We even used the same categories of ImageNet100 for all methods, as most papers did not mentioned what random categories they have chosen for evaluation. This explains the performance discrepancies to the reported results in the original papers.\n\n$\\textbf{(2b) Variant baseline which replace multi-loss functions with proposed single loss.}$ Thanks. That's actually how we implemented our method with the proposed single loss -- we take an existing model and replace its multiple losses with our single one. For instance, our OrthoCos is based on CSQ [1]. The original CSQ model has no classification objective, but contains hash center as code target. We thus replace the CSQ loss functions with our single loss function resulting in our OrthoCos. We will make that clearer. \n\n$\\textbf{(3a) Typo in the section 3.}$ Thanks for pointing it out.  $o_i$ is indeed a column vector. A corrected version of the sentence is $o_i \\in [o_1, \\dots ,o_C]^\\intercal = O\\in\\\\{-1,+1\\\\}^{C \\times K}$. We will correct all these issues.\n\n$\\textbf{(3b) Why unsupervised Bihalf layer is used in supervised learning.}$ Indeed,  Bihalf was initially proposed by [3] to solve unsupervised hashing problem. In [3], the Bihalf layer was appended into a FC layer to balance the output of every bit and solve the unsupervised hashing with unsupervised loss from [4]. Such a balance is also desirable under the supervised setting. Therefore,  in our experiment, we adopt the Bihalf layer in our supervised learning model. We will make it clearer.\n\n$\\textbf{(3c) Evaluation Metric/Protocol.}$ Please refer to the answer from \"Evaluation Protocol.\" in the reply to Reviewer MNiD.\n\n$\\textbf{(4) Should compare more newly deep hashing methods in the experiments.}$ It is worth noting that the latest and popular baseline methods were indeed selected in Tables 1 and 2 at the time of submission. For instance, we have included the latest work from CVPR2020 (CSQ [1]), IJCAI2020 (DPN [2]) and AAAI2021 (Bihalf [3]). We will try to include newer works if accepted.\n\n$\\textbf{Reference:}$\n1. Li Yuan, Tao Wang, Xiaopeng Zhang, Francis EH Tay, Zequn Jie, Wei Liu, and Jiashi Feng. Central similarity quantization for efficient image and video retrieval. CVPR 2020.\n2.  Lixin Fan, Kam Woh Ng, Ce Ju, Tianyu Zhang, and Chee Seng Chan. Deep polarized network for supervised learning of accurate binary hashing codes. IJCAI 2020.\n3. Yunqiang Li and Jan van Gemert. Deep unsupervised image hashing by maximizing bit entropy. AAAI 2021.\n4. Shupeng Su, Chao Zhang, Kai Han, and Yonghong Tian. Greedy hash: Towards fast optimization for accurate hash coding in cnn. NeurIPS 2018.\n5. Venice Erin Liong, Jiwen Lu, Gang Wang, Pierre Moulin, and Jie Zhou. Deep hashing for compact binary codes learning. CVPR 2015.", " $\\textbf{(2) Under what conditions the proposed method would fail?}$ One such condition is when the number of bits is too small (<8 bits), especially when number of classes is much greater than the number of bits. In this case, there will be overlapping in the generated target code (i.e., the number of maximum unique codes is equal to $2^K$ where $K$ is number of bits). Overcoming this limitation is part of the  future work. We will add this discussion.\n\n$\\textbf{(3) Modification on our proposed method with pairwise constraints.}$ Good questions! We think it is possible to treat this problem as instance-level retrieval (e.g., image matching [2]) where every pair are considered as a single instance pair. This way, we can treat each instance (e.g., if pair A <-> B, and pair B <-> C. As such, we can possibly deduce that all A, B, C are the same instance) as a single class and perform the experiments like how we did in Section 4.2 (instance-level retrieval). \n\n$\\textbf{(4) Results explanation in Table 1.}$ Thank you. We think it is the combination of both factors: \na) The category-level retrieval task is relatively easy and the latest results reported on both datasets are clearly saturating. This explains the small gaps between different methods in Table 1.  We therefore tested our method on the more difficult  instance-level retrieval task (Section 4.2 and Table 2). For this task the advantage of our method over the SOTA alternatives is much clearer. \n(b) NUS-WIDE/MS-COCO are multi-labeled data (and with 21/80 classes), hence it might be easier for pairwise/triplet-wise losses based methods. For them, as long as one of the labels appeared in both query and retrieved images, it is considered as positive, otherwise negative. The retrieval evaluation follows exactly the same setting, therefore favors these pairwise/triplet-wise methods. In contrast, our method treats multi-labeled as a classification problem with  harder constraints on  the output having an equivalent probability for the labeled classes based on the label-smoothing concept. This mismatch in the training objective and test setting thus gives our method a disadvantage.\n\n$\\textbf{(5) Relation with semantic hashing [1].}$ Great suggestion! We will cite and compare with this work as suggested. These two works are indeed related but also have vital differences. More specifically, the method in [1] is based on the probability of collision under SimHash [4],  and describes the hashing problem under pairwise constraint. In contrast, our method is pointwise. In the loss design our single loss is also very different from theirs. \nOverall, we think pointwise method would be better than pairwise/triplet-wise based methods in learning to hash. \nThis is because  pairwise/triplet-wise based methods need to solve a sample mining problem as well (also discussed in [1]), making the optimization problem harder. \n\n$\\textbf{(6) Overlap between two distributions in Figure 6.}$ Thank you. We believe you meant Figure 3, as we do not have a Figure 6 in our paper and supplementary. As requested, we measured the overlapping by using histogram intersection, and the results are 0.2365, 0.2049 and 0.2586 for Figure 3a,b,c respectively. However, this does not represent the true performance as these overlapping area were happened mostly at the region of inter-class (specifically, at hamming distance of 16 to 40), where this distance are used to positioned the retrieved images in end of the retrieval.\n\nTherefore, we consider it more appropriate to look into on those regions where the hamming distance ranges between 0 to 15. In this way, other than overlapping area, we also measure the sum of area as well. The sum of area (the bins of intra-class only) are 0.3946, 0.5414 and 0.5613 and the overlapping are 0.0024, 0.0029 and 0.0014 for Figure 3a,b,c respectively. As we can see, while the overlapping area is too insignificant to verify, the sum of area from 0 to 15 are proportional to both the distance between two distributions (i.e., 13.90, 17.34 and 18.31) and the performance (i.e., 0.573, 0.659 and 0.711). While the exact performance may be explained by multiple factors, we believe that the distance between two distributions in Figure 3 can help explain the superiority of our method.\n\n$\\textbf{Reference:}$\n1. Levi Boyles, Aniket Anand Deshmukh, Urun Dogan, Rajesh Koduru, Charles Denis, Eren Manavoglu. Semantic Hashing with Locality Sensitive Embeddings. https://openreview.net/forum?id=sFDJNhwz7S\n2. https://image-matching-workshop.github.io/\n3. Ye, Mang, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling Shao, and Steven CH Hoi. Deep learning for person re-identification: A survey and outlook. TPAMI 2021.\n4. Moses S Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of the thirty-fourth annual ACM symposium on Theory of Computing, pages 380â388, 2002.", "The paper proposes a deep hashing model with only a single learning objective. It avoids the problem that existing deep hashing models are difficult to train due to a large number (>4) of losses. Specifically, it maximizes the cosine similarity between the continuous codes and their corresponding binary orthogonal codes to ensure both the discriminative capability of hash codes and the quantization error minimization. Besides, it adopts a Batch Normalization layer to ensure code balance and leverages the Label Smoothing strategy to modify the Cross-Entropy loss to tackle multi-labels classification. Extensive experiments show that the proposed method achieves better performance compared with the state-of-the-art multi-loss hashing methods on several benchmark datasets.  The paper proposes a deep hashing model with only a single learning objective. It avoids the problem that existing deep hashing models are difficult to train due to a large number (>4) of losses. Specifically, it maximizes the cosine similarity between the continuous codes and their corresponding binary orthogonal codes to ensure both the discriminative capability of hash codes and the quantization error minimization. Besides, it adopts a Batch Normalization layer to ensure code balance and leverages the Label Smoothing strategy to modify the Cross-Entropy loss to tackle multi-labels classification. Extensive experiments show that the proposed method achieves better performance compared with the state-of-the-art multi-loss hashing methods on several benchmark datasets.\n\nStrengths: \n1. This work is meaningful for effectively optimizing the deep hashing models. It unifies the training objectives of deep hashing under a single classification objective.\n2. The motivation is well presented by the authors in the manuscript.\n3. The paper carries out extensive experiments and the supplementary material is rich in content.\n\nWeaknesses:\n1. According to the experimental results in Table 1, the proposed method does not consistently outperform the baseline methods, which is inconsistent with the sentence\"outperforming the state-of-the-art multi-loss hashing models on three large-scale instance retrieval benchmarks, often by significant margins\" mentioned in Abstract. In addition, the performance improvement of the proposed method is not significant.\n\n2. As the main contribution of this work is to propose a unified objective function, to verify its superiority, the authors should compare the results of the following two experiments:\na) Baseline methods without any change \nb) The variant baselines which replace the multiple loss functions with the proposed single loss \n\n3. There are several issues that the authors may want to explain.\na) In the second paragraph of section 3 OrthoHash: One Loss for All, the authors write \"o_i denotes a column vector belongs to i-th class\". Since the dimension of the binary orthogonal targets O is CÃK, I think o_i is a row vector.\nb) The proposed method is supervised. Please expain the reason why using the unsupervised Bihalf as the baseline?\nc) Why mAP@1k is used for ImageNet100 and mAP@5k for MS COCO for performance evaluation, especially when the two datasets are close in size. Besides, the metrics of performance evaluation for instance-level benchmark datasets are also different.\n\n4. For experimental evaluation with SOTA methods, the authors should compare more newly deep hashing methods in the experiments. 1. According to the experimental results in Table 1, the proposed method does not consistently outperform the baseline methods, which is inconsistent with the sentence\"outperforming the state-of-the-art multi-loss hashing models on three large-scale instance retrieval benchmarks, often by significant margins\" mentioned in Abstract. In addition, the performance improvement of the proposed method is not significant.\n\n2. As the main contribution of this work is to propose a unified objective function, to verify its superiority, the authors should compare the results of the following two experiments:\na) Baseline methods without any change \nb) The variant baselines which replace the multiple loss functions with the proposed single loss \n\n3. There are several issues that the authors may want to explain.\na) In the second paragraph of section 3 OrthoHash: One Loss for All, the authors write \"o_i denotes a column vector belongs to i-th class\". Since the dimension of the binary orthogonal targets O is CÃK, I think o_i is a row vector.\nb) The proposed method is supervised. Please expain the reason why using the unsupervised Bihalf as the baseline?\nc) Why mAP@1k is used for ImageNet100 and mAP@5k for MS COCO for performance evaluation, especially when the two datasets are close in size. Besides, the metrics of performance evaluation for instance-level benchmark datasets are also different.\n\n4. For experimental evaluation with SOTA methods, the authors should compare more newly deep hashing methods in the experiments.", "In this paper, authors propose a novel deep hashing model with only a single learning objective and claim that state of the art papers generally use a large number (>4) of losses. Specifically, authors show that maximizing the cosine similarity between the continuous codes and their corresponding binary orthogonal codes can ensure both hashcode discriminativeness and quantization error minimization. Authors show significance of their method using extensive experiments. \n  1) Main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. Proposed method is simple but novel. The submission is technically sound and well supported by experimental results. \n2) Under what conditions the proposed method would fail? Can authors comment more on this?\n3) Using orthogonal targets is a simple but clever idea. Can authors comment on how can modify this method when one doesnât have categorical labels but have some pairwise constraints and the goal is to learn quantized representation learning for better retrieval? \n4) In Table 1, proposed method is not always the best. From the discussion it was not clear why? Can authors comment more on why? Is it because the state of the art methods were already close to the best possible results or there is some drawback of the method that does not improve the results on some datasets? \n5) Angular margin used in the paper seems very related to semantic hashing [1] where angular similarity was used to generate representations better for hashing retrieval. Can authors compare and comment? \n6) In Figure 6, not only the distance between two distributions (intra class and inter class) matter but also the overlap between them matters. Can authors calculate and comment on the overlap between two distributions?  \n\n[1] https://openreview.net/pdf?id=sFDJNhwz7S  I don't see any potential negative societal impact of their work. ", "This paper presents OrthoHash, a new deep hashing model that is trained by maximizing the cosine similarity between the continuous codes and the corresponding binary orthogonal target. In addition, code balancing is achieved by using batch normalization. Different from prior works that typically involves multiple loss functions, the training objective in this paper is simpler and easy to optimize.   Strength:\n- Neat idea and good novelty: the proposed training objective is well designed with the idea of cosine similarity. The final formulation with softmax-like loss function is very interesting, and it is simpler than the existing multi-loss approaches. Additionally, the use of BN for balancing code is elegant. \n- Strong results on multiple datasets. It is highly competitive compared to multi-loss approaches. \n- Good ablation studies to demonstrate the performance impact of different factors.\n- The paper is well-written and easy to understand.\n\nWeakness:\n- One possible weakness is that it may have large performance degrade when the bit length is short. That is because in the target code generation, the target is sampled from Bern(0.5). One may expect larger information loss with short code.\n- Evaluation: For ROxf and RPar, the authors use multi-scale features for instance retrieval. It is unclear to me if the compared exiting deep hashing methods also use such a multi-scale scheme. Does it a fair comparison?\n- Evaluation: The authors use mAP@1K for ImageNet100, but mAP@5k for NUS-WISE and COCO. In instance retrieval, the authors use mAP@100 and mAP@all instead. They are inconsistent and not easy to interpret the performance. The authors may want to provide some intuitions/discussions about their design choices. For example, how many returned images are required to better reflect the  retrieval performance?  What's the effect of evaluating all returned images?  \n- In current presentation, the proposed method performs only on par with prior works in category-level retrieval. This gives an impression that only ordinary improvements are achieved. I believe this paper has many advantages that are not well demonstrated in current form. One shining point should be its scalability to very large dataset. I was hopping to see at least one experiment at a large scale, to clearly differentiate itself from pair-wise and triplet-wise methods. This would strengthen the paper significantly. \n\n\nOverall:\n- Overall this is a nice paper. My main concerns are in their evaluation. But this paper has merits that outweigh the flaws.\n- This paper presents a simpler learning objective that can reduce the difficulty of training deep hash models. Given its effectiveness and the simplicity, I think this paper could serve as a strong baseline to facilitate future research. In my opinion, it would bring insightful contributions to our community. My initial recommendation is Accept. Yes"], "review_score_variance": 2.0, "summary": "Authors propose a novel deep hashing model with only a single learning objective which is a simplification from most state of the art papers generally use lots of losses and regularizer. Two of the three reviewers liked its effectiveness and the simplicity, they believe it would bring insightful contributions to our community. Reviewers liked the final formulation with softmax-like loss function. They also found the use of Batch normalization for balancing code elegant.\n\nOne of the reviewers raised certain concerns.  The authors in rebuttal clarified that most of the negative argument raised by reviewers are misunderstanding and are not valid. It is clear that the concerns raised were already addressed in the main draft. \n.\nOverall, the merits of the paper justifies the publication.  ", "paper_id": "nips_2021_2pJZSVcSZz", "label": "train", "paper_acceptance": "accept"}
{"source_documents": ["Generative models have achieved impressive results in many domains including image and text generation. In the natural sciences, generative models have lead to rapid progress in automated drug discovery.  Many of the current methods focus on either 1-D or 2-D representations of typically small, drug-like molecules. However, many molecules require 3-D descriptors and exceed the chemical complexity of commonly used dataset. We present a method to encode and decode the position of atoms in 3-D molecules along with a dataset of nearly 50,000 stable crystal unit cells that vary from containing 1 to over 100 atoms. We construct a smooth and continuous 3-D density representation of each crystal based on the positions of different atoms. Two different neural networks were trained on a dataset of over 120,000 three-dimensional samples of single and repeating crystal structures. The first, an Encoder-Decoder pair, constructs a  compressed latent space representation of each molecule and then decodes this description into an accurate reconstruction of the input. The second network segments the resulting output into atoms and assigns each atom an atomic number. By generating compressed, continuous latent spaces representations of molecules we are able to decode random samples, interpolate between two molecules, and alter known molecules.", "We uploaded a new version of the paper that we think is a large improvement. We would like to thank the reviewers for very helpful feedback. Below, we outline the specific changes made:\nâ¢ The major concerns of Reviewers 1 & 3 was a comparison with existing methods and the choice of representation. We added a section in the main text to specifically address this, and also clarified various other parts of the main text. We tried to make it clear that existing 1-D and 2-D methods are not appropriate for the task we consider.\nâ¢ Based on the comments of Reviewer 2, we have added M and S to Fig. 2. Thank you for this suggestion. We also added a new section in the supplement on the choice of grid size and spacing. \nâ¢ Based on the suggestion of Reviewer 3, we have tried to make the results section more streamlined. We have moved a few pieces around and added more descriptive text when needed. We also added more section numbers, to help clarify the flow of the text.\nâ¢ We expanded Table 1 to include species information, at the suggestion of Reviewer 3.\nâ¢ We made minor changes, including clarifying that q is the approximate posterior. \nâ¢ Added a few new references (Hoffmann  & NoÃ©), those suggested by Reviewer 1 (Alperstein, Cherkasov, and Rolfe) and those suggested by Reviewer 3.\nâ¢ We improved the clarity throughout the text of the paper.\n[EDIT]\nâ¢ To try to further improve the understanding of the current state of the randomly generated samples (due to the difficult in comparing to current baselines), we added a new Figure (Fig. S7) that shows more random samples of varying complexity. We will release pre-trained models and code after all deadlines. ", "Hello,\nWe want to ask if there is anything more we can clarify in the next day or any more comments you may have about the paper?\nThanks!", "Hello,\nWe want to ask if there is anything more we can clarify in the next day or any more comments you may have about the paper?\nThanks!", "Thank you for your feedback on our work and useful comments.\n\nThank for for the suggestion  on Figure 2. We will update this in the draft and upload shortly.\n\n>> In Section 3.1, why do the authors use a cube with side of â10â Angstrom? And why divide the cube into â30â bins?\nThis is a useful point, and we should be more clear in the text. We played with a variety of sizes for both values, and  found that this was a balance between (a)  including as much data as possible, (b) having grid sizes that enabled reasonable compute time under our conditions and (c ) was meaningfully accurate.\n\n>> The paper revolves on vanilla 3D convolutions of the crystal structures. Have the authors considered how the results would change if SO(3) rotation invariant convolutions were used instead. The SO(3) convolutions would empower capturing all possible rotations of the crystal other than only its canonical form.\nThis is a very useful question, and something  we are actively working on. However, we generally found that the encoder-decoder accuracy is quite good and the weakness is in the difficult segmentation task.\n", "Thank you for spending the time to read the paper carefully and provide very useful feedback. This will be very helpful as we try to revise our manuscript. We wanted to address a few points:\n\nFirst, thank you for acknowledging that the new domain is a strength of the paper. We hope to inspire more work in an important area and think that by combining physical intuition for input representation and using a VAE/U-Net, this is a useful paper tackling a very important problem in a field related to, but in many ways quite different from, that of drug discovery. \n\n>> Limited methodological novelty\nRegarding the limited methodological novelty: A key problem for a new domain is often the choice of representation. Some times, a new representation and method go hand-in-hand. However, we feel in this case the main hinderance is not existing methods, but instead a representation. Work published since this submission uses a similar representation to ours [1]. \n\n>> Lack of comparison with other methods/baselines:\nComparing to other baselines is a very important aspect of science. Unfortunately, to our knowledge there is no method that produces materials of the complexity we consider. CrystalGAN produces a composition rather than a specific 3D description (that would be needed) and only considers a small subset of material types. Graph networks like those used by Xie and Grossman have been very successful for predicting material properties. However, using this approach to generate the 3-D locations and species of many atoms in space (with a variable number of atoms) is not a problem we are aware of being tackled yet. We will make this more clear in an updated version of the paper. See for example [1] which is a paper that came out after the submission of this work that uses many very similar ideas and also does not perform such evaluations.\n\n>> No quantitative evaluation of the generative capabilities of the network:\nRegarding quantitative evaluation: As you point out, logP and QED scores are commonly used for drug-like molecules. However, these are not applicable for the molecules we are looking at. Additionally, because of the physical constraints, creating materials that relax is quite difficult. Therefore, we report distances as in [2]. We will try to include more metrics in an updated version shortly, but unfortunately there is not yet a community accepted set of criteria as generating valid molecules is already a very difficult problem. \n\n>> Presentation of quantitative results: \nRegarding the presentation of quantitative resultsâ thank you for your feedback. We will restructure the results to be more clear. Thank you!\n\n>>Lack of novelty:\nRegarding the novelty- the use of a 3-D representation  based on the physically meaningful atomic number is something that we found to work well as a possible choice of representation for these 3-D crystal  structures. We feel that the application is important, general, and something that benefits from a discussion as to how to best represent molecules, due to the unique requirements of the field. \n\n\nThanks again very much for the very constructive comments. We really appreciate the time you spent to provide feedback on this manuscript. We are clarifying the repeated unit cell text and will update the text with the word approximate. Also, thanks for the additional references! :)\n\n\n[1] Inverse design of Solid-State materials via a Continuous Representation. Juhwan Noh, et. al. Matter.\n[2] Symmetry-adapted Generation of 3D point Sets for the Targeted Discovery of Molecules, Gebaueer, Gastegger, and SchÃ¼tt. \n", "First,  thank you for your comments on our draft. We are in the process of updating the draft, but want to address a few points. \n\nRegarding Gebauerâs work on SchNet,  their current method works with a smaller subset of the chemical space than is  in the dataset that we use. The specifically state that future work  will explore this direction,  but the increased variation in composition makes this very challenging. Therefore, we focus more on using physical  intuition to try to develop and test a possible data representation. \n\nRegarding a comparison with 1D and 2D methods: this is a very interesting point, and something we thought about quite a bit. For most molecules where ML tools has been successful, SMILES strings or graphs have proved an effective data representation. These tend to be relatively small organic molecules from the QM9 database. There are only a handful of possible species types. In all these cases, a string or graph is generated and there is a mechanism by which this string or graph is âtranslatedâ back to a molecule. For crystal structures, the most effective representation  has not been explored. Surely, when a 1D or 2D representation works, it  is far better. For this reason, we found it  very difficult to compare  since these 3D molecules do not have an appropriate SMILES or graph representation that we can use to recover both the 3D geometric arrangement and the composition.  In part, we are hoping to encourage the community to try to begin exploring a more complicated chemical space (similar to G-SchNet). Since we submitted, two new papers have come out in this area [1,2].  Notably, [2] uses ideas very similar to the ones we presented but also does not provide a clear baseline, due to the novelty of the domain. We will add a section in the supplement and main text that more clearly addresses this.\n\n>> Another concern is that the authors claim that the errors in atomic numbers differ only by 1 or 2.\nThis is a very real concern, and something that we thought about considerably. Originally, we tried encoding and decoding group/period rather than atomic number, and found (understandably) improved accuracy. In fact, we found we were able to reconstruct group with just over 93% accuracy. However, for many applications (such as random structure searching) we felt that predicting atomic number and then introducing potential post-processing steps was a preferable alternative. We will add our results on group to the main text.\n\nThanks  again for reading the paper and providing very helpful feedback. We will post an updated manuscript soon, but hope that we have addressed some of your concerns. \n\n[1] âGenerating valid Euclidean distance matricesâ Moritz Hoffmann, Frank NoÃ© arXiv:190.03131\n[2] Inverse design of Solid-State materials via a Continuous Representation. Juhwan Noh, et. al. Matter.\n", "The paper deals with accurately encoding and decoding 3D atomic positions and the crystalâs species using 2 sets of neural networks a) a VAE that builds a compressed latent space representation of a crystal and b) a UNET for segmenting the latent space into atoms and assigns each atom to its atomic number. Experiments were conducted on over 120K 3D samples of crystals and the results seem to be promising.\n\nThe paper is neatly written and well organized.\n\nComments:\n\nA) Figure 2: For completion, consider marking M and S as outputs of the VAE and U-Net respectively. \n\nB) In Section 3.1, why do the authors use a cube with side of â10â Angstrom? And why divide the cube into â30â bins?\n\nC) The paper revolves on vanilla 3D convolutions of the crystal structures. Have the authors considered how the results would change if SO(3) rotation invariant convolutions were used instead. The SO(3) convolutions would empower capturing all possible rotations of the crystal other than only its canonical form.", "PAPER SUMMARY: This paper addresses the problem of encoding and decoding 3D chemical structures, with the ultimate goal of generating 3D crystal structures. The authors propose an auto-encoder framework for encoding the 3D locations of atoms in the crystal to a latent representation and then decoding that representation back into 3D structure. The paper's contributions can be summarized as follows:\n1) A data representation that converts the 3D atom locations to a 3D voxel density map, so that they can be encoded by a standard 3D convolutional network.\n2) A decoder network that first estimates a 3D density map from the latent vector using upsampling and convolutions and then classifies the atom type (atomic number) per voxel using a 3D segmentation.\n3) The network is applied on unit cells of crystals or repeated unit cells from a dataset of crystal structures and the network is shown to be able to accurately reconstruct the 3D density maps, predict the number of atoms in the cell and perform fairly well in their classification into atomic types.\n\nI appreciate that the paper addresses an interesting problem that is not sufficiently explored and can motivate the development of novel methods that generate 3D molecules with particular structure and multiple types of atoms, however the current work combines existing methods, without any architectural modifications that exploit the new domain.  If the presentation of results and experiments is improved, this could be a good application paper in material science with an interesting combination of techniques from machine learning and computer vision. It would be more appropriate to submit this paper to a domain-specific venue rather than to ICLR. Therefore, I cannot justify its acceptance to ICLR in its current format.\n\nStrengths\n-----------------------\n1.\tNew domain: The paper addresses an interesting problem in a new domain. Previous work on generative models for molecules have concentrated on molecules with 1D structure (which can be represented as strings) or 2D structure (which can be represented as planar graphs). Generating 3D molecule structures is a challenging and interesting problem.\n2.\tEfficient data representation: The authors bypass the difficulties associated with modeling sets of 3D points with arbitrary structure by proposing a canonical, voxel-based density representation.\n3.\tJoint VAE + UNet training: The joint training of the encoder-decoder VAE and the 3D segmentation network results in a decoder that can reconstruct atom locations and atom types, being robust to mistakes in the density map reconstruction.\n4.\tQualitative results: There are nice visualizations of the reconstructed molecules and of the effect of the latent variable z. Figure 3 in particular does a great job at elucidating the outputs of the network and the reconstruction quality.\n\nWeaknesses\n------------------------\n1.\tLimited methodological novelty: The methods used in the paper, i.e. the data representation (see detailed comments), the encoder network, the decoder network and the segmentation network are all existing methods without any (or with only minor) modifications.  Their combination also seems straightforward, except for the joint training of the VAE and the segmentation network, which has not been tried before to the best of my knowledge. Networks have not been modified to exploit the intricacies of the new domain/task, such as symmetries, repetition of unit cells, large number of atoms.\n2.\tLack of comparison with other methods/baselines: There is no quantitative comparison with alternative baselines or methods or even discussion of such alternatives. It is understandable that the paper addresses a relatively new problem, however the method could be compared to CrystalGAN. Also, there is no justification of the advantages of using a voxel-based density map representation along with 3D convolutions vs, for instance, a graph representation along with graph convolutions (Xie & Grossman, [3]) or a point cloud GAN (Achlioptas). \n3.\tNo quantitative evaluation of the generative capabilities of the network: In the case of drug molecule generation, logP and QED scores  [4,5] are used to evaluate their drug-likeness. Without such scores in the crystal generation domain, it is not easy to judge how good a generated crystal is.  The need for some type of quantitative evaluation is especially important, since it is not as easy to qualitatively judge the quality of crystals obtained by sampling random latent vectors (Fig 5B) as in the case of generated images or text. The only such evaluation presented in the paper is related to the distribution of distances between atoms which seems to be close to their actual distribution in nature. \n4.\tPresentation of quantitative results: The quantitative results are scattered in text throughout the results section without being summarized in a table. Results about the distance of generated atoms w.r.t to their true location, the predicted atom counts, predicted atom types etc. for the cases of single unit cells and repeated unit cells should be added to a table to complete a rigorous experimental evaluation. \n\nAdditional Comments\n-------------------------\n1.\tLack of novelty: The data representation, which is presented as one of the core contributions of the paper, is not new. Radial basis functions have been used to generate 3D density maps (e.g. [2]) and 3D Convolutional VAEs (Brock2016, [1]) have been employed on density maps (e.g. occupancy maps) on computer vision tasks. \n2.\tConfusing description of the repeated unit cells case: It is not clear, especially for a general audience, how the repeated unit cells data are generated (in which directions are the unit cells repeated, how many times). It is also not clear how the training routine (conditioning) and the output post-processing (connected components + majority voting) is modified.\n3.\tAfter Eq. (2), q is the --approximate-- posterior â¦\n\nSuggested References\n--------------------------\n\n1.\tVoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition\n2.\tVV-NET: Voxel VAE Net with Group Convolutions for Point Cloud Segmentation\n3.\tDynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs\n4.\tGraph Convolutional Policy Network forGoal-Directed Molecular Graph Generation\n5.\tGrammar Variational Autoencoder\n6.\t3D U-Net: Learning Dense VolumetricSegmentation from Sparse Annotation\n7.\tPointConv: Deep Convolutional Networks on 3D Point Clouds\n", "The authors describe a method to encode and decode the position of atoms in 3-D molecules. An encoder-decoder architecture is used to create a representation of a molecule and to reconstruct the molecule from its representation. Then a second Neural Network segments the output and assigns an atomic number. Prior work on this task has used 1D (SMILES) and 2D (Graph) representations. The authors argues that exploiting 3D structure can create better representations.\n\nAs the paper's related work section shows, this is not the first attempt to use 3D structure to create molecular representations. Unfortunately, the paper does not compare their work to prior work on 3D structure representations (e.g Gebaur et al 2019). Also, it is not clear whether the 3D representation is better than 1D or 2D representations especially since there have been many new 1D models that perform very well for tasks like molecular property prediction (For example All SMILES VAE https://arxiv.org/abs/1905.13343 ). I think the community will benefit if the authors perform a comparison with state of the art 1D and 2D models. I think this is a main drawback of this work.\n\nAnother concern is that the authors claim that the errors in atomic numbers differ only by 1 or 2. But doesn't this show that the network has not learnt a good representation? Because atoms that differ in atomic number by 1 or 2 will have different valencies and hence exhibit different properties? On the other hand, if the authors can show that the errors in atomic numbers suggest they correspond to similar atoms (may be along the same column in the periodic table), then one can have better confidence that the network has learned a meaningful representation."], "review_score_variance": 8.666666666666666, "summary": "This paper presents an encoder-decoder based approach to construct a compressed latent space representation of each molecule. Then a second neural network segments the output and assigns an atomic number. Unlike previous works using 1D or 2D representations, the proposed method focuses on the 3D representations.\n\nThe reviewers have several major concerns. Firstly, the novelty of the paper seems to be limited as the proposed method mainly use the existing techniques. Secondly, there is no clear baseline to compare with. Finally, there is no clear quantitative results to measure the proposed method. The rebuttal did not well address these problems.\n\nOverall, this paper did not meet the standard of ICLR and I choose to reject the paper.\n", "paper_id": "iclr_2020_S1enmaVFvS", "label": "train", "paper_acceptance": "reject"}
{"source_documents": ["Dropout is a simple yet effective technique to improve generalization performance and prevent overfitting in deep neural networks (DNNs). In this paper, we discuss three novel observations about dropout to better understand the generalization of DNNs with rectified linear unit (ReLU) activations: 1) dropout is a smoothing technique that encourages each local linear model of a DNN to be trained on data points from nearby regions; 2) a constant dropout rate can result in effective neural-deactivation rates that are significantly different for layers with different fractions of activated neurons; and 3) the rescaling factor of dropout causes an inconsistency to occur between the normalization during training and testing conditions when batch normalization is also used.  The above leads to three simple but nontrivial improvements to dropout resulting in our proposed method \"Jumpout.\" Jumpout samples the dropout rate using a monotone decreasing distribution (such as the right part of a truncated Gaussian), so the local linear model at each data point is trained, with high probability, to work better for data points from nearby than from more distant regions. Instead of tuning a dropout rate for each layer and applying it to all samples, jumpout moreover adaptively normalizes the dropout rate at each layer and every training sample/batch, so the effective dropout rate applied to the activated neurons are kept the same. Moreover, we rescale the outputs of jumpout for a better trade-off that keeps both the variance and mean of neurons more consistent between training and test phases, which mitigates the incompatibility between dropout and batch normalization. Compared to the original dropout, jumpout shows significantly improved performance on CIFAR10, CIFAR100, Fashion- MNIST, STL10, SVHN, ImageNet-1k, etc., while introducing negligible additional memory and computation costs.", "Thanks for your comments! We add a thorough ablation study as you suggested, but do not agree with other points.\n\nQ1: However, I find the intuitive reasoning unclear and have to lean much more on empirical evidence.\n\nR1: Modification 1 and 2 are theoretically supported by the rigorous analysis of ReLU DNNs in Section 2, i.e., a ReLU DNN equals to a set of local linear models defined on a set of respective convex polyhedra in the input space, each containing a few data points. Modification 1 improves the local smoothness of the generalization of each linear model (associated with a specific polyhedron), by training it also on data points located at other nearby polyhedra with higher probability. Modification 2 ensures the homogeneity of the local smoothness, i.e., it generalizes each linear model to the nearby polyhedra of equal distance to the original one (measured by the number of different activation patterns) with the same probability. Modification 3 aims to reduce and balance the mean drift and variance drift when applying dropout together with batch normalization.\n\nQ2: For instance, the motivation for modification 2...However, if preventing co-adaptation is a reason to dropout neurons then the issue of conditional correlation (or co-activation given related inputs) will remain regardless of the number of active neurons in a layer, thus changing the dropout rate as a function of ReLU activation is not fully justified.\n\nR2: It is wrong to entirely block co-adaptation. Dropout aims to weaken co-adaptation but not to entirely remove it, since exploring the correlation between hidden nodes is an important part of optimizing the model weights (considering backpropagation for example). Comparing to dropout, jumpout allows slightly more co-adaptation, but the amount is extremely small and negligible. Because the adaptive dropout rate is a single number applied to hundreds of thousands of hidden nodes in a layer and a mini-batch. Considering how much a single number can describe the correlation among hundreds of thousands of variables: its influence is negligible. \n\nIn addition, it is worth noting that fixing dropout rate can be catastrophic during training. As shown in Figure 1, since existing training methods do not have any control on the ratio of activated neurons per layer, it is very possible that some layers have many activated nodes while some have very few. For the former, a relatively large dropout rate is required to avoid overfitting. However, applying the same large dropout rate to the latter will almost cut the information flow sent from input to deeper layers. In this case, the output will almost independent to the input, which is catastrophic.\n\nQ3: Similarly, modification 3 ârescale outputs to work with batch normalizationâ proposes exponentiation by -0.75 with weak justification as a compromise.\n\nR3: Modification 3 is theoretically derived from the given analysis of mean/variance drift in Section 3.3. In order to balance the reduced mean drift and variance drift, the power in the rescaling factor should be between $-0.5$ and $-1.0$. Without any extra information about the weight matrices of the following layers, -0.75 provides a good trade-off between reducing the mean drift and the variance drift (as shown in Figure 3), and also shows promising and consistent performance boost in our experiments. So modification 3 does not rely on any \"weak justification\".", "We appreciate all the reviewers for their comments and suggestions! As suggested by the reviewers, in the updated draft (Table 1), we added a thorough ablation study of all the possible combinations of the three modifications proposed in this paper, and show the effectiveness of each of them on four datasets. \n\nWe also emphasized in our response that the three modifications are based on rigorous analysis and new insights to ReLU networks (most in Section 1.1 and Section 2, which might be ignored but are important) rather than sheer heuristics or empirical evidence only. \n\nIn addition, we are not proposing a variational dropout method. Instead, we are modifying the vanilla dropout to make it consistent the new analysis. Jumpout requires the same cost as the vanilla dropout for both training and test, has a very simple implementation, and improves the performance consistently and dramatically. ", "Q4: I find the empirical evidence and support for the three modifications lacking in detail.  The authors provide results of the combined Jumpout technique on a number of tasks, but do not demonstrate the effectiveness and contribution of individual modifications on error rates on the tasks they evaluated.\n\nR4: We provided a thorough ablation study on multiple datasets in the updated draft (Table 1). The ablation study compares the performance of all the 7 different combinations of the three modifications. It shows that 1) each modification brings improvement to the vanilla dropout; 2) adding any modification to another brings further improvements; and 3) applying the three modifications together achieves the best performance. \n\nQ5: I also find the baseline systems to be on the weaker side (e.g. on CIFAR100 many systems now have higher than 82% accuracy with best being over 84, on STL-10 many systems now are well above 85%).\n\nR5: 1) It is not fair to justify a dropout technique by comparing its performance on two different systems; 2) The purpose of this paper is not to pursue SOTA performance on CIFAR100 and STL10 by combining several complicated tricks or employing an extremely large and costly model. Instead, our goal is to provide an easy-to-implement, efficient and effective dropout technique. This has been verified by the experimental results, i.e., jumpout always brings promising improvements (~2% on CIFAR100 and >2.3% on STL10) on the same model (WideResNet and ResNet) with negligible extra computation; 3) Achieving SOTA performance is usually much more expensive, either due to the extremely large size of model or complicated data augmentation/regularization; 4) dropout/jumpout improves generalization by preventing overfitting, which usually happens when training relatively small neural networks on small data (CIFAR100 and STL10), but not for severely over-parameterized models achieving SOTA performance (recent theoretical papers proved that over-parameterized model is harder to overfit).", "Thanks for your comments! We added the ablation study you suggested, and briefly explained the locally linear region in the following. The three modifications are based on theoretical analysis and are not heuristic.\n\nQ1: Unfortunately, I did not understand the arguments on locally linear regions and ReLu and its relationship with the monotone dropout scheme, or why the half Gaussian is chosen.\n\nR1: Intuitively, we show that a ReLU DNN equals to a set of linear models defined on a set of respective convex polyhedra in the input space. Each linear model is only applied to the data point within the respective polyhedron. The monotone dropout rate encourages the local smoothness of the generalization of each linear model, by training the linear model also on data points located at other nearby polyhedra with higher probability. Sampling from the half Gaussian ensures that the data points from closer polyhedra have a higher probability to be used to train the linear model.\n\nQ2: However, I could not make out much of why each step is done, and could not find empirical tests of the value of each step...it is important to test their separate effects...It should be easy to perform an ablation analysis...\n\nR2: In the updated draft (Table 1), we provided a thorough ablation study on multiple datasets. It compares the performance of all the 7 different combinations of the three modifications. This will provide a complete answer to your question. \n\nQ3: All the proposals seem very heuristic.\n\nR3: This is not true. Modification 1 and 2 are theoretically supported by the rigorous analysis of ReLU DNNs in Section 2, while modification 3 is derived from the given analysis of mean/variance drift in Section 3.3 (it aims to balance the reduced mean drift and variance drift).", "Thanks for your comments! We added the ablation study to demonstrate the effectiveness of every individual modification. We further emphasize that jumpout is not a variational dropout approach, and can scale to very large networks. We also added a comparison with concrete dropout[1] as suggested (Table 3 in Appendix).\n\nQ1: Overview of the paper: \"Specifically, the proposed method attempts to address the obvious drawbacks of Dropout: (i) the need to heuristically select the Dropout rate; and (ii) the universality of this selection across a layer.\"\n\nR1: \"(i) the need to heuristically select the Dropout rate\" is merely one observation of the paper and 1/3 of the drawbacks we aim to address, and we never attempt to address \"(ii) the universality of this selection across a layer\", i.e., for all nodes on a layer, jumpout applies the same drop rate. The primary purpose of jumpout is to improve the original dropout performance without introducing extra computational costs. The truncated Gaussian distribution aims to improve the dropout performance based on the linear model geometry of ReLU networks. The change of dropout rate based on ReLU pattern tries to address the dropout rate selection problem. The change of rescaling factor resolves the disharmony between dropout and batchnorm, so for a network with both kinds of layers, the performance gets boosted.\n\nQ2: Comparison to [1],[2] and [3].\n\nR2: We note that jumpout is NOT a variational approach of dropout, which does not require Bayesian training or inference. Jumpout does not introduce extra inference cost, and it also has similar training costs as the original dropout. Jumpout can therefore work on modern networks with deep and wide structures, whereas the variational approaches [2] and [3] do not scale to the networks we include in the paper. For [1], we add comparison experiments and show that jumpout significantly outperforms [1]. \n[1],[2] and [3] tries to address the problem similar to our observation 2, namely, selection of the dropout rate. Jumpout has 2 other major changes: we choose to impose a truncated Gaussian distribution on the dropout rate based on the linear model geometry of the ReLU network, and we change the rescaling factor to account for the disharmony between dropout and batchnorm. \n\nQ3: They suggest sampling from a truncated Gaussian, but they do not elaborate on why this selection solves the problem.\n\nR3:The truncated Gaussian distribution is a natural choice based on the intuition based on the linear model geometry of ReLU networks. Again, jumpout is not a variational approach, and the truncated Gaussian distribution is not aimed to solve the dropout rate selection problem. The truncated Gaussian is applied because the original dropout has the uniform preference for both nearby and faraway linear models, while in principle, close linear models should be preferred. Also, for [3], the beta distribution is selected with no clear reason at all.\n\nQ4: No one knows without these the method would not work.\n\nR4: We add thorough ablation studies on all combinations of the 3 modifications to show that 1) they all have positive impacts on the performance, and 2) 3 modifications can work together to get the best performance. ", "Authors propose three modifications to dropout, specifically in context of dropout applied to deep networks utilizing the ReLU non-linearity.  The three modifications seem independently motivated and aim to overcome separate potential shortcomings of the current dropout approach.  These three modifications are combined into a new approach termed Jumpout.\n\nOverall I find this to be a weak paper requiring further work, for the following main reasons:\n\n* The proposed modifications are intuitively motivated and then empirically supported.  However, I find the intuitive reasoning unclear and have to lean much more on empirical evidence.  For instance, the motivation for modification 2 âdropout rate adapted to number of active neuronsâ, is that in case ReLU causes a large number of neurons to âshut downâ then the dropout rate in that layer should be reduced (or increased, depending on how it is defined) causing fewer neurons to further dropout.  However, if preventing co-adaptation is a reason to dropout neurons then the issue of conditional correlation (or co-activation given related inputs) will remain regardless of number of active neurons in a layer, thus changing the dropout rate as a function of ReLU activation is not fully justified.  Similarly, modification 3 ârescale outputs to work with batch normalizationâ proposes exponentiation by -0.75 with weak justification as a compromise.\n\n* I find the empirical evidence and support for the three modifications lacking in detail.  The authors provide results of the combined Jumpout technique on a number of tasks, but do not demonstrate effectiveness and contribution of individual modifications on error rates on the tasks they evaluated.\n\n* I also find the baseline systems to be on the weaker side (e.g. on CIFAR100 many systems now have higher than 82% accuracy with best being over 84, on STL-10 many systems now are well above 85%).\n", "This paper proposes jumpout, which is a 3 step modification based on dropoout\nthat is designed to work better with batch normalization. Unfortunately, I did not understand the arguments on locally linear regions and ReLu and its relationship with the monotone dropout scheme,\nor why the half Gaussian is chosen.\n\nStill, jump out the procedure is fairly clear in Algorithm 1, and the results seems good.\nHowever, I could not make out much of why each step is done, and could not find empirical tests of the value of each step.\n\nI think the paper needs more work. All the proposals seem very heuristic, and it is important to test their separate effects. It should be easy to perform a ablation analysis since the 3 proposed steps are pretty independent and can be tested separately. Since two of these have to do with modifying the dropout rate, it would be important to compare with carefully cross-validated dropout rates, which I also do not see.", "The paper proposes yet another variant of the celebrated Dropout algorithm. Specifically, the proposed method attempts to address the obvious drawbacks of Dropout: (i) the need to heuristically select the Dropout rate; and (ii) the universality of this selection across a layer. \n\nAs the authors have admitted in the paper (Sec. 1.2), there is a variety of methods already addressing the same problem. They argue that contrary to some of these methods \"jumpout does not rely on additional trained models: it adjusts the dropout rate solely based on the ReLU activation patterns. Moreover, jumpout introduces negligible computation and memory overhead relative to the original dropout methods, and can be easily incorporated into existing model architectures.\"\n\nHowever, this is argument is certainly untrue and rather misleading. The works of Kingma et al. (2015) and Molchanov et al. (2017), that the authors cite, does not introduce additional trained models. In addition, there is additional related work that the authors do not cite, but ought to: \n\n[1] Yarin Gal, Jiri Hron, Alex Kendall, \"Concrete Dropout,\" Proc. NIPS 2017.\n[2] Yingzhen Li, Yarin Gal, \"Dropout Inference in Bayesian Neural Networks with Alpha-divergences,\" Proc ICML 2017.\n[3] Harris Partaourides, Sotirios Chatzis, âDeep Network Regularization via Bayesian Inference of Synaptic Connectivity,â J. Kim et al. (Eds.): PAKDD 2017, Part I, LNAI 10234, pp. 30â41, 2017. \n\nThese methods also address a similar problem, without introducing extra networks or imposing extra costs art inference time. Thus, citing them, as well as COMPARING to them, is a necessity for this paper to be convincing.\n\nThese crucial shortcoming aside, there are various theoretical claims in this paper that are not sufficiently substantiated. To begin with, the arguments used in the last paragraph of page 4 seem at least speculative; then,  the authors proceed to propose a solution to the alleged problem in the beginning of page 5. They suggest sampling from a truncated Gaussian, but they do not elaborate on why this selection solves the problem; they limit themselves to noting that other selections, such as the Beta distribution, may also be considered in the future. We must also underline that [3] have suggested exactly that; sampling from a Beta. \n\nFinally, the last two modifications the authors propose seem reasonable, yet they are extremely heuristic. No one knows (which can be guaranteed through theoretical proofs or solid experimental evidence) that without these the method would not work. In addition, previous papers, e.g. [1-3] achieve similar goals in a principled fashion (ie by inferring proper posterior densities); without experimental comparisons, nobody knows which paradigm is best to adopt. \n\n"], "review_score_variance": 0.22222222222222224, "summary": "The paper introduces a new variant of the Dropout method. The reviewers agree that the procedure is clear. However, motivations behind the method are heuristic, and have to lean much on empirical evidence. A strong motivation behind the procedure is lacking, and the motivation behind the method is unclear. Furthermore, the empirical evidence is lacking in detail and could use better comparisons with existing literature.", "paper_id": "iclr_2019_r1gRCiA5Ym", "label": "train", "paper_acceptance": "rejected-papers"}
