{"source_documents": ["Graph classification is currently dominated by graph kernels, which, while powerful, suffer some significant limitations. Convolutional Neural Networks (CNNs) offer a very appealing alternative. However, processing graphs with CNNs is not trivial. To address this challenge, many sophisticated extensions of CNNs have recently been proposed. In this paper, we reverse the problem: rather than proposing yet another graph CNN model, we introduce a novel way to represent graphs as multi-channel image-like structures that allows them to be handled by vanilla 2D CNNs. Despite its simplicity, our method proves very competitive to state-of-the-art graph kernels and graph CNNs, and outperforms them by a wide margin on some datasets. It is also preferable to graph kernels in terms of time complexity. Code and data are publicly available.", "The authors propose to use 2D CNNs for graph classification by transforming graphs to an image-like representation from its node embedding. The approach uses node2vec to obtain a node embedding, which is then compacted using PCA and turned into a stack of discretized histograms. Essentially the authors propose an approach to use a node embedding to achieve graph classification.\n\nIn my opinion there are several weak points:\n\n1) The approach to obtain the image-like representation is not well motivated. Other approaches how to  aggregate the set of node embeddings for graph classification are known, see, e.g., \"Representation Learning on Graphs: Methods and Applications\", William L. Hamilton, Rex Ying, Jure Leskovec, 2017. The authors should compare to such methods as a baseline.\n\n2) The experimental evaluation is not convincing:\n- the selection of competing methods is not sufficient. I would like to suggest to add an approach similar to Duvenaud et al., \"Convolutional networks on graphs for learning molecular fingerprints\", NIPS 2015.\n- the accuracy results are taken from other publications and it is not clear that this is an authoritative comparison; the accuracy results published for state-of-the-art graph kernels are superior to those obtained by the proposed method, cf., e.g., Kriege et al., \"On Valid Optimal Assignment Kernels and Applications to Graph Classification\", NIPS 2016.\n- it would be interesting to apply the approach to graphs with discrete and continuous labels.\n\n3) The authors argue that their method is preferable to graph kernels in terms of time complexity. This argument is questionable. Most graph kernels compute explicit feature maps and can therefore be used with efficient linear SVMs (unfortunately most publications use a kernelized SVM). Moreover, the running of computing the node embedding must be emphasized: On page 2 the authors claim a \"constant time complexity at the instance level\", which is not true when also considering the running time of node2vec. Moreover, I do not think that node2vec is more efficient than, e.g., Weisfeiler-Lehman refinement used by graph kernels.\n\nIn summary: Since the technical contribution is limited, the approach needs to be justified by an authoritative experimental comparison. This is not yet achieved with the results presented in the submitted paper. Therefore, it should not be accepted in its current form.", "The paper introduces a method for learning graph representations (i.e., vector representations for graphs). An existing node embedding method is used to learn vector representations for the nodes. The node embeddings are then projected into a 2-dimensional space by PCA. The 2-dimensional space is binned using an imposed grid structure. The value for a bin is the (normalized) number of nodes falling into the corresponding region. \n\nThe idea is simple and easily explained in a few minutes. That is an advantage. Also, the experimental results look quite promising. It seems that the methods outperforms existing methods for learning graph representations. \n\nThe problem with the approach is that it is very ad-hoc. There are several (existing) ideas of how to combine node representations into a representation for the entire graph. For instance, averaging the node embeddings is something that has shown promising results in previous work. Since the methods is so ad-hoc (node2vec -> PCA -> discretized density map -> CNN architecure) and since a theoretical understanding of why the approach works is missing, it is especially important to compare your method more thoroughly to simpler methods. Again, pooling operations (average, max, etc.) on the learned node2vec embeddings are examples of simpler alternatives. \n\nThe experimental results are also not explained thoroughly enough. For instance, since two runs of node2vec will give you highly varying embeddings (depending on the initialization), you will have to run node2vec several times to reduce the variance of your resulting discretized density maps. How many times did you run node2vec on each graph? \n\n", "The paper presents a novel representation of graphs as multi-channel image-like structures. These structures are extrapolated  by \n1) mapping the graph nodes into an embedding using an algorithm like node2vec\n2) compressing the embedding space using pca\n3) and extracting 2D slices from the compressed space and computing 2D histograms per slice.\nhe resulting multi-channel image-like structures are then feed into vanilla 2D CNN.\n  \nThe papers is well written and clear, and proposes an interesting idea of representing graphs as multi-channel image-like structures. Furthermore, the authors perform experiments with real graph datasets from the social science domain and a comparison with the SoA method both graph kernels and deep learning architectures. The proposed algorithm in 3 out of 5 datasets, two of theme with statistical significant."], "review_score_variance": 2.888888888888889, "summary": "The submission proposes a strategy for creating vector representations of graphs, upon which a CNN can be applied.  Although this is a useful problem to solve, there are multiple works in the existing literature for doing so.  Given that the choice between these is essentially empirical, a through comparison is necessary.  This was pointed out in the reviews, and relevant missing comparisons were given.  The authors did not provide a response to these concerns.", "paper_id": "iclr_2018_HkOhuyA6-", "label": "train", "paper_acceptance": "rejected-papers"}
{"source_documents": ["In this work we consider data-driven optimization problems where one must maximize a function given only queries at a fixed set of points. This problem setting emerges in many domains where function evaluation is a complex and expensive process, such as in the design of materials, vehicles, or neural network architectures. Because the available data typically only covers a small manifold of the possible space of inputs, a principal challenge is to be able to construct algorithms that can reason about uncertainty and out-of-distribution values, since a naive optimizer can easily exploit an estimated model to return adversarial inputs. We propose to tackle the MBO problem by leveraging the normalized maximum-likelihood (NML) estimator, which provides a principled approach to handling uncertainty and out-of-distribution inputs. While in the standard formulation NML is intractable, we propose a tractable approximation that allows us to scale our method to high-capacity neural network models. We demonstrate that our method can effectively optimize high-dimensional design problems in a variety of disciplines such as chemistry, biology, and materials engineering.", "Updated review\n----\n----\n\n# Summary\n\nThis work proposes an approach for model-based optimization based on learning a density function through an approximation of the normalized maximum likelihood (NML). This is done by discretizing the space and fitting distinct model parameters for each value. To lower the computational cost, the authors propose optimizing the candidates concurrently with the model parameters. Each model's distribution is encoded as a neural net outputting a scalar which is then encoded using a thermometer approach using a series of shifted sigmoid. Candidates are optimized based on the average value of the scalar of each model evaluated using parameters obtained from an exponentially weighted average of its most recent parameters.\n\n# Reason for score\n\nThis work proposes a reasonable approximation to an interesting estimator and demonstrate it is capable of achieving good consistent performance. This is likely to be of interest to the community and, as far as I'm aware, is sufficiently novel. Given that I see no noteworthy issues and all of my major concerns have been addressed, I don't see any reason for rejection. I strongly support acceptance.\n\n# Pros\n\n* Using estimates of the NML for model-based optimization is an interesting idea.\n* This work shows that the NML can be successfully approximated with a relatively coarse discretization and that both the optimization of the candidate and the various model parameters can be optimized in tandem. This suggests that this type of approach is viable and possibly warrants further investigation.\n\n\nInitial review\n----\n----\n\n# Summary\n\nThis work proposes an approach for model-based optimization based on learning a density function through an approximation of the normalized maximum likelihood (NML). This is done by discretizing the space and fitting distinct model parameters for each value. To lower the computational cost, the authors propose optimizing the candidates concurrently with the model parameters. Each model's distribution is encoded as a neural net outputting a scalar which is then encoded using a thermometer approach using a series of shifted sigmoid. Candidates are optimized based on the average value of the scalar of each model evaluated using parameters obtained from an exponentially weighted average of its most recent parameters.\n\n# Reason for score\n\nThere are a lot of typos and issues with the notation which make this paper unfit for publication in its current state. Otherwise, the work seems interesting but I find the experiments don't provide much insight. Though the comparison with the selected method is favorable, it's hard to consider them significant when there is a notable difference in only one of the three datasets of an unpublished benchmark. Additionally, the fact that the reported results only cover half the datasets from this benchmark raises some questions. Despite the negative tone of this paragraph, I want to note that the severity of some of these issues is subjective while others can be easily fixed. My mind isn't made up and I hope the authors can clarify anything I might have missed.\n\n# Pros\n\n* Using estimates of the NML for model-based optimization is an interesting idea.\n* This work shows that the NML can be successfully approximated with a relatively coarse discretization and that both the optimization of the candidate and the various model parameters can be optimized in tandem. This suggests that this type of approach is viable and possibly warrants further investigation.\n\n# Cons\n\n* The current notation is often confusing and even ambiguous at times. This will probably transpire in some of my other comments, but I do consider these issues to be superficial and easily fixed. I've provided some suggestions below for how to improve the notation. I understand that notation preference is a very subjective thing so the authors should feel free to opt for something different, but I do think the notation needs to be improved.\n* The \"thermometer encoding\" of the output seems like an odd choice, especially given how it is done here. If I understood the approach correctly, this seems to be a hacky way of using the output of the NN, $o_{int}$, as parameters to a logistic distributions. Why not treat it this way directly? My interpretation of this approach is that the $o^k_{int}$s are parameters for logistic distributions and the mean is optimized through the unnormalized probs by optimizing each $o^k_{int}$ directly. Would this be a fair description of the approach? I was expecting the discretization to be used directly to approximate the integral in eq. (2). \n* The experimental results aren't very conclusive, only showing a clear benefit in a single case. Without additional results, it is difficult to say much about the behavior and properties of this method. A visualization of the distribution of the value of the candidates might help convey some additional information in favor of this method. It's possible that I am missing some context to appreciate these results. If that is the case, it would help if the authors could provide the context I need to appreciate these results.\n\n# Questions\n\n* I don't think I understand what makes the appendix results an ablation study. From what I understand, these results only compare with the case where there is no learning of the model parameters. What are the models initialized to? Where does the data come into play?\n* Have the authors considered using points selected from some fixed quadrature method instead of a uniform grid?\n* A common theme for the comparison methods is the idea of not diverging too much from the data. Was the validity of the outputs of this method evaluated in any way? How can I know that the method isn't just exploiting some quirk in the learned models used to evaluate it while some of the other methods avoid doing this?\n* Were comparisons with a simple approaches Bayesian optimization tried, e.g., Gaussian process?\n* What happens when doing more iterations on the log likelihood before updating $x$? How much do we lose when only doing a single update? Does using a more accurate approximation of the NLM improve/worsen performance? (I was hoping this would be part of the ablation study)\n* How do the run times compare?\n* Were the other datasets from the Design-bench benchmark tried?\n\n# Misc and typos\n\n* page 2, \"in that it has shown to be\", missing word?\n* page 2, \"to discuss how approximate this intractable\", missing word?\n* page 3, when $p_{NML}$ is formally written, the meaning of $y$ is ambiguous since it is on the LHS and also being redefined by $\\max_y$ in the RHS.\n* page 3, \"The notation $D \\cup (x, y)$ refers to an augmented dataset $D \\cup (x_{N+1}, y_{N+1})$\", this wasn't very informative and felt a bit tautological. I would recommend sticking with one of the two notations.\n* page 3, \"where $D$ is fixed to the given offline dataset, and $\\theta_{D \\cup (x,y)}$ [...]\", this sentence is a bit confusing as a whole. The start of the sentence talks only about $D$ so the $\\theta$ mention is unexpected when reading.\n* page 3, right after eq. (2), $(x, y)$ is on the LHS but then also part of the expectation on the RHS. What is the expectation being taken over? Are $x$ and $y$ being redefined?\n* page 4, \"for y in 1 ... K do\", is $y$ I don't believe that $y$ is assumed to be an integer. \n* page 4, algorithm 4?\n* page 4,  \"this would produce a distribution over output values $p(y|x)$\", this doesn't \"type check\" for me. If I understood correctly, $p$ or $p(\\bullet | x)$ represent distributions and $y$ are output values. Also, it might be good to reuse the \"$\\hat p_{NML}$\" notation to get the following sentence: \"this would produce a conditional distribution, $\\hat p_{NML}(\\bullet | x_t)$,  over output values.\n* page 4, \"we can optimize $x_t$ with respect to some evaluation function $g(y)$ of this distribution, such as the\nmean\", this is confusing since algorithm 1 has $\\mathbb{E}[ g(y)]$. What does it mean for the evaluation function $g$ to be the mean in this context? How should I interpret the expectation of mean(y)? Also, I assumed that what was meant is that $g$ is the evaluation function, not $g(y)$, or is $g$ meant to return a function given a $y$?\n* page 6, \"a straightforward choice for the evaluation function g is the identity function $g(x) = x$\", it might be best to stick to a consistent variable name, e.g., $g(y) = y$, to avoid confusion about what the domain of $g$ is.\n* Proof of thm 4.1, equation under \"using these two facts, we can show:\", $q$ should be replaced with $p_{NML}$ in the RHS.\n* Proof of thm 4.1, going from TV to KL, looking up the bound returns a $1/2$ factor inside the root rather than a $2$. I could have missed a detail but thought worth mentioning in case I haven't.\n* Proof of thm 4.1, missing a \"]\" when bounding the KL divergence.\n* Algorithm 2, there is some undefined superscript $k$ and $y$ in the loop over $x^m_t$.\n\n# Notation suggestions\n\n* When writing expectations, I would strongly recommend making it explicit over which variables they are, e.g., $\\mathbb{E}\\_{y \\sim p(\\bullet | x, \\theta)}$. Introducing some shorthand notation might help make this more concise, e.g., $p_{x, \\theta}(y) := p(y | x, \\theta)$.\n* When referring to a function, only mention its name/symbol and reserve the form that includes inputs to refer to the output of the function given those inputs, e.g., a function $g$, an evaluation score $g(y)$.\n* Avoid relying on the readers pattern matching abilities for assigning meaning to variables and make sure variables, e.g., $\\mathbf{x}$ and $y$, are always explicitly defined. By explicitly defined, I include defining $y$ compactly with something like $\\max_{y \\in \\mathcal{Y}} g(y)$, for example. There is not need to be overly verbose but it should never leave room for interpretation. This is related to the point about expectation notation.\n* I usually prefer explicit domains, e.g., $\\sum_{y \\in \\mathcal{Y}}$ instead of $\\sum_y$, but I consider it fine to omit it if variables names are always reserved to the same domain when reused. This was not the case for $x$ in this paper.\n* When writing pseudocode, either loop over integer indices or over elements of a set, it is confusing to use \"for y in 1 ... K\" when $y$ isn't an integer. Additionally, using both makes it difficult to tell that $k$ is associated with $y$ inside a loop.", "Summary: The paper proposes an approximation method, called NEMO (Normalized maximum likelihood Estimation for model-based optimization)  to compute the conditional normalized maximum log-likelihood of a query data point as a way to quantify the uncertainty in a forward prediction model in offline model-based optimization problems. The main idea is to construct a conditional NML (CNML) distribution that maps the high-dimensional inputs to a distribution over output variables. In addition, the paper provides a theoretical motivation that estimating the true function with the CNML is close to the best possible expert even if the test label is chosen adversarially, which is a great challenge for an optimizer to exploit the model. By using this CNML on three offline optimization benchmark datasets (Superconductor, GFP, MoleculeActivity) with gradient ascent-based optimization, the NEMO outputs all the other four baselines on the Superconductor dataset by almost 1.4x to 1.7x, the generate comparable results as the other four baselines method on the GFP and MoleculeActivity datasets.   \n\nTypo: \nIn section 4, “We outline the high-level pseudocode in Algorithm 4” -> “…. in Algorithm 1”\n\nQuestions: \n1. When sampling a batch of data points at each step of algorithm 1, is the sampling performed with or without replacements?  \n2. What’s the variance across the 16 random runs? Is the score of the best algorithm in the average performance across 16 random runs significantly different from the baseline algorithms?\n3. When estimating the CNML, what is the number of models in the experiments? Are the number of models differ from dataset to dataset? How to choose the number of models in practice?\n4. Since the output y needs to be discretized in the NEMO algorithm, how difficult for the algorithm to scale when y is multivariate?\n\n------------------------------------------------------------------------------------------------------------\nUpdate: \nI think the authors did a great job of addressing my concerns, I'm happy to increase my score to 6\n", "# summary\n\nThis paper proposed a method based on NML and provided a principled approach\nto estimate the uncertainty for OOD data.\n\n\n# pros\n\n1.  The method proposed in this work is a principled way to handle uncertainty\n    for novel points out of the original dataset compared with, for example,\n    deep ensemble.\n2.  One clear advantage of ths proposed approach is this method can scale to\n    large dataset, compared with GP, which scales cubically.\n\n\n# cons\n\n1.  The authors claim using a supervised approach is brittle and prone to\n    failure as the uncertainty of inputs outside of training data is\n    uncontroled. However, this is not true and uncertainty can be well\n    controlled depending on the model, which can be non-parametric or\n    parametric, distribution-free or distribution-dependent. For example, to\n    measure uncertainty on novel point, GP could be viewed as the ground truth\n    under some conditions. My question is why not directly compare your\n    approach with a GP approach, then combine the posterior with an acquisition\n    function, such as EI. The offline MBO problem presented in this work is\n    similiar to an online MBO, except we have only one online sample, and\n    we are tying to optimize this point. Comparing eq(1) of this paper with the\n    formula of EI, it is easy to see eq(1) is exactly EI, if we assume there\n    exists (x\\*,y\\*) such that y\\* is larger than objective values in the training\n    data set. Thus the problem presented in this work can be effectively solved\n    through **one step** of conventional BO. Given the datasets used in the\n    experiments of this work is are not of large scale, I think comparing with\n    a GP-based BO is necessary.\n2.  In Figure 3, although not a major concern, I don't think the comparison\n    with the ensemble is fair. Although this work uses bootstrap and ensemble,\n    MSE cannot capture uncertainty, thus it is not an ideal metric in this\n    setting. For example, to obtain a similar uncertainty estimation compared\n    with NML (middle column), we can use a deep ensemble, which optimizes NLL\n    instead of MSE.\n3.  The experimental results, in my opinion, are not sufficient and there is\n    only one table presenting empirical results. I don't want to judge\n    sufficiency by the quantity of tables or figures, but considering the\n    theoretical analysis is not strong enough, I think more empirical study\n    should be performed.\n4.  The uncertainty estimation seems too conservative, and this could make the\n    estimated uncertainty less useful, especially for high-dimensional\n    problems.\n\n\n# questions\n\nThe approach proposed in this paper seems very similiar with conformal\nprediction. In conformal prediction, the target value y\\* for a novel point x\\*\n(adversarial input) is chosen so that y\\* is compatible with the original\ndataset. As I am not familiar with the evaluation protocol in Brookles2019 and\nFannjiang2020, the metric used in Table 1 is not clear to me. Can the authors\nsay more about that?\n\nupdate:\n\n---\nOverall speaking, the added GP-BO results address my  concerns, and I've updated the score from 5->6. A final update will be given later.", "Hello, we were wondering if there were any additional concerns you had, and we would be happy to clarify them. Thank you!\n", "Again, we appreciate the constructive feedback on the paper. We are wondering if there are any outstanding issues you think should be addressed, or if the current presentation is satisfactory. Thank you.\n", "The new presentation for the discretization looks good! Since new results showing # NLM steps vs performance seem to suggest that better approximations improve performance, I wonder how big of an effect a more powerful numerical integral approximation scheme, e.g., a fixed Bayesian quadrature approach, would have. Maybe the authors are equally curious to know and my curiosity will be satisfied in follow up work :)\n\n### How do the run times compare?\n\nThose are a bit better than I expected. Given the setting, runtime isn't all that important unless it starts to get prohibitively expensive but I wanted to do my due diligence and make sure nothing was swept under rug. It might be useful to add a few words with describing the \"order of\" what to expect in case a reader has the same concern, but that is far from necessary. I'm sure we can think of small things to add/change until the end of time and I don't want the authors to think they need to indulge me at this point! I think the authors have already done enough.\n\n### Design-bench benchmark ~omitted~\n\nUgh, I can't believe I missed that! That makes my comment quite silly...\n\n### Closing thoughts\n\nI am quite happy with how the paper has progressed. The authors have addressed all my concerns and have considerably improved the overall quality of this paper. At this point, I believe this paper is ready for publication and I consider it clearly above the novelty and quality threshold for acceptance at ICLR. I will wait until later in the reviewer discussion phase before updating my review but I think it is reasonable for the authors to expect a notably improved score from me after my revision.", "On the thermometer encoding:\n\nThank you for the clarifications. The quadrature + logistic distribution interpretation does seem to be a cleaner way to interpret the method, and we’ve updated Section 4.2 to use this interpretation rather than the original thermometer encoding. The implementation of the method itself remains unchanged. This should better motivate the use of quantization as an approximation scheme to computing a full integral, and as you mentioned, shed some light into potential future ways to improve the method.\n\n> “How do the run times compare?”\n\nTo provide some concrete runtime numbers, here are runtimes we obtained on AWS c5.large instances for the Superconductor task.\n\nOptimizing a forward ensemble of 40 networks directly took on average 0.11s / gradient step on x. Adding 5 steps of NML model optimization (this was the setting used in our results reported in Table 1 & 2) brings this cost up to 0.35s / gradient step. So, when using equivalent neural network architectures and ensemble sizes, NEMO takes roughly 3x longer due to the cost of additional model training per iteration. In total, we ran our experiments for 50k gradient steps, which took a bit under 5 hours for NEMO on the Superconductor task, although 50k steps was more than the number of steps we needed to reach convergence.\n\n> “Why were some of the Design-bench benchmark omitted? With the new results, I don't think the authors need to add them but I think the question is still relevant.”\n\nWith the inclusion of the robotics tasks, all 6 tasks in the Design-bench benchmark are now included in Table 1 & 2.\n\nOverall, thank you for the numerous suggestions. We think the paper has been greatly improved by your (and other reviewer’s) feedback.\n", "The detailed clarifications, the updated notation and additional results have made this a significantly stronger submission. I am pleased to see that authors appear to have invested a notable amount effort polishing this paper.\n\n### Notation\n\nNotation is significantly improved and makes the paper much easier to read. There doesn't appear to be any noteworthy notation issues remaining.\n\n### New experimental results\n \n* The addition of GP-BO complement well the previous results and help appreciate previous results. \n* The addition of the mujoco results cover what I considered a major blind spot in the previous draft. These new results serve as a useful reference point which makes me more confident about the significance of the previous results. Furthermore, the mujoco results appears to support the claim that NEMO performs consistently well across tasks while other methods don't exhibit the same level of consistency. (The authors might want to render their best policy for hopper at some point. While it doesn't always payoff, the quirks of the mujoco simulator can be a great source of comical videos for talks, especially with a good optimization algorithm that can exploit them!)\n* I appreciate that the authors have added more ablation results. As a general statement, I find results like these to be much more likely to inspire future research ideas or other minor conceptual breakthroughs, even if these results don't necessarily fit nicely into the \"story\".\n\n### Thermometer\n\nFirst, I'd like to apologize for my choice of words which made my comment about thermometer encoding unconstructive at best. I should have caught that before submitting my initial review. What I should have said is that I was surprised by its introduction. I also believe I did a poor job expressing why so I will give it another go.\n\nGiven the presentation up until that point in the paper, I expected that the conditional probability density would be approximated by a family of parametric probability density functions where the parameters would come from the output of some NN, and the denominator of (2) would then be approximated using a discrete set of points covering Y, i.e., a form of quadrature approximation of the integral. What tripped me up was just how similar the proposed approach was to what I expected but derived using the ideas of discretization and thermometer encoding.\n\nThe authors should correct me if I'm wrong, but I believe that up until their assumption that $g(y) = y$ and other simplifications, the proposed method is equivalent to what I described when using a family of parametric logistic distribution with $\\sigma = 1$. I think what I found \"odd\" was more the motivation/derivation of the method than the resulting method itself.\n\nAlso, while I appreciate the addition of the softmax results, it was not my intention to suggest approximating that probability density with a softmax categorical distribution. I think the overall approach proposed by the authors is good and thought provoking. I appreciate papers that make you wonder what various extensions or generalization would look like and this was the case for me here.\n\nIf my claim that the proposed method is equivalent (or almost) to using a parametric probability density and taking a quadrature approximation of the integral, the authors might want to consider using those concepts to introduce their method. I believe it would make the presentation more intuitive for many readers as well as allow the use of theory and ideas from quadrature methods in follow up work.\n\nTo be clear, I'm writing this simply because I think it could improve this paper (provided I didn't misunderstand something, of course). The authors should feel free to opt for keeping the presentation as is. I consider the thermometer motivation to be sufficient and I don't believe changing it is necessary for publication.\n\n### Remaining questions\n\n* How do the run times compare?\n* Why were some of the Design-bench benchmark omitted? With the new results, I don't think the authors need to add them but I think the question is still relevant.\n\n### Misc comments and typo\n\n* Algorithm 2, \"for $x_t^m$ in $1 ... M$\", should this be 'for $m$ in $1 ... M$'? Given the other changes to the notation, I suspect this might be an accidental omission.", "Thank you for your comments. We have clarified several points below, and please let us know if this addresses your concerns.\n\n> “When sampling a batch of data points at each step of algorithm 1, is the sampling performed with or without replacements?”\n\nSampling is done with replacement. The model training step is identical to minibatch optimization methods used in standard supervised learning.\n\n> “What’s the variance across the 16 random runs? Is the score of the best algorithm in the average performance across 16 random runs significantly different from the baseline algorithms?”\n\nWe have updated Table 1 with standard deviations for all methods. The average score for NEMO is significantly higher than baselines for the Superconductor and newly added HopperController tasks, and is still competitive with state-of-the-art methods for the other tasks.\n\n> “When estimating the CNML, what is the number of models in the experiments? Are the number of models differ from dataset to dataset? How to choose the number of models in practice?”\n\nWe use one model per discretization bin. These values are reported in Appendix A.2.2. In practice, we found a value of 20-40 to perform quite well for the problems we tried, and we did not need to tune this hyperparameter.\n\n> “Since the output y needs to be discretized in the NEMO algorithm, how difficult for the algorithm to scale when y is multivariate?”\n\nBecause standard optimization problems generally deal with scalar valued objectives, we don’t foresee the need to consider the case when y is multivariate in the context of optimization. However, multi-objective optimization would be a great direction to explore for future work, and it would interesting to see how NML could fit into the analysis of a Pareto frontier.\n", "Thank you for your detailed feedback. We believe your main concerns are with the depth of empirical study and writing/notational issues in the paper. We have significantly expanded the experimental evaluation with additional ablation studies on the architecture choice and optimization parameters, as well as including BO baseline, and additional benchmarking tasks. Additionally, we have corrected typos and updated the writing in the paper using many of your suggestions. Please let us know if this addresses your concerns.\n\n> \"The current notation is often confusing and even ambiguous at times... I understand that notation preference is a very subjective thing so the authors should feel free to opt for something different, but I do think the notation needs to be improved.\"\n\nWe have clarified ambiguous notation regarding expectations, loop indices in the pseudocode, and functions in the updated paper according to your suggestions. X and Y now always refer to the input and output variables, each expectation is now explicitly defined with the variable it is taken over, and summations are defined with the domain. \n\n> \"Were comparisons with a simple approaches Bayesian optimization tried, e.g., Gaussian process?\"\n\nWe have included an additional Bayesian optimization baseline based on maximizing expected improvement on a Gaussian process posterior. We use an RBF kernel for the Superconductor task, and a dot-product kernel for the discrete GFP/Molecule tasks. \n\n> \"The \"thermometer encoding\" of the output seems like an odd choice, especially given how it is done here...  the o_int_ks are parameters for logistic distributions and the mean is optimized through the unnormalized probs by optimizing each o_int_k directly. Would this be a fair description of the approach? \"\n\nYour description is accurate - therefore we justified optimizing o_int_k directly by making a monotonicity argument, and optimizing the o_int_k simplifies the algorithm. However, we don’t consider this an “odd” choice since similar ideas have been applied previously - for example, the “discretized logistic” is used in works such as PixelCNN to convert a single scalar value into a categorical distribution.\n\nWe have included an additional ablation study regarding the architecture choice in Appendix A.3.2, comparing the “thermometer” approach to the perhaps more straightforward method of training a model to predict discretized values directly. The results overall favor the thermometer architecture choice in performance and reliability. However, note that this is largely an architectural implementation detail -- a discretization design would be nicer, it's often the case that getting good results with deep neural networks requires careful design decisions, and we wanted to describe our decisions in detail for the sake of reproducibility.\n\n> \"I don't think I understand what makes the appendix results an ablation study. From what I understand, these results only compare with the case where there is no learning of the model parameters. What are the models initialized to? Where does the data come into play?\"\n\nThe purpose of this ablation study was to measure the effect of using NML during optimization compared to optimizing a forward model directly.The models are initialized by pretraining on the dataset. Therefore, the difference between the two results is due to additional NML training. We have clarified this point in the appendix.\n\n> \"How can I know that the method isn't just exploiting some quirk in the learned models used to evaluate it while some of the other methods avoid doing this?\"\n\nIn order to provide a “grounded” result, we have included additional results in the robotics domain from the Design-Bench benchmark. The designs in these tasks are evaluated in the MuJoCo simulator, and therefore high scores can be trusted to correspond to valid outputs. We find that the similar trends still hold true on these tasks - NEMO scores consistently high on each task.\n\nUnfortunately, it is difficult to gauge the validity of results on the material and molecule design tasks without synthesizing the outputs in real life. As discussed in Brookes et. al. 2019, the model class of the ground truth is a random forest and therefore belongs to a very different model class from the estimated function, and our model is trained on ground-truth values and not on the output of the random forest. Therefore, it is still a nontrivial problem to achieve high scores on these problems.\n\n> \"What happens when doing more iterations on the log likelihood before updating x?\"\n\nWe have included a more detailed study in the Appendix A.3.3, comparing the ratio of NML model learning steps to optimization steps on x. When the model learning rate is small, taking additional steps appears to strictly improve the convergence speed of the method (as measured by overall algorithm steps). However, as somewhat expected, taking too many steps with a large learning rate can cause some instabilities.\n", "Thank you for the comments and feedback. Our overall impression from your review is that while the method is promising, the empirical study is not thorough enough to back our claims. Therefore, we have significantly expanded our evaluations according to your suggestions, and included a GP-based baseline, additional evaluations on new tasks, and ablation studies for justifying our design choices. Please let us know if this addresses your concerns, or if there are any further modifications you would like us to make.\n\n> “My question is why not directly compare your approach with a GP approach… Thus the problem presented in this work can be effectively solved through one step of conventional BO.”\n\nYou are correct in that a Bayesian method such as a GP can be used to accurately measure uncertainty on out-of-distribution inputs in some cases (especially in low-dimensional tasks) and that one step of a method such as BO can be used to solve the offline MBO problem. We have included results for a GP-based method by selecting the design maximizing the expected improvement acquisition function. We use an RBF kernel for the continuous Superconductor task, and a dot-product kernel for the GFP and MoleculeActivity tasks, since they are discrete. \n\n> “The experimental results, in my opinion, are not sufficient and there is only one table presenting empirical results. I don't want to judge sufficiency by the quantity of tables or figures, but considering the theoretical analysis is not strong enough, I think more empirical study should be performed.”\n\nIn order to strengthen the empirical analysis, we have included additional experimental results on a 3 robotics domains from the design-bench, where we see similar experimental trends hold true: NEMO consistently ranks among the top performing algorithms and on the HopperController task significantly outperforms prior methods. An additional benefit for these tasks are evaluated in the MuJoCo robotics simulator, and therefore we can be confident the designs are valid.\n\nTo justify our design choices, we have also included additional ablation studies on the choice of architecture (thermometer architecture vs standard feedforward architecture) and hyperparameter settings such as learning rates and number of optimization steps, included in Appendix A.3.\n\n> “The approach proposed in this paper seems very similiar with conformal prediction. In conformal prediction, the target value y* for a novel point x* (adversarial input) is chosen so that y* is compatible with the original dataset. “\n\nThe connection with conformal prediction is an interesting one we were not aware of. It does seem like conformal prediction could be used in a similar manner to CNML, by providing per-instance confidence intervals that could prevent model exploitation. Additionally, the NML regret may have connections to conformity measures, as they both provide a measure of you close a datapoint is to the rest of the dataset. We think this could be an interesting connection to explore in future work, and could reveal additional theoretical insights on the algorithm. We have added a brief discussion of this to the related work (Section 2).\n\n> “As I am not familiar with the evaluation protocol in Brookles2019 and Fannjiang2020, the metric used in Table 1 is not clear to me. Can the authors say more about that?”\n\nRegarding the metric in Table 1, we report max/median ground truth scores across a batch of 128 candidate designs output by the algorithm. The max score corresponds to what would commonly be done in practice - one would synthesize a batch of candidate designs and use the best performing one. We report the median to verify that most designs perform decently well, because some algorithms may get lucky on scoring a single high-performing design. In terms of physical units, the Superconductor task is reported in Kelvin, the Molecule task in IC50 scores, the GFP in log-fluorescence scores (see Sarkisyan et. al. 2016), and the remaining robotics task in rewards (derived via velocity) from the underlying reinforcement learning MDP.\n\n> “In Figure 3, although not a major concern, I don't think the comparison with the ensemble is fair… For example, to obtain a similar uncertainty estimation compared with NML (middle column), we can use a deep ensemble, which optimizes NLL instead of MSE.”\n\nWe are indeed using NLL for both methods to keep the comparison fair. To be more specific, we discretized the output values, and the ensemble method predicts these using a softmax cross-entropy loss. We have updated Section 5.1 to clarify this.\n"], "review_score_variance": 0.888888888888889, "summary": "This work proposes a model-based optimization using an approximated normalized maximum likelihood (NML). It is an interesting idea and has the advantage of scaling to large datasets. The reviewers are generally positive and are satisfied with authors' response.  ", "paper_id": "iclr_2021_FmMKSO4e8JK", "label": "train", "paper_acceptance": "poster-presentations"}
{"source_documents": ["We aim to understand grokking, a phenomenon where models generalize long after overfitting their training set. We present both a microscopic analysis anchored by an effective theory and a macroscopic analysis of phase diagrams describing learning performance across hyperparameters. We find that generalization originates from structured representations, whose training dynamics and dependence on training set size can be predicted by our effective theory (in a toy setting). We observe empirically the presence of four learning phases: comprehension, grokking, memorization, and confusion. We find representation learning to occur only in a \"Goldilocks zone\" (including comprehension and grokking) between memorization and confusion. Compared to the comprehension phase, the grokking phase stays closer to the memorization phase, leading to delayed generalization. The Goldilocks phase is reminiscent of \"intelligence from starvation\" in Darwinian evolution, where resource limitations drive discovery of more efficient solutions. This study not only provides intuitive explanations of the origin of grokking, but also highlights the usefulness of physics-inspired tools, e.g., effective theories and phase diagrams, for understanding deep learning.", " Thank you for your extensive rebuttal. You have addressed a number of my questions and concerns, and I am increasing my score to a 6: weak accept. I appreciate the new appendices, they add extra depth and understanding to the paper. The one remaining concern is scale: results on large-scale systems would still add considerably to the paper’s contribution. However, as it stands I would argue in favor of accepting the paper, because it reveals interesting dynamics in the learning process that are likely to also be present at larger scales.\n\nRegardless of whether the paper gets accepted eventually, I would encourage you to work on showing that your findings here are relevant to training large-scale systems. I believe they might very well be, even if that is outside the scope of your current submission.\n\nYou say, rightly, that a perfect representation is harder to define for more complex data. That is indeed the reason to use toy/small-scale datasets, but in order for the findings on small-scale data to be relevant to large-scale settings, and hence to the research community as a whole, it is very important to show that the conclusions generalize to situations that are harder to inspect.\n\nApologies for my misreading of figure 1 - I thought the circular arrangement was found through t-SNE instead of PCA. Thanks for the clarification.\n\nI have another question or remark about the MNIST results, just for your consideration: The relative ordering of the phases is different than in the arithmetic task: memorization and confusion do not border each other, while memorization and grokking do (they also do in some places in the arithmetic task, but not to the same extent as in the MNIST case). The explanation in terms of absorbing information at different rates sounds like it would suggest a particular phase diagram, which one might expect to be consistent between different tasks, since the explanation does not refer to the task. It would be interesting to understand this in more detail.", " _\"How strongly does the result for the critical training set size to obtain a linear representation depend on the simple effective loss you are using? Would it be possible to perform a similar analysis for image classification (or language modeling, or any other application you fancy)?\"_\n* The critical training set size does not depend on the loss, per se. The theoretical model offers an explanation for the existence of a critical set in the toy setting i.e. a phase transition in the probability of obtaining a linear structure (which would lead to generalization). The argument here is that different operations  (for various algorithmic datasets) will have similar behavior based on a generalized version of parallelograms and \"linear structure.\" This is something we can already visualize in the modular addition case in Figure 1.\nDetermining the critical training set size requires the knowledge of ground truth representation, which is not well-defined in image or language tasks. It may require more extensive domain knowledge to do the critical size analysis, which we would like to investigate in the future.\n\n_\"Regarding the p=53 modulo addition experiment: is there a linear subspace in which you find the pizza wheel representation, like for the toy task? I understand it’s hard to visually inspect the 256D embeddings, but it might be interesting to see how many ‘effective dimensions’ there are - e.g. how many PCA components have significant explanatory power. It would be reasonable to expect a low-dimensional effective embedding, given how neatly t-SNE puts all the numbers in a circle. What happens if you take the two most explanatory principal components and make the same decoder plot there that you have for the toy task?\"_\n* Figure 1 shows exactly that! There we plot the first two principal components from a PCA on the embeddings. (N.b., the fact that modular addition can be represented by a circle like this is why we chose this example.) We also added plots showing that a small number of principal components of the input embeddings is enough to reproduce perfect generalization.\n\n_\"How do your findings relate to the lottery ticket hypothesis? That also addresses generalization in unexpected situations - in overparameterized networks as opposed to late in training. Would having more highly overparameterized representation networks be another potential way to avoid grokking, regiven the lottery ticket hypothesis that bigger networks have higher chances of containing subnetworks with good initializations, that would therefore learn quickly?\"_\n* This is an interesting thought! We’ve added some discussion of the LTH to Appendix J. While the connection to LTH is not perfect, we observe that along the right components, the embeddings are already roughly structured at initialization. So perhaps one could view generalization (grokking) as a process where superfluous structure is pruned out by SGD, leaving behind simple subnetworks which generalize. The connection between LTH and grokking is definitely a promising direction for future investigation.\n\n_\"In the particle system analogy: What are the ‘internal particle interactions’? If they arise from the gradients computed from the loss, then they really originate ‘externally’ too, don’t they?\"_\n* The effective loss only contains so-called two-body interaction terms i.e. terms that depend on the relative positions of the particles (embeddings). This is what we meant by internal interactions. We updated the draft to reflect this.\n\n[1] Papyan, Vardan, X. Y. Han, and David L. Donoho. \"Prevalence of neural collapse during the terminal phase of deep learning training.\" Proceedings of the National Academy of Sciences 117.40 (2020): 24652-24663.", " **Questions**\n\n_\"Can you be a bit more explicit about the loss in the beginning? It would help reading the paper.\"_\n* We have updated the explanation of the losses we use in the revised version of the paper. \n* To clarify, we would like to distinguish two kinds of losses used in our work: (i) actual training loss (mean-squared error or cross entropy) to obtain empirical results; (ii) effective loss, as a simplification of the actual training loss.  We use the term \"effective loss\" to invoke the idea of an “effective theory” in physics -- a theory which describes a system’s behavior without fully modeling the true causes of that behavior. \n* One can think about our effective loss as a postulated (approximate) implicit loss induced by the training procedure (the architecture, the actual loss being minimized, the optimizer used, etc.). Its purpose is to model the training dynamics of representations in a way that can be more directly analyzed. In particular, the mechanism described in Section 3 is that the neural network will try to simply learn parallelograms in embedding space. This simple view seems to be able to predict numerous phenomena: the criticality of training set size for generalization, the structure of the learned representations in embeddings space, and, to a lesser extent, the dynamics of the embeddings throughout training.\n\n_\"If I read the paper correctly, you are saying that grokking happens when different parts of the network absorb information at different rates; in particular, when the last part (the decoder) is much faster than the preceding part. Is that a reasonable formulation? And if yes, is it possible to quantify that statement?\"_\n* This is a reasonable formulation. Looking at phase diagrams in Figure 5 and 6, grokking occurs for a particular combination of decoder learning rate and weight decay, which we consider two of the major contributors to “learning speed”. We believe the phase diagrams shed some light on the roles of these hyperparameters. We cannot quantify the statement beyond our empirical results yet. However, the main conclusion should be that grokking is pathological and can be remedied by changing hyperparameters.\n\n_\"What constitutes a part, for the purposes of the above question? (see also discussion of A1 in the ‘strengths and weaknesses’ section)\"_\n* We have defined the encoder to be the learned embedding while the decoder is the rest of the neural network, for most of our experiments. As discussed earlier, this depends on the task, but it is well-justified theoretically for the toy model and well-justified empirically for transformers being trained on algorithmic tasks. \n\n_\"How generally does your analysis apply? [...] The answers you give - linear representation, relative learning speeds of the decoder and upstream subnetwork - are not specific to the type of problem you study, so it would be very interesting (and make the paper much stronger) if you could also include results on other types of task.\"_\n* As the question noted, our analysis applies to arithmetic datasets in which the Grokking phenomenon was first observed. One reason for the focus on arithmetic datasets, and specifically on the toy setting, is precisely because one can predict the proper representations necessary for generalization -- linear for addition and circular for modular addition. In the algorithmic datasets, we know exactly what to expect as a perfect representation, that is the representation that makes the task \"as easy as possible\" for the decoder.\nIn the addition setting, for instance, the best way to encode numbers is along a line, such that addition in the representation space can simply be achieved exactly and generally by adding the embedding vectors. To summarize, we started with problems that humans can solve, since in these special cases we know what to expect.\n* For tasks like image classification, defining a perfect representation is a much harder task.\n* However, your question prompted us to try to apply parts of our analysis to image classification, with good results. We include the effective theory analysis in Appendix H, and phase diagram analysis in Appendix I. To summarize our findings here: (i) The effective theory predicts the training dynamics similar to the neural collapse phenomenon [1]. (ii) The effective theory gives rise to a novel representation learning method (iii) However, the effective theory is unable to determine the exact critical training set size, see the question below. (iv) MNIST classification manifests all four learning phases (including grokking), and its phase diagram is very similar to that of algorithmic datasets. The resemblance seems to imply the universality of our phase diagram analysis, though additional studies on other types of problems are still needed to confirm universality", " Dear Reviewer k44Q -- Thank you for your detailed review and constructive feedback! Here is our response to your questions and concerns:\n\n**Summary**\n\n**Weakness:**_\"[...] There is a secondary result on the minimal training set size required to induce a representation in which different classes are linearly separable.\"_\n* Just to clarify here: In our discussion of the toy model (Section 3), when we refer to a “linear structure”, we mean that the embeddings are arranged along a straight line (i.e. $E_k = a + kb$ where a and b are vectors and k is an integer.), rather than that they are linearly separable. In the toy model for addition, this arrangement of the embeddings leads directly to generalization -- see Section 3.1.\n\n**Weakness:** _\"the paper is quite limited in terms of bigger systems, which makes it difficult to assess how general the results are.\"_\n* Our primary goal with this paper was to provide theoretical insight into the empirical results of Power et al., who first observed grokking in a relatively narrow domain: relatively small neural networks being trained on small algorithmic datasets. Accordingly, we have focused on this relatively small-scale setting.\n* However, we are also interested in whether our analysis of grokking extends to more general tasks/architectures. To address your concern, we have added more experiments which show that both our effective theory and phase diagrams can generalize to image classification. We include the new results in Appendix H and I, including the first demonstration, to our knowledge, of grokking on an image classification task. While our image classification setting is still relatively small-scale (MNIST), we believe these new results will be of significant theoretical interest, since they demonstrate that grokking is a more general phenomenon in deep learning than was previously known.\n\n**Weakness:** _\"The current formulation of A1 (the answer to question Q1, when does generalization happen), while true, seems both a bit trivial and a bit vague. You say generalization happens ‘with a good representation’, or a representation whose ‘structure is appropriate for the task’. This answer leaves open the questions which representation needs to be good (is it the penultimate layer of the network? the input to the decoder? but where does the decoder start and the representation network end?) and when a representation is ‘good’ or a structure ‘appropriate’.\"_\n* Representations, in any layer of a network, are good insofar as they allow the rest of the network to easily compute, in a manner that generalizes, the desired output from that representation. Exactly what constitutes a good representation thus depends on the network and on the task. Empirically, we observe that ordered ring structures in the embeddings of transformers are correlated with generalization on tasks like modular arithmetic, the setting of Power et al. Given some network, figuring out what part to consider the encoder vs the decoder is an empirical question -- almost a matter of interpretability, answered by carefully studying the internal workings of trained models. In transformers, since we observed intricate structure emerge in the embeddings (Figure 1), we considered the model embedding to be the encoder and the transformer “decoder” to be the decoder. Once a decision has been made, one can then study how various hyperparameters affect the formation of structure within the network.\n* In our toy model for addition, where addition between embeddings is part of the architecture, the linear structure in the embeddings is a good representation. This is the advantage of the toy model -- one can directly see that the embeddings are the representations that matter, and that a particular structure (a linear structure) is good, since it guarantees generalization. \n\n", " Dear Reviewer tEaJ -- Thank you for reviewing our paper and for your detailed feedback!\nBelow, we address your questions. Please let us know if anything remains unclear.\n\n### Weaknesses\n\nQ: _\"One of the intriguing observations from Power et al [1] is that grokking can be seen both in unregularized models with sufficient data, or in regularized models with a small amount of training data. I think a sufficient explanation of the phenomenon needs to provide a justification for both behaviors. However, the discussion around the effect of data size seems lacking. As far as I can tell, neither the theoretical nor the empirical analyses shed light on why more data speeds up generalization while less data can lead to grokking.\"_\n* A: Our effective theory does indeed capture dependence of grokking time on data set size, although this was not adequately explained in our original manuscript. We have added new discussion and results on this in Appendix G.\n* The basic idea is this: We argue that the learning speed in our setting is governed by $\\lambda_3$, the third-smallest eigenvalue of $A^T A = H$. $A$ is a matrix representing the parallelogram equations, see Equation 7 in Section 3.2. The larger $\\lambda_3$, the faster the training. In the plot in Appendix G, we show the size of $\\lambda_3$ depends on the train data fraction. In Figure 4b one can find the empirical results for the toy model.\n* Additionally, we show empirically how the size of the training dataset influences grokking/delayed generalization in image classification, see Appendix I, Figure 20. \n\nQ: _\"The behavior of the phase diagram is qualitatively different between the setting in [1] and the toy model, namely that instead of models going from memorization to comprehension to grokking, in the real task setting it is going from memorization to grokking to comprehension. There is insufficient explanation to bridge this gap.\"_\n* A: Thanks for pointing that out, we should have been clearer here! The phase diagram's main point is to show that grokking is pathological, a result of improperly tuned hyperparameters.  Going from X to Y implies choosing a path on the plane, and depending on the choice the results are different. In that sense, one can almost always find a way in which everything is adjacent to everything else, as long as they have a common border. Note also that the phase diagrams only show a part of the plane!\n* We admit that the phase diagrams in Figure 5 and 6 are qualitatively different. This could be due to several factors. One possibility is that it is due to differences in embedding dimensions (1D and 256D, respectively). While 1D prefers linear representation, 256D prefers the ring representation.\n\n### Questions\nQ: _\"Is the analysis on \"time towards the linear structure\" independent of dataset size? Or can it shed light on the question of data size vs grokking / comprehension?\"_\n* A: It can indeed shed light on the dependence of grokking on data size. The dynamics of effective theory is dependent on the training data size, since the effective loss is the sum of four-body terms within the training set. As we discuss in Appendix G, the time of grokking scales as $1/\\lambda_3$, and we find that $\\lambda_3$ is (on average) zero below some train set fraction and then an increasing function of of the train set fraction (Figure 17a), so grokking time decreases as data size grows larger. \n\nQ: _\"What type of task characteristic makes a model most susceptible to grokking? If the mismatch between decoder learning speed and representation learning speed is key to grokking, would you expect that the four phases in the phase diagram to exist in general (e.g. for natural vision or nlp tasks)? If it's not general, what is it about the specific tasks and models studied in this work that leads to these different regions?\"_\n* A: We expect that four phases of learning to exist in general, but the ease of obtaining them in practice depends on specific tasks. We think grokking is easy to get on datasets in which initial representation is very far from a good, final representation. From comparing (a) arithmetic datasets and (b) image classification: (a) the embeddings of numbers are initialized as random vectors, thus far from the desired, structured representations. (b) Images, although pixelization may destroy or obfuscate some semantic information, other information (e.g. topology of  the image manifold) is still preserved, making it faster to learn a good representation. In fact, we are able to observe grokking on MNIST classification, if we manually construct a suboptimal initial representation by using a large initialization scale. The dependence on training set size and the phase diagrams are consistent with theory developed in the paper, see Appendix I.\n", " _\"3. I have a hard time understanding lines 148-153. Would you explain why the third eigenvalue is the one that dominate the dynamics?\"_\n* Of course!\nWe say a representation is a solution if it incurs zero effective loss.\nAs already discussed above, a line in embedding space, $E_k = a + k b$, does that.\n* Now we look at the solution $R(t)$ to the differential equation (9). We know that $H$ has at least two zero eigenvalues. All contributions to $R(t)$ with non-zero $\\lambda_i$ will go to 0 with increasing time, while the first two contributions remain constant. The two remaining eigenvectors will define the line $E_k$. As the other eigenvalues \"interfere\" with the line representation, we have to wait for them to decay with time. And since $\\lambda_3$ is the smallest one, that decay will take the longest. Thus, the size of $\\lambda_3$ determines the time for generalization. If $\\lambda_3$ is also 0, we will never get to a line representation and thus not achieve perfect generalization. That defines the critical training set size. See Figure 17(a) for how $\\lambda_3$ depends on the training set size.\n\n\n_\"4. The analogy in lines 197-201; Don't you agree that it is the decoder that encourages the representations to have a structure rather than their \"internal interactions\". If the is not decoder, what forces the representations to change structure?\"_\n* The effective loss only contains so-called two-body interaction terms i.e. terms that depend on the relative positions of the particles (embeddings). This is what we meant by internal interactions. You are correct that the decoder determines the dynamics under real training. However, we find that our effective theory, which is independent of the decoder, nevertheless captures the core behavior of the learning dynamics. We updated the language of the draft for clarity.", " Dear Reviewer Sd61 -- Thank you very much for your review, especially for the references to the related papers! Below we address your concerns and questions. Please let us know if something remains unclear.\n\n### Weaknesses\n_\"At parts, it is difficult to follow certain arguments. For example, the term \"effective\" has a different meaning that the one that initially comes to mind.\"_\n* We apologize for the confusion. The concept of an effective theory in physics is similar to model reduction in computational methods, aiming to describe complex phenomena with a simpler theory which is tractable to analyze but still captures the basic behavior being studied. In our revised manuscript, we have clarified the definition at the beginning of Section 3.\n\n_\"Cite related works\"_\n* Thanks for pointing to these nice papers!. We have added them in the related works.\n\n_\"Limited experimental results\"_\n* We have added more experiments to strengthen our theory/method. In particular, we show that both our effective theory and phase diagrams can generalize to image classifications (run on the MNIST datasets). Please consider the new Appendices G, H, and I, marked blue in the revised paper!\n\n### Questions\n_\"1. The choice of Parallelogram-ratio for quantifying the quality of a representation is rather arbitrary. What is the idea behind using such a quantity?\"_\n* Thanks for pointing that out, we should be more clear on that! We have made the motivations more clear in the paper.\n* The intuition is as follows:\n    * (i) the numbers are embedded into a vector space\n    * (ii) if $E_1 + E_4 = E_2 + E_3$ (with $E_X$ being the embedding of number $X$), the learning process correctly placed those embeddings relative to each other.\n    * (iii) Parallelograms are explicit causes of generalization. For example, Suppose the pair (1, 4) appears in the training set, and the pair (2, 3) appears in the validation set. If $(E_1, E_2, E_3, E_4)$ forms a parallelogram, i.e., $E_1 + E_4 = E_2 + E_3$,  then the training sample $(E_1, E_4)$ can immediately generalize to the validation sample $(E_2, E_3)$. If all possible parallelograms are set up correctly,\ni.e. $E_X + E_Y = E_Z + E_W$ for all $X,Y,Z,W$ where $X+Y = Z+W$, then the embeddings will lie on one line and the scalar addition of the inputs becomes equivalent to the vector addition in the embedding space and we have perfect generalization. This is why the fraction of parallelograms learned is a good indicator of the generalization capability of the network.\n\n_\"2. Why don't you model the actual dynamics of GD on the training loss? Instead, you derive the dynamics of the 'effective' loss (defined in Eq. 5) which again appears arbitrary & 3. By defining the 'effective' loss, in essence the learning dynamics of the encoder is now decoupled from the decoder. I am right?\"_\n* Yes -- in reality, gradients for the embeddings are backpropagated through the decoder MLP. However, it turns out that we can model the learning dynamics of the embeddings as if they didn’t depend on the details of the decoder, but instead just on their relative positions. With this simplification, learning dynamics become much easier to theoretically analyze. Despite this simplification, we find that the dynamics under the effective loss capture some important properties of the true model, notably the dependence on the train data fraction. Lastly, we’ll note here that one can derive our “effective loss” from the true loss in a certain limit, with some assumptions, as demonstrated in Appendix K of our revised manuscript.\n\n", " Firstly, we want to thank all reviewers for their high-quality reviews and constructive feedback. This first round of reviews prompted us to make a number of improvements to our manuscript. The revised version not only contains improvements in clarity, but also adds substantial new analysis and results that we believe make our work materially stronger and also much more general. In the updated manuscript, major revisions, as well as passages relevant to our answers, are highlighted in blue. Here is a summary of these revisions/additions:\n\n**1. Better justification and analysis of our “effective theory”**\n* We add additional discussion and results showing how our effective theory predicts the dependence of grokking on training set size. See Appendix G for more details.\n* We clarify the language in Section 3 for what we mean by “effective theory” and “linear structure”.\n* In response to reviewer Sd61, we hope to motivate the effective loss a little better by showing how it can be derived in the linear regression setting from actual gradient flow dynamics. See Appendix K.\n\n**2. Extension of our analysis beyond algorithmic datasets**  \nA primary concern of reviewer k44Q was whether our analysis was too limited in scope. They mentioned image classification as an application where we could extend our analysis, and suggested that doing so would make our paper much stronger. This was a fruitful suggestion, which led to the following:\n* We develop an effective loss function for image classification problems. This effective theory gives rise to a self-supervised learning method free of global collapse. It also aligns with the phenomenon of neural collapse. We visualize how representations evolve under this effective loss on the MNIST dataset. See Appendix H.\n* We demonstrate that grokking (significantly delayed generalization) can occur in networks trained on MNIST. To our knowledge, this is the first time that grokking has been observed beyond the algorithmic datasets studied in Power et al. We include experimental results on MNIST in Appendix I. These include a phase diagram as well as a study of how time to generalization depends on the amount of training data. We find that grokking in this setting can again be remedied using the same prescription we develop for algorithmic datasets -- with proper tuning of weight decay and learning rates.\n\nWe have also made some miscellaneous other edits. For instance, in order to both respond to reviewer concerns and also stay within the page limit, we have removed some parts of “Related Work”. We want to add these back for the “camera ready” version. \n", " The main contribution of this paper is a study of how different loss minimization and generalization outcomes are realized: confusion (no loss minimization), memorization (loss minimization without generalization), fast generalization, and grokking (generalization, but significantly slower than the loss is minimized). There is a secondary result on the minimal training set size required to induce a representation in which different classes are linearly separable. All tasks studied in the paper are related to arithmetic operations like addition, with an appendix explaining that the setup studied in the paper can be used for all Abelian groups. The paper contains a thorough exploration of a toy system, which is used to analyze and illustrate the above-mentioned outcomes. Some results are included on a larger task with larger networks.  - Strength: the exploration of the toy system is very thorough and offers an interesting ‘peek inside the black box’. It suggests very plausible mechanisms for how the learning process plays out.\n- Weakness: the paper is quite limited in terms of bigger systems, which makes it difficult to assess how general the results are.\n- Weakness: The current formulation of A1 (the answer to question Q1, when does generalization happen), while true, seems both a bit trivial and a bit vague. You say generalization happens ‘with a good representation’, or a representation whose ‘structure is appropriate for the task’. This answer leaves open the questions which representation needs to be good (is it the penultimate layer of the network? the input to the decoder? but where does the decoder start and the representation network end?) and when a representation is ‘good’ or a structure ‘appropriate’. Presuming that those questions are answered, it comes down to the statement that the mapping from the representation to the final required output is easy to learn. For the toy task you analyze you do go into the structure of the representation quite extensively, but since that analysis is limited to the toy setup, it is not obvious that the answers you find there generalize to harder / bigger tasks, deeper networks, and higher-dimensional representations. NB: A2 is only as meaningful as A1 is clear, which makes this issue more relevant.\n - Can you be a bit more explicit about the loss in the beginning? It would help reading the paper.\n- If I read the paper correctly, you are saying that grokking happens when different parts of the network absorb information at different rates; in particular, when the last part (the decoder) is much faster than the preceding part. Is that a reasonable formulation? And if yes, is it possible to quantify that statement?\n- What constitutes a part, for the purposes of the above question? (see also discussion of A1 in the ‘strengths and weaknesses’ section)\n- How generally does your analysis apply? I can see from the appendices that it should generalize to other Abelian groups, but can you comment on different applications like image classification? The answers you give - linear representation, relative learning speeds of the decoder and upstream subnetwork - are not specific to the type of problem you study, so it would be very interesting (and make the paper much stronger) if you could also include results on other types of task.\n- How strongly does the result for the critical training set size to obtain a linear representation depend on the simple effective loss you are using? Would it be possible to perform a similar analysis for image classification (or language modelling, or any other application you fancy)?\n- Regarding the p=53 modulo addition experiment: is there a linear subspace in which you find the pizza wheel representation, like for the toy task? I understand it’s hard to visually inspect the 256D embeddings, but it might be interesting to see how many ‘effective dimensions’ there are - e.g. how many PCA components have significant explanatory power. It would be reasonable to expect a low-dimensional effective embedding, given how neatly t-SNE puts all the numbers in a circle. What happens if you take the two most explanatory principal components and make the same decoder plot there that you have for the toy task?\n- How do your findings relate to the lottery ticket hypothesis? That also addresses generalization in unexpected situations - in overparameterized networks as opposed to late in training. Would having more highly overparameterized representation networks be another potential way to avoid grokking, given the lottery ticket hypothesis that bigger networks have higher chances of containing subnetworks with good initializations, that would therefore learn quickly?\n- In the particle system analogy: What are the ‘internal particle interactions’? If they arise from the gradients computed from the loss, then they really originate ‘externally’ too, don’t they?\n The main limitation comes from the size of the systems studied, as mentioned above. In principle the analysis could apply quite widely, but it is difficult to assess that from the experiments in the paper.", " This paper studies the grokking phenomenon under a physics-inspired \"effective theory\", which studies the phenomenon under toy settings. The grokking phenomenon refers to when generalization happens much later during training, long after training accuracy have reached 100%. The theory proposes the following explanations to grokking.\n1) generalization is correlated to when the models learns a sensible representation of the inputs.\n2) the critical training size (amount of data needed for generalization) relates to the amount of data needed to determine a good representation.\n3) grokking represents a particular region in phase space of hyperparameters, and can be removed by using better hyperparameters. ### Strength:\n\n- The paper proposes an original view into the grokking phenomenon, which has been an interesting and unexplained phenomenon in the community. The phase diagram view into the landscape of training regimens illustrates interesting relationships between memorization, generalization, and grokking. One of the interesting questions about grokking is whether there is some magical inductive bias of gradient descent that leads to generalization after an abnormally long training period. The result of this paper would suggest that grokking is more of a pathological phenomenon that is the result of improper hyperparameter tuning. Though as I'll mention in the weakness section, I think that the support for this argument is not sufficient if it is indeed the conclusion of the authors.\n\n- The paper proposes an explanation into the grokking phenomenon in the form of competition between representation learning and decoder overfitting. This seems plausible and is in line with a number of other works suggesting that later layers drive overfitting.\n\n- Although the theory is based on a very simplified toy model, it provides an illuminating view into why a minimum number of training examples may be required for generalization. The predicted phase transition in Figure 4 is interesting to see.\n\n### Weaknesses:\n\n- One of the intriguing observations from Power et al [1] is that grokking can be seen both in unregularized models with sufficient data, or in regularized models with a small amount of training data. I think a sufficient explanation of the phenomenon needs to provide a justification for both behavior. However, the discussion around the effect of data size seems lacking. As far as I can tell, neither the theoretical nor the empirical analyses shed light on why more data speeds up generalization while less data can lead to grokking.\n\n- Most of the results use an extremely toy setting. Only one set of experiments (modulo addition with varying learning rate and weight decay) is performed on the original grokking task from [1], while [1] contains a number of tasks and hyperparameter settings. Crucially the effect of data size is not studied, which was an important observation from [1]. It is unclear how much this theory really explains the original phenomenon, particularly given that the behavior of the phase diagram is qualitatively different between the setting in [1] and the toy model, namely that instead of models going from memorization to comprehension to grokking, in the real task setting it is going from memorization to grokking to comprehension. There is insufficient explanation to bridge this gap. - Is the analysis on \"time towards the linear structure\" independent of dataset size? Or can it shed light on the question of data size vs grokking / comprehension?\n\n- What type of task characteristic makes a model most susceptible to grokking? If the mismatch between decoder learning speed and representation learning speed is key to grokking, would you expect that the four phases in the phase diagram to exist in general (e.g. for natural vision or nlp tasks)? If it's not general, what is it about the specific tasks and models studied in this work that leads to these different regions? The authors address the limitations of the toy model. Though it is fine to use toy models to study these type of questions, I think that there is nonetheless insufficient work to show whether the insights from the toy model can actually generalize to other settings.", " This work studies the Grokking phenomenon -- a phenomenon in which generalization occurs long after mastering the training set. This work poses three questions with regard to this phenomenon:\n1. When the generalization happens? and the answer is: when the representations become well-structured.\n2. How the size of the training set affects this phenomenon? and the answer is: the critical size of the dataset is the amount required for learning a 'good' representation.\n3. What leads to a delay in generalization? and the answer is: different phases of learning between \"memorization\" (perfect training acc but with lack of generalization) and \"comprehension\" (perfect training and generalization acc)\n\nFrom a physics perspective, this work build the answers outlined above and provide supporting experiments. **Strengths**:\n- The problem is interesting and relevant: The Grokking phenomenon is a perplexing phenomenon which understanding it opens the doors to a better theory of generalization.\n- The form of presentation is neat: I very much liked that the message is clear from the get go.\n- The phase diagrams provide high-level intuitions: The same applies to the phase-diagrams that clearly distinguish between the 4 different phases.\n\n** Weaknesses**:\n- Some of the choices in the paper appears rather arbitrary and not fully justified (see questions below).\n- At parts, it is difficult to follow certain arguments (at least for me): I am not from a physics back ground. For that it took me several wikipedia searches to grasp the meaning of some concepts in the paper. For example the term \"effective\" has a different meaning that the one that initially comes to mind.\n- Very light on the related work: There has been a couple of papers that use tools from physics and provide phase diagrams for understanding the generalization of neural networks:\n    + Generalisation error in learning with random features and the hidden manifold model by Gerace et al\n    + Multi-scale Feature Learning Dynamics: Insights for Double Descent by Pezeshki et al\n    + The Gaussian equivalence of generative models for learning with two-layer neural networks by Goldt et al\n    + Statistical mechanics for neural networks with continuous-time dynamics by Kuhn et al\n- Very limited experimental results. 1. The choice of Parallelogram-ratio for quantifying the quality of a representation is rather arbitrary. What is the idea behind using such a quantity?\n2. Why don't you model the actual dynamics of GD on the training loss? Instead, you derive the dynamics of the 'effective' loss (defined in Eq. 5) which again appears arbitrary.\n3. By defining the 'effective' loss, in essence the learning dynamics of the encoder is now decoupled from the decoder. I am right?\n4. I have a hard time understanding lines 148-153. Would you explain why the third eigenvalue is the one that dominate the dynamics?\n5. The analogy in lines 197-201; Don't you agree that it is the decoder that encourages the representations to have a structure rather than their \"internal interactions\". If the is not decoder, what forces the representations to change structure? No negative societal impact is foreseen by this reviewer.\nAuthors talk about the limitations of the theory in line 160-165."], "review_score_variance": 0.22222222222222224, "summary": "There was a consensus among reviewers that this paper should be accepted. In particular, reviewers felt, that the contribution of studying the rate at which different parts of a neural net absorb information, and the effects it has on learning and generalization is a worthwhile one that would enrich the literature and that the paper is a solid contribution to the NeurIPS literature.", "paper_id": "nips_2022_6at6rB3IZm", "label": "train", "paper_acceptance": "Accept"}
