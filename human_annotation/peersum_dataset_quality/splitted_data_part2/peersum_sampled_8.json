{"source_documents": ["As an important problem of causal inference, we discuss the estimation of treatment effects under the existence of unobserved confounding. By representing the confounder as a latent variable, we propose Counterfactual VAE, a new variant of variational autoencoder, based on recent advances in identifiability of representation learning. Combining the identifiability and classical identification results of causal inference, under mild assumptions on the generative model and with small noise on the outcome, we theoretically show that the confounder is identifiable up to an affine transformation and then the treatment effects can be identified.  Experiments on synthetic and semi-synthetic datasets demonstrate that our method matches the state-of-the-art, even under settings violating our formal assumptions.", "Summary:\nThe present paper introduces Counterfactual VAE (CFVAE), a generative learning method to estimate treatment effects under a latent unconfoundedness assumption. It builds on variational autoencoders (VAE) to learn causal representations. The authors provide identification results using recent results on nonlinear ICA (Khemakhem et al., 2020). They show that the confounder is identifiable up to an affine transformation.\nThe main contributions of this work are presented in Sections 5.1 and 5.2, where they derive identifiability of the treatment effect via identifiability of representation.\nThe theoretical claims are complemented with various synthetic and semi-synthetic experiments which also show that the proposed models can compete with and in certain cases improve upon state of the art.\n\nRecommendation:\nReject. In summary, I am rather convinced that the contribution of this paper is important, but lacks clarity in its arguments and clarifications w.r.t. its positioning in the standard causal inference framework which it claims to make a new contribution to and its causal model/underlying assumptions about confounding are not stated clearly enough. I will read the rebuttal carefully and am willing to increase the score if the authors address the raised concerns.\n\nStrong points: \n - This work provides a simplified and yet still practically useful framework that leverages recent nonlinear ICA result to provide an identification result in the causal inference framework.\n - The simulations, especially in Section 6.1 are well presented and commented.\n\nIssues/Points that require clarification:\n - The positioning w.r.t. CEVAE and more generally w.r.t. to classical unconfoundedness assumptions is not very explicit in that the authors first state that their method requires weaker assumptions than CEVAE but from the identifiability discussion (Sec. 5.2) and the experiments (Sec. 6.1) it seems that the models are more different and not a weaker, resp. stronger version of each other. A simplified causal graph for CFVAE (maybe together with one for CEVAE) could make the comparison more explicit.\n - To my understanding, the balancing covariate assumption implies that $z$ is not directly related to $t$ (meaning in a causal graph there would be no arrow from $z$ to $t$) but only through $x$. So if one were to adjust for the treatment bias using only $x$, this should be sufficient to estimate the treatment effect, for instance by inverse propensity weighting. Thus again, could the authors provide a causal graph representing their assumptions about the role of each variable, most importantly $z$ and $x$, similar to Figure 1 of Louizos et al. (2017)?\n - The theory seems solid for the univariate case. How challenging would an extension to a multivariate case (i.e., multiple latent confounders) be? As a first step, have the authors looked at the empirical behaviour in case of multivariate confounders? \n - Related to the previous point, are there some concrete examples where the assumption of a latent univariate confounder is plausible/sufficient to capture all confounding?\n - The main claim of this work relies on results from Khemakhem et al. (2020), it would be helpful to have at least 1-2 sentences summarizing the main idea/result for identifiability of the iVAE in this cited article, so that the present work is more self-contained.\n - Concerning the pre-treatment prediction (p.6), have the authors verified empirically how their proposed alternative in the absence of post-treatment observation $y_t$ performs?\n - Would it be possible to add the parametric probabilistic PCA based approach by Mao et al. (2018) to the experiments? To my knowledge, this is the only work that gives a proved consistent estimator in the case of latent confounding. Also, this reference should be added in the related work section.\n\nMinor comments (that did not impact the score):\n - p. 1: \"effects of public policies or clinical trials\" $>>$ replace clinical trials (clinical trial is a mean to estimate the effect of a new drug/treatment, not something that has an effect in itself)\n - p. 2: In many work $>>$ In many works\n - p. 2: if we apply $>>$ if we applied\n - p. 3: casual effects $>>$ causal effects\n - p. 3: CATE can be understood is an $>>$ CATE can be understood as an\n - p. 3: introduce/define $D_{KL}$ notation\n - p. 5: introduce/define $\\delta$ notation\n - p. 6: check caption of Figure 2.\n - p. 7: descriptoins $>>$ descriptions\n\nReferences:\n - Nathan Kallus, Xiaojie Mao, and Madeleine Udell. Causal inference with noisy and missing covariates via matrix factorization. In Advances in Neural Information Processing Systems, pp 6921–6932, 2020.\n - Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational autoencoders and nonlinear ica: A unifying framework. In International Conference on Artificial Intelligence and Statistics, pp. 2207–2217, 2020.\n - Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and Max Welling. Causal effect inference with deep latent-variable models. In Advances in Neural Information Processing Systems, pp. 6446–6456, 2017.\n\n===================\n\nPost Rebuttal Update:\n\nWhile I think the proposal is interesting, I still think the proposed methodology lacks a clear presentation of the studied (causal inference) problem and statement of its underlying assumptions, as it has also been pointed out by other reviewers. Despite some clarifications from the authors I still vote for rejection as I believe the paper requires a major revision.", "We thank the constructive comments, and reply the reviewer's concerns as following:\n\nGeneral points:\n\n- We totally agree that \"*Identifiability is a property of the data generating process*\". In this work, we made a *parametric* assumption, that the true conditional prior distribution p(z|x,t) should be in an exponential family. Some readers would be confused and think using exponential family conditional prior \"will make the causal effect identifiable\".  We **updated** the paper to make this clearer, rewording Theorem 2 (now Theorem 3) and adding explanations above it.\n- (Non-parametric identification?) We are not sure if the reviewer were suggesting us to first present conditions for *non-parametric* identification. We plan to leave this to future work, by analysis of our assumptions in light of, for example, (Kuroki and Pearl 2014) and (Miao et al 2018). We would be grateful if the reviewer could give further suggestions on this.\n- (Reviewer's point 2 and 3 replied together) Given the experimental results, we tend to believe some assumptions we made are not necessary. And we will follow the reviewer's suggestion, examine and highlight which assumptions are really necessary to eliminate certain causal situations, and which assumptions are \"non-causal\" and could thus be relaxed. As a first step, we **updated** the paper, listing our assumptions more clearly.\n\nSpecific concerns:\n\n- The reviewer's second argument (by adjustment eq.) requires that p(t|x) should be *positive* at any points. This opens the possibility that ignorability might also fail when positivity does not hold. We would like to emphasize that our method works *without* positivity given x, but naïve regression does not. Moreover, even if positivity given x is satisfied, naïve regression still has practical issues: given finite data and the high dimensionality of x, it is quite possible that for many values of x, we do not actually have data for both t. But since our method learns a lower dimensional z, this problem will be largely addressed.\n\nOther concerns:\n\n- (Rightmost side of eq (2)) Sorry, this was a typo, and we **fixed** it.\n- (Individual causal effects?) We are not sure if we understand this concern correctly, but we reply as following. By \"individual level\" causal effects, we mean the CATE is conditioned on a high dimensional and highly diverse set of covariates x, such that *in practice* an individual can be identified by x. We do *not* intend to tackle the problem of identifying individual causal effects in the *proper* sense. Note also that, under 0 noise limit, given (z, t), y_t=f(z, t) is deterministic.\n- (Noisy y and f^{-1}(y)) We agree the outcome noise will complex the problem and in particular the recovered z' will not be a deterministic function of true z (as shown in 6.1), but the f^{-1}(y) notation itself is still meaningful even if y is noisy.", "We reply the reviewer's concerns point by point:\n\n- (Comparison to CEVAE) We admit the statement \"*Except this independence, we do not make any specific causal assumptions on the covariates*\" in the introduction was a bit misleading and is **replaced** now. Compared to CEVAE, the main additional assumption of our method is the balancing covariate assumption, which is required to learn a affine transformation of the true confounder. However, CEVAE assumes directly that the true confounder can be recovered, and this, as implied by our Sec. 5, is arguably an even stronger assumption than balancing covariate. The graphical model (that needs *not* to have causal implications) for the decoder of CFVAE is presented in Figure 1, while the graphical model of our encoder is standard and thus omitted (see the caption of Figure 1). The encoder of CEVAE is nonstandard and more complex than ours. Please also see the reply to reviewer3 on our novelty compared to CEVAE and iVAE.\n- (Example satisfying our assumptions) For a proxy variable (e.g., Figure 1 of Louizos et al. (2017)) to further satisfy (t indep z|x), one possibility is noiseless proxy, defined in Appendix 8.5 (note that noiseless requirement violates causal faithfulness).\n- Our Theorem 2 (Theorem 3 in the updated version) in itself is not constrained to univariate latent z, but instead requires that the dimensionality of z is not larger than that of outcome y. So there might be a practical constraint because y is usually univariate. Experiments on multivariate artificial data will be added. Also see the following point.\n- (When univariate latent is sufficient) We can think in turns of the complexity of confounding. Imagine, if we have 10 independent latent confounders, and they have *linear* effects on both t and y. Then it is plausible that the total confounding effects can be modeled by single latent variable z, but complex enough functions relating t, y and z. On the other hand, as seen in 6.2 and 6.3, we can use multivariate z to achieve better performance even if y is univariate. Hence we think the practical usefulness of our method is promising.\n- We **updated** the paper, adding more on iVAE in 3.2.\n- Yes, we evaluated the pre-treatment performance (without observation y_t) in each experiment, and the results constantly matches that of post-treatment, or only slightly worse. In figure 3 6.1 and table 2 6.3, the reported results are pre-treatment. In table 1 6.2, we reported both pre/post-treatment results.\n- Thank you for the reference to Mao et al. (2018). It is indeed relevant. We **added** it in the related work. We will also add experimental comparison.\n- Thank you for the careful reading. We have **fixed** the typos and careless omissions.", "We reply the reviewer's concerns point by point:\n\n- The existence of observed confounders is allowed. The description of z as \"unobserved confounders\" in the paper was a bit confusing. Actually, since z is the latent variable(s) for VAE and is learned from covariates x by VAE, it can contain *all* confounders in principle. If there are observed confounders in x, our method will extract that part of x into z. We **updated** the paper to make this clear.\n- (Definitions of ignorability and propensity) We believe this point is also addressed by the above.\n- (Example satisfying our assumptions) A typical proxy variable x satisfies (y indep x|z, t) (e.g., Figure 1 of Louizos et al. (2017)), and for a proxy variable to further satisfy (t indep z|x), one possibility is noiseless proxy, defined in Appendix 8.5 (note that noiseless requirement violates causal faithfulness). On the other hand, our Fig. 1 shows the graphical model, that needs *not* to have causal implications, of the decoder of CFVAE (which is built to easily satisfy the assumptions of iVAE).\n- Graphical models for generating synthetic datasets are now **added** in Appendix. Note that eq (10) is a general equation which will be used to generate 3 different causal settings, as described in the paragraph \"We experiment on three different causal settings...\".\n- More comparisons on synthetic datasets will be added.", "We reply the reviewer's concerns as following:\n\n### Comparison with CEVAE\n\n- The similarity to CEVAE is mainly on the initial motivation by proxy variable and the employment of VAE.\n- CFVAE is quite different to CEVAE in *architecture*, both its decoder and (simpler) encoder. Please see Figure 1 and its description.\n- CEVAE assumes implicitly p(x,t|z)=p(x|z)p(t|z) (x \\indep t |z) in their eq (2) for building the decoder. This means direct edge from X to t is actually not allowed, unless causal faithfulness is violated.\n- We will add more detailed comparison (with figures), possibly in Appendix.\n\n### Assumptions from iVAE\n\n- The beginning of Sec. 5.1 introduced the assumptions under our own setting, though in a less formal way. We **updated** the paper, presenting a formal theorem, and also saying more about iVAE in 3.2.\n\n### More comments on novelty\n\n- CEVAE is tailored and limited to the setting of standard proxy variable (as in their Fig. 1), but CFVAE works under more general settings, given only the independence is satisfied.\n- On the other hand, the derivation of the variational lower bound of CFVAE from (5) to (6) is more principally built on identifiability of VAE and identification equation (2).\n- iVAE did not make any connection to causal inference. In particular, first, we motivate the independence of iVAE by that of proxy, and second, there is no place of the treatment variable in iVAE and thus it is not directly applicable to the estimation of treatment effect, and we add the conditioning on treatment to address this.", "We thank the reviewers for their time and thoughtful comments. We have updated the paper, and the major changes that would be interesting to all are as following:\n\n1. Presenting the identifiability of our model in a formal theorem (now Theorem 2).\n2. Rewording Theorem 2 (now Theorem 3) and adding explanations above it.\n3. Listing our assumptions more clearly.", "The paper proposes a new variant of VAE for estimating conditional treatment effects under unobserved confounding. It provides theoretical results on the identifiability of the confounder and the treatment effects under some assumptions. Experimental results are provided to demonstrate the performance of the proposed method. \n\nPros:\n\n-The paper addresses an important problem\n\n-The paper provides a theoretical analysis of the identifiability of representation and treatment effect.\n\nCons:\n\n-I’m confused by the role of $x$ and its relation with the unobserved confounder $z$. In the works that do not consider unobserved confounders, I believe the covariates $x$ in the treatment effect $\\mu_t(x)$ are the observed confounders and assumed to satisfy the ignorability condition. In this paper, are you assuming there exist no observed confounders? That would be a very unreasonable assumption.\n\n-I think the ignorabiltiy assumption made before Eq. (2) is not correct. For (2) to hold, you’d need (y(0), y(1) independent t |z,x) to hold. I believe the propensity score should be p(t=1|z,x).\n\n-The paper made several assumptions that look to me quite arbitrary, including (y indep x|z, t) and (t indep z|x). I’m not sure they are consistent with each other or with the ignorabiltiy assumption. What graphical models satisfy all these assumptions? The CFVAE structure in Figure 1 does not look like satisfying all these assumptions.\n\nOverall, I vote for reject. I think the paper made some unreasonable assumptions.\n\nOther comments:\n\n-What is the graphical structure of the data-generating model in (10)?\n\n-Why only compare with CEVAE on the synthetic datasets but not other methods?\n\n\n", "Summary:\n\nThis paper provides a method for using a VAE with proxy variables to estimate CATE in a model with latent confounding by recovering a conditional distribution over the latent confounders. Building upon results from Khemakhem et.al. 2020, the confounding can be identified if the latent variable is parameterized by an exponential family distribution dependent on the proxy variable, and the outcome variable is an injective function of the latents with (small) additive noise. Conceptually, the paper is similar in goal to Louizos et al. 2017, with the main difference seeming to be a stronger theoretical base.\n\nReasons for score:\n\nOverall, I would rate this paper as a borderline accept. The work seems to provide a strong theoretical background for VAE-based models to be used for identification purposes with latent confounding when proxy variables are present, allowing to explicitly specify conditions under which approaches modelling confounders for adjustment can work. I find this an interesting result that validates the intuitions behind previous works. My main concern with the paper is an unclear comparison to Louizos et. al, as well as an unclear specification of the necessary assumptions, which only becomes clear after reading Khemakhem et. al. Finally, given that this work seems to be a direct combination of these two previous works, I have some lingering doubts about the result's novelty, which would be greatly helped if the paper included a more detailed comparison.\n\nPros:\n\n- As mentioned, provides a strong theoretical backing for a VAE-based approach to CATE, giving a clear set of conditions under which such approaches can be used\n- Provides convincing experimental evidence that the proposed method, CFVAE, compares favorably with existing approaches.\n- Very clearly lays out the relevant background literature, allowing to cleanly place this work in context.\n\nCons:\n\n- The difference between CEVAE and CFVAE should be clarified. For example, the statement \"CEVAE assumes a specific causal graph where the covariates should be independent of the treatment given the confounder.\" seems to contradict the end of page 3 in Louizos et. al., where it looks like they state that there can be a direct edge from X to t. Could the authors comment on this?\n- While the fact that the paper was using results from Khemakhem et. al. was made very clear, I needed to go to the original paper for a clear specification of the precise conditions required for the method to work (exponential family, outcome is function of latent with small additive noise, etc). It would be useful if these conditions were listed clearly and centrally in the paper, without assuming familiarity with this previous work.\n\nComments:\n\nI think it might be useful to include CEVAE in Fig 1, and add it to the discussion in section 3.2/4. In general, showing the precise difference between the two VAE formulations would go a long way towards making the contribution's novelty easier to understand.", "3342 Identifying Treatment Effects Under Unobserved Confounding\n# Summary\n\nThe authors propose a representation learning method for estimating causal effects in the presence of unobserved confounding when covariates that act as proxies for a latent confounder are available. The authors connect the problem to some recent results on the partial identifiability of VAE and non-linear ICA models. The authors lay out some conditions under which the causal effect may be identifiable and propose a VAE-based model, CFVAE, that enforces some of these conditions on the estimated model. They compare CFVAE to a previously proposed VAE algorithm, CEVAE, and other methods that are designed to work under unconfoundedness.\n\n# Feedback\n\nIdentification of causal effects in the presence of proxy variables for unobserved confounders is an important but subtle problem. However, this means that the bar for making contributions in this area needs to be high. Because the conditions for identification can’t be falsified in a particular application, making unclear statements about when the effects of interest are and are not identifiable is very important. Readers who misunderstand will only experience silent failures and make poor decisions as a result.\n\nUnfortunately, I don’t believe this paper meets this bar of clarity. I think that the authors explore some interesting connections to recent work on identifiability in latent variable models, and understanding what these results imply for causal inference is important. In particular, the observation that one might estimate a decoder up to a _different_ affine transformation in the treated and control arms seems like a useful insight. But the results in the paper are incomplete, disorganized, and in some cases wrong. I’ll list out a few general points here, then discuss some substantive issues with the paper’s specific argument.\n\n## General points about identifiability arguments\n\n### Identifiability is not a property of the method\n\nIdentifiability is a property of the _data generating process_ and not a property of the model being used to do estimation. If the causal effect of interest is not identifiable in the process that generated the data, then using an “identifiable model” to estimate the causal effect will not solve the problem.\n\nThe exposition in the paper seems to argue that using the right model will make the causal effect identifiable. This may just be a matter of unclear writing, but this is a broader point of confusion in the ML community, so it’s important that the authors be clear on this point.\n\nHere, I think it would make sense for the authors to state what their assumptions are about the data generating process, separately from the parameterization of their model. Reading the paper, the distinction between these two layers of assumptions was unclear.\n\n### Relaxing identifying assumptions is not an option\n\nIn the same vein, if there are assumptions that the authors need to make to eliminate identification failure modes, then showing that the model “works” when those assumptions are relaxed does not inspire confidence. Unlike standard model misspecification which can be detected and debugged based on observables, making the wrong identifying assumptions in a model can result in silent failure, where the model can fit the observed data perfectly, but return a causal effect estimate that converges to the wrong place, or doesn’t converge at all.\n\nIf one is able to relax the identifying assumptions and still see success in experiments, this means either (a) the assumptions were unnecessary, or (b) the experiments did not probe the method well enough.\n\n### Assumptions needs to be stated clearly, with implications clearly highlighted\n\nWhen making identification arguments, assumptions play the role of eliminating equally plausible causal explanations of the observed data, until only the true one can remain (if the assumptions are true). These are not the kinds of assumptions that eliminate exotic corner cases; instead, they eliminate cases like the most obvious explanation for the observed data like the absence of unobserved confounding. Bounding arguments like the Manski and Kallus et al papers that are cited in the introduction construct the full set of causal explanations that identifying assumptions must narrow down to a point.\n\nAll of this is to say that clearly stating assumptions, and the cases they eliminate, is essential for any identification argument. In the paper as it is written now, many assumptions are made implicitly or in passing, and it is unclear which assumptions are made for illustration (e.g., the noise on the outcome going to zero) and which assumptions are essential for the argument in general. The assumptions are always framed as “mild” and do not highlight situations that the assumptions eliminate (i.e., in which cases they would fail to hold).\n\n## Specific Concerns\n\n### Balancing covariate implies that the naive estimator “just works”\n\nThe primary identifiability result of the paper involves the assumption that the observed covariates are “balancing covariates”, satisfying t \\indep z | x. The authors argue that this is a weaker condition than requiring that x satisfy unconfoundedness. This may be true, but the gap between the two assumptions is, at most, a set of knife-edge violations of faithfulness. In terms of estimating causal effects, simply doing the standard covariate with x and ignoring z would give the right answer.\n\nThis can be shown in two ways. First, graphically, t \\indep z | x implies that there is no backdoor path through z from t to y when you condition on z. So z doesn’t induce any non-causal association between y and t. Secondly, using the standard adjustment formula:\n\n\\mu_t(x) = E[ E[Y | X = x, T = t, Z = z] ]\n= \\int_z E[Y | X = x, T = t, Z = z] p(z | x) dz\n= \\int_z E[Y | X = x, T = t, Z = z] p(z | x, t) dz  (using the balancing covariate property)\n= E[Y | X = x, T = t]\n\nIn particular, the naive regression function E[Y |  X = x, T = t] only fails in cases where p(z | x, t) \\neq p(z | x); i.e., when the distribution of the latent variable is different in the two observed treatment arms even after conditioning on x. The balancing covariate property eliminates this possibility.\n\nThis also means that the VAE model specified can only generate data where the latent variable z does not introduce confounding.\n\n## Other Concerns\n\n * The adjustment formula in equation (2) is wrong. The last integral should be with respect to p(z | x), not p(z | x, t) (see the argument above).\n\n * There is a substantive difference between estimating individual level causal effects given the observed outcome (a counterfactual query) versus estimating the CATE. The authors do divide this into “pre-treatment” and “post-treatment prediction”, but the counterfactual query presents additional identification questions. In particular, whether the reported expectation is correct depends on Cov(y(1), y(0) | z, t), which is never observable. None of the identifying assumptions in the paper make any arguments about this quantity, so the models in the paper are making implicit strong assumptions here.\n\n * The f^{-1}(y) notation in the paper is very unclear in the case that there is actually outcome noise for y. The function f relates the latent z to the expectation of y, not y itself. When y includes independent noise, the distribution of f^{-1}(y) does not yield the marginal distribution of z; you need to deconvolve the independent noise in y, which is non-trivial.\n"], "review_score_variance": 1.1875, "summary": "The reviewers noted that this is an important, interesting but difficult topic. They appreciated that the authors clarified their assumptions in the theorem statements. Nevertheless, they recommend the authors to detail in depth when the method work better than the method where only the covariates are adjusted. They still think that the paper would require major modifications to be considered for publication hence the decision is rejection the paper. \n", "paper_id": "iclr_2021_D3TNqCspFpM", "label": "test", "paper_acceptance": "withdrawn-rejected-submissions"}
{"source_documents": ["Dynamical phenomena, such as recurrent neuronal activity  and perpetual motion of the eye, are typically overlooked in models of bottom-up visual perception. Recent experiments suggest that tiny inter-saccadic eye motion (\"fixational drift\") enhances visual  acuity beyond the limit imposed by the density of retinal photoreceptors. Here we hypothesize that such an enhancement is enabled by recurrent neuronal computations in early visual areas. Specifically, we explore a setting involving a low-resolution dynamical sensor that moves with respect to a static scene, with drift-like tiny steps. This setting mimics a dynamical eye viewing objects in perceptually-challenging conditions. The dynamical sensory input is classified by a convolutional neural network with recurrent connectivity added to its lower layers, in analogy to recurrent connectivity in early visual areas.  Applying our system to CIFAR-10 and CIFAR-100 datasets down-sampled via 8x8 sensor, we found that (i) classification accuracy, which is drastically reduced by this down-sampling, is mostly restored to its 32x32 baseline level when using a moving sensor and recurrent connectivity, (ii) in this setting, neurons in the early layers exhibit a wide repertoire of selectivity patterns, spanning the spatiotemporal selectivity space, with neurons preferring different combinations of spatial and temporal patterning, and (iii) curved sensor's trajectories improve  visual acuity compared to straight trajectories, echoing recent experimental findings involving eye-tracking in challenging conditions. Our work sheds light on the possible role of recurrent connectivity in early vision as well as the roles of fixational drift and temporal-frequency selective cells in the visual system. It also proposes a solution for artificial image recognition in settings with limited resolution and multiple time samples, such as in edge AI applications.", " Thank you for your detailed responses!\n\nVerifying that the teacher is not the crucial ingredient is great.\n\nI like the new figure S10 as a start to bridge to biology, although I think more could be done, e.g.: test _directly_ if the proposed model's saccades match those observed experimentally (e.g. test on the same tasks as in the linked papers https://www.nature.com/articles/s41467-020-14616-2 and https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0240660). I don't think you necessarily have to train on the same images, as long as your model is image-computable you should be able to just run the same stimuli on it.\n\nI have increased my score because I think the attempt at connecting to biology is taking this in the right direction, but I still believe key comparisons are missing: either a stronger link to biology (e.g. as suggested above), and/or explicit comparisons to alternative models in ML tasks. I hope you will continue pushing on this front, I think this is very promising, and will make for a much stronger paper once you demonstrate the model's utility!", "The paper's main claim is that recurrence aids to enhance visual acuity in settings with limited resolution, such as the one imposed by limited photoreceptors in the retina. The authors therefore build a convolutional network with recurrent connectivity in its early layers (termed DRC) that receives a time-series of low resolution frames and learns representations -- for classification in CIFAR -- from a teacher network receiving full resolution inputs. DRC outperforms a low-resolution baseline and approaches standard resolution performance. Additionally, the paper visualizes the DRC's learned features. pros:\n* as far as I can tell, this is a novel setting and I have not seen much work investigating the impact of low retinal resolution on object recognition models\n* the results on CIFAR-10 and CIFAR-100 are clearly described and show that the recurrent DRC model aided by a full-resolution teacher can regain most of the performance of a standard resolution model\n* the paper provides good background on the biological motivation for modeling low-resolution photoreceptors\n\ncons:\n* lack of connection to biology: the proposed model is motivated from biological observations, but model predictions are never tested against any experimental results. Are the model's resulting features any more brain-like? Does it exhibit the same hyperacuity as observed in biology?\n* requirement of a teacher: the DRC is only tested when learning representations from a teacher which both has a non-obvious connection to biology and is an unfair comparison to the non-recurrent baselines which do not use a teacher. Would any of the baselines perform better when trained with a full-resolution teacher in the same way as the DRC?\n* unclear benefits for computer vision: it is not obvious to me if/where processing sensory data with low resolution but many temporal samples will be helpful to the machine learning community. Some connection is made in the very last paragraph to always-on cameras such as body worn cameras but it is not made clear if those are really in the regime of low-resolution and high temporal sampling.\n\nminor: \n* some more discussion of related recurrent models, e.g. https://papers.nips.cc/paper/2019/hash/7813d1590d28a7dd372ad54b5d29d033-Abstract.html and https://www.pnas.org/content/116/43/21854.short, would be helpful to contextualize the work\n* the second-to-last paragraph on page 4 states that the ResNet+RNN \"achieved accuracy lower by 3.5% and 10%\" respectively for CIFAR-10/100, but Table 2 has its accuracy as 83.94/59.61 compared to the standard resolution 96.83/82.94 -- this seems to be inconsistent\n* it is not clear to me what to take from the visualization of features (figs. 2 and 3). In general, I found the paper a bit hard to follow at times, the consistent story is not clear to me. E.g. how do the feature visualizations support the main claim?\n* figure 3 caption has a typo: \"The wors case\" The paper lacks a clear demonstration of usefulness: either an improved fit to biological data (since the motivation starts from limited sampling in the retina), or a clear use case in computer vision. Since neither is demonstrated, I find it really hard to contextualize the work and cannot tell if the proposed model makes any improvements over previous models (see the main review for detailed suggestions). The use of a full-resolution teacher network is also not well motivated especially in connection to biology, and the second half of the paper is a bit hard to follow (i.e. what to take from the feature visualizations).\n\nREBUTTAL UPDATE: I have increased my score following the authors' attempts at connecting to biology more directly, but I still believe key comparisons are missing: either a stronger link to biology and concretely relating model predictions to experimental results, and/or explicit comparisons to alternative models in ML tasks.", " We thank the Reviewer for their comments. We went over the points raised by the Reviewer and addressed them as detailed in the following text. We believe that addressing these important comments improved our manuscript significantly.\n\n**Comment**:*A central claim made by the authors is that spatio-temporal computations in the front-end of the network are important. The main evidence here is that the ResNet+RNN network, i.e. putting the recurrent computations on the back-end, does not work nearly as well ... It is also unclear how the ResNet+RNN network incorporates spatial information since I could not find the parameters related to \"Input Trajectory\" in the appendix. It is also unclear how the ResNet+RNN network was trained.*\n\n**Reply**: We agree that ResNet + convRNN is a more fair control. We performed this control as the Reviewer requested. The results did not improve in this setting. We explored a few options, including playing with the learning rate, optimizer type, altering ImageNet pre-trained vs. fresh network and rescaling the position signal.  We were not able to obtain a better performance with any of these options. We are not surprised by these results, because the top layer of CNNs typically develop a translation-invariant representation and suppress the information about small displacements (which DRC, as well as standard MISR, rely on).\n\n**Comment**:*Figure 2 demonstrates that the DRC network uses a mixture of spatial and temporal computation, ... Much more analysis is needed to be able to interpret the importance of what's shown in Figure 2.*\n\n**Reply**: We thank the Reviewer for raising this point. We checked the repeatability of Activation Maximizations (AM), and improved the regularization of the generator. The repeatability of feature AM is now demonstrated in supplementary figures and details of regularizations are provided in Appendix A, which also reports that different realizations of student learning from the same teacher were examined and found to exhibit similar spatial and temporal AMs. We hope that this strengthens our interpretation of the results, namely that spatial  and temporal AMs do not emerge randomly but rather  “there exist specific  coding  benefits  for spatio-temporal fields in our dynamic network”.\n\nAs for ablation tests: We did not find a systematic impact of ablations of either purely spatial, or purely temporal neurons on the performance. According to our understanding, it is not obvious that there should be such a tendency in the first place - the literature suggests that removing neurons with interpretable selectivity might work in both directions. Some successes were reported in removing specific neurons based on their selectivity (e.g.  Bau, Belinkov et al ‘18) while other works suggested that selective neurons compromise performance (e.g. Leavitt and  Morcos ICLR2021 and references therein). We therefore believe that further work is needed in order to clarify the significance of ablations in our setting.\n\n**Comment**:*Another central claim is that the trajectory of images over time is important (Fig. 3). In general this result is under-analyzed ...*\n\n**Reply**: We performed the shuffling experiments that the Reviewer requested and are now reporting the results in Fig. 3 of the revised version. We also replaced the presentation of averages with depicting the distributions of all data points, demonstrating the low variability in each group. As now discussed in the main text, trajectories with high curvature (negative kappa), for which the shuffled trajectories are visually similar to the non-shuffled ones, shuffling does not affect accuracy. Shuffling does affect trajectories with low curvatures. This is in line with our understanding of how the DRC works - over-sampling of a local region to compensate for a low spatial resolution in order to estimate local features.\n\nWe have added the definition of the curvature index (as taken from the literature) to the Appendix and measured the curvature indices for our ensemble of trajectories. A comparison to psychophysical experiments showed that biological vision, at least in the conditions tested in those experiments, operates in the interval of kappa~0, right at the edge of the region where shuffling begins to affect classification accuracy.\n\nIt is important to mention that the curvature index we are using is a measure, in a way, of the global curvature rather than the local, meaning we calculate one index per trajectory and the index is not necessarily sensitive to the point-by-point local curvature along the trajectory. Therefore, with this definition of curvature, our claim that more curved trajectories result in better performance is not in contradiction (but rather with agreement) with the Reviewer’s interpretation that the overall statistics or the displacement from the center is what contributes to the improved performance. \n", " **Comment**: *unclear benefits for computer vision: it is not obvious to me if/where processing sensory data with low resolution but many temporal samples will be helpful to the machine learning community. Some connection is made in the very last paragraph to always-on cameras such as body worn cameras but it is not made clear if those are really in the regime of low-resolution and high temporal sampling.*\n\n**Reply**: We acknowledge that in this work we did not benchmark ourselves against realistic tasks and worked with a synthetic handmade dataset. We level with, and given a sufficient number of samples, are better than a corresponding work (Xi et al. 2020). Based on that, as well as on the comparison with other architectures that we explored, we believe that a DRC-like architecture is indeed relevant for computer vision, and not only as a bio-mimetic model.    \nAs for the relevance of processing sensory data with low-resolution-many-temporal-samples to the machine learning community, we refer to the subfield of machine-learning and computer-vision dealing with multi-image super-resolution (MISR; references had been added in the revised main text), where multi low-resolution images are combined to produce a high-resolution image. Although our solution is not MISR, implementations of MISR illustrate such relevant scenarios and their general relevance to the field of machine-learning. A paragraph with a short description of the MISR sub-filed with a few relevant examples was added to the Introduction (paragraph starting with “Using the information available from over-sampling...”).\n\n\n**Comment**: *some more discussion of related recurrent models, e.g. https://papers.nips.cc/paper/2019/hash/7813d1590d28a7dd372ad54b5d29d033-Abstract.html and https://www.pnas.org/content/116/43/21854.short, would be helpful to contextualize the work*\n\n**Reply**: We thank the Reviewer for these very relevant references. We have added them to the Introduction (3rd paragraph) along with a short description of their significance.\n \n**Comment**: *the second-to-last paragraph on page 4 states that the ResNet+RNN \"achieved accuracy lower by 3.5% and 10%\" respectively for CIFAR-10/100, but Table 2 has its accuracy as 83.94/59.61 compared to the standard resolution 96.83/82.94 -- this seems to be inconsistent*\n\n**Reply**: The text was modified following some more controls that were conducted. The modified text reads: “At their best, these models achieved accuracy lower by approximately 4% and 7% for CiFAR-10 and CiFAR-100 datasets respectively, compared to 5-step DRC w/o positional information.”. In the modified sentence, the relevant control is clearly stated to prevent confusion. \n\n**Comment**: *it is not clear to me what to take from the visualization of features (figs. 2 and 3). In general, I found the paper a bit hard to follow at times, the consistent story is not clear to me. E.g. how do the feature visualizations support the main claim?*\n\n**Reply**: Thank you for this comment. We have tried to improve the flow of the text and added several explanations where needed. Specifically regarding our feature visualization we would like to repeat  that we think that the spatiotemporal feature analysis included in the manuscript is a first step in trying to gain a mechanistic explanation to the function of the network and the source of its performance-gain compared to the baselines; as well as a first step in gaining data that can be compared against, or provide predictions for, biological experiments.\nThe feature visualizations also support the main claim by illustrating the access information contained in the spatio-temporal dynamic features compared to spatial-only and temporal-only features.\n\n**Comment**: *figure 3 caption has a typo: \"The wors case\"*\n\n**Reply**: Thank you. Figure 3 as well as its caption were modified and the misspelled word was removed.\n\n**Comment**: *The paper lacks a clear demonstration of usefulness: either an improved fit to biological data (since the motivation starts from limited sampling in the retina), or a clear use case in computer vision. *\n\n**Reply**: We thank the Reviewer for motivating us on this point. A fit to biological data was added - Figure S10 and the relevant text in sections 2.4 and B.3. Yet, we have no way to evaluate whether it’s an improvement compared to other models since, as far as we know, such models do not yet exist. Please also see our reply above to the more specific suggestions you have raised. We would also like to suggest that a fit to biological data is not the only way to achieve biological usefulness - another way is by having a model that provides support to an existing hypothesis. In this regard, our DRC model provides a possible motor-sensory mechanism underlying biological hyperacuity just by the mere facts that it is based on biological insight and it is functional.   ", " We thank the Reviewer for their comments. We went over the points raised by the Reviewer and addressed them as detailed in the following text. We believe that addressing these important comments improved our manuscript significantly.\n\n**Comment**:*lack of connection to biology: the proposed model is motivated from biological observations, but model predictions are never tested against any experimental results. Are the model's resulting features any more brain-like? Does it exhibit the same hyperacuity as observed in biology?*\n\n**Reply**: The reviewer raises valid points and we do intend to continue the research in-line with the Reviewer’s suggestions. Specifically, we plan to design and conduct experiments to test specific DRC predictions. We would like to stress that such experiments would be technically challenging as they will require, in addition to the behavioral or neurophysiological recording, high-precision eye-tracking. Neuronal responses in primates were rarely recorded in parallel to the ocular drift, and when done (e.g., Snodderly et al 2001) datasets were, as far as we know, not publicly released. The same point applies, as far as we know, to public brain-like scoring frameworks - their datasets do not include eye-motion recordings (at least not with the required precision). As for the last question raised by the Reviewer- Does it exhibit the same hyperacuity as observed in biology?   As far as we know, biological hyperacuity was measured either with simple artificial tasks such as Vernier acuity or with Snellen tables, but never with natural images. These settings, with a very small vocabulary of stimuli, are not suitable for training a neural network. As a part of future work we can evaluate a DRC that trains on a large dataset and then learns by few-shot learning to classify Snellen optotypes.\n\nFollowing the Reviewer’s concern, we have made an effort to provide a quantitative comparison between our model and available relevant biological data. We have added a graph that displays the distribution of curvature-index as defined in Grubber & Ahissar, 2020 for two trajectory families with different kappa values (Figure S10). Comparing this graph to Figure 3 (‘Natural-small’  conditions) in the mentioned paper (please see details in the main text section 2.4 and in Appendix B.3) demonstrates that (a) the range of kappas we are using is biologically relevant, and (b) our model can be used for exploring the mechanistic details underlying the biological control preferring such curvatures, as it demonstrates their advantage in recognition.\nWe think that the spatiotemporal feature analysis included in the manuscript is a first step in trying to gain a mechanistic explanation to the function of the network and the source of its performance-gain compared to the baselines; as well as a first step in gaining data that can be compared against, or provide predictions for, biological experiments.\n \n**Comment**:*requirement of a teacher: the DRC is only tested when learning representations from a teacher which both has a non-obvious connection to biology and is an unfair comparison to the non-recurrent baselines which do not use a teacher. Would any of the baselines perform better when trained with a full-resolution teacher in the same way as the DRC?*\n\n**Reply**: We conducted a control of applying a teacher to the baselines, as the Reviewer suggested, and in one case found that it leads to a small improvement over what we previously reported (Table 1).  We agree with the reviewer that the student-teacher feature learning has no explicit correlate in the brain. On the other hand, the vast majority of biological learning follows some curriculum of increasing difficulty and an assumption that recognition in challenging cognitions, such as poor resolution, relies on  feature sets that emerge in less challenging conditions is reasonable. A partial support to this assumption can be found in the studies, referred to in our Discussion, describing the development of visual hyperacuity that follows the development of standard acuity:\n\n* Ann M. Skoczenski and Anthony M. Norcia. *Late Maturation of Visual Hyperacuity.* PsychologicalScience, 13(6):537–541, November 2002\n\n* Yi-Zhong Wang, Sarah E. Morale, Robert Cousins, and Eileen E. Birch. *The Course of Devel-opment of Global Hyperacuity Over Lifespan.* Optometry and vision science : official publi-cation of the American Academy of Optometry, 86(6):695–700, June 2009\n\n\n\n\n", " We greatly thank the Reviewer for their positive and encouraging feedback, as well as their specific comments. We went over the points raised by the Reviewer and addressed them as detailed in the following text. We believe that addressing these important comments further improved our manuscript.\n\n**Comment**:*It took me a while to parse Figure 2B, but once I figured it out, it was reasonably clear.*\n\n**Reply**: We thank the Reviewer for this comment. The Figure’s caption was modified to make it clearer.\n\n**Comment**:*It is unclear what the representation of the positional information is.*\n\n**Reply**: The positional information fed into the network contained the (x,y) normalized coordinates of the lower left corner of each frame relative to the center position of the original CiFAR image. It was  integrated into the network by first broadcasting it to the 2d dimensions (height and width) of a single input frame (8,8,2) and then concatenating it with the frame. resulting in a (8,8,5) input for each time-step (three dimensions for RGB and two dimensions for location).\n\n**Comment**:*Is the RNN an LSTM network?*\n\n**Reply**: In the control ResNet50+RNN we used GRU. Following the Reviewer’s concern we now refer to this model as ResNet+GRU/convGRU.\n\n**Comment**:*In general, I'm confused about the role of \"Small-net\" in this paper. Please clarify.*\n\n**Reply**: The main roles of Small-net are two-fold: \n1. Confirm that the results using the ResNet based design are not somehow specific to the very specific reference design. \n\n2. Enable feature visualization, which was challenging with the full-scale ResNet50.\n\nWe have revised Table 2, which reports the Small-net results.  The Table now describes a fixed teacher network and 4 settings: combinations of network depth (3 or 6 layers) and adding positional info (enable/disable).\nWe have also modified the text (mostly, 2nd and 3rd paragraphs in section 2.1 and second paragraph in section 2.1.1) in a way that makes this point clearer.\n\n**Comment**:*The procedure by which the generative network determines the optimal features is not clear - this could be described more clearly. The supplementary material is insufficient in this regard. You have an unused half-page in the main text, so that should be enough room to elucidate how this is done.*\n\n**Reply**: We agree. Given the space limitations in the revised version we have added the missing explanation in the supplementary material.\n\n**Comment**:*\nMinor comments, wording, etc.\n\nWording errors*\n\n**Reply**: Thank you. We have corrected these errors.\n\n**Comment**:*Also in this paragraph, I initially thought you were saying you applied feature distillation to an 8X8 layer using 56X56 features, which is not what you did. This is one of those places where the role of Small-net is unclear.*\n\n**Reply**: We agree that this paragraph wasn’t clear enough. It was modified (as well as other related sections) in accordance with the Reviewer’s comment. We believe that the teacher-student training procedure and the role of small-net are clearer now.  \n\n**Comment**:*In Figure 2B, you say you are showing predominantly temporal, predominantly spatial, and mixed examples here. If that's the case, I would expect one call-out to be from the far left point in the upper-left hand corner (predominantly spatial - your choice is reasonable here, but the point to the left of it would be even better), a point in the lower right-hand corner (predominantly temporal), and then the third one you show. The point you use from the lower left hand corner corresponds to 0 temporal and low spatial. So, there isn't a \"predominantly temporal\" example here. Can you pick one from the lower-right hand corner instead?*\n\n**Reply**: We hope that the examples are clearer in the revised version. Importantly, the third point that was chosen is in the low pure-spatial, low pure-temporal (bottom left) region as designated by the coordinates, yet, its spatio-temporal activation (indicated by the radius of the dot and its dark color) is high; we hope the Reviewer agrees that presenting this feature is valuable.\n\n**Comment**:*Wording suggestion for Discussion...*\n\n**Reply**: Thank you. We have fixed these errors. We have also explained what “same” padding means in the legend of Table S3, the first place where this conv-net jargon term is used.\n", " We thank the Reviewer for their comments. We went over the points raised by the Reviewer and addressed them as detailed in the following text. We believe that addressing these important comments improved our manuscript significantly.\n\n**Comment**:*None of the baseline models considered the most important control: what if you just feed in a series of static images without motion to the DRC-FE? As is, all that we can conclude from this paper is that a recurrent network in the early stages somehow helps, but it is not clear that the motion in the input image has anything to do with this.*\n \n**Reply**: We thank the Reviewer for this suggestion. We have performed this control and included the results in Table 1 of the revised paper.\n \n**Comment**:*The 8x8 images were created by downsampling from the 32x32 images with bicubic interpolation - essentially smoothing or lowpass filtering. If you simply move and resample a lowpass filtered image, there is no new information that can be exploited by later information processing, assuming that it was lowpass filtered below the nyquist rate for an 8x8 image (which presumably it was, an important detail that is missing) - this is given by basic signal processing.*\n\n**Reply**: We thank the Reviewer for this comment. Indeed we did not emphasize in the original version that the bicubic interpolation in the OpenCV  does not include an anti-aliasing filter (as conjectured by the Reviewer). This information is now added (first paragraph of the Results).\n\n**Comment**:*... I'm not sure what we learn here from a neuroscience point of view.*\n \n**Reply**: We have revised the paper to better explain the contribution of our work to the neuroscience of vision. A timely question in the neuroscience of vision is whether temporal dynamics within the visual system (expressed by, among other behaviors, ocular motion and phasic neuronal responses) and sensitivity to temporal features (in the visual input), that are evident in the works cited in our Introduction and Discussion, are epiphenomena or essential features. Our work provides evidence supporting them being essential processing components. We now better explain these contributions in the Discussion (in the paragraph starting with “The results of this work can be used when constructing specific hypotheses ...”), including references that demonstrate the long-term debate (starting at 1942) about the function of the ocular drift in vision\nOur use of the deep conv net engineering is guided by specific interpretations of the biological data and thus any success of our approach can be considered as a support to the selected interpretations.  Thus, the success of positioning  recurrency in the low layers, which is driven by a timescale separation between early and high visual areas, supports recurrent functioning of early visual networks in biology. Furthermore the use of Gated Recurrent Units (GRU) can be justified by multiplicative gating at V1 (Burak et al 2010) \nPhasic neuronal responses were not explicitly addressed - our hypothesis is that they should emerge, in part, as a result of network dynamics and in part as a result of phasic neuronal transfer functions. Following the Reviewer’s comment we have realized that mentioning phasic visual responses may indeed be confusing and we have removed it (from the paragraph starting with “On the other hand, temporal dynamics, and sensitivity to temporal features,...”). \n \n**Comment**:*There is no overall theory presented ..., yielding a non-transparent solution providing little insight into how the brain might actually solve this problem.*\n \n**Reply**: Indeed, the original text has hidden the assumed overall theory in the sentence “The same drift motion could potentially improve acuity if spatiotemporal computations are employed (Burak et al., 2010; Ahissar & Arieli, 2012).”. Now we have modified this paragraph to (a) add a reference to the earlier work of Rucci, (b) cite Burak 2010 more accurately, (c) cite Ratnam’s and Anderson’s papers and (d) provide more details about the repertoire of possible spatiotemporal computational approaches (in the text starting with  “The same drift motion could potentially improve acuity if spatio-temporal computations are employed.”)\nSuper-resolution: We have added a paragraph (starting with “Using the information available from over-sampling low-resolution…”) describing these powerful tools, referring to the super-resolution (including the book by Milanfar as suggested by the Reviewer), and in particular to multi-image super-resolution (MISR), as well as the low-resolution object recognition literature.\n  \n**Comment**:*The introduction does not properly attribute prior work...*\n\n**Reply**: We have revised the Introduction. Rucci’s work and hypothesis are better described now. Burak’s 2010 paper is now correctly cited. Ratnam’s and Anderson’s papers are now cited also in the Introduction (see also our Reply to point 4 above). \n", " **We are grateful to the Reviewers for their valuable feedback which helped to improve our paper!**\n\nWe recently uploaded a revised version, incorporating the Reviewers’ comments. Detailed rebuttals are posted as a reply to each Reviewer. Here we summarized the main changes made in the revised version of the paper.\n\n-Control with not moving DRC (per request of **Reviewer  zsN3**): \nDone and demonstrates a DRC advantage.\n\n-Control with teacher-student on the baseline (per request of **Reviewer  9EUA**):\nDone and demonstrates a DRC advantage.\n\n-Control with ResNet+convGRU (per request of **Reviewer  RJzH**):\nDone and demonstrates a DRC advantage.\n\n-Reporting repeatability of feature generator ResNet+convGRU (per request of **Reviewer  RJzH**):\nDone. Activation Maximizations are now shown to be repeatable both between generator runs and between DRCs that learn with the same teacher. To achieve that, we improved the regularization of the feature generator. As a part of that we normalized the input differently, which resulted in a better performance on the Small-Net. \n\n-Trajectories of DRC (Sec 2.4) - showing distribution and a shuffling experiment (per request of **Reviewer  RJzH**):\nDone. Repeatability is demonstrated. Shuffling results are discussed in Section 2.4.\n\n-Trajectories of DRC - a preliminary comparison to psychophysical results is added (to partially address the request of **Reviewer  9EUA** for link to biology):\n\n-Clarity regarding Small-net (**Reviewer vR8C**):\n Table 2 was reorganized to be clearer and more systematic. Specific settings are reported instead of “version 1”, “version 2” etc.\n\n-Errata: There was a corrupted result in one cell of Table 1: DRC-w/o position-5 steps-CiFAR 100. The previous result: $68.27 \\pm 2.10$ is now replaced with the correct result: $67.23 \\pm 0.30$. All other results were thus double-checked and found to be valid.  The correction is within the standard deviation of the previously reported result and does not affect any of the paper’s conclusions.\n", " The issue is not the cropping per se but rather the downsampling followed by blurring.  Here is what is stated in the paper:\n\"The sensor’s frames were obtained by cropping a 32x32 pixels window from the scene, around the sensor position. Resolution was then reduced to 8x8 using a standard OpenCV (Bradski, 2000) function with bi-cubic interpolation.\"\nThe bi-cubic interpolation is essentially a smoothing or lowpass filtering operation, presumably intended by OpenCV to prevent aliasing.  In this case, you would lowpass filter the image to 4 cycles/image in each dimension so that 8 pixels in each dimension are sufficient to sample the image without aliasing.  Assuming that was done, then simply shifting the underlying 32x32 image and then blurring and resampling will not provide any new information about the underlying image that wasn't already contained in the first downsampled image.  If the downsampled images were noisy then you could potentially benefit by combining multiple frames to denoise.  But I see no mention of this.  \n\nSuper-resolution via a moving camera works when the image is downsampled *without* lowpass filtering below the nyquist rate.  The idea is that by sequentially sampling an underlying high-resolution image in different locations with a coarse-resolution array, you can combine these (assuming they have been properly registered) to obtain a reconstruction of the original high-res image.  If the OpenCV bicubic interpolation function happened to leave intact some high spatial-frequency information beyond 4 cy/image, then it is possible that downstream processing could exploit this information.  But there is no mention of this.\n\nRegarding the cropping:  I can see where information around the borders could get filled in as you move the image back and forth.  Is that what you are counting on here?  If so, then that should be made more explicit in the paper.  But then it also begs the question of what this corresponds to in the retina or the visual cortex.  There is no aperture problem like this in the visual system.\n\nRegarding the control of using a sequence of static images, yes I mean a sequence of repetitions of the same static image.  That is an important control because it seems likely that the recurrent network is effectively just giving you more layers of processing (by unrolling in time) as opposed to exploiting the motion per se.\n\n\nAgain, I feel the overall direction of this paper is interesting and important.  But there are basic image processing issues that should be addressed.  It would also be useful to incorporate as much as possible what we know about this problem from engineering - such as the methods of super-resolution (e.g., book by Milanfar).  Perhaps the brain doesn't work this way, but then we should try to understand why, or if it has discovered a better solution.\n", " Hi Reviewer vR8C,\n\nto give more detail on what I imagined the authors could do to compare the model to biology:\n* test if the proposed model's saccades match those observed experimentally (e.g. test on the same tasks as in the linked papers https://www.nature.com/articles/s41467-020-14616-2 and https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0240660 -- right now the paper makes only a _very_ loose connection by stating that curved trajectories give better CIFAR performance, whereas both linked papers describe a Gaussian distribution of curvature from what I can tell)\n* test if the proposed model makes more behaviorally consistent class predictions than previous models (e.g. https://www.jneurosci.org/content/38/33/7255)\n* test if the proposed model's internals are more brain-like than previous models (e.g. https://www.biorxiv.org/content/10.1101/407007v2, https://arxiv.org/abs/2104.13714v1)\n\nThe model does not need to be trained on the same images from the experiment, an image-computable model can make predictions on all of these tests.\n\nAt least to me, the fact that something is novel alone does not warrant publication. There are a lot of novel models to be built, but if we don't compare against related models and experimental data, then I don't know how our field can make progress without drowning in hundreds of novel models that lack comparative evaluation.", " Hi folks -\n\nSince the reviews are so skewed (because of me!) - 10/3/3/3, it's up to me to argue for acceptance, despite having a grant due in three days!\n\nI gave the paper a 10 because I thought it was well-written, biologically-inspired, it got good results, and as far as I know, there is no one who has done this before. I.e., it is *completely novel*. \n\n1. It is inspired by data on small eye movements during fixation, i.e., fixational drift. As the authors state in their paper: \"the dynamics of low-level visual processes, occurring early in the bottom-up visual hierarchy and sensitive to the fixational drift ... remains largely overlooked in models of vision as well as in bio-inspired computer vision systems.\" I agree with their assessment - in almost all of the recurrent models I know of, the recurrence is largely in later layers.\n\n2. They give what I considered a reasonable review of the literature on fixational drift and the hyper-resolution of the human visual system, which inspires their work.\n\n3. They obtain excellent results - the system is almost as good as a system that is given 32x32 images. \n\nIt seems to me that most of the objections have to do with whether they cited all of the literature they should have - or where they cited it - and they didn't have enough controls. These don't seem to be killer arguments to me, when the work itself is so obviously novel. Seriously, can you think of *any* paper out there that models fixational drift in a deep learning system? This is the first.\n\nResponse to reviewer zsN3: I'm unclear what your point is about giving a static sequence of images to their model as a control (and neither do the authors - they asked you for clarification). Isn't that essentially what \"DRC 5 steps, w/o position input\" in Table 1 is? That results in worse performance. \n\nAlso, you state that \"there is no new information that can be exploited by later information processing, assuming that it was lowpass filtered below the nyquist rate for an 8x8 image\". Clearly they would not have the results they have if that were what they were doing. \n\nAnother criticism is that they have no theory; quite often, theory follows empirical results, rather than preceding it; examples in the literature are legion. \n\nThe critique of them not citing earlier papers by Rucci is easily fixed in revision. Finally, you ding them for not citing work earlier in the paper than they did. It seems odd to worry about where in a paper something is cited. And the Ratnam paper is not about a neural network model of retinal drift, but a human experiment. If it were a competing model, then I would expect it to be cited earlier.\n\nFinally, they asked you for clarifications of your comments, but you didn't respond.\n\nResponse to reviewer 9EUA. You also see it as novel, but downgrade the paper for not comparing more directly to experimental results. It is unclear to me how you would do that in this setting. As far as I know, CIFAR images have not been used in human psychophysics experiments. What they do do is show that they get hyperacuity in their model, and that the curved paths that have been experimentally observed improve their model's results significantly, suggesting that their model could be analyzed further to show why this is the case. The fact that they got that result in the first place seems pretty cool to me. \n\nMore controls: You suggest an additional control of using distillation for a non-recurrent model; that seems like something that could easily be done in a revision or later work, but I don't see that as killing this paper. \n\nFinally, while they mention potential engineering advantages, I don't see that as the point of this paper. This is about computational neuroscience, not engineering. Yes, they require distillation first to get the model off the ground; this seems like a detail that could be relaxed in later work.\n\nFinally, reviewer RJzH wants many more controls. Ok, I don't have a lot more to say about that, as I am running out of steam here.\n\nMy two cents; sorry if I come off as confrontational here!\n\n\n", " We thank the Reviewer for their important feedback and we are currently working on a revision to mitigate their critics.\nTo proceed efficiently, we would kindly ask for further clarification about the critics of our downsampling procedure and the fundamental concern about the recovery of presumably unavailable information. We identify two plausible methods to extract time series information from a static image using motion and downsampling:\n\n*Downsample after cropping (our case)*: When sampling from the high resolution image, we first crop a 32x32 image and only then downsample it into a 8x8 version. The motion is represented by moving the 32x32 frame before cropping. This is reminiscent of what happens in a moving eye or camera: the naturel (high resolution) stimulus is sampled by the retinal photoreceptors or by the camera sensor array.\n\n*Downsample before cropping*: In this case the full resolution image would be downsampled once and the motion will be implemented by moving the location of the image. Indeed the recovery of spatial information by temporal resampling would be impossible in this setting.\n\nIn the situation of option #1 information that is not available in a single frame is extractable from the frame sequence, a fact that is exploited in several approaches in engineering and computational neuroscience.  \nBased on the comment of the Reviewer we assume that our description in the paper was confusing, such that it was not clear that we followed option #1. Yet, before submitting a revised paper we would appreciate the Reviewer’s feedback on this issue.\n\nAnother important issue for which we would appreciate the Reviewer’s clarification relates to the requested control with a “sequence of static images”. Specifically, we are not sure whether it  refers to a sequence of repetitions of the same static image or to something else. \n", "The authors train a neural network to do object recognition on downsampled, moving images of objects.  They show that by using a recurrent neural network in the early layers, it can learn to produce representations that result in recognition performance nearly as good as with static, full resolution images.\n\n The overall thesis here is very interesting, however there are numerous concerns:\n\nOne of the main claims of this paper is that dynamic retinal input combined with recurrent processing is key to how the brain manages to do hyperacuity.  The results of Table 1 seem at first glance to support this.  However none of the baseline models considered the most important control:  what if you just feed in a series of static images without motion to the DRC-FE?  As is, all that we can conclude from this paper is that a recurrent network in the early stages somehow helps, but it is not clear that the motion in the input image has anything to do with this.  In fact, if the motion did help, it would beg even more questions.  The 8x8 images were created by downsampling from the 32x32 images with bicubic interpolation - essentially smoothing or lowpass filtering.  If you simply move and resample a lowpass filtered image, there is no new information that can be exploited by later information processing, assuming that it was lowpass filtered below the nyquist rate for an 8x8 image (which presumably it was, an important detail that is missing) - this is given by basic signal processing.   It seems plausible that recurrent computation in the early layers helps - it is essentially like making a deeper network - but it would appear the effect has nothing to do with the motion in the input.\n\nThe paper seems motivated by neuroscience and psychophysics, but there is very little attempt to tie anything about the neural architecture of the model to substrates in the brain.  For example it is mentioned that neurons exhibit temporal dynamics with phasic responses, but none of this is incorporated in the model.  This seems like run of the mill deep convnet engineering as opposed to neuroscience.  I'm not sure what we learn here from a neuroscience point of view.\n\nThere is no overall theory presented as to how the brain could benefit from motion of the sensor in building a higher acuity representation enabling tasks such as hyperacuity.  There is much verbal reasoning in the introduction, however there is now much engineering and mathematical know-how about how such problems can be solved - e.g., super-resolution.  These works are mentioned at the end in the discussion, but then almost immediately dismissed because they reconstruct the image rather than doing recognition.  This is a shame because the theory behind these models is exactly what the authors need to implement their idea.  Instead, all of the requisite established theory is tossed aside and the authors resort to training a neural network to solve the problem, yielding a non-transparent solution providing little insight into how the brain might actually solve this problem.\n\nThe introduction does not properly attribute prior work.  First, Rucci et al have been writing and talking about the benefits of image motion for more than a decade now, but you wouldn't know this by reading the intro.  Although Rucci is cited, it is about drift motion in general and not with regard to his theory of *why* image motion is helpful, which is well known in the vision science community.  Burak's (2010) important earlier work is cited but misattributed as providing an account for how how drift motion could improve acuity, which is wrong.  Burak's model shows how the cortex could disentangle shape from motion from retinal spike trains so as to recover shape information on the retina, but does not address the question of why the motion may be beneficial to begin with.  Also missing in the intro is any mention of Ratnam et al. (2017) and Anderson et al. (2020).  Those works are brought up in discussion at the end, but given the high degree of relevance of these prior works to the authors' thesis it is baffling why they are not brought up earlier, especially with regard to what the authors hope to do here that goes beyond or improves upon this prior work.\n An interesting idea but implementation is problematic.\n", "This paper takes inspiration from the biological phenomenon of fixation drift, slow, low-amplitude movements during fixation that are believed to result in hyper-resolution in human vision. They hypothesize that this phenomenon can be explained by a model that has a recurrent convolutional front end that integrates over fixation drift, feeding into a well-trained back-end from a conventional model (ResNet 50). They demonstrate that this \"Dynamical Recurrent Classifier\" (DRC) is capable of restoring performance on 8X8 images to nearly the performance on \"high\" resolution 32X32 CIFAR images (actually, no one would call 32X32 high resolution!). They analyze the representations learned by the model and show they have strong spatio-temporal features, with some learned features emphasizing spatial features, some emphasizing temporal features, but most combine the two. Finally, they show that using curved trajectories improves performance over more random walks, which can potentially explain recent results in humans. They suggest this model can be useful in AI applications involving limited resolution but with multiple samples over time.  This paper is well-written, proposes a highly innovative model that is consistent with behavioral and neural data, and obtains excellent results. The paper addresses a long-neglected aspect of human vision (fixation drift) in neurocomputational models of human vision, and shows that it has efficacy in challenging conditions. They don't stop at demonstrating that the model improves accuracy over a static model that uses multiple images. They develop a method for assessing the dynamical features of their model, because the usual activation maximization technique doesn't work in this setting. Finally, they demonstrate that a recently discovered phenomenon, curved paths in the fixational drift, promotes higher classification accuracy.\n\nThe front end of the model is a two-layer, recurrent convolutional network. It is trained by feature distillation from a layer of ResNet 50. The ResNet 50 is pre-trained on imagenet and then fine-tuned on either CIFAR-10 or CIFAR-100. The recurrent net is provided with 8X8 images, and trained to match the features activated by the 32X32 versions in the ResNet. The inputs are shifted slightly based on dynamical difference equations with random perturbations that determined the x,y coordinates of the next input, simulating fixation drift. The network was trained to reproduce the activations of the teacher network after 5 or 10 inputs. Then, the output of this front end was input to the remaining layers of the ResNet50 network, which was then fine-tuned to improve performance. The baselines are quite reasonable: a network trained directly on the 8X8 images, the same network, but using the average prediction over the 5 or 10 images, a ResNet + RNN network trained on a sequence of 5 8X8 images, with or without positional information. They show that with increasing number of inputs (5 images or 10 images), performance of the DRC improves, while the static network flatlines at 5 images. Positional information also improves performance. In the end, a 10-step DRC network with positional information achieves performance nearly as good as the original 32X32 ResNet50 in both CIFAR 10 and CIFAR 100. \n\nThey then go on to analyze the features. They perform the usual gradient ascent procedure to obtain maximally-activating 32X32 inputs for the features of the ResNet50 network used to train the features of the DRC network. They find that this same procedure doesn't converge for the DRC network, so they have to invent a novel technique for finding the optimal features. They use the idea of the generative network (Nguyen, et al., 2016), modified for their setting. The generative network has to learn to generate a *sequence* of 8X8 images that maximally activate the DRC features. These resemble the corresponding ResNet features they were trained on, but obviously have dynamics. To evaluate the spatial and temporal aspects of these units, they also apply the same procedure, but only allow the generative network to generate one image that is repeated, giving the best spatial activation of the feature, or, they only allow the generative network to vary the images, but all the images have to have the same pixel everywhere, giving the best temporal activation, but without form. These activations generally aren't as high as the unconstrained optimization. It took me a while to parse Figure 2B, but once I figured it out, it was reasonably clear. \n\nFinally, they set up the fixation location dynamics in such a way that they can control the curvature of the drift. They find that more curvature in the drift dynamics, the better the accuracy. In fact, an enforced \"spiral\" dynamics gives the best results. It turns out the performance data is based on this model, which is about 4% better than the less constrained model. This is interesting because it accords with recent human data from Michele Rucci's lab that finds curved drifts are used by subjects when the recognition problem is challenging. (I haven't read that paper, so I don't know how faithful they are to Rucci's data, or that this correctly describes his results).\n\nWeaknesses, with concrete, actionable feedback\n\nThe weaknesses are mainly in the exposition: I had several clarification questions:\n\nIt is unclear what the representation of the positional information is.\n\nIs the RNN an LSTM network?  \n\nIn general, I'm confused about the role of \"Small-net\" in this paper. Please clarify.\n\nThe procedure by which the generative network determines the optimal features is not clear - this could be described more clearly. The supplementary material is insufficient in this regard. You have an unused half-page in the main text, so that should be enough room to elucidate how this is done. \n\nMinor comments, wording, etc.\n\nPage 1, 3rd line from the bottom: dominate -> have dominated\n\nFirst sentence in section 2.1.1: -> We applied a feature distillation learning paradigm...\n\nNext paragraph: therefor -> therefore. Spell check!\n\nAlso in this paragraph, I initially thought you were saying you applied feature distillation to an 8X8 layer using 56X56 features, which is not what you did. This is one of those places where the role of Small-net is unclear. \n\nstackup -> processing stack\n\ncosyne -> cosine\n\nOur model was mostly implemented in *the* Keras package ... with *the* convolutional GRU...\n\nIn the sentence beginning \"The accuracy of the reference (teacher)...\", it isn't clear which entry in the table you are referring to here. I believe it is \"Naive training\", so call it that in this sentence.\n\nMiddle of page 6: \nsaptio-temporal -> spatio-temporal. Spell check!\n\nproperty -> properties\n\nVarious places: use two left apostrophes (below the tilde on the standard keyboard instead of \" in LaTeX on the left side of a word. (e.g., \"spirals\" near the bottom of page 6).\n\nIn Figure 2B, you say you are showing predominantly temporal, predominantly spatial, and mixed examples here. If that's the case, I would expect one call-out to be from the far left point in the upper-left hand corner (predominantly spatial - your choice is reasonable here, but the point to the left of it would be even better), a point in the lower right-hand corner (predominantly temporal), and then the third one you show. The point you use from the lower left hand corner corresponds to 0 temporal and low spatial. So, there isn't a \"predominantly temporal\" example here. Can you pick one from the lower-right hand corner instead?\n\nThird line of Figure 2 caption: students -> student's\n\nthird line from the bottom of page 7: reported at Tables... -> reported in Tables...\n\nlast sentence in Figure 3 caption: wors -> worse. Spell check! Don't annoy your reviewers!\n\nWording suggestion for Discussion: \n\nThis setting is novel and has been hardly addressed in the...->\nThis setting is novel and has been mostly neglected in the...\n\nmiddle of page 8: stack-up -> architecture\n\nlast word in third paragraph from the bottom of page 8 is not the one you want!\n\nprepossessing. -> preprocessing step.\n\nFurthermore -> Furthermore,\n\nto idealistic -> to the idealistic \n\nFirst line, last paragraph: it sets -> our work sets\n\nlast sentence, last paragraph: This is not really a sentence in english. \nRewrite as: This is enabled by a solution...\n\ncaption of supplementary Figure S4: The second sentence is garbled. It needs a \"right\" somewhere, or \"compared to\"\n\nI don't know what \"same\" means in the padding column in your supplementary tables. Same as what?\n This paper is well-written, proposes a highly innovative model that is consistent with behavioral and neural data, and obtains excellent results. The paper addresses a long-neglected aspect of human vision (fixation drift) in neurocomputational models of human vision, and shows that it has efficacy in challenging conditions. This result suggests that it can be used in engineering applications where the stimuli are low-resolution. It has some confusing parts, but these can be fixed by the authors. ", "Here, the authors attempt to leverage spatio-temporal computations for object recognition on the standard CIFAR-10 and CIFAR-100 datasets. In short, they use a network with a front-end of recurrent units (ConvGRU) to recognize objects given spatially jittered downsampled images – effectively approximating an active sensor. The network is trained in a student-teacher configuration, where weights in a temporal pooling layer after the recurrent layers are trained to match the weights of a feature layer inResNet50. Next, the network is fine-tuned to increase classification accuracy.\n\nAltogether, the authors are asking if spatio-temporal computations are enough to produce a feature layer similar to a larger network train on full-res images and, in turn, if this feature layer supports object recognition on par with full-res performance of ResNet50. The authors demonstrate that their network is almost as performant as ResNet50 with 4x downsampled images, especially when the down-sampled images are jittered in a spiral formation. They also present analysis demonstrating that the network is in fact performing spatio-temporal calculations. Strengths:\n\n1. The use of an \"active sensor\" (i.e. jittering the input image) is an interesting idea that capitalizes on recent developments in neuroscience and psychology.\n\n2. On first blush the results are relatively strong (however with a caveat that more controls are needed to interpret them)\n\nMajor issues:\n\n1. A central claim made by the authors is that spatio-temporal computations *in the front-end of the network* are important. The main evidence here is that the ResNet+RNN network, i.e. putting the recurrent computations on the *back-end*, does not work nearly as well. In fact, ResNet+RNN appears to do no better than simply averaging the prediction of ResNet over 5 frames, which is surprising. This needs to be evaluated much more systematically, since it is not an apples-to-apples comparison. Why is the RNN only used *after* the global average pooling layer? Here, the DRC is using convGRU while the comparison is made with vanilla GRU units in the ResNet+RNN network, so they do not have access to spatial information. So really, the comparison is spatiotemporal computation for the DRC network and temporal only computation for ResNet+RNN.  It is also unclear how the ResNet+RNN network incorporates spatial information since I could not find the parameters related to \"Input Trajectory\" in the appendix. It is also unclear how the ResNet+RNN network was trained.\n\n2. Figure 2 demonstrates that the DRC network uses a mixture of spatial and temporal computation, but the results are under-analyzed.  Does the network produce a similar distribution across different random initializations? Does the performance covary with these distributions? What happens if units with specific criteria are ablated? Much more analysis is needed to be able to interpret the importance of what's shown in Figure 2. \n\n3. Another central claim is that the trajectory of images over time is important (Fig. 3). In general this result is under-analyzed and difficult to interpret as is. As k becomes more negative and the trajectories more curved, trajectories are likelier to remain closer to the center and have more overlap, yet I could not find any analysis of this. If indeed curvature matters, then the authors must show that curved trajectories are better-performing than other trajectories with less curvature but similar aggregate statistics (e.g. the trajectories are a similar distance from the center-point and have similar degrees of overlap). A very simple control here is to shuffle the trajectories over time, in this case the statistics should be the same, but the degree of curvature from point to point will be destroyed. If the DRC network performs just as well with the shuffle, then the overall statistics matter more than curvature.  This is essential to understanding what allows the DRC network to perform well.\n\nMinor issues:\n\n1. Please check for typos\n2. Figure 3 please provide a legend\n3. Figure 3 I do not understand why the authors are plotting an average of *2* datapoints.  Many more points should be computed to estimate the distribution properly, so we can visualize a reasonable confidence interval for each parameter setting. In summary, I found the core idea of author's network interesting; however, the author's claims are currently not justified by the results. Many more controls are needed to ensure that the DRC network performs better than alternatives. If the DRC network does perform better than all other control networks, then additional analysis is required to understand how the network is able to improve its performance."], "review_score_variance": 8.1875, "summary": "This paper explores the idea that fixational drift of a sensor over an image (something that primate eyes do) could be used to achieve visual hyperacuity, i.e. image recognition with low resolution images equivalent to what would be achieved with high resolution images. The authors construct networks where the bottom of a deep convnet is replaced by recurrent networks and the network is then trained on low-resolution versions of high-resolution images that are sampled with fixational drift across the image. The authors show that this approach allows their system (dynamical recurrent classifier, or DRC) to get much better classification performance on CIFAR images than can be achieved without the early recurrence and drift. The authors also show that the most robust classification mandates drift trajectories with higher curvature, and they show that this matches some of the properties of visual drift trajectories in humans. \n\nThe reviews on this paper were highly divergent (ranging from 3 to 10). Three of the reviewers felt this paper should be rejected, but one felt very strongly it should be accepted. The primary concerns from the negative reviewers were lack of appropriate controls, lack of insight into why the system works, lack of appropriate references to past work, and lack of connection to biology. The authors made a very concerted effort to attend to all of the reviewers' comments. They ran all of the requested control experiments, updated the text to better reflect past literature, and included some comparison to psychophysics data. In the end, only one reviewer increased their score, though, leading to final scores of 3, 10, 5, and 3. Discussion did not lead to any more consensus. \n\nThus, this paper was still very much in the borderline zone, and required AC consideration. After reading through the paper, reviews, and rebuttals, the AC felt that the authors really had addressed the primary concerns as best as could be hoped for in the time-frame for ICLR, and that the paper was sufficiently interesting and informative for ML and neuroscience to be worthy of publication. Some of the negative review points stand, e.g. there are still some mysteries as to why this works and there is certainly a lot more that could be done to make this paper informative for neuroscience. Nonetheless, in total, the AC felt that this paper deserved to be accepted, given that the authors did most of what the reviewers requested of them.", "paper_id": "iclr_2022_p0rCmDEN_-", "label": "train", "paper_acceptance": "Accept (Poster)"}
{"source_documents": ["Generative adversarial networks (GANs) have achieved outstanding success in generating the high-quality data. Focusing on the generation process, existing GANs learn a unidirectional mapping from the latent vector to the data. Later, various studies point out that the latent space of GANs is semantically meaningful and can be utilized in advanced data analysis and manipulation. In order to analyze the real data in the latent space of GANs, it is necessary to investigate the inverse generation mapping from the data to the latent vector. To tackle this problem, the bidirectional generative models introduce an encoder to establish the inverse path of the generation process. Unfortunately, this effort leads to the degradation of generation quality because the imperfect generator rather interferes the encoder training and vice versa. \n      In this paper, we propose an effective algorithm to infer the latent vector based on existing unidirectional GANs by preserving their generation quality.\n      It is important to note that we focus on increasing the accuracy and efficiency of the inference mapping but not influencing the GAN performance (i.e., the quality or the diversity of the generated sample).\n      Furthermore, utilizing the proposed inference mapping algorithm, we suggest a new metric for evaluating the GAN models by measuring the reconstruction error of unseen real data.\n      The experimental analysis demonstrates that the proposed algorithm achieves more accurate inference mapping than the existing method and provides the robust metric for evaluating GAN performance. ", "Thanks for your comments,\nWe thank the Reviewer1 for constructive feedback. Reviewer1 suggests the comparison among the results of various unidirectional GANs to strengthen the experimental evaluation. We agree that the comparison mentioned by Reviewer1 helps improving the quality of the paper. To reflect this comment, we now revise our manuscript to supplement the qualitative comparison among unidirectional GANs.\nFurthermore, Reviewer1 suggests that you should provide the inception score or MS-SSIM of other datasets. It would be helpful to show that the existing metrics are meaningful only on the specific datasets. As we mentioned on section 3.2, MS-SSIM scores are almost zero for CIFAR10 or Fashion MNIST, which have multiple classes. Meanwhile, the inception score utilizes the classifier, thus it is not appropriate to apply onto the single class dataset (i.e., CelebA). As a result, the scores from those cases would present meaningless scores. Due to limited space, we did not include them in our main manuscript. \nWe hope our additional results can answer your comments.", "Thanks for your comments,\nWe appreciate the constructive feedback from Reviewer 2. As Reviewer 2 pointed out, we missed the comparison with the relevant existing work (iGAN) which also adopts independent inference mapping method to the GAN training; our experimental evaluation focused on the limitation of the bidirectional generative models that trains generation and inference mapping together. We agree that the comparison with iGAN and the ablation study using the pre-trained feature extractor would improve the quality of our paper. To reflect this valuable feedback, we conduct the experiment as follows. Please note that all changes are reflected in our revision (page 6-7, 13). \n\niGAN compares three different inference prediction methods. 1) The first method is a direct inference mapping through naïve encoders. 2) The second method is to adopt the non-convex optimization, which minimizes the pixel wise difference between the original image and the image generated from the estimated latent vector. 3) Finally, their proposal is a hybrid method that carries out encoder mapping followed by the non-convex optimization. Similarly, we compare the proposed method with 1) naïve encoder mapping, 2) iGAN (naïve encoder followed by optimization) and 3) hybrid method using the proposed method (discriminator with CN followed by optimization). Furthermore, to investigate the capability of the discriminator as the feature extractor, we directly compare our inference mapping (discriminator with CN network) with the VGG-16 based inference mapping (pre-trained VGG with CN network). Note that the pre-trained VGG16 network is trained on ImageNet 1k. The quantitative evaluation is summarized as follows. Please also see qualitative results on our revision.  \n\n\n                Ours\tnaïve\tiGAN\tHybrid+Ours\tVGG16\nSSIM\t0.5214\t0.4872\t0.5624\t0.5710\t\t0.5199\nPSNR\t16.85\t15.28\t18.21\t18.52\t\t16.81", "Our algorithm successfully synthesizes the attributes in various faces, unlike the na{\\\"i}ve encoder. As reported in iGAN, we confirm that adopting the non-convex optimization for inference mapping significantly enhances the quantitative score (i.e., SSIM and PSNR). It is because the non-convex optimization directly minimizes the pixel-wise difference between test images and reconstructed images; the goal of the non-convex optimization is nearly equivalent to the goal of PSNR. Hence, the hybrid method improves the PSNR of any baseline encoder mapping. When we replace the na{\\\"i}v encoder with the proposed inference mapping, its quantitative results are better than iGAN. It is because our inference mapping predicts more accurate initial latent vector. \n\nHowever, these quantitative results do not exactly match with qualitative results. The quantitative results demonstrate that hybrid inference mapping is the most effective among all others. Meanwhile, the qualitative results from the hybrid methods are generally blur or have missing important components (e.g., eye glasses, mustache, gender, wrinkles, detailed hair lines, etc.). Because the hybrid inference mapping optimizes the inference mapping in the image domain (i.e., minimizing the pixel-wise difference), the inference network finally chooses the latent vector corresponding to an average-like image. Note that there exists average-like faces among many possible faces. We conjecture that, although the generator can produce sharp images, the hybrid inference mapping strategically selects average-like faces to reduce its loss function. Meanwhile, our method (also VGG16 based inference mapping) optimizes the inference mapping in the latent domain. Thus, our inference results are sharp and better preserve semantically important attributes. From examples shown in Fig ~\\ref{figure04} and Appendix Fig  \\ref{appendix fig}, pixel-wise loss based methods  (i.e., iGAN and Hybrid+ours) fail to capture glasses, but latent vector loss based methods  (i.e., ours and VGG16) reproduce the glasses. In fact, for the same reason, VEEGAN chooses to minimize a reconstruction loss on the latent vector to solve mode collapse.   \nBy replacing the discriminator as feature extractor by the pre-trained VGG16 network, we observe that its inference results are also as sharp and realistic as our results. However, considering the semantic similarity between the original and reconstructed image, our inference mapping can restore unique attributes (e.g., mustache, race, age, etc.) better than the VGG16 based inference mapping. Moreover, utilizing the pre-trained VGG16 require additional memory overhead while our method does not. In terms of network capacity, VGG16 has the much deeper network than the discriminator. Thus, we conclude that the proposed inference mapping is more efficient than the VGG16 based inference mapping. From these results, we confirm that recycling discriminator as a feature extractor is effective for improving inference accuracy and reducing the computational complexity.\n\nIn conclusion, we adopt the inference mapping through optimizing on the latent space because this preserves the properties of GANs that generate sharp images and semantic attributes. Moreover, since preserving the semantic attributes could be interpreted as how well GAN understand the images, the non-convex optimization that prefers average images even omitting semantic attributes is not appropriate for suggesting the GAN evaluation metric. In the same context, since recycling the discriminator directly affect the inference mapping accuracy, our method is suitable for evaluating whole GAN framework (i.e., both the generator and the discriminator).", "Thanks for your comments,\nFirst of all, we would like to solve important misunderstanding about our methodology. Especially, the  major difference of our work compared to three techniques mentioned by Reviewer 3 is summarized as follows. Unlike the existing techniques (i.e., InfoGAN, CycleGAN and ALICE), our inference mapping using the connection network is completely independent of both generator updates and discriminator updates. We would like to stress that this is why we could maintain the quality of generation in the baseline GAN model; other inference mapping techniques influence the generation quality.\n\nOur key idea of inference model is to reuse the discriminator network as feature extractor and learn a direct mapping from the feature vector to the GAN's latent space, as mentioned by Reviewer 1. Because we utilize the well-educated discriminator to extract the meaningful features, the direct mapping is learned after the training of both generator and discriminator end. On the other hand, infoGAN, CycleGAN, and ALICE intend to design the inference mapping that controls the generation process; the above three techniques all affect the generator updates for their own purpose. Again, we emphasize that the reconstruction loss of the connection network is different from the conditional entropy of infoGAN and the cycle consistency of CycleGAN and ALICE in two aspects. First, our model does not affect both the generator update and the discriminator update. Secondly, our goal is to build the inference mapping without affecting the baseline GAN performance. Meanwhile, three techniques develop new GAN models for learning interpretable representation (infoGAN), unsupervised domain transfer (CycleGAN), or alleviating the mode collapse (ALICE).\nWe decide to separate the generation mapping (from z to x) and the inference mapping (from x to z) because of the convergence issue reported in bidirectional GANs. For example, ALICE reported in appendix E.3, “As a trade-off between theoretical optimum and practical convergence, we employ feature matching, and thus our results exhibit a slight blurriness characteristic”. We believe that this convergence issue is caused by the error propagation from both generator and encoder. Furthermore, their inception score is 6.015, which is slightly worse than the average inception of unidirectional GAN, 6.5. This experimental result demonstrates that existing bidirectional methods (either using cycle consistency or joint distribution matching) have shown the limited generation performance. Although we did not compare the performance to ALICE directly in the paper, the experimental results of ALI/BiGAN could represent the limitation of bidirectional GANs trained for joint distribution matching.\n\nWe agree that the reconstruction loss could be interpreted as the negative log likelihood, and this is equivalent to mutual information and conditional entropy. However, such a scheme is applicable for techniques that handle the joint distribution (both generation and inference or both generation and latent code) matching. Meanwhile, our model disconnects the inference mapping from generation process.  \n\nWe now revise our manuscript to clarify those of difference of ours and three existing techniques. We hope our explanations can answer your concerns.  ", "Paper Summary: \nThis paper proposes to reconstruct the generated images to the their corresponding latent code. As claimed, the goal is to improve the accuracy and efficiency of inference mapping better than other inference mapping techniques, while maintaining their generation quality.\n Instead of using an independent encoder, the authors propose to share the encoder parameters with the discriminator: a Connection Network (CN) is built on top of the features extracted by the discriminator. The weight-sharing machisme shows better performance in Figure 1.\nThe proposed method has two benefits: : a) manipulating the image by disentangling the latent space and b) suggesting a new metric for assessing the GAN model by measuring reconstruction errors of real data.\n\nGeneral Comments:\nIn term of algorithm, the paper essentially adds the conscontruction term (CN) to the standard GAN loss, and partially shares the weights of the “encoder” and discriminator. However, it is almost identical to the existing works, which are NOT cited, and the connections are not discussed.\n\nConnection to InfoGAN: To relate the generated images to the latent code,  the proposed method employs the reconstruction loss, InfoGAN employs the mutual information. Note that reconstruction loss = negative log likelihood, and effectively is equivalent to Mutual Information and Conditional Entropy in the case. Please see the discussion in Lemma 3 and Appendix A of [3] for detailed discussion.  Further, InfoGAN has proposed to to sharing weights of the encoder and discriminator, exactly the same with this submission. The claimed advantage is to disentangle the latent space. It is not surprise at all, once the authors see the connection to InfoGAN, which was originally proposed to disentangle the latent codes.\n\nConnection to CycleGAN: CycleGAN consists of four losses: two reconstruction losses and two standard GAN losses. As shown in Section 4 of [3] “Connecting ALI and CycleGAN”, one reconstruction loss and  one standard GAN loss is sufficient to achieve CycleGAN’s objective, the other two losses would only help to accelerate. In another word, the proposed method is exactly half of the CycleGAN losses.\n\nThe author mention in Abstract that “the bidirectional generative models introduce an encoder to establish the inverse path of the generation process. Unfortunately, their inference mapping does not accurately predict the latent vector from the data because the imperfect generator rather interferes the encoder training.” This is the non-identifiable issue of ALI/BiGAN discovered in [3]. Please clarify. \n\nThe proposed method should compare with [1] and [2] in great detail, to demonstrate its own advantages. Given the missing literature, the current experimental comparisons seem not that meaningful, because the baseline methods are not really the competitors. \n\nOne interesting contribution of the submission is to consider the reconstruction errors to measure the quality of GANs. To my best knowledge, it is original. \n\n\nReferences:\n\n[1] InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets, NIPS 2016\n[2] Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, ICCV 2017\n[3] ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching, NIPS 2017\n", "The paper proposes using the GAN discriminator for inference mapping, mapping an image to a latent code that would be used to generate the image by the encoder, based on the argument that the discriminator can be used as a powerful feature extractor because it has seen both real and fake data during training. The paper compares the proposed approach to several approaches that train an inference model together with a generator. \n\nWhile the paper compares its approach to several baselines, they are not the most relevant ones. In fact, the most relevant baseline is not cited and compared. As a result, the novelty of the paper is not justified. Specifically, the baselines the paper compare to are mostly methods that jointly learn an inference model and a generation model, while the proposed approach first learns a generation model and then fits an inference model (it is referred to as the connection network in the paper). In this regard, the paper should compare its approach to methods that first learns a generation model and then learns an inference model. The iGAN work by Zhu et. al. ECCV 2016 is arguably most relevant approach. Especially, they also use the discriminator architecture for the inverse mapping. Unfortunately, the work is neither cited nor compared.\n\nIn addition, pretrained networks such as VGG and ResNet have been known to be powerful feature extractor. It would be ideal the paper can compare the proposed approach to that using VGG and ResNet for finding the z for a given image.\n\nFinally, the paper seems to lack of comprehensive knowledge on how the inference mapping has been investigated in the GAN literature. For example,  the statement that \"BEGAN (Berthelot et al., 2017) made the first attempt to solve the inverse mapping from x to z using the non-convex optimization\" in the introduction section is incorrect. The scheme is used in at least two 2016 papers (Liu and Tuzel NIPS 2016 and Zhu et. al. ECCV 2016).", "This paper describes a novel method to provide inference mapping for GAN networks. The idea is to reuse the discriminator network's feature vector (output of layer before last) and learn a direct mapping to the GAN's latent space. This can be done very efficiently since the dimensionality of both layers are relatively small. Also, the mapping does not interfere with the learning process of the GAN itself and thus can be applied on top of any GAN method without affecting its performance. \n\nInference mapping is useful in the GAN context for several reasons that are well described in the paper. First it allows to more efficiently generate \"edited\" images as the mapping provides a good starting point in the latent space. Second it provides a sound way to evaluate GAN's performance as the reconstruction of a given image through the inference mapping and the generator provides auto-encoder-like capabilities. Comparison of GAN models have been difficult due to a lack of adequate evaluation technique. This paper proposes a novel evaluation scheme that is both fair and technically simple.\n\nIn the experimental part, the authors first compare their approach to the 'naive encoder' approach where the last layer of the discriminator is removed after training, a feature layer of the size of the encoder's latent space is added, and the rest of the discriminator's layers are frozen. The proposed approach outperforms the naive encoder approach on the CelebA dataset. The second set of experiments investigates reconstruction accuracy of various GAN models. Figure 2 shows reconstructed images for 7 GANs and 36 examples from 3 datasets. Unfortunately, no subjective comparison can be attempted since the examples are different for each GAN. In Figure 3, editing in performed on the CelebA dataset, but again, subjective comparison among the GAN's is precluded by the fact that different examples are chosen. This oversight does not affect the paper's relevance, since those comparison would be purely subjective, however it would add some visual interpretation to the quantitative comparison given in table 1. I also wish the authors would have provided the inception score for FashMNIST and CelebA and also provide the more recent FID (Frechet Inception Distance). Inception scores are trained on ImageNET and are too commonly applied to CIFAR-10 and CelebA. It would be good to compare them against the proposed method on those datasets to show that there are not good for datasets other than those on which they were trained.\n\nThe article is technically sound. The citations are adequate. The English is fine with some extraneous articles being the only issue. The article lacks a graphic for the architecture of the system and many of the figures are too small to interpret when printed out. Also there's a typo on table 1. where the inception score for WGAN-GP on CelebA should be 6.869 and not 0.6869.\n\nOverall, I find this paper provides a simple, novel significant method for evaluating GAN models and making better use of their latent space arithmetic editing capabilities. Due to the algorithm's simplicity, most of the paper is devoted to experiments and discussions.\n \n"], "review_score_variance": 3.5555555555555554, "summary": "The paper presents a method to learn inference mapping for GANs by reusing the learned discriminator's features and fitting a model over these features to reconstruct the original latent code z. R1 pointed out the connection to InfoGAN which the authors have addressed. R2 is concerned about limited novelty of the proposed method, which the AC agrees with, and lack of comparison to a related iGAN work by Zhu et al. (2016). The authors have provided the comparison in the revised version but the proposed method seems to be worse than iGAN in terms of the metrics used (PSNR and SSIM), though more efficient. The benefits of using the proposed metrics for evaluating GAN quality are also not established well, particularly in the context of other recent metrics such as FID and GILBO. \n", "paper_id": "iclr_2019_HkgnpiR9Y7", "label": "val", "paper_acceptance": "rejected-papers"}
