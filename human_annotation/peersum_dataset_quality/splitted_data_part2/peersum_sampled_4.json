{"source_documents": ["In the genome biology research, regulatory genome modeling is an important topic for many regulatory downstream tasks, such as promoter classification, transaction factor binding sites prediction. The core problem is to model how regulatory elements interact with each other and its variability across different cell types. However, current deep learning methods often focus on modeling genome sequences of a fixed set of cell types and do not account for the interaction between multiple regulatory elements, making them only perform well on the cell types in the training set and lack the generalizability required in biological applications. In this work, we propose a simple yet effective approach for pre-training genome data in a multi-modal and self-supervised manner, which we call $\\textbf{\\texttt{GeneBERT}}$. Specifically, we simultaneously take the 1d sequence of genome data and a 2d matrix of (transcription factors × regions) as the input, where three pre-training tasks are proposed to improve the robustness and generalizability of our model. We pre-train our model on the ATAC-seq dataset with 17 million genome sequences. We evaluate our GeneBERT on regulatory downstream tasks across different cell types, including promoter classification, transaction factor binding sites prediction, disease risk estimation, and splicing sites prediction. Extensive experiments demonstrate the effectiveness of multi-modal and self-supervised pre-training for large-scale regulatory genomics data. ", " Thank you to the authors for responding to the comments. Based on the other reviews and responses, I intend to maintain my score. ", " Thank you to the authors for their response to my comments. I still believe the authors would need to do more to explain, demonstrate, and interpret their model before it would make for a compelling publication in ICLR. Therefore, I maintain my score.", " Thanks for your response. I read the authors' answers and my concerns for the paper are not addressed. I think if the authors apply all the comments of the reviewers, the paper will be more complete and interesting.", " We appreciate your insightful comments, below we address the concerns mentioned.\n\n**Q1: One more interesting experiment for finding enhancers?**\n\nAns: We thank the reviewer for the insightful comments. We acknowledge the fact that finding the promoters is not very challenging. We include this task to show the versatility of our model, and to better compare it with the performance of other methods. We agree with the reviewer's comments that identifying the functional enhancers is a harder and more biologically meaningful task, with immediate implications in a lot of biological fields. Currently, we are experimenting with different finetuning models to do that and hopefully can extend them to a fully-fledged version of this paper. \n\n**Q2: Why not show the performance on all the TFs?** \n\nAns: We agree with the reviewer's comments that to fully evaluate the performance of this model, we need to evaluate it on different types of TFs. We are currently performing these experiments and hopefully can extend them to a journal version of this paper.\n\n**Q3: Interpretation of the t-SNE plots.**\n\nAns: We agree with the reviewer that two t-SNE plots look quite similar. We will continue to look into this issue in future research.\nFor quantitative comparison, we did report the results of promoter classification task by ablating the 2D modaltiy using $L_{srm}$ or not in Table 5. We can observe that adding $L_{srm}$ to the final objective further improves the results by 0.103, 0.142, and 0.169, in terms of all metrics. This further demonstrates the effectiveness of our 2D modality and sequence-region matching loss in learning the alignments between sequence and region of genome data.\n\n**Q4: Confusion about Table 6.**\n\nAns: Table 6 contains the prediction of CTCF binding sites in different cell types.", " \n**Q6: Confusion about the disease risk estimation task.**\n\nAns: We agree with the reviewer's comments that the description for disease risk estimation tasks a lot of details. The study was performed as an exploratory. Also, we noticed that the disease risk prediction result showing in our current version is pre-mature, as it does not beat SNP or MARVEL (https://genome.cshlp.org/content/30/11/1618) based baseline. We decided to remove this part of the experiment from this study for now and will try to further improve the fine-tuned model for this task.\n\n**Q7: Details about Splicing task.**\n\nAns: We used the pre-trained model together with a classification head to do the splicing prediction task on the SpliceAI dataset. Regarding the dilated CNN, we are referring to SplieAI itself. Actually in SpliceAI, 400nt sequence as input can already produce quite a good result, implying that cell-type agnostic prediction of splicing site is actually similar to promoter prediction, is a relatively simple task. Since transformer-based models have a larger modeling capacity, that might explain the improvement over the dilated CNN model.\n\n**Q8: Interpret what the model has learned that alternative approaches have not?**\n\nAns: The 2D modality in our work takes advantage of the combinatorial interaction and the implicit cell-type specific information. This combinatorial interaction between regulatory regions and transcription factors varies across different cell types such that precise control of expression in different cell types is possible. We believe that our method uniquely captures this kind of information, which can benefit various downstream tasks. In future work, a more rigorous analysis will be conducted.\n\n**Q9: Confusion about t-SNE plots.**\n\nAns: The purpose of the t-SNE plots is to qualitatively compare the quality of pre-trained representations in a uni-modal and multi-modal manner. We agree that the two scenarios are quite similar, we are still investigating the embedding produced by the two approaches.\nFor quantitative comparison, we did report the results of the promoter classification task by ablating the 2D modality using $L_{srm}$ or not in Table 5. We can observe that adding $L_{srm}$ to the final objective further improves the results by 0.103, 0.142, and 0.169, in terms of all metrics. This further demonstrates the effectiveness of our 2D modality and sequence-region matching loss in learning the alignments between sequence and region of genome data.\n\n**Q10: Mutagenesis experiment?**\n\nAns: We agree with the reviewer that mutagenesis/saliency methods will provide a nice interpretation of the trained model. We are currently working on an improved model and will definitely add the mutagenesis analysis.", " \nWe appreciate your insightful comments and suggestions on improving the paper, below we mainly answer your questions and discuss the limitations of this work. We will also update the paper according to the suggestions.\n\n**Q1: More sufficient details about input data?**\n\nAns: Concatenating 10 consecutive accessible regions can be considered as an approximation of the average peak within one topologically associating domain (TAD). In future work, we will investigate more about the input size based on the results of this pilot study.\nThe ATAC-seq comes from the public human fetal cerebrum single-cell chromatin accessibility data in the Descartes database.\nWe have added a detailed caption for Figure 1 in the updated manuscript.\n\n**Q2: Isn’t each position in the sequence depth 1?**\n\nAns: Actually we used both the 1D sequence (length L*10) and a 2D matrix (size 10*111, 10 for 10 consecutive peaks, 111 for number of motifs, different motif have different PWM, so the motif scanning result is not 1D) in the training. \n\n**Q3: Why use ImageNet pre-trained weights?**\n\nAns: Although we loaded the weights of the 2D transformer pre-trained on ImageNet, we updated the parameters of the 2D backbone during the pre-training process. \n\n**Q4: Try to predict gene expression level?**\n\nAns: We agree with the reviewer's comments that expression modeling is a harder and more meaningful task, with immediate implications in a lot of biological fields. Currently, we are experimenting with different finetuning models to do that and hopefully can extend them to a fully-fledged version of this paper.\n\n**Q5: Why not compare to any of the leading methods?** \n\nAns: We understand the reviewers' concerns about not directly comparing our method with some other leading methods.  However, DNABERT, the method that we outperformed, has already shown its superiority over the SOTA methods in various downstream applications. For the promoter classification task, it outperformed models using other neural networks structures, including CNN, CNN+LSTM, and CNN+GRU. For the TFBS classification task, it outperformed methods including DeepBind, DeepSea, Basset, DeepSite, DanQ, and BESSO. Therefore, we believe our method is competitive with the SOTA methods.\n", " \nWe appreciate your helpful comments, below we answer your questions.\n\n**Q1: Whether this work is of broad interest to the ICLR attendees?**\n\nAns: The main focus of this work is to propose a simple yet effective method for large-scale genome data pre-training in a multi-modal and self-supervised manner. Our work not only brings meaningful biological improvements but makes an important contribution to the machine learning community, by introducing a novel multi-modality construction. For example, the ‘visual’ modality in our work is constructed based on the regulatory property of the ‘language’ sequential units. This inspires researchers in the ICLR community to build a new ‘visual’ modality, based on text matching, part-of-speech tagging, and named entity recognition, etc., for the study of NLP.\n\n**Q2: Does the genomic distance between regulatory elements matter?**\n\nAns: We agree with the reviewer that distance plays an important role in the enhancer-promoter targeting relationship. Our current implementation is indeed not ideal. In future work, we will add distance into consideration, potentially with an exponential weighting scheme based on 1D distance or 3D contact probability.\n\n**Q3: The 2D representation is not a true image representation?**\n\nAns: The 2D modality in our work takes advantage of the combinatorial interaction and the implicit cell-type specific information. This combinatorial interaction between regulatory regions and transcription factors varies across different cell types such that precise control of expression in different cell types is possible.", " **Q6: More disease types for generality?**\n\nAns: We agree with the reviewer's comments that this study would be much valuable in terms of biological implication if we can show generalizability for different disease types. Actually, we are currently finalizing the method and applying it to other disease types for a biological journal version of this method.\nAlso, we noticed that the disease risk prediction result showing in our current version is pre-mature, as it does not beat SNP or MARVEL (https://genome.cshlp.org/content/30/11/1618) based baseline. We decided to remove this part of the experiment from this study for now and will try to further improve the finetune model for this task.\n\n**Q7: misusing different datasets (train orange to predict apple)?**\n\nAns: We are not fully sure which part of this study is the reviewer mentioning.\nWe suppose the reviewer is referring to the splicing-related task, which indeed is a bit weird given that we are using open chromatin data to pre-train the model. We are aware of the fact that splicing is performed on RNA rather than DNA. However, there are also splicing factors (RNA-binding proteins) that have sequence-specific binding patterns to RNA, which might leave their trace in the DNA sequence.\nAlthough there is no direct connection between open chromatin and splicing sites, we do find that a considerable amount of known splicing sites actually overlaps with the open chromatin data we used, which seems to be enough for the simple splicing site prediction task.\nOverall, this part is only a exploratory analysis rather than a strong arguments that we want to highlight in manuscript.", " We appreciate your insightful comments and suggestions on improving the paper, below we mainly answer your questions.\n\n\n**Q1: Details about datasets and data processing.**\n\nAns: We apologize for the inaccurate description of the data processing procedure. For TFs, we used the motif scanning result provided by the original dataset and combined similar motifs into one row in 2D modality by\n1. map each TF motif to one of the architype described in https://www.vierstra.org/resources/motif_clustering. \n2. Sum up their motif matching score in a peak.\nFor cell types, we want to clarify that we are aggregating all cells that belong to the same cluster as one pseudobulk sub-cell-type, while we did not merge across the 17 cell types in the brain. The cell types we used are listed in https://descartes.brotmanbaty.org/bbi/human-chromatin-during-development/dataset/cerebrum, which includes:\n- Astrocytes\n- Astrocytes/Oligodendrocytes\n- Cerebrum Unknown.3\n- Excitatory Neurons\n- Inhibitory Neurons\n- Limbic System Neurons\n- Skor2 Npsr1 Positive Cells\n- Vascular Endothelial Cells\n\n**Q2: Is scATAC-seq sparse and noisy?**\n\nAns: We agree with the reviewer that scATAC-seq data is sparse and noisy. This is exactly the reason we choose to aggregate the cell-level signal to pseudo-bulk level but add up the signal from cells that belongs to the same cluster (performed by the author of the original dataset). We believe binarizing the data after this aggregation is less prone to false positives, and we also chose a $>$10 counts threshold rather than $>$0, which is not a loose cut-off.\n\n**Q3: Missing many regulatory regions for various tissue types and cell types?**\n\nAns: We thank the reviewer for these insightful comments. We do agree that the data we used is in the fetal stage rather than both fetal and adult. However, we would like to point out a new study (https://www.cell.com/action/showPdf?pii=S0092-8674%2821%2901279-4) which integrates a new adult ATAC-seq dataset (30 tissue types) with the fetal dataset (15 tissue types) we used. After the integration, the increase of total ATAC-seq peaks is only 10\\%.\nThus, we believe our current dataset has reached a good coverage of the human regulatory genome landscape. We are now pre-training our model on the new dataset with randomization over both fetal and adult tissues to get a less biased version of the model.\n\n**Q4: Why not predict other TFs?**\n\nAns: We agree with the reviewer's comments that to fully evaluate the performance of this model, we need to evaluate it on different types of TFs. We are currently performing these experiments and hopefully can extend them to a fully-fledged version of this paper.\n\n**Q5: ENCODE TF ChIP-seq data were for cell lines?**\n\nAns: We thank the reviewer for these insightful comments. We do agree that the ENCODE TF ChIP-seq data is mainly for cell lines, different from the primary fetal tissue data we used in our study.\nThis difference could include two aspects: \n1. the genome DNA of cell line might have extensive change compared to primary tissue \n2. the regulatory landscape of cell lines is more akin to tumor cells due to their proliferation ability.\nIn recent epigenomic study common practice, usually, aspect 1 is not fully accounted for, as ENCODE consortium aligned all sequencing data to the reference genome.\nFor aspect 2, since the goal of our study is to build a model that is robust and useful to out-of-sample data, we actually purposefully used a different data source to validate the ability of our model to generalize outside the training dataset.", "In this paper, the authors developed a transformer-based model, GeneBERT to align DNA sequences with regulatory elements. In particular, GeneBERT first applies transformers to learn representations of sequencing data and regulatory regions (e.g., open chromatin), and then aligns the representations of two modalities for identifying region-aligned sequences. The authors applied GeneBERT to recent scATAC-seq data in fetal and use aligned sequences to predict promoters, CTCF binding sites, a disease type and RNA splicing sites. Strengths\n•\tA novelty is that GeneBERT combines the losses for representation learning and alignment and simultaneously estimate the model parameters.\n•\tLarge-scale multi-modal data integration\n\nWeaknesses\n•\tThe authors need to elaborate on the details about datasets and data processing such as how to select TFs and binding motifs to construct 2D modality, how to merge 17 cell types, which cell types in fetal data were used. Fig 2 only shows cell cluster numbers, for instance.\n•\tscATAC-seq is sparse and noisy. Binarizing data by non-zero can also introduce many false positive open regions.\n•\tMany regulatory regions for various tissue types and cell types are yet open in fetal stage, so they are likely missed.\n•\tCTCF is a general TF with strong binding activities. The authors should predict other TFs, especially cell-type-specific TFs. Also, ENCODE TF ChIP-seq data were for cell lines, which may not match the cell types that the authors used.\n•\tThe authors also should try more disease types for generality.  The paper was organized logically. However, the applications were not well presented. Many details are unclear and missing, especially on datasets, data processing, feature selection, cell types. Also, it seems that the authors don’t fully understand cell-type gene regulation for misusing different datasets (train orange to predict apple).\n\n", "The manuscript describes GeneBERT, a self-supervised and multi-modal pre-training approach for genomic data. GeneBERT combines 1D genome sequence data with a 2D representation of regulatory elements in different cell-types in three different pre-training tasks. A large-scale single-cell ATAC dataset is used to identify pseudo-bulk ATAC profiles for 17 cell-types spanning > 1M genomic locations. The sequence of these genomic locations provides the 1D representation of the data which are used in self-supervised training with masked genome modeling and next genome segment prediction loss functions, both inspired by the BERT framework. A 2D representation is derived using accessibility per regulatory element and cell type, and is self-supervised using a infoNCE loss. The pre-training framework is then compared in a variety of biological tasks including promoter prediction, TFBS prediction, splice site prediction. The authors then present ablation studies highlighting the importance of the different components of the loss function. Strengths of the paper\n* The simple-yet-effective approach for pre-training of genome sequence data works remarkably well for very diverse tasks \n* The wide availability of multi-modal data provides an opportunity for adaptation by the field. \n* Benchmarking and ablation studies are well designed. \n* The paper is well written and makes effort to explain diverse concepts from biology and machine learning to a wide audience\n\nWeaknesses \n* While the formulation is indeed a novel multi-modal construction as claimed by the authors, the generalization beyond genome data is not clear and hence I am not sure if this will be of broad interest to the ICLR attendees. \n* An important feature of gene regulation is that the genomic distance between regulatory elements matter - the fact that 10 consecutive accessible regions are grouped into one sample breaks this relationship and might affect interpretability. \n* The 2D representation is certainly interesting but it is not a true image representation. This might be a case of defining a representation to fit the model but the performance certainly shows there is value to such a representation. \n The problem tackled by the manuscript address an important need in repressing genome data using both sequences and additional measurements. The results presented both in performance and diversity of tasks measured is impressive. However, I am not completely sure if the submission will be of broad interest to the ICLR audience. ", "The authors describe a pipeline to perform self-supervised learning on genome sequences, guided by accessible chromatin peaks. Their learning procedure is inspired by the NLP method BERT and uses several tasks from that work and its successors. After pre-training transformer layers, they fine-tune on several regulatory sequence classification tasks and demonstrate high performance. The authors’ description of the method does not contain sufficient details to understand it. Specifically, the input data and neural network layers that process it are difficult to decipher. For example, why are 10 sequences concatenated? Where does the ATAC-seq data come in? A caption for Figure 1 would be very helpful.\n\nThe authors transform the 1D nucleotide sequence to a 1D sequence of recognized PWM vectors. As I understand it, the sequence is then treated as a 2D object and processed with transformer layers. Isn’t each position in the sequence depth 1? The authors suggest that they used a model pre-trained on ImageNet. How could it be that this pre-trained model has learned anything useful for this very different task?\n\nClassifying promoters from DNA is a very easy task that the field has had high performance on for many years and does not solve a useful problem. If the authors can demonstrate that their model can predict gene expression level, particularly in diverse cell types, it would be far more compelling. In addition, there are strong competing methods based on neural networks to which the authors could compare their different approach.\n\nClassifying TF binding sites is an important task, but the authors do not compare to any of the leading methods, such as DeepBind, DeepSea, Basset, DanQ and many successors. Furthermore, CTCF has a long informative PWM, making it arguably the easiest TF to predict. The authors should show results from more diverse TFs.\n\nThe disease risk estimation task is impossible to understand because there are so few details. What is an example in this task? For example, are genetic variants the examples? Where did the labels as disease-related or not come from? What procedure did you use to include negative examples? How did you choose Hirschprung disease among the many options? Since the model focuses on gene regulation rather than protein coding sequence, how should we think about coding gene mutations in your task?\n\nThe authors observation that their model outperforms SpliceAI using shorter sequences is very surprising. Could the authors include more details about the full model that they used for this task, and how their pre-trained model is used within it? When the authors write “dilated CNN” in their Table 4, are they referring to SpliceAI itself or to the author’s own implementation of a dilated CNN?\n\nNew methods in the space are typically compared to competing alternatives, but also interpreted. Can the authors interpret what their model has learned that alternative approaches have not? Referring to Figure 2, the authors state, “We can observe that our GeneBERT pre-trained representations form more separated clusters that are distributed more uniformly on the space in terms of all cell types when compared to the pre-trained representations generated by the uni-modal model. Furthermore, the representations pre-trained by our multi-modal self-supervised method have much smaller distances inside each cluster of cell type.” This doesn’t appear to be true to my eye. The authors should compute summary statistics to make this claim convincing. However, this visualization also doesn't really help interpret the model; mutagenesis or saliency methods applied to DNA sequences would be better. The authors explore a new method for self-supervised pre-training before tackling several regulatory sequence analysis tasks, but are not able to deliver a clear method description or compelling empirical results.", "This work proposes an approach, called GeneBERT, for pre-training genome data in a multi-modal and self-supervised manner. They take the 1d sequence of genomic data and a 2d matrix of (transcription factors × regions) as the input and optimize three pre-training tasks to improve the robustness and generalizability the model. Specifically, they introduce two main objectives for sequence pre-training, Masked Genome Modeling (MGM) and Next Genome-Segment Prediction (NGSP). GeneBERT consists of three main components: sequence pre-training, region pre-training, and sequence-region matching. The main contribution of the paper is that they use transcription factor information in genomic regions which makes the model more generalizable to other cell types than the previous DNABERT that only uses the sequence information.  Strengths:\n\n- The idea of using 2d information of (transcription factor by regions) in transformer models to learn meaningful genomic embeddings is cool. Using such information is a win over DNABERT which is not so informative. \n\n- The authors have used the recent advanced models from NLP and have employed three losses whose optimization all together leads to significant improvement over the DNABERT. \n\n- The authors has thoroughly ablated the model and analyzed the impact of each component of the model as well as hyper parameters and batch size. It is convincing that it is all three loss together that gives rise to such improvement. \n\nWeaknesses:\n\n- Some of the claims in the paper regarding the biological importance are not so true. For example: \"As the promoters play an important role in gene regulation, using machine learning methods to predict promoter sites accurately is one of the most popular problems in bioinformatics.\" Finding the promoters are not a challenge. The genes and their transcription start sites (TSS) are nearly annotated in the genome and it is known that the promoters are next to the TSS's. Instead, the most challenging part is to find functional enhancers in the genome which is a fundamental question in biology. The majority of enhancers are distal regulatory elements, meaning that they are very far from their target genes (could be 1Mb away). So, one interesting experiment for GeneBERT is to check and see if it can find enhancers or not (functional enhancers can be validated by CRISPRi perturbation data that are available in some cell lines such as K562).\n\n- For transcription factor binding site (TFBS), the paper only shows the results for CTCF in Table 2. I was expecting to see the results on many TFs. Why did the authors not show the other TF's results? The CTCF motif is not so complicated and maybe that is the reason why we see such great performance. I think the authors should provide the performance on all the TFs and discuss which ones the model finds well and which ones not and why.\n\n- On the effect of 2D modality on pre-training, the t-SNE plots in Figure 2 are not so informative. They look similar and it is hard to get any conclusion from that. The authors can provide a quantitative metric to show how 2D modality helps the embeddings. \n\n- It is not clear in Table 6, what are the classification tasks. The authors only mentioned they chose different proteins from various cells or tissues in the human body. But it should be stated clearly what TFs are used in each cell.\n\n- Typo: several times in the paper including the abstract, \"transaction factor\" has been used instead of \"transcription factor\". \n The paper tries to address an important problem in genomics: how to effectively embed DNA sequence for downstream tasks. They have used some recent transformer architectures and loss function to effective do this. Their main contribution, which is an important one, is to use the transcription factor information in the accessible genomic regions which are cell type specific. Therefore, the model would be more generalizable than the previous methods like DNABERT. The ablation study of the model and hyperparameters are comprehensive. However, some biological experiments and their importance are overstated. I provided some comments for the authors to enrich the experiments. I think the paper is interesting and if some comprehensive and important experiments are coupled with it to show that it can really find interesting biological events, it would be a great paper. As such, I choose the score 6 for the paper. "], "review_score_variance": 4.5, "summary": "While several reviewers acknowledge that the paper contains potentially useful ideas related to multi-modal self-training applied to genomic data, they also point out a number of weaknesses and room for improvement that the discussion with authors did not fully address. This includes in particular the need to better explain the details of what is done in the paper; the choice of experiments which is not relevant (eg, predicting promoter regions) or complete (eg, showing results on only one transcription factor); the lack of comparison with existing methods, etc... We therefore consider that the paper is not ready for publication in its current form, but hope that the reviews will help the authors work on a revision addressing the issues.", "paper_id": "iclr_2022_DSCsslei9r", "label": "train", "paper_acceptance": "Reject"}
{"source_documents": ["Consider the task of learning an unknown concept from a given concept class;  to what extent does interacting with a domain expert accelerate the learning process? It is common to measure the effectiveness of learning algorithms by plotting the \"learning curve\",  that is, the decay of the error rate as a function of the algorithm's resources (examples, queries, etc). Thus, the overarching question in this work is whether (and which kind of) interaction accelerates the learning curve. Previous work in interactive learning focused on uniform bounds on the learning rates which only capture the upper envelope of the learning curves over families of data distributions. We thus formalize our overarching question within the distribution dependent framework of universal learning, which aims to understand the performance of learning algorithms on every data distribution, but without requiring a single upper bound which applies uniformly to all distributions. Our main result reveals a fundamental trichotomy of interactive learning rates, thus providing a complete characterization of universal interactive learning. As a corollary we deduce a strong affirmative answer to our overarching question, showing that interaction is beneficial. Remarkably, we show that in important cases such benefits are realized with label queries, that is, by active learning algorithms. On the other hand, our lower bounds apply to arbitrary binary queries and, hence, they hold in any interactive learning setting.", " Thank you for the clarifying comments. These are indeed nice directions for future research.", " > *What are the precise technical differences between your proofs and those of [BHM+21] for the passive setting?*\n\nWe now explain the technical differences between our work and [BHM+21] in detail.\n\n* Upper bound (no infinite Littlestone tree): We use the extension of the SOA that guarantees a finite number of mistakes when the learner is playing against an adversary who uses a class $H$ with only finite Littlestone trees. This was developed in [BHM+21], but the reader does not need to be familiar with the implementation of the algorithm – we treat it as a black box. We utilize this subroutine to develop a novel *active* learning algorithm, which uses only label queries, and achieves arbitrarily fast learning rate. Both the algorithm and its analysis are very different compared to the ones that appeared in [BHM+21].\n\n* Upper bound (no infinite VCL tree): The first step in our approach is the same as in [BHM+21], i.e. we use the Gale-Stewart game that was developed in [BHM+21] to end up with partial concept classes that have a bounded VC dimension. We utilize some results from [BHM+21] which show that the partial concept classes that correspond to batches of a (labeled) dataset satisfy some important properties (see Lemma 5). Then, we aggregate these partial classes into a ``majority’’ class (see Lemma 7). From that point forward, our approach diverges from the one in [BHM+21]. We develop a novel interactive learning algorithm which uses binary queries on subsets of the unlabeled data in order to figure out the correct labels of, roughly, $2^n$ points using $O(n)$ such queries. Then, we feed these points to the one-inclusion graph predictor that guarantees linear error in the number of points. Since the points are exponential in $n$, we get the result. In [BHM+21] the authors feed the labeled dataset of $n$ points that they have access to directly to the one-inclusion graph predictor, which gives an error rate that is linear in $n$.\n\n* Lower bounds: Both of the lower bounds we present are technically more challenging to establish than the ones in [BHM+21]. This challenge stems from the fact that our bounds hold in the case where the learner has *full* access to the marginal distribution $P_{X}$. Thus, the instances which give rise to the lower bounds are more complicated compared to their counterparts in the passive setting. Moreover, our proofs utilize a connection between learning algorithms and coding schemes with low distortion rate, which was not necessary in the proofs in [BHM+21]. For example, to show the lower bound in the case where $H$ has an infinite Littlestone tree, [BHM+21] consider a random path in the infinite given tree, then they define the marginal distribution $P_X$ to be supported only on that path and the target labels are determined by the path. However, in our setting this approach does not work because we show a lower bound in the case where the learner has full access to $P_X$. Our first step is to pre-process the tree and make sure that all the nodes have a different $x$-instance associated with them. Then, we define a distribution $P_X$ that is supported on every node of the tree. We also pick a random path and the labels of the nodes that are on the path are also determined by this choice. The next step is to pick the labels of the nodes that are not on the path in a consistent way. The last step is to establish a connection between coding schemes with low distortion rate and interactive learning algorithms and leverage some information-theoretic lower bounds in order to show the desired lower bounds on the learning rates. There are similar technical difficulties that we need to overcome in the case of an infinite VCL tree.\nWe will add this explanation to the next version of our manuscript.", " We would like to thank the reviewer for finding our paper very well-written and our results significant. We will try to give more intuition about our algorithms and make them easier to grasp.\n\n> *The algorithms used in the proofs seem quite impractical. Do the presented results and conclusions have any concrete implications? Is it practically feasible to achieve the faster described rates with any computationally efficient algorithm?*\n\nWe would like to first address the comment regarding the practicality of our algorithms. Notice that our main result characterizes the query complexity of interactive universal learning and is general in the sense that it applies to every class without assuming any structure on it. Thus, inevitably, the algorithm we use is very abstract. We hope and believe that for specific natural classes it will be possible to find explicit and efficient algorithms, and leave this as an important direction for future research. Relatedly, for specific natural classes and distributions we believe one will be able to get explicit bounds on the query and sample complexities. We also believe that our approaches will inspire practitioners to come up with heuristics that perform better in practice compared to the state of the art. We will add a discussion to the next version of our manuscript.\n\n> *How vital is the assumption of realizability? Is this a technical simplification, or do the results completely break down in its absence?*\n\nThe techniques certainly rely on realizability, and we do not believe the same rates would hold for the agnostic setting.  Nevertheless, our work serves as a starting point for understanding the agnostic setting in future work.\n\n> *What are the remaining questions in this line of work?*\n\nWe believe that there are a lot of interesting questions that remain open in this line of work. These include characterizing the unlabeled sample complexity (i.e. is it possible to achieve optimal rates both in terms of the number of queries and the number of samples?), designing efficient and practical algorithms for natural classes, and studying natural types of interactions such as relative queries (e.g. comparison queries). Moreover, the question you raised about the agnostic setting is also important.\n", " We would like to thank the reviewer for their conscientious reading of our work and for finding our paper well-written, our algorithmic techniques novel and the theoretical contributions important. We will add a short discussion about Long’s paper to the next version of our manuscript. \n\n> *1. The learner can make any binary query*\n\nWe would like to first address the concern regarding the general interactive model we consider. We highlight that the algorithm which achieves arbitrarily fast rates requires only *label* queries on individual points, so it is captured by, arguably, the simplest interactive learning model, i.e. the active learning model. Moreover, the fact that the learner can use powerful queries makes our lower bounds particularly strong: in any natural interactive setting, the learner cannot achieve universal rate better than exponential if the class has an infinite Littlestone tree or better than arbitrarily slow if the class has an infinite VCL tree. We view our characterization as a first result in universal interactive learning and we hope and believe that it will lead to characterizations for more specialized models that allow more realistic types of queries.\n\n> *2. The learner has access to an unbounded number of unlabeled samples*\n\nWe focussed on the query complexity for simplicity, as this is the main theme of this work, and of most works in the literature on interactive learning. This is inspired by the fact that unlabeled data are usually considered abundant, while the labels are more expensive.  For this reason, we also did not attempt to optimize the dependence on the number of unlabeled examples in the proofs.  That said, a careful inspection of the proofs reveals that for u unlabeled examples, when there is no infinite VCL tree, our rate in terms of u would be 1/u (matching the supervised rate, in contrast to the exponential rate in the number of queries).  When there is no infinite Littlestone tree, our proofs do no improve over this, and we hope this may be improved in future work\n\n> *Is it the case that the rate of error decay with respect to unlabeled data is substantially worse than in its unlabeled counterpart? If so is it known this is necessary to the characterization?*\n\nWe kindly refer to the previous part of our response.\n", " > *The paper is overall very well written. The only thing is that it may distinguish the techniques in this paper from the priors. The algorithmic designs are very clearly presented but might be hard for the reader to compare with the prior works, e.g. BHM+21, without reading them.*\n\nWe would like to thank the reviewer for their conscientious reading of our work and for finding our paper very well-written. In the following, we explain the differences in the techniques between our work and [BHM+21].\n\n* Upper bound (no infinite Littlestone tree): We use the extension of the SOA that guarantees a finite number of mistakes when the learner is playing against an adversary who uses a class $H$ with only finite Littlestone trees. This was developed in [BHM+21], but the reader does not need to be familiar with the implementation of the algorithm – we treat it as a black box. We utilize this subroutine to develop a novel *active* learning algorithm, which uses only label queries, and achieves arbitrarily fast learning rate. Both the algorithm and its analysis are very different compared to the ones that appeared in [BHM+21].\n\n* Upper bound (no infinite VCL tree): The first step in our approach is the same as in [BHM+21], i.e. we use the Gale-Stewart game that was developed in [BHM+21] to end up with partial concept classes that have a bounded VC dimension. We utilize some results from [BHM+21] which show that the partial concept classes that correspond to batches of a (labeled) dataset satisfy some important properties (see Lemma 5). Then, we aggregate these partial classes into a ``majority’’ class (see Lemma 7). From that point forward, our approach diverges from the one in [BHM+21]. We develop a novel interactive learning algorithm which uses binary queries on subsets of the unlabeled data in order to figure out the correct labels of, roughly, $2^n$ points using $O(n)$ such queries. Then, we feed these points to the one-inclusion graph predictor that guarantees linear error in the number of points. Since the points are exponential in $n$, we get the result. In [BHM+21] the authors feed the labeled dataset of $n$ points that they have access to directly to the one-inclusion graph predictor, which gives an error rate that is linear in $n$.\n\n* Lower bounds: Both of the lower bounds we present are technically more challenging to establish than the ones in [BHM+21]. This challenge stems from the fact that our bounds hold in the case where the learner has *full* access to the marginal distribution $P_{X}$. Thus, the instances which give rise to the lower bounds are more complicated compared to their counterparts in the passive setting. Moreover, our proofs utilize a connection between learning algorithms and coding schemes with low distortion rate, which was not necessary in the proofs in [BHM+21]. For example, to show the lower bound in the case where $H$ has an infinite Littlestone tree, [BHM+21] consider a random path in the infinite given tree, then they define the marginal distribution $P_X$ to be supported only on that path and the target labels are determined by the path. However, in our setting this approach does not work because we show a lower bound in the case where the learner has full access to $P_X$. Our first step is to pre-process the tree and make sure that all the nodes have a different $x$-instance associated with them. Then, we define a distribution $P_X$ that is supported on every node of the tree. We also pick a random path and the labels of the nodes that are on the path are also determined by this choice. The next step is to pick the labels of the nodes that are not on the path in a consistent way. The last step is to establish a connection between coding schemes with low distortion rate and interactive learning algorithms and leverage some information-theoretic lower bounds in order to show the desired lower bounds on the learning rates. There are similar technical difficulties that we need to overcome in the case of an infinite VCL tree.\nWe will add this explanation to the next version of our manuscript.\n", " We would like to thank all the reviewers for dedicating time to read the work and for their suggestions and positive feedback. Indeed, the reviews raise important questions such as characterizing the unlabeled sample complexity (i.e. is it possible to achieve optimal rates both in terms of the number of queries *and* the number of samples?), designing efficient and practical algorithms for natural classes,  and studying natural types of interactions such as relative queries (e.g. comparison queries).\n\nThese research directions are beyond the scope of the main result in this work and addressing them seems to require new ideas and techniques. We think that the questions the reviewers raise demonstrate the potential impact the current work can have in inspiring future research directions in interactive learning.\n", " This work studies the universal learning rate for interactive learning, which, unlike the uniform learning setting that captures an upper envelope of the learning curves over families of distributions, concerns the best possible rate of convergence for every data distribution. It shows that interactive learning does have benefits over passive learning by showing much superior convergence rates in each category of the fundamental trichotomy. In addition, the bounds established can apply to arbitrary types of queries as long as they are binary.\n This paper considers a very important problem (in the modern machine learning community) of interactive learning and provides insights through the lens of universal learnability which are quite interesting and inspiring. For example, just by a mild rearrangement of the quantifiers in its definition, the learning rates characterize the learnability of problems from a totally different perspective. The authors were able to show the benefit of interactive learning in two different categories, that is, the optimal learning rate for different distributions can be arbitrarily fast rates and exponential, while in the passive learning paradigm, the rates were exponential and polynomial respectively. The different categories appear to be closely related to the dimensionality of the infinite Littlestone tree and the VCL tree. The authors propose algorithms together with rigorous analysis with techniques built upon some tools from the prior works that study passive settings, fitting into the interactive learning scenario.\n The paper is overall very well written. The only thing is that it may distinguish the techniques in this paper from the priors. The algorithmic designs are very clearly presented but might be hard for the reader to compare with the prior works, e.g. BHM+21, without reading them.\n No concerns.\n", " The authors study interactive learning in [BHMYH21]’s, universal learning model. Universal learning studies the asymptotic error rate of classification in a distribution-dependent fashion, in the sense that a class $H$ is learnable at rate $R(n)$ if for every (realizable) joint distribution over the data there exist constants $c$,$C$ such that $\\mathbb{E}[err(h_n)] \\leq CR(cn)$. This better models practical ML settings where a target distribution is fixed before the number of samples is chosen. In the interactive setting, the algorithm is allowed to ask arbitrary binary queries  (the learner may also use a finite but unbounded number of unlabeled samples), and $n$ denotes the number of queries rather than number of samples.\n\nThe authors show that interactive learning in the universal setting is characterized by the same trifecta as passive learning, but with strictly improved rates. Namely, they show that a hypothesis class satisfies exactly one of the three following \n\n1. Learnable at arbitrarily fast rates\n2. Learnable at exponential rate\n3. Learnable at arbitrarily slow rates\n\nFurthermore, they give a combinatorial characterization of which classes fall into these categories: classes with no infinite Littlestone tree satisfy 1, classes with no infinite VCL tree satisfy 2, an class with an infinite VCL tree satisfy 3. [BHMYH21] showed the same trifecta for passive learning, where the rate of 1 is exponential, 2 is linear, and 3 is arbitrarily slow. Thus the authors in this work fully resolve when and by how much interaction helps in the universal setting. This paper resolves the problem of characterizing interactive learning with arbitrary binary queries in the universal setting (a question in a sense first proposed in [BHW08]). This is an important theoretical contribution to the field of active learning and the study of the universal model. The techniques in the work are a mix of strategies developed in [BHMYH21]’s original study of passive universal learning (e.g. analysis of Gale-Stewart games) and novel algorithmic techniques such as reduction to interactive partial learning. The paper is well-written and the authors do a nice job of fitting their work into the literature (though it might be nice to also mention Long’s original work on partial learning “On agnostic learning with {0,*,1}-valued and real-valued hypotheses”). The work is likely to be of strong interest to the (theoretical) active learning community and of fair interest to the learning theory community at large.\n\nThe main weakness of the work lies in the very strong model of interactivity it considers. Namely the work makes two main assumptions that are atypically strong for the active learning literature:\n\n1. The learner can make any binary query\n2. The learner has access to an unbounded number of unlabeled samples\n\nLearning with arbitrary binary queries is of theoretical interest, but is not particularly practically relevant. Even the membership query model (which allows arbitrary label queries) is too strong to be of interest in most applications. Similarly, it is typically assumed in active learning that the error rate with respect to unlabeled data also exhibits reasonable decay (in particular with a similar rate to what is enjoyed in the passive setting). Indeed in the uniform setting this can always be achieved. It is unclear from this work whether these issues are inherent and greatly affect the characterization of interactive universal learning, or merely artifacts of the proof. Is it the case that the rate of error decay with respect to unlabeled data is substantially worse than in its unlabeled counterpart? If so is it known this is necessary to the characterization? Yes", " In this paper, the universal learning framework is studied. In universal learning, unlike in uniform learning, the goal is to obtain decay rates for learning curves where the constants are potentially distribution-dependent. This can lead to improvement as compared to the distribution-independent formulation of uniform bounds. In contrast to previous work on universal learning, which focused on standard passive learning, this paper considers interactive learning, where the learner is allowed to send binary queries to an oracle. This includes the active learning setting, where the learner requests labels of unlabelled data. For this setting, three regimes of learning rates are shown, which give potentially dramatic improvements as compared to passive learning results for universal learning. The paper is overall very well-written, and gives significant results in a pertinent area. While the extension from passive to interactive is in a sense straight-forward, the technical details are non-trivial and the proofs use new algorithmic ideas. While I did not go through the proofs in detail, everything seems sound and well-written based on a skim reading. The main body of the paper does a commendable effort in trying to convey the intuition of the proofs and results.\n\n\nIn terms of weaknesses, the paper could benefit from a discussion of practical aspects. The algorithms used in the proofs seems highly impractical in terms of actual learning tasks due to the computational requirements, such as running learning algorithms on all possible classifications of a set of unlabelled data. The intuitive description of the algorithms are still not entirely easy to grasp, but I understand if it is hard to simplify the presentation more. The algorithms used in the proofs seem quite impractical. Do the presented results and conclusions have any concrete implications? Is it practically feasible to achieve the faster described rates with any computationally efficient algorithm?  \nWhat are the precise technical differences between your proofs and those of [BHM+21] for the passive setting?  \nHow vital is the assumption of realizability? Is this a technical simplification, or do the results completely break down in its absence?  \nWhat are the remaining questions in this line of work?\n\n\nMinor comments:  \nLine 176 \"and and\"  \nMissing space on line 342   The authors adequately discuss the limitations. Possibly, the practical considerations of the limitations of realizability, iid, and access to the unlabelled data distribution can be discussed further."], "review_score_variance": 0.6666666666666666, "summary": "This paper provides a characterization of learning rates for interactive learning in the universal learning framework. All reviewers praised the novelty and quality of this submission.", "paper_id": "nips_2022_dTTKMy00PTJ", "label": "train", "paper_acceptance": "Accept"}
{"source_documents": ["Object counting and localization in dense scenes is a challenging class of image analysis problems that typically requires labour intensive annotations to learn to solve. We propose a form of weak supervision that only requires object-based pairwise image rankings. These annotations can be collected rapidly with a single click per image pair and supply a weak signal for object quantity. However, the problem of actually extracting object counts and locations from rankings is challenging. Thus, we introduce adversarial density map generation, a strategy for regularizing the features of a ranking network such that the features correspond to an object proposal map where each proposal must be a Gaussian blob that integrates to 1. This places a soft integer and soft localization constraint on the representation, which encourages the network to satisfy the provided ranking constraints by detecting objects. We then demonstrate the effectiveness of our method for exploiting pairwise image rankings as a weakly supervised signal for object counting and localization on several datasets, and show results with a performance that approaches that of fully supervised methods on many counting benchmark datasets while relying on data that can be collected with a fraction of the annotation burden.", "In this work, authors propose to learn to count objects in weakly supervised setting where only pairwise ranking information is used as supervision. The annotation cost is expected to be much lower than the widely-used dot annotation. Besides, an adversarial density map regularization method is proposed to enforce the output remain the properties of a density map. Strength:\n1. The weakly supervised setting could reduce the annotation cost and still achieve acceptable performance in several object counting benchmarks under specific conditions .\n\n2. The proposed adversarial density map regularization technique could further improve the performance.\n\nWeakness:\n1. Novelty: The main contribution of this work is to use ranking information in object counting, as explained by the authors, ranking information is already used in previous object counting work[1,2], this work only extends  it to inter-image setting, which requires extra annotation while the original intra-image ranking is purely self-supervised[1,2] without extra annotation information ( in [1,2] the supervised loss is also used but the ranking loss itself is purely self-supervised ). Therefore, the novelty of this paper seems to be trivial. \n\n2. Supervision: The pairwise ranking weak supervision can be highly unreliable in real world applications. For example for two images with both people and trees, and the first image contains both more people and trees than the other one, therefore, the ranking information can be vague as the model may not know which object to be counted, people or trees?\n\n3. Adversarial Density Map: the point maps are sampled uniformly thus does not really capture the location information, I am a bit surprised that such pseudo point map could really help as it does not bring extra information to the network.\n\n4. Annotation Cost: This pairwise ranking supervision has less annotation cost if the number of pair used is similar as the number of images in the dataset. However, if we have 500 images in the training set and there are can be up to 124,750 different pairs, then the most efficient way to obtain the ranking information is to annotate object number of all the images. Besides, if the object number is similar, it is hard to get the ranking information. Therefore, I think this setting does not generalize well in a broader setting.\n\n5. Experiments: Missing evaluation with dense crowd counting benchmarks, for example ShanghaiTech PartA, UCF_QNRF, NWPU and UCF_CC_50 that are often followed by previous work and could be a good example to show the generalization of the proposed approach. Without such experiments, we cannot judge the generalization of the proposed approach.\n\n\n\n\n\n[1]Xialei Liu, Joost Van De Weijer, and Andrew D Bagdanov. Leveraging unlabeled data for crowd counting by learning to rank. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7661–7669, 2018.\n\n[2]Xialei Liu, Joost Van De Weijer, and Andrew D Bagdanov. Exploiting unlabeled data in cnns by self-supervised learning to rank. IEEE transactions on pattern analysis and machine intelligence, 41(8):1862–1878, 2019.\n\n\n\n\n\n\n\n\n\n------------------------------Post Rebuttal Comments--------------------------------------------------------------------------------------\n\n\n\nI would like to thank the authors for providing the rebuttals, however, as explained in the post rebuttal comments, it does not solve my concerns and actually shows the limitation of this work. I therefore choose to keep my original ratings. This paper propose an interesting setting of pairwise ranking information as weak supervision to count objects. However, as I mentioned in the main review part, there are several limitations in both methodology and experiments. I hope the authors could address my concerns listed above and I think we could not accept this paper at current stage.", "This paper proposed an object counting and localization method which uses pairwise image ranking information for training. A count-based ranking objective is proposed for model optimization and an adversarial density map generation strategy is introduced to regularize the features of the network. The method was evaluated with multiple datasets. Strengths:\n\nThe idea of using count-based ranking to train a weakly supervised object counting and localization method is interesting. The authors attempted to analyze the model performance vs the annotation burden. The overall writing is clear.\n\n\nWeaknesses:\n\nThe comparison of the annotation burden is not convincing. Experiments were conducted under different settings. For example, annotation costs are referred from different works under different settings. The ranking cost should be relevant to the difference between two images. While, the experiment was conducted using randomly sampled image pairs. We don't know the count differences. Overall, the cost comparison needs more evidence.\n\nThe technical contribution is also limited.\n\n\n------------post rebuttal--------------------\nAfter reading the authors's rebuttal, I decide to keep my original score as 3. This work is interesting, but needs to be improved.\n\nFirst, reducing annotation cost and improving model accuracy are the key motivations for weakly supervised localization. It is important to have a fair comparison between the proposed pair-wise ranking cost and other existing annotation costs to justify the reason of using the pair-wise ranking. The current version does provide some comparison, but it is not convincing. Different experimental settings may lead to significantly different cost estimations. I would suggest the authors do some annotation cost study in a fair setting.\n\nSecond, the performance comparison with existing methods is not convincing. As mentioned earlier, two factors (annotation cost and model accuracy) are used to evaluate weakly supervised methods. But, the annotation costs are all estimated using different sources. Besides, not only the ranking cost, but also the model optimization might be different given different pairs of ranked images. This paper only provides one set of randomly sampled image pairs, but does not analyzes how the model performance will be affected by the randomness. The authors mentioned that \"Weber-Fechner law, which states that a human's ability to perceive a difference between the magnitude of two stimuli is related to the ratio between those stimuli\". This is for the annotation cost, but empirical experiments are needed to verify if this is the case for model accuracy as well. The idea of using count ranking is interesting. The annotation cost comparison is not convincing and the work is lack of technical contribution. ", "The paper propose a learning to rank surrogate approach as surrogate for the problem of counting and localization in dense senses. It is claimed that this labeling approach significantly reduces the labeling time and it sufficient to learn a dense map which can be used for counting and localization. In addition to the pairwise ranking loss, the paper use an adversarial training strategy to make the predictions look more like dense maps. The main contributions of the paper are important and interesting, the paper is well written and easy to follow, and the experiments seem to show the advantages of using the adversarial and rank loss. I still have few concerns that needs to be addressed:\n\n1) Using GAN to learn structuring the output is interesting and important aspect of the paper. My question is if this approach has been used before in any previous work? Specifically, using GAN to structure the predictions like heatmap or segmentation mask where the \"real\" examples for training the GAN are synthetically generated.\n\n2) I do not understand why only 2K pairs are sampled from each dateset? This is very small for all the datasets but specifically for Penguins with 82K images. If this is a saturation point for the accuracy it should be clearly mentioned in the paper otherwise the authors should provide the effect of increasing N in the final performance for some of these datasets.\n\n3) Related the point (2), I also think it is important to show the effect of reducing the training set size for the state-of-the art methods and see how much their performance drops if fewer images are labeled. At the minimum, author can compare their method to the state-of-the-art when both method uses datasets with equal *annotation time*. Only then one can judge if using weak ranking labels is beneficial for learning to count.\n\n4) What is the ratio of object counts in the sampled 2K pairs for each dataset (mean and std)? It is important to know this ratio is in the range in which human can easily do the ranking. Reviewer believes the paper has some interesting novelties but more ablation studies are necessary to support the points made in the paper. I am looking forward to reading the rebuttal and will adjust my score accordingly. I am willing to update my score to accept if the rebuttal addresses my concerns.\n\n\n----------------- Post Rebuttal -------------------------\n\nThanks for addressing most of my concerns. I increased my score to 6 with regard to the rebuttal. I think the paper could be stronger with more experiments including dataset size vs. accuracy plots which show where the accuracy plots saturate. \n\nMost of the points raised by the reviewers are just and important. However, I disagree with reviewer yR54 on the importance of using \"Adversarial Density Map\". As the reviewer mentioned, the problem of weakly supervised learning is typically ambiguous as the network might find to detect non-relevant features that still can decrease the final ranking loss. As this is also the case for other weakly supervised methods including object detection, I don't think the authors should be penalized for this. Instead, we need to focus on the novelties that can address some of these ambiguities. Adversarial density map loss put a meaningful constraint on the predictions to make them look like a mixture of gaussian among all the possible distributions that the network can generates. It is unlikely that it can solve all the ambiguities in the task but it should resolve some.", " Hello reviewer! Thank you so much for your detailed and insightful review! We would like to respond to the points you have raised.\n\nNovelty: With regards to the works of Liu, et al., which you referenced in [1,2], their work leverages the fact that, given an unlabelled image and a cropped portion of that image, the crop must have less-than or equal the number of objects when compared to the whole image (intra-image ranking pairs). However, this self-supervision is not the only source of supervision that they use; their method also relies on fully supervised density maps which capture target object locations. The authors readily admit that their method does not work without the training signal provided by the density maps and provide an experiment where they perform pre-training with the intra-image rankings and fine-tune the model using density maps. They then perform the same fine-tuning experiment with an ImageNet pre-trained model and find that the intra-image ranking model underperforms by comparison. This suggests that intra-image ranking is a poor supervisory signal in isolation. The authors identify that this setup leads to trivial solutions, as there is no specified target object that determines the ranking. For example, there may be a few really simple patterns in the full image that are also captured in the crop. Then, the model can simply re-identify those patterns. The novelty of our work is that we are able to extract object counts and locations using only inter-image rankings and without any density maps.\n\nSupervision: Consider your example with trees + people. There is only ambiguity if you have a single image pair. However, if you have N examples containing ranking constraints and image pairs, the only way for that ambiguity to persist is if the numbers of both trees and people occur in the exact same inequality (rank) across every single example where they occur. Imagine that you have access to some model that is able to detect every single instance of every object class within an image. The challenge, then, is to select relevant objects. As the number of examples N increases, we would expect there to be fewer ways to select irrelevant objects and also satisfy every ranking constraint where those objects occur. While the “tree + people” example exists at the semantic level, there is also serious consideration for simple spurious patterns that the model could use to cheat. However, applying augmentation seems to relieve this risk.\n\nAdversarial Density Map: The purpose of our novel method is for the discriminator to structure the output of the generator to look like a density map, which another reviewer has highlighted as an important aspect of the paper. Our experiments have demonstrated empirically that our method improves results. We have a few hypotheses about why this should help. First, it explicitly connects the ranking task to the detection task, and forces each detection to correspond to a single countable unit. Second, by uniform sampling object locations across the density map, we encourage the output of the generator to seek spatially diverse density maps. While the objects are not uniformly spatially distributed in practice, they are still spatially distributed. Suppose there is some trivial pattern that occurs within an image ranking pair, such as a man in a yellow shirt occurring in one image and a man with a red shirt occurring in another image. The model could learn a trivial solution by pooling density over the colored shirts. However, a discriminator which selects for uniform distributions will prevent the model from pooling density over trivial features. Thus, the ranking loss and the uniform distribution loss together encourage the model to select against irrelevant objects.\n\nAnnotation Cost: While it’s true that labeling images with their counts requires a smaller number of images to annotate in order to generate a target number of ranking pairs, there is a hazard involved, which is that using a smaller number of images implies a less diverse set of images used for training (even if the total number of target ranked image pairs is met).  For example, to collect 2000 ranked image pairs, we could either randomly sample 2000 image pairs from 500 images vs. explicitly counting the number of objects in 64 images to generate ~2000 (64*63/2=2016) image pairs. The former approach (our proposal) generates greater image diversity and reduces the risk of overfitting.\n\nExperiments: This is a good point. We agree that evaluation using these datasets would strengthen the paper. However, dense crowd counting (images with greater than 200 objects) is not the target for this method. But, we can run the experiments and include them in the experimental section to help quantify where the method works and where it fails. While this method does not necessarily generalize to dense crowd counting, we do have the option of dividing images to reduce the total number of objects per image.", " Hello Reviewer,\n\nThank you for your review and your insights! We would like to address the concerns you have raised.\n\nWith respect to the point about the technical contribution being limited, we would argue that this is a novel use of GANs on a novel version of the weakly supervised object counting problem. Further, we are the first to use inter-image ranking annotations to solve the weakly supervised counting problem. And we are attempting to tackle a fundamental underlying problem in the weakly supervised object counting space, which is that global labels underperform due to a lack of correspondence with local object features.\n\nTo your point about the annotation burden being unconvincing, it’s true that we are pulling different annotation costs from different sources. There is, at present, no comprehensive source that analyzes the burden under various setups. To compensate for this, we provide a range where available and draw attention to the fact that we are working with estimates from different sources. The point of the paper isn’t to explicitly prove something about the true underlying nature of the burden of these various annotation types. We simply seek to provide motivation for this version of the weakly supervised object counting problem. In the limiting case, ranking two images requires a user to count the objects in both images. So, the ceiling for the annotation burden for ranking is the cost of counting annotations. Given that the cost for the counting problem is an order of magnitude [1,2,3] greater than the floor for the ranking problem, the annotation burden estimates should be sufficient to motivate the problem and give users a ballpark for what to expect. We don’t know of many examples of Weakly Supervised Counting and Weakly Supervised Object Localization papers that provide any information actually comparing the annotation burden between their respective fully and weakly supervised annotations. So, we have gone a step beyond many of these works by providing estimates, backed up by literature, for our method.\n\nWith respect to the point about the relevance of the difference in the number of objects between two images, the more important measure is the ratio of the quantity of objects (not their absolute counts). This is known as the Weber-Fechner law, which states that a human's ability to perceive a difference between the magnitude of two stimuli is related to the ratio between those stimuli. We performed calculations for the Trancos and Penguins datasets. We will provide 3 metrics; the mean/std for the absolute difference for the object counts of the ranking pairs, the mean/std for the ratio of object counts between the smaller count and larger count, and the probability of selecting a pair that violates the Weber-Fechner ratio. Here, the Weber-Fechner ratio for object ranking is reported to be 10:11 or 0.91. For the Trancos dataset, we report a mean absolute difference of 15.3 objects with a std of 12.3 objects, and a mean ratio of 0.66 with a std of 0.20. Note that the ratio must be greater than 0.91 to violate the Weber-Fechner ratio. We also report a probability of 13.2% for selecting an image pair that violates the Weber-Fechner ratio. If we assume that an annotator presented with an image pair that violates this ratio will get 50% of these labels wrong, then we expect an error rate of 6.6% within the Trancos dataset. For the Penguins dataset, we report an mean absolute difference of 19.38 objects with a std of 20.0 objects, and a mean ratio of 0.52 with a std of 0.27. We also report a probability of 8.3% for selecting an image pair that violates the Weber-Fechner ratio and an expected error rate of 4.2%. To add further context, it is known that many popular image analysis datasets contain similar error rates. For example, it has been reported that the ImageNet test set has an error rate of 5.83% and the CIFAR-100 test-set has an error rate of 5.85% [4]. So the error rate would be comparable to other benchmarks, but with the context that errors for ranking pairs are made on examples that have similar object counts. So, 93.5 of the examples used during training contain image pairs that should be correctly labeled, regardless.\n\n[1] Hisham Cholakkal, Guolei Sun, Salman Khan, Fahad Shahbaz Khan, Ling Shao, and Luc Van Gool. Towards partial supervision for generic object counting in natural scenes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.\n\n[2] Pierre Pica, Cathy Lemer, Veronique Izard, and Stanislas Dehaene. Exact and approximate arithmetic in an amazonian indigene group. Science, 306(5695):499–503, 2004.\n\n[3] Justin Halberda and Lisa Feigenson. Developmental change in the acuity of the” number sense”: The approximate number system in 3-, 4-, 5-, and 6-year-olds and adults. Developmental psychology, 44(5):1457, 2008\n\n[4] Northcutt, Curtis G., Anish Athalye, and Jonas Mueller. \"Pervasive label errors in test sets destabilize machine learning benchmarks.\" 2021\n", " Hello Reviewer,\n\nThank you for all of the insight and feedback you have offered! We would like to address the 4 points that you have raised.\n\nTo your first point, we have yet to find a paper that uses GANs to structure the output of a network to have the properties of density maps using a synthetic distribution. The most similar work to ours is the work in [1], where the authors use a discriminator to incorporate priors about the structure of the human body into the output of a pose estimation network. Our work uses synthetic density maps to encourage the network output to look like a density map. Their work uses real pose maps to learn a prior about the relationship between body parts. So far as we can tell, our work is a novel application of GANs.\n\nTo your second and third points, we agree that we should include results for error rate and dataset size. The motivation behind choosing only 2K pairs was simply to minimize the experiment burden while demonstrating the effectiveness of the method. We had performed some experiments with this method previously and not found 2K pairs to be a saturation point, but we agree that the results would be stronger with this information. However, this method should not arbitrarily saturate at 2K pairs; imagine that we have a model which is already able to detect every instance of every object class within an image. Now, we merely need it to determine which of the detected objects are relevant. And suppose we have N ranking pairs which inform the model about which objects are relevant. If N is small then there are potentially many object selections that can satisfy the N ranking constraints. However, as N increases, the likelihood of violating a ranking constraint increases, which should select for models which ignore irrelevant objects.\n\nTo your fourth point, we performed calculations for the Trancos and Penguins datasets. We will provide 3 metrics; the mean/std for the absolute difference for the object counts of the ranking pairs, the mean/std for the ratio of object counts between the smaller count and larger count, and the probability of selecting a pair that violates the Weber-Fechner ratio. Here, the Weber-Fechner ratio is the ratio between magnitudes of a pair of stimuli necessary to recognize a difference, and for object ranking it is reported to be 10:11 or 0.91. For the Trancos dataset, we report a mean absolute difference of 15.3 objects with a std.dev of 12.3 objects, and a mean ratio of 0.66 with a std.dev of 0.20. Note that the ratio must be greater than 0.91 to violate the Weber-Fechner ratio. We also report a probability of 13.2% for selecting an image pair that violates the Weber-Fechner ratio. If we assume that an annotator presented with an image pair that violates this ratio will get 50% of these labels wrong, then we expect an error rate of 6.6% within the Trancos dataset. For the Penguins dataset, we report an mean absolute difference of 19.38 objects with a std.dev of 20.0 objects, and a mean ratio of 0.52 with a std.dev of 0.27. We also report a probability of 8.3% for selecting an image pair that violates the Weber-Fechner ratio and an expected error rate of 4.2%. To add further context, it is known that many popular image analysis datasets contain similar error rates. For example, it has been reported that the ImageNet test set has an error rate of 5.83% and the CIFAR-100 test-set has an error rate of 5.85% [2]. So the error rate would be comparable to other benchmarks, with the additional context that errors for ranking pairs are made on examples that have very similar object counts. So, randomly flipping these examples wouldn’t be expected to have a large impact on the success of the method.\n\n\n[1] Chen, Yu, et al. \"Adversarial posenet: A structure-aware convolutional network for human pose estimation.\" Proceedings of the IEEE International Conference on Computer Vision. 2017.\n\n[2] Northcutt, Curtis G., Anish Athalye, and Jonas Mueller. \"Pervasive label errors in test sets destabilize machine learning benchmarks.\" (2021)."], "review_score_variance": 2.0, "summary": "Even though reviewers found some responses by the authors satisfactory, several concerns regarding the paper still remain. The authors are strongly encouraged to:\n\n1) Explore how dataset size impacts accuracy.\n2) Reason about annotation costs via empirical experiments.\n3) Including benchmark datasets in experimental evaluations.", "paper_id": "iclr_2022_Y3cm4HJ3Ncs", "label": "test", "paper_acceptance": "Reject"}
