{"source_documents": ["Logistic regression training over encrypted data has been an attractive idea to security concerns for years. In this paper, we propose a faster gradient variant called quadratic gradient to implement logistic regression training in a  homomorphic encryption domain, the core of which can be seen as an extension of the simplified fixed Hessian. We enhance  Nesterov's accelerated gradient (NAG) and Adaptive Gradient Algorithm (Adagrad) respectively with this gradient variant and evaluate the enhanced algorithms on several datasets. \nExperimental results show that the enhanced methods have a state-of-the-art performance in convergence speed compared to the naive first-order gradient methods. We then adopt the enhanced NAG method to implement homomorphic logistic regression training and obtain a comparable result by only 3 iterations.", " I am glad to help with the questions in my paper.\nAnd I am very grateful for the time you and other reviewers spent reading my work.", " Thanks for the meaningful response. And after reading my colleague's comments, I decided to maintain my scores.", " We would like to thank the reviewers for their input and appreciate their comments.\n\nThe approximate Hessian proposed by (Kim et al., 2020) is very similar to the quadratic gradient but different in some ways: (a) they only used the diagonal elements of the fixed Hessian to build the approximate Hessian, together with a regularization term. They have to decide the parameter in this term. The quadratic gradient takes advantage of all the elements of the fixed Hessian. (b) (Kim et al., 2020) probably cannot be applied to numerical optimization. The underlying theory of quadratic gradient has been proved to meet the converge condition of the fixed Hessian method, so it might be probably used for general optimization problems (future work needs to be done to justify this claim). \n\nAlgorithm 1 is the pseudo-code to implement the enhanced NAG method in the clear. The algorithm in the clear is really the same as  the one on ciphertexts, from the logistic perspective. Moreover, the baseline work presented a detailed description of the algorithm incorporating operations on ciphertexts. We have no new contributions to the ciphertext packing and the ciphertext-computation process, so our first mission is to clearly represent the algorithm logistic behind the enhanced NAG. \n\n\nWe implement our privacy-preserving logistic regression training based on the open source code of the baseline method. Our learning-rate setting $\\alpha_{t+1} = (1 + \\sqrt{1 + 4\\alpha_t^2})/2  $ is based on the line 128 in their source code at: https://github.com/kimandrik/IDASH2017/blob/master/IDASH2017/src/TestGD.cpp \n  (the first author changed the link from https://github.com/kimandrik/HEML to https://github.com/kimandrik/IDASH2017). \nThe NAG method has many variants and the one that the baseline method used doesn't need to tune the learning rate very much. Although the authors claimed in their paper that they chose a learning rate $\\alpha_t = 10/(t + 1)$, we found no such setting in their open source code.\n\n\nWe don't know the reason why the enhanced Adagrad method had a different result in the iDash dataset. Our guess is that the enhanced Adagrad method might not perform well at the first several iterations because it has no enough prior knowledge to get a  good learning-rate setting (the raw Adagrad has a less serious problem with this) but catches up with the raw Adagrad after it has enough quadratic gradients to obtain its learning rate. It might be probably the case that the raw Adagrad performs very well in the iDash dataset and when the enhanced Adagrad method finally catches up, there is no much room for both methods to improve in their performance.\nWe should not have included the enhanced Adagrad method, which is not related very much to the topic of this paper and whose performance we don't fully understand. Maybe we should leave it to future work.\n\nFor a fair comparison, we adopt the same parameters of the HE setting as the baseline work.  The security parameter $\\lambda$ is determined by the parameters $\\log N$ and $\\log Q$ according to the security estimator of (Albreht et al. 2015). (Kim et al. 2018) derived a lower-bound on the ring dimension as $N \\ge \\frac{\\lambda + 110}{7.2} \\cdot \\log Q$ to get $\\lambda$-bit security level. Since HEAAN uses the complex number to facilitate the FFT computation, it needs another $N$ slots to store the complex conjugates. The formula is actually $N \\ge 2 \\cdot \\frac{\\lambda + 110}{7.2} \\cdot \\log Q$ for HEAAN.\nThe results with $\\lambda = 128 $ can be obtained if we set $\\log N = 17$ and $\\log Q = 1200$.\n\n\nIn table 2, that the auc score of Ours on nhanes3 is too low is probably because the nhanes3 is a too large dataset and only 3 iterations is not enough to achieve a decent auc score. We doubted it was a typo before and therefore implemented the bootstrapping in our code to perform more iterations, just to see the further outcomes. After many experiments, the results show that enough iterations enable the enhanced method to have a comparable auc score compared to the baseline method. Here is one running result: https://anonymous.4open.science/r/IDASH2017-245B/IDASH2017/Debug/No.vCPU2_RAM16GB_Datasetnhanes3_kdeg5_fold5_numIter17_nohup.out\n                         So we don't think it is a typo. \n\n\n\n(Albreht et al. 2015) M. R. Albrecht, R. Player, and S. Scott. On the concrete hardness of learning with errors. Journal of Mathematical Cryptology, 9(3):169{203, 2015 \n\n(Kim et al. 2018) Kim, M., Song, Y., Wang, S., Xia, Y., and Jiang, X. (2018b). Secure logistic regression based\n262 on homomorphic encryption: Design and evaluation. JMIR medical informatics, 6(2):e19.", " We would like to thank the reviewers for their input and appreciate their comments.\n\nOur method and the baseline method have very similar computation processes and our method adopts the ciphertext packing technique proposed by the baseline work. Therefore, the two works have very similar consumption of memory in the HE domain. \n\nFor some large-scale datasets such as the MNIST dataset, the calculation of $\\tilde B$ indeed needs to walk through the whole dataset.  However, in these cases, we usually adopt the mini-batch version of the NAG method rather than the full batch version and partition the large-scale dataset into multiple same-size batches. The mini-batch version of the enhanced NAG method would be an open future work.\n\nWe don't know the reason why the convergence speed of Enhanced Adagrad is worse than the raw Adagrad. Our guess is that the enhanced Adagrad method might not perform well at the first several iterations because it has no enough prior knowledge to get a  good learning-rate setting (the raw Adagrad has a less serious problem with this) but catches up with the raw Adagrad after it has enough quadratic gradients to obtain a better learning rate. It might be probably the case that the raw Adagrad performs very well in the iDash dataset and when the enhanced Adagrad method finally catches up, there is no much room for both methods to improve in their performance.\n\n\nThe SFH method, as an extension of the fixed Hessian method, has one main drawback that it cannot be used on some datasets like the MNIST datasets, in which case the simplified fixed Hessian is singular (The fixed Hessian method also has this weakness). This is one reason that the SFH method, as well as the fixed Hessian method, might not be applied to more general NNs.  Another important reason is that it might be difficult or even impossible to find a \"fixed\"  good lower bound of the Hessian matrix for the SFH method. On the other hand, the quadratic gradient can be constructed from the Hessian matrix directly, which doesn't rely on finding a fixed replaced matrix, and can be invertible in any case. This suggests that quadratic gradient might be able to be applied to more general NNs training like CNN, LSTM or Transformer. ", " We would like to thank the reviewers for their input and appreciate their comments.\n\n\nThe motivation why we proposed such a gradient variant is that quadratic gradient might unite the first-order gradient (descent) method and the second-order Newton's method, probably substituting and superseding the line-search method used in Newton's method. We think that the two enhanced methods might be super-quadratic algorithms (we might be wrong but we believe so) although we cannot prove it now. \nWe proposed the quadratic gradient with the hope of constructing something that possesses both the merits of the gradient descent method and Newton's method.  We think that quadratic gradient can enhance Newton's method and fill the gap between the gradient methods and Newton's method.\n\n\nWe did try to make some comparisons with the work [1]. However, [1] used the mini-batch version of NAG while we adopt the full batch version of NAG. Therefore, It might be difficult to design experiments to fairly compare the two different types of NAG methods. We would like to leave the mini-batch version of the enhanced NAG method to future work, in which we of course would like to take [1] as the baseline work. Python experiments have already shown that the enhanced NAG outperforms the original NAG even in the mini-batch version.  Our work and our baseline work are both based on a single model, whereas [2] is not. Therefore, it may be inappropriate to compare our work with [2].\n\n\nIt is true that the enhanced method in the iDash dataset of figure 1 is not robust and may be worse than the baseline method. we don't know the reason. Our guess is that the enhanced Adagrad method might not perform well at the first several iterations because it has no enough prior knowledge to get a  good learning-rate setting (the raw Adagrad has a less serious problem with this) but catches up with the raw Adagrad after it has enough quadratic gradients to obtain a better learning rate. It might be probably the case that the raw Adagrad performs very well in the iDash dataset and when the enhanced Adagrad method finally catches up, there is no much room for both methods to improve in their performance.\n\n\nThe enhanced NAG method has worse accuracy on the Edinburgh dataset probably because it can only be performed for 3 iterations while the baseline method was performed for 7 iterations. ", " This paper studied the problem of privacy-preserving logistic training on encrypted data. The quadratic gradient is proposed to implement logistic regression training in the homomorphic encryption domain. The proposed method can be seen as an extension of the simplified fixed Hessian and an enhancement of the Adaptive Gradient Algorithm (Adagrad). The implemented homomorphic logistic regression training obtains a comparable result by only 3 iterations. Strengths: \n\nThis paper studies an important problem. Privacy-preserving logistic regression training is one popular method to protect the training data privacy, especially when logistic regression is ubiquitous in the real world.\nThe paper provides extensive experimental details and implemented codes that are helpful for reproducibility. \n\nWeakness:\n\nThe writing is not clear to distinguish between the proposed methods and previous methods. Most of section 3 are previous works, instead of proposed methods. For example, section 3.1 about the Simplified Fixed Hessian method is actually from previous works. It is better to write it in a background section. Section 3.2 seems to be the proposed method. Thus, it should be highlighted and clarified using diagrams, and clear descriptions. More importantly, it is not clear to know the motivation why the authors propose a quadratic gradient. What are the observations and insights that we have to using quadratic gradient? \n\nTypos: for example: lines 183-184. Ower-->owner\n\nMore comparisons are needed. Although the authors mentioned [1], it is required to have a result comparison with [1] since the techniques are applicable to logistic regression training. It is also better to compare with [2]. \n\nThe results are not robust or generalized. For example, the enhanced method in the iDash dataset of figure 1 is not robust and may be worse than the baseline method. It would be better to explain more about the reason. Otherwise, it may show that the proposed method is not generalized to other datasets. Also, in table 2, the proposed method has worse accuracy than the baseline method on the Edinburgh dataset.  \n\n[1] Han, K., Hong, S., Cheon, J., & Park, D. (2019). Logistic regression on homomorphic encrypted data at scale. In Proceedings of the AAAI conference on artificial intelligence (pp. 9466–9471).\n\n[2] Bergamaschi, F., Halevi, S., Halevi, T., & Hunt, H. (2019). Homomorphic training of 30,000 logistic regression models. In International Conference on Applied Cryptography and Network Security (pp. 592–611).\n\n - -", " This paper presents a faster gradient variant to implement logistic regression training in a homomorphic encryption domain. The results show the new optimization method could speed up the convergence of training on small datasets. Strength:\n1. This paper is well-written and clearly organized, including sufficient theoretic analysis of the proposed Simplified Fixed Hessian method. I like such a solid line of work.\n2. The targeted topic – privacy-preserving domain of logistic regression is very promising and deserves insightful hard work to resolve the application of homomorphic encryption.\n3. The results of convergence speed are clear and convincing.\n\nWeakness:\n1. The experiments are somewhat not enough, considering the lack of experimental support for the claim in line 44 “without compromising much on computation and storage.” It will be better if a detailed comparison of computation and storage is presented.\n2. The algorithm may lack of generalization to some large-scale datasets, considering the calculation of $\\bar{\\math{B}}$ needs to walk through the whole dataset, which is quite resource-hungry\n3. Lack of further analysis of the results of the experiment, such as in Figure 1(a), why the convergence speed of Enhanced Adagrad is worse than the raw Adagrad.\n 1. In Figure 1(a), why the convergence speed of Enhanced Adagrad is worse than the raw Adagrad?\n2. I am curious about the consumption of memory in the HE domain, it will be better if the authors include a comparison between the proposed method and the baseline method.\n3. I like the idea of the second-order optimization and wonder whether the SFH method could be applied to more general NNs like CNN, LSTM or Transformer?\n Yes, the authors have included the limitations in line 119, which includes (a) the numerical condition of the SFH method and (b) the singular problem for some high-dimensional sparse matrix datasets.", " This study suggests an enhanced gradient method on logistic regression with homomorphic encryption (HE)\nThe contents of the paper generally follows (Kim et al., 2018), so for who read the paper the contents are familiar.\nThe others modified existing algorithm by replacing learning rate with quadratic gradient, which is a variant of simplified fixed Hessian.\nExperimental results show that the proposed method converges better than existing method in (Kim et al., 2018). Strengths\n- In table 2, It is empirically shown that the proposed method can achieve similar accuracy compared to (Kim et al., 2018) with much less computation time. (the ratio of the computation time is bigger than 7/3 for most datasets, which seems because bigger $logq=60$ was used for the proposed method)\n- By transferring the role of computing $\\tilde{B}$ to the data owner, the proposed method does not require more multiplicative depth compared to (Kim et al., 2018).\n- The paper seems clear and easy to follow.\n\nWeaknesses\n- The novelty seems rather shallow; (Kim et al., 2020) has already proposed an approximate Hessian which is very similar to quadratic gradient method. The authors should check the paper.\n- Algorithm 1 seems rather meaningless. The algorithm should incorporate operations on ciphertexts, which deal with ciphertext packing.\n\n(Kim et al., 2020)  Kim, M., Lee, J., Ohno-Machado, L., & Jiang, X. (2019). Secure and differentially private logistic regression for horizontally distributed data. IEEE Transactions on Information Forensics and Security, 15, 695-710. 1. Parameter setting is an important issue when using HE, because parameter search is extremely unfriendly with HE. In algorithm 1, the authors set $\\alpha_{t+1}=(1+\\sqrt{1+4\\alpha_{t}^2})/2$, which is different to the setting in (Kim et al., 2018), which is $\\alpha_t=10/(t+1)$. Can the parameter choice be justified?\n\n2. In the experiments, iDash dataset showed different results than other datasets. Can it be explained?\n\n3. Can the results with $\\lambda=128$ be given?\n\n4. In table 2, the auc score of Ours on nhanes3 is too low. Is it a typo? The authors have not address the limitations of their work. Because the study is about privacy-preserving logistic regression, it seems to have a positive societal impact.\n\nAs mentioned above, the biggest limitation of their work is that there is no significant improvement compared to the existing methods."], "review_score_variance": 0.6666666666666666, "summary": "Reviewers remained concerned about the novelty of the contribution, about the extent/limitations of experiments/comparisons to other methods, as well as about the fact that the method does not seem to outperform competitors in certain cases.", "paper_id": "nips_2022_6tRhLrki6b8", "label": "train", "paper_acceptance": "Reject"}
{"source_documents": ["Spherical CNNs are convolutional neural networks that can process signals on the sphere, such as global climate and weather patterns or omnidirectional images. Over the last few years, a number of spherical convolution methods have been proposed, based on generalized spherical FFTs, graph convolutions, and other ideas. However, none of these methods is simultaneously equivariant to 3D rotations, able to detect anisotropic patterns, computationally efficient, agnostic to the type of sample grid used, and able to deal with signals defined on only a part of the sphere. To address these limitations, we introduce the Gauge Equivariant Spherical CNN. Our method is based on the recently proposed theory of Gauge Equivariant CNNs, which is in principle applicable to signals on any manifold, and which can be computed on any set of local charts covering all of the manifold or only part of it. In this paper we show how this method can be implemented efficiently for the sphere, and show that the resulting method is fast, numerically accurate, and achieves good results on the widely used benchmark problems of climate pattern segmentation and omnidirectional semantic segmentation.", "Thank you for your comments. It is true that the present paper does not introduce a new framework like the paper by Cohen et al. However, although that paper contained a detailed continuous mathematical theory, as well as a discretized implementation of the idea for the icosahedron, it lacked an explanation of how exactly the method is to be implemented for manifolds that are not locally flat like the icosahedron. This is certainly not a trivial matter, as there are many ways one might derive a discrete algorithm from the continuous theory, all of which will have different characteristics in terms of runtime efficiency, numerical accuracy, and task performance. After careful consideration and preliminary experiments, we have settled on the convolution algorithm described in the paper, which is based on a non-trivial interpolation scheme.\n\nNevertheless, the reviewer is correct that this is not a theoretical paper. In order to strengthen the theoretical side of the paper, we have added appendix E, which contains a theoretical analysis of the equivariance of the regular non-linearity under SO(2) gauge transformations. It shows that pointwise nonlinearities, which tend to perform best, can be used for continuous groups and the number of samples can be selected to trade off computational cost with the equivariance error.\n\nWe have also worked to further strengthen the experimental section of the paper. We added comparisons to more prior work and an isotropic baseline to the Spherical MNIST; we demonstrate state of the art on molecular energy prediction; and demonstrate scalability on a high resolutions cosmology dataset - a task Fourier based spherical convolutions would fail at. We hope that the additional experiments added to the revised paper, such as the baseline using only scalar features and isotropic filters, help showing the strengths and weaknesses of various convolutional methods on the sphere.\n", "Thank you for your comments.\n\n1. We have fixed this typo, thank you for pointing it out.\n2. When an equivariant method uses only scalar features, it necessarily must use isotropic kernels. This can be seen from Case III in Proposition 1 of [1]. This limits expressivity and complicates the detection of orientable features, such as lines. We show this empirically with the Isotropic baseline in the MNIST experiment. The main motivation for using non-scalar features is that it allows for equivariant anisotropic kernels. Additionally, some datasets consist of non-scalar signals, like optical flows, SIFT-like features that measure local properties in different directions, wave polarization, or wind direction. In these cases one has no choice but to use non-scalar features in the input space.\n3. The higher computational complexity of S2CNN [2] prevents scaling up to high resolution grids, as noted by [3]. Our method does not suffer from this limitation. The fact that in the MNIST experiments, which use a relatively coarse grid, the runtimes are quite similar, is possibly due to the custom CUDA kernels used in [2]. We expect custom kernels can yield big improvements in the runtime of our method and are investigating possible implementations.\n\n[1] Kondor & Trivedi 2018\n[2] Cohen et al. 2018\n[2] Perraudin et al. 2019\n", "Thank you for your comments and kind words on the readability of our paper.\n\n- We agree that the regular non-linearities and log map could be clarified more and attempted to do so in the revised version. We have added a proof that shows in the limit of infinite samples in the regular non-linearity, we achieve full equivariance and provide bounds on the equivariance error for finite number of samples.\n- We have added a comparison to Kondor et al’18 to our MNIST experiment and further comparisons in the additional experiments.\n- We agree that the difference between the IcoCNN is marginal, except crucially for a task that directly tests equivariance, which is the NR/R experiment for rotated MNIST. This experiment shows that the icosahedral approximation leads to poor equivariance under SO(3) rotations. The other experiments don’t test for generalisation to arbitrary rotations and thus show less difference between the IcoCNN and our method.\n", "We thank the reviewers for their valuable comments, which we have taken into account in our revised version.\n\nFirst, we would like to stress the difference between our method and other equivariant spherical convolution methods:\n- Isotropy vs Anisotropy: Methods using only scalar features are restricted to using isotropic filters in order to be SO(3) equivariant, which includes graph methods such as [1]. Alternatively, one can pool over orientations immediately after convolution (as in [2]). In either case, however, detecting the direction of orientable patterns such as lines is complicated. We show this empirically in the isotropic baseline we added to our revised version (see below).\n- Computational Efficiency: Fourier based methods, such as [3] and [5], are automatically SO(3) equivariant, but scale poorly to high resolution grid, due to the nonlinear complexity of the Fourier transform and various difficulties implementing it efficiently in current deep learning frameworks. Additionally, they can’t be easily applied to grids on part of the sphere.\n- Icosahedral CNN: The Icosahedral CNN [4] is fast to compute thanks to the use of conv2d routines and exactly gauge equivariant and as a result automatically equivariant up to 60 discrete symmetries of the icosahedron. However, it is not equivariant to SO(3), while our model is fully SO(3)-equivariant. Additionally, Icosahedral CNNs assume a particular sampling grid whereas our Spherical CNN can admit arbitrary grids. This feature is particularly important when considering the fact that in many applications spheres are discretized differently than the one Icosahedral CNNs and others presume.  Finally, it is not straightforward to adjust Icosahedral CNNs to operate over partially observed spherical inputs in an efficient manner as our method.\n\nAll in all, to the best of our knowledge, our method is the first spherical convolution which is SO(3) equivariant, supports anisotropic filters, and scales to arbitrary high resolution grids.\n\nSecondly, the reviewers have pointed out that the experimental validation could be improved. To address this, we add two new experiments to the revised version. With these experiments, we would like to emphasize both scalability and flexibility aspects of our method using a different sampling strategy and larger dimensional spherical signals as well as cases where our anisotropic filters could potentially lead to better performance in comparison to isotropic counterparts. We list our experimental revisions below.\n\n- Additional Experiment #1 (Atomization Energy Prediction): In this dataset, we achieve state of the art compared to other sphere-based methods, in order to expand the experimental validation of our method. \n- Additional Experiment #2 (Cosmological Model Classification): The resolution of the signals in this problem are a few orders of magnitude larger in comparison to the existing spherical datasets.  Also, it requires a different spherical sampling scheme namely, Healpix. Thus we use it to demonstrate our approach's scalability and grid-agnostic aspects. \n\nIn addition to those, the following aspects have changed in the revised version:\n- We added a comparison to [5] in our Spherical MNIST experiment, as requested by reviewer #1.\n- We added a baseline to our Spherical MNIST experiment in which we use only scalar features and isotropic filters. The results show that anisotropic filters are important in this task, as mentioned by reviewer #2.\n- We prove a bound on the error to the gauge equivariance of the Regular NonLinearity in Appendix E, as requested by reviewer #1.\n- We clarified the log map, as requested by reviewer #1. We moved the sections regarding the spherical geometry to the appendix to make space for the additional experiments.\n- We added a figure of the icosphere with exponential/log map to visually support equations as requested by reviewer #1.\n\n[1] Perraudin et al. 2019\n[2] Masci et al. 2015\n[3] Cohen et al. 2018\n[4] Cohen et al. 2019\n[5] Kondor et al. 2018\n", "The paper proposes SO(3) equivariant layer, derived from the recently introduced Gauge equivariant CNN framework. The novel contributions are in taking the Gauge equivariance CNN and finding efficient ways to perform logarithmic mapping, parallel transport, and convolution by the equivariant kernel when applied to the sphere. An interpolation scheme for an improved approximation to global SO(3) symmetry is also discussed. Experimental results on spherical MNIST, climate pattern segmentation, and omnidirectional semantic segmentation demonstrate the usefulness of the proposed method for prediction on a sphere.\n\nFor the most part, the paper is very clearly written despite the challenging and technical nature of the topic. In particular, the first four pages provide a nice overview of related work and a clear explanation of the Gauge equivariance framework of Cohen et al’19. Two sections that can benefit from further clarification are the proposed \"regular non-linearities\" (where it would be nice to show in equation why we have equivariance), and equation (4), the logarithmic map (where I had a hard time mapping the discussion in words to the equation). \n\nHowever, the experiments, while satisfactory, are not impressive: one issue is that the results are mostly compared to the results of two relevant papers by Cohen and colleagues. In recent years, there have been other proposals for deep learning on the sphere, and I wonder why experiments do not try to compare with these works?  (see Kondor et al’18, Coors et al’18, and others cited in the paper.) Moreover, although in theory, the proposed framework improves the Icosahedral CNN of Cohen et al’19 (by directly operating on the sphere rather than an Icosahedral approximation), the practical improvements over the Icosahedral CNN seem to be often marginal (with one exception in spherical MNIST). Do you have any explanation for this? Is there any setup where you expect the proposed approach would give a substantial improvement?\n", "1. (p1) ``in almost in all cases\" to \"in almost all cases\" \n2.  (1.1) The authors could explain more about why we would want to consider tensor features. \n3. They conducted experiments on different datasets, including the MNIST dataset. They achieved good results comparing to baseline spherical CNNs. However, the advantage of this method over S2CNN can be further elaborated, as S2CNN already achieved high accuracy; it seems like the one improvement is the complexity (improved from S2CNN's $O(N \\log N)$ to their model's $O(N)$), but the reduction of complexity is not significantly reflected in the training time per epoch (from 380s to 284 s). \n4. Overall, the paper provides clear theoretical backgrounds on gauge CNNs that justifies their definition of convolution operator only uses the intrinsic structure of the manifold (does not reply on higher dimensional embedding).", "Cohen et al. recently proposed the \"Gauge equivariant CNN\" framework for generalizing convolutions to arbitrary differentiable manifolds. The present paper instantiates this framework for the case of the sphere.\n\nThe sphere is the simplest natural non-trivial manifold to try out gauge invariant networks on, and spherical CNNs have several applications. However, other than the details of the interpolation etc., there is really very little in this paper that is new relative to the original paper by Cohen et al., it reads a bit more like an extended \"experiments\" section. \n\nUnfortunately the experimental results are not all that remarkable either, probably because the tasks are relatively easy, so other SO(3) equivariant architectures do quite well too. Given that there is essentially no new theory in the paper, I would have welcomed a much more thorough experimental section, comparing different architectures, different discretization strategies of the sphere and different interpolations/basis functions."], "review_score_variance": 5.5555555555555545, "summary": "The paper extends Gauge invariant CNNs to Gauge invariant spherical CNNs.  The authors significantly improved both theory and experiments during the rebuttal and the paper is well presented. However, the topic is somewhat niche, and the bar for ICLR this year was very high, so unfortunately this paper did not make it. We encourage the authors to resubmit the work including the new results obtained during the rebuttal period.", "paper_id": "iclr_2020_HJeYSxHFDS", "label": "val", "paper_acceptance": "reject"}
{"source_documents": ["Though word embeddings and topics are complementary representations, several\n      past works have only used pretrained word embeddings in (neural) topic modeling\n      to address data sparsity problem in short text or small collection of documents.\n      However, no prior work has employed (pretrained latent) topics in transfer learning\n      paradigm. In this paper, we propose a framework to perform transfer learning\n      in neural topic modeling using (1) pretrained (latent) topics obtained from a large\n      source corpus, and (2) pretrained word and topic embeddings jointly (i.e., multiview)\n      in order to improve topic quality, better deal with polysemy and data sparsity\n      issues in a target corpus. In doing so, we first accumulate topics and word representations\n      from one or many source corpora to build respective pools of pretrained\n      topic (i.e., TopicPool) and word embeddings (i.e., WordPool). Then, we identify\n      one or multiple relevant source domain(s) and take advantage of corresponding\n      topics and word features via the respective pools to guide meaningful learning\n      in the sparse target domain. We quantify the quality of topic and document representations\n      via generalization (perplexity), interpretability (topic coherence) and\n      information retrieval (IR) using short-text, long-text, small and large document\n      collections from news and medical domains. We have demonstrated the state-ofthe-\n      art results on topic modeling with the proposed transfer learning approaches.", "Thanks for increasing your rating and leaning towards accept!\n\nThanks for acknowledging contribution of our proposed transfer learning approaches in topic modeling.", "Thanks for your reviews and positive comments, e.g., \"well written\". \n\nThe extensive experimental results (Table 5, 6 and 7) have shown significant improvements in terms of perplexity (PPL), topic coherence (COH) and IR scores using 7 datasets.  The improvements are EXPLICITLY mentioned in Tables 5, 6 and 7 (see \"Gain%\"). Also, see plots 2 (a,b,c,d,e), where our proposed model outperforms all the baselines at all the fractions in terms of retrieval precision.\n\nBeyond perplexity, we have also shown large gains in topic coherence scores due to improved topic quality and noticeable gains in precision for IR task.  \n\nFollowing are the 20 (some) EVIDENCES of significant improvements:\n\n\"Gain% vs DocNADE baseline\" (Table 5): \nOn 20NSshort: 10.9% (COH), 8.28% (IR)\nOn TMNtitle: 7.22% (PPL), 6.06% (COH) and 9.21% (IR)\nOn 20NSsmall: 37.9% (COH), 20.7% (IR).\nOn Ohsumedtitle: 4.01% (PPL) and 13.8% (IR) (Table 7)\nOn Ohsumed: 12.3% (PPL) and 4.35% (IR) (Table 7)\n\n\n\"Gain% vs DocNADEe baseline\" (Table 6): \nOn 20NSshort: 9.95% (COH), 8.84% (IR)\nOn TMNtitle: 4.60% (COH) and 7.04% (IR)\nOn 20NSsmall: 39.3% (COH). \nOn Ohsumedtitle: 17.3% (PPL) and 4.0% (IR) (Table 7)\nOn Ohsumed: 8.5% (PPL) and 4.91% (IR) (Table 7)\n\nAdditionally, #R4 and #R2 have acknowledged the noticeable gains achieved in this paper.", "Dear Reviewers,\n\nThanks again for reviewing our paper! We have responded to your queries and we are looking forward to discuss further. \n\nEven though there is NO negative/critical criticism, the ratings are NOT positive. We mostly found clarification queries that we have addressed in our response. We have also highlighted our contributions and SIGNIFICANT GAINS that our proposed methods achieved. \n\nWe would appreciate if the reviewers could participate in the rebuttal and raise further questions, if any.  \n\nAlso, we would acknowledge if you could justify your negative ratings or update them accordingly based on our response below.\n\nThanks!", "On the basis of existing topic modelling approaches, the authors apply a transfer learning approach to incorporate additional knowledge to topic models, using both word embeddings and topic models. The underlying idea is that topic models contain a global view that differs on a thematic level, while word embeddings contain a local, immediate contextual view. The combination of both local and global view transfer to enhance a topic model is the main contribution of this paper, especially when using multiple sources (therefore the title: multi-source multi-view transfer).\nGiven a document collection, DocNADE is used to generate the topic-word matrix. In the local view transfer step, the pre-trained WordPool is used, from which knowledge is transferred on the target document. The global view transfer is done by transferring knowledge from the pre-trained TopicPool to the target. As described in Algorithm 1 in the paper, both Word- and TopicPool are jointly used in the transfer learning process. \nFor evaluation, three different measures are taken into account: Perplexity, Topic Coherence and Precision (Information Retrieval). In comparison to a DocNADE only approach, all values are better in the settings that use the transfer learning approach. Compared to DocNADE + word embeddings, the results are competitive as well. In both experiments, the multi-source setting evaluates best overall.\n\nIn conclusion, the paper shows that exploiting multiple sources and views in transfer learning leads to an overall improvement in the given tasks. The main contribution is the usage topic models in a transfer learning framework. Additionally the use of multi-source word embeddings is novel too, especially in the joint setting with the topic model transfer. The paper shows how the DocNADE approach is enhanced to make use of both local and global view transfer and how this enhancement leads to improved performance on various related tasks. \nStill, the overall contribution is mostly in combining existing methods and can be judged as rather incremental.\n\nMinor note: A small mistake has been found in Table 5. The best perplexity value in the first column is not the bold 638, but the 630 in the local-view transfer setting.\n\nEdit after rebuttal: In my review I did not value the contribution of the transfer learning approach enough. So, when also considering the extensive evaluation I am now leaning towards accept.", "Thanks for your reviews, positive comments about \"novelty\" and acknowledging gains obtained by our proposed modeling.\n\nAs far as we know, this is the first/novel work that introduces:\n(1) single-source pre-trained topic embeddings,\n(2) single-source joint pre-trained word and topic embeddings, and  \n(3) multi-source transfers using pre-trained topics and word embeddings jointly in neural topic modeling under transfer learning paradigm.\n\nThis work DOES NOT focus on introducing a new topic model; however, we focus on introducing a novel transfer learning mechanism in neural topic modeling using complementary representations. Therefore, we have used the existing neural topic model, i.e., DocNADE to address data sparsity issues. \n\nThe experimental results have clearly shown noticeable gains in topic modeling due to the proposed transfer learning methodology using 7 target datasets from several domains (e.g., news, medical, etc.), evaluated using perplexity, topic coherence and information retrieval task.  \n\nThanks for the minor comment. We will correct it and will update the gain% as well. :)", "As far we we know, we have covered all the experimental settings, where we have clearly/individually shown contributions of each of the components. \n\nSee Table 5, 6 and 7, where the scores are reported due to: \n(1) only single-source word embedding transfer, i.e., LVT, \n(2) multi-source word embedding transfer, i.e., MST+LVT, \n(3) only single-source topic embedding transfer, i.e., GVT, \n(4) multi-source topic embedding transfer, i.e., MST+GVT, \n(5) single-source joint word and topic embeddings transfers, i.e., MVT=LVT+GVT, and \n(6) multi-source joint word and topic embeddings transfers, i.e., MST+ MVT. \n\nNotice that the topic-embedding transfer is performed via the regulalrization term. Also, mentioned in algorithm #1.\n\nWe are happy to answer if something is still not clear. Please point out precisely. ", "Thanks for your (emergency) reviews.\n\nThanks for your positive comments on experimental setup and acknowledging that our transfer learning approaches introduced in neural topic modeling clearly outperform several baselines.\n\n>> \"Word Embedding Alignment\"\nYes, we do.\nPlease see section 3, page 6 in \"Reproducibility\" paragraph (line 3). Also, mentioned in caption of figure 6 as well as in Appendix C.4 (the last paragraph).\n\nWe perform the word embeddings alignment in all the \"+Glove\" settings (Table 6) to \n(1) overcome the DocNADEe (baseline topic model) limitation (word-embedding size must be same as the number of topics), and \n(2) align vector spaces of word-embeddings obtained from several sources as well as from several different training processes, e.g., from Glove, FastText and word embeddings from topic models. \n\nThe focus of our work is to demonstrate the joint word and topic embeddings transfer in neural topic models from one or many sources. ", "The paper proposes a multi-source and multi-view transfer learning for neural topic modelling with the pre-trained topic and word embedding. The method is based on NEURAL AUTOREGRESSIVE TOPIC MODELs --- DocNADE (Larochelle&Lauly,2012). DocNADE learns topics using language modelling framework. DocNADEe (Gupta et al., 2019) extended DocNADE by incorporating word embeddings, the approach the authors described as a single source extension of the existing method.\n\nIn this paper, the proposed method adds a regularizer term to the DocNADE loss function to minimize the overall loss whereas keeping the existing single-source extension. The authors claimed that incorporating the regularizer will facilitate learning the (latent) topic features in the trainable parameters simultaneously and inherit relevant topical features from each of the source domains and generate meaningful representations for the target domain. The analysis and evaluation were presented to show the effectiveness of the proposed method. However, the results are not significantly improved than the based line model DocNADE. \n\nOverall, the paper is written well. However, it is not clear to me that the improved results are resulted due to multi-source multi-view transfer learning or for the better leaning of the single-source model due to the incorporation of the regularizer. \n\n\n", "This is an emergency review.\n\nThis work proposes a novel method to use pre-trained topic embeddings and pre-trained word embeddings obtained from various corpora in the transfer learning framework. \n\nTheir model architecture is based on DocNADE, unsupervised neural-network based topic model, and the authors propose two strategies to use pre-trained topic embeddings and pre-trained word vectors.\n1) Addition of a weighted sum of pre-trained word embeddings and the hidden vector of DocNADE.\n2) L2-Regularization term between topic embedding of DocNADE and pre-trained topic embeddings. They propose to align these two embeddings by multiplying align matrix \"A\" to the topic embedding of DocNADE.\n\nThey show the transfer learning performance of their model on various source/target domain datasets, including medical target corpora, and verify that their model outperforms on a short text and small document collection.\n\nStrengths.\n1. Comparison with the data augmentation baseline shows the performance gain is not only from bigger training data. Even though comparison with the naive baseline (data augmentation) seems too obvious, I think the results clearly show their claim about the importance of using transfer learning in neural topic modeling domain.\n2. As the first approach that introduces a novel transfer learning framework with pre-trained topic embeddings, they show tons of experimental results with various datasets and metrics to show the specification of their method. Their experimental setting is well designed.\n\nWeaknesses and comments:\nTheir method to combine pre-trained word embeddings and pre-trained topic embeddings is too simple. Since this is the first approach to use topic embedding in the transfer learning field, the simplicity of the proposed method is somewhat necessary. However, a weighted sum of pre-trained topic/word vectors seems not enough to transfer multisource knowledge. For instance, word vectors obtained from individual training processes do not share embedding vector space. As you apply the alignment method to topic embeddings from various sources, you should align word embeddings too."], "review_score_variance": 2.0, "summary": "This paper presents a transfer learning framework in neural topic modeling. Authors claim and reviewers agree that this view of transfer learning in the realm of topic modeling is novel.\n\nHowever, after much deliberation and discussion among the reviewers, we conclude that this paper does not contribute sufficient novelty in terms of the method. Also, reviewers find the experiments and results not sufficiently convincing.\n\nI sincerely thank the authors for submitting to ICLR and hope to see a revised paper in a future venue.", "paper_id": "iclr_2020_ByxODxHYwB", "label": "train", "paper_acceptance": "reject"}
