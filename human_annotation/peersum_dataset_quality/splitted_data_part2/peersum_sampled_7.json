{"source_documents": ["    The ability to identify influential training examples enables us to debug training data and explain model behavior. Existing techniques to do so are based on the flow of training data influence through the model parameters. For large models in NLP applications, it is often computationally infeasible to study this flow through all model parameters, therefore techniques usually pick the last layer of weights. However, we observe that since the activation connected to the last layer of weights contains \"shared logic\", the data influenced calculated via the last layer weights prone to a \"cancellation effect\", where the data influence of different examples have large magnitude that contradicts each other. The cancellation effect lowers the discriminative power of the influence score, and deleting influential examples according to this measure often does not change the model's behavior by much. To mitigate this, we propose a technique called TracIn-WE that modifies a method called TracIn to operate on the word embedding layer instead of the last layer, where the cancellation effect is less severe. One potential concern is that influence based on the word embedding layer may not encode sufficient high level information.  However, we find that gradients (unlike embeddings) do not suffer from this, possibly because they chain through higher layers. We show that TracIn-WE significantly outperforms other data influence methods applied on the last layer significantly on the case deletion evaluation on three language classification tasks for different models. In addition, TracIn-WE can produce scores not just at the level of the overall training input, but also at the level of words within the training input, a further aid in debugging. ", " Dear reviewer,\n\nWe hope that our response clarifies our motivation example 3.1 and the meaning of cancellation. If there are any remaining questions we hope to have a chance to answer before discussion deadline.", " Dear reviewer,\n\nWe would like to thank you for your feedback. We hope that our rebuttal has addressed any concerns you may have for our paper. If you have any unresolved concerns, please let us know so we could try to address them during the author-reviewer discussion period (ending following Tuesday).", " Dear reviewer,\n\nWe would like to thank you for your feedback. We hope that our rebuttal has addressed any concerns you may have for our paper. If you have any unresolved concerns, please let us know so we could try to address them during the author-reviewer discussion period (ending following Tuesday).", " Dear reviewer,\n\nWe would like to thank you for your feedback. We hope that our rebuttal has addressed any concerns you may have for our paper. If you have any unresolved concerns, please let us know so we could try to address them during the author-reviewer discussion period (ending following Tuesday).", " We thank the reviewer for the constructive reviews.\n\n– Unique logic VS shared logic:\n\nOne way to quantify the shared logic level of weight w is to measure the expected gradient similarity, E_xa, xb {COS_SIM [dl(xa)/ w , dl(xb)/ w]}, where COS_SIM is the cosine similarity. This measures the expected gradient cosine similarity between two examples. The expected gradient similarity for testing examples between different layers in the CNN classification are: first 0.035, second 0.075, third 0.21, last 0.23. This verifies that the latter layers in the neural network have more aligned gradients between examples, and thus share more logics between training examples.\n\n– ablation study for using special tokens only:\n\nThe AUC-DEL+ and AUC-DEL- for special tokens (CLS and SEP) only on AGnews is -0.029, 0.018, and the AUC-DEL+ and AUC-DEL- for special tokens (CLS and SEP) only on Toxic is 0.016 and 0.014. The special tokens alone perform much worse than TracIn-WE as they do not consider the information for overlapping words, validating the importance of overlapping words.\n\n– How do you control related variables to be consistent for different methods? Does the random seed of data sampler influence the model performance significantly?\n\nFor fair comparisons, we always do a random shuffle before the training of each batch. We also average the retraining result over 10-runs to reduce the randomness of the random seed (while we do not manually set the random seed in our experiments).\n\n– Is it essential to use gradient-based similarity as the similarity term:\n\nWe have tested using TF-idf similarity as the similarity term, but while it performs well on simpler tasks such as AGnews and Toxic, it performs very badly on mNLI (worse than TracIn-Last). Our interpretation of this result is that similarity terms based on gradients can better capture task-related semantic information , while similarity terms based on word similarity may be only useful when the task is highly-related to word similarity (AGnews and Toxic are very dependent on word similarity, but not mNLI). There might be other model-dependent similarity terms that make sense, but currently we do not know any other good similarity terms to get good influence scores.\n\n– The proposed method only works well when word embedding is fixed:\n\nJust to clarify, when we only want to find influential training examples without word overlap, both the proposed TracIn-WE and any other baselines (influence function, TracIn, Representer point) do not work well (see Fig. 4 in appendix) when word embedding is not fixed. On the contrary, when we want to find any influential training examples, the proposed TracIn-WE works very well under both fixing and training the embeddings settings. This hints that when the word embedding is trainable, sentences with no word overlaps do not carry strong influential signals. This is not a limitation of TracIn-WE but instead a phenomenon for all training data influence methods that we know of.\n", " We thank the reviewer for the constructive reviews.\n\n– example 3.1 – sparse representation VS dense representations:\n\nThe key point of Example 3.1 is that gradient updates from individual examples to the weight vector w are sparse, and are less likely to cancel each other out. On the other hand, gradient updates to the bias are likely to cancel each other out as all training examples update the bias. Similarly, updates to word (token) embedding parameters are also sparse in that each input sentence only updates the parameters of tokens present in the sentence. Concretely, if the vocabulary size is 25000 and the embedding dimension is 128 then the embedding matrix contains 25000 * 128 trainable parameters. For each input sentence with length 128, at most 128*128 weight parameters will be updated by the resulting weight gradients. Thus, each embedding parameter is only updated by 128/25000 of the training data on average. This is quite sparse (in terms of the vocabulary dimension 25000) in comparison to the bias parameters which are updated by all training data. Thus, the updates to the bias parameter would be subject to a stronger cancellation effect than updates to the embedding parameters.\n\n– potential vanishing gradient problem using gradient of word embeddings:\n\nWe do not observe gradient vanishing issues for word embedding gradients, mainly because modern deep neural networks (self-attention based architectures) contain residual blocks and ReLU activations to prevent gradient vanishing. If the model suffers from gradient vanishing, using later layers for data influence should have better deletion curve compared using the first layer as the gradient from the first layer may have reductive information; however, we observe that the first layer performs better than later layers for both self-attention models and CNN architectures, hinting that the gradient to word embeddings does not suffer from vanishing gradient.\n\n– It is not clear how the proposed method solves those fundamental problems (e.g., learning and loss relevance) when calculating the data influence based on the embedding layer.\n\nThe loss gradient of the word embedding layers contains the information of loss by chain law. As dL(x_t)/dw = dL(x_t)/df(x_t) * df(x_t)/dw, the loss gradient with respect to w contains a term that is solely based on the loss, and thus is relevant to the loss.\n\n– What is the interpretation of cancellation ratio? Why remove Bias for influence calculation?\n\nThe cancellation ratio is the ratio of A=|(sum of magnitude of influence caused by the weight parameters across training examples and testing examples)| and B=|(sum of influence caused by the weight parameters across training examples and testing examples)|.\n\nInterpretation of A:\nThe absolute total influence amount change caused by the weight parameters. If A is large, the weight parameter greatly affects the influence scores.\n\nInterpretation of B:\nThe absolute total estimated loss change of the testing examples caused the weight parameters after training. If B is small, this means that the weight parameters are not crucial to the change of loss.\n\nWhen the cancellation ratio is large (such that A >>> B), this means that a non-crucial weight parameter W greatly changes the influence score, which may not be ideal. Applying this interpretation to the cancellation of bias parameters, the intuition is that the bias parameters are not mainly responsible for the reduction of testing example loss change during the training process (since term B is small). However, they dominate the total influence strength due to their dense nature (term A is large). We could consider the following motivating example:\n\nAssume in the training process, the batch size is 2, and one positive example and one negative example are always guaranteed for each mini-batch of data. Assume that during the training, the bias parameter is always 0 since the positive example and negative example gradient for the bias will always cancel each other out. In this scenario, the bias should not have any impact on the loss change, and term B will be 0. However, term A will still be large, as the influence of the positive example and the influence of the negative example are both non-zero, but they cancel each other out so that term B is 0. This is an example where even though the bias does not affect the model at all, it affects the actual influence distribution, which leads to infinite cancellation score. The influence score change caused by the bias would not be meaningful, and should be removed.\n", " We thank the reviewer for the constructive reviews.\n\n– A straightforward extension lacks novelty: \n\n​​The main novelty/contribution of our paper lies in making two observations: (1) Using the last layer to calculate data influence suffers from cancellation and leads to an inferior measure of influence (as evidenced by our case deletion eval), and (2) Using the first layer mitigates the issue, and leads to a significantly better measure of influence (in some cases >10 times on the \"case deletion\" evaluation).  (1) is surprising as restricting to the last layer is the most popular choice for approximating influence computation and widely used, yet this issue has not be identified before. While the contribution (2) seems to be a straightforward extension, it is actually surprising as one does not expect the first (embedding) layer to carry high-level / semantic information, and thus such a simple extension has never been considered in the field. Fortunately, first layer gradients chain sufficient semantic information from higher layer layers to offer a meaningful measure of influence; see Table 2 for examples. We argue that the simplicity of our extension is actually a strength of the paper as it addresses a significant existing issue that was not identified before.\n\n\n—Have you ever tried to visualize the reducing cancellation effect for each layer?\n\nWe added a cancellation by layers plot in Fig.5 of appendix.\n", " This paper revisits the common practice of computing training data influence, especially methods for computing the influence only from the last layer and investigates improving the estimation of examples' influence.\nThe model size used in the NLP field is huge because we often incorporate a pre-trained language model as a base model.\nWe usually compute influence functions only from the final layers.\nThis paper also reveals that last layer representations in text classification models may suffer from the cancellation effect when computing the influence function on training samples.\nThis paper proposes a technique called TracIn-WE to mitigate the cancellation effect.\nTracIn-WE is a modification of the existing method, TracIn, to compute the influence of each sample on the word embedding layer instead of the last layer.\nThis paper conducts experiments on three text classification tasks and shows that TracIn-WE significantly outperforms other data influence methods applied on the last layer.\n Strengths:\n* This paper reveals the issues of existing influence computation methods by calculated from the last layer.\n* The empirical evaluation shows the better performance on the proposed method.\n* This paper tackles the important research theme involved in XAI.\n\n\nWeaknesses:\n* The method is actually a straight forward extension, so that the technical novelty is marginal.\n * Have you ever tried to visualize the reducing cancellation effect for each layer? This paper has limitation and potential negative societal impact sections in Appendix.\nThere are no additional concerns.", " The paper proposes TracIn-WE that will produce data influence scores based on the level of the overall training input and at the level of words within the training input. The method mitigates an effect called “cancellation effect” resulting from the data influence extraction methods based the last layer weights. The word-embedding layer based proposed method (TracIn-WE) is built on the existing method called TracIn which is constructed on the last layer.  Strengths: \n\nThe method is based on the observation of existing data influence method’s cancellation across the data influence attributions (i.e., distributed test data loss) to training examples, i.e., the sign of attributions across training examples disagree and cancels each other out.  This kind of observation might be useful to develop improved methodologies for explainable NLP.\n\nWeakness\n\n1.\tExample 3.1 is not clear to me as the example is based on a sparse representation whereas the word embedding inputs are dense representations. \n\n2.\tThe word embeddings provide the input representation. As a result, the similarity of inputs (train and test) would better capture by the embedding space. However, the data’s influence on a prediction is related to the output layers (last layer) which is closely linked to loss function. Using embedding space for data influence extraction has some limitations in terms of deep neural network training (e.g., vanishing gradient problem). It is not clear how the proposed method solves those fundamental problem (e.g., learning and loss relevance) when calculating the data influence based on the embedding layer.  \n\n3.\tBias plays different rules than other weight. Removing Bias from the TracIn Calculation to Reduce Cancellation Effect needs further justification. What is the interpretation of cancellation ratio?  \n What is the interpretation of cancellation ratio? \n\n They have discussed limitations.\n", " This paper thoroughly examines the effectiveness of different parameters serving as data influence measurement. It provides two important insights on the informativeness of parameters in lower layers and cancellation effect of parameters used by more training examples. Motivated by these insights, it develops a novel data influence method by replacing parameters of the last layer used in TracIn with embedding layer. Empirical results verify the insights and the effectiveness of the proposed method. **Strengths**\n* This paper provides two valuable insights: \n  * First, the gradients of the embedding weights capture high-level information due to gradient chain.\n  * Second, parameters used by more training examples suffer more from 'cancellation effect' (i.e. gradients of multiple instances have large magnitude, but different signs).\n* The proposed method takes both low- and high-level similarity into consideration. \n* Good writing and clear presentation.\n\n**Weakness**\n* The terms of 'share logic' and 'unique logic' are vague. Could you provide explanations and evidence?\n* There should be an ablation study for using special tokens only on full dataset to demonstrate the effectiveness of word overlap.\n* Removing data points also change the training process (i.e. schedule of batches). How do you control related variables to be consistent for different methods? Does the random seed of data sampler influence the model performance significantly? * Is it essential to use *gradient-based* similarity as the similarity term $S$?\n* Some typos:\n  * line 36: parametersin -> parameters in\n  * line 194: \"share\" logic -> \"share logic\"\n  * line 215: multiple periods.\n  * Appendix Figure 4: or -> on\n * The proposed method only works well when word embedding is fixed."], "review_score_variance": 1.5555555555555554, "summary": "This paper studies an important problem, of identifying influential training examples. It exposes a potential shortcoming in prior work, of focusing on the last layer, and proposes an alternative method. The approach cleverly assures looking at word overlap via overlapping word embeddings while still aggregating high level information from the back flowing gradients. The reviewers appreciated the topic, the insights and observations, and the empirical observations.  Overall this paper is making novel contributions to an important area. \n\nThere was some worry of novelty but I agree that the findings are novel and meaningful. The reviewers also raised various questions about vagueness of terms, which the authors addressed in the their response. There were also comments on missing controls and ablations, which the authors partly addressed in their response. To this, during the discussion, a suggestion was made to \"to add the variance of multiple runs or a significant test, since the robustness towards randomness is also very important to a reliable data influence measurement\". I strongly agree. \n\nSome technical questions by Reviewer iZ6x were answered. Please make sure to include the answers to them in the next revision, as well as all the clarifications and ablations provided in the author responses. \n\nFinally, I would strongly suggest adding experiments with at least one stronger model besides BERT, such as RoBERTa or DeBERTa. This would help give confidence that the approach is relevant for newer models. \n\nAC  ", "paper_id": "nips_2022_yfrDD_rmD5", "label": "train", "paper_acceptance": "Accept"}
{"source_documents": ["From a young age humans learn to use grammatical principles to hierarchically combine words into sentences. Action grammars is the parallel idea; that there is an underlying set of rules (a \"grammar\") that govern how we hierarchically combine actions to form new, more complex actions. We introduce the Action Grammar Reinforcement Learning (AG-RL) framework which leverages the concept of action grammars to consistently improve the sample efficiency of Reinforcement Learning agents. AG-RL works by using a grammar inference algorithm to infer the “action grammar\" of an agent midway through training, leading to a higher-level action representation. The agent's action space is then augmented with macro-actions identified by the grammar. We apply this framework to Double Deep Q-Learning (AG-DDQN) and a discrete action version of Soft Actor-Critic (AG-SAC) and find that it improves performance in 8 out of 8 tested Atari games (median +31%, max +668%) and 19 out of 20 tested Atari games (median +96%, maximum +3,756%) respectively without substantive hyperparameter tuning. We also show that AG-SAC beats the model-free state-of-the-art for sample efficiency in 17 out of the 20 tested Atari games (median +62%, maximum +13,140%), again without substantive hyperparameter tuning.", ">The k-Sequitur algorithm runs in linear time in the length of the presented action sequence. Hence, in computational terms it is easily feasible. Furthermore, the entropy regularisation deployed in the technique makes it more than a greedy compression technique.\n\nI think it's fairly clear that k-Sequitur does more than greedy compression, however my point was that I don't see a discussion about what this additional complexity buys to the policy learning process, and what the tradeoffs are of using Sequitur rather than - say - greedy search.\n\n\n>Instead, the main point that we want to raise is that the grammatical inference procedure obtains a hierarchical representation of actions. A key advantage of this symbolic procedure is the interpretability of such representations. For now, we leave this for future work.\n\nRight, but if this \"key advantage\" is not exploited (as far as I can see), then it is not an advantage at all, at least wrt this particular publication.\n\nThink about this issue from the perspective of someone that needs to build on your work: what is the \"simplest\" combination - of the ideas you have introduced - that shows the properties you have demonstrated through this method? What is the _scientific knowledge_ that one gains from reading your paper?\n\n\n>the simple moving average based heuristic has sufficed and reduces the complexity of the proposed algorithm. \n\nI think even just the fact that learning termination functions is a common HRL problem tells me that it is fundamentally important to deal with multi-stage policies, and it's unwise to present \"abandon ship\" without comparing it to previous work in the area.\n\nHowever, ultimately my main concern is that the heuristic is just that, a heuristic: it's bound to have corner cases and fail to generalise to interesting settings, and a proper evaluation of the system would include a discussion on failure cases and unexpected behaviour, which I don't really see in the manuscript?\n\n\n>We agree and have updated the manuscript to include a more detailed literature review, see section 2 of the revised paper.\n\nThank you for that, it looks better.\n\n\n>Yes, we agree. It is easier to infer effective macro-actions based on already successful on-policy rollouts.\n\nWould it be possible to add any experiment / analysis showing the degree of how much this matters?\n\n\n>And again, the agents do experience a significant speed up  in learning after the first grammar is inferred (see figure 4, performance after 100,000 transitions). \n\nRight, but *why* is that the case? Does it mean that the policies are just facilitated in exploration? Do the initial few macros still retain usefulness towards the end of the training stage? What is the evolution of the distribution in terms of action usage across these tasks?\n\nSample complexity is a poor way of analysing this sort of methods, since it's difficult to disentangle behaviour caused by task settings rather than properties of the methods, so the analysis would be better if it were to be augmented with some qualitative, method-specific, data.", "Dear reviewer 1,\n\nWe are very thankful for your comments and believe that multiple issues of importance are being raised. \n\nRegarding point 1: The k-Sequitur algorithm runs in linear time in the length of the presented action sequence. Hence, in computational terms it is easily feasible. Furthermore, the entropy regularisation deployed in the technique makes it more than a greedy compression technique. Instead, the main point that we want to raise is that the grammatical inference procedure obtains a hierarchical representation of actions. A key advantage of this symbolic procedure is the interpretability of such representations. For now, we leave this for future work.\n\nRegarding point 2: The relationship between abandon ship and termination policies is a very interesting observation. We have not attempted to learn the termination in an end-to-end fashion. Our current understanding is that this poses significant challenges to options (see concurrent work by Harutyunyan et al., 2019  https://arxiv.org/pdf/1902.09996.pdf) and it is not entirely trivial how to combat this additional non-stationary component. For now, the simple moving average based heuristic has sufficed and reduces the complexity of the proposed algorithm.  \n\nRegarding point 3: We agree and have updated the manuscript to include a more detailed literature review, see section 2 of the revised paper.\n\nRegarding point 4: Yes, we agree. It is easier to infer effective macro-actions based on already successful on-policy rollouts. We want to highlight that this provides a potential future research direction, i.e. skill distillation/imitation learning via action grammar inference. Furthermore and to address your point, the results of the Action Grammar SAC agents are obtained without pre-training. And again, the agents do experience a significant speed up  in learning after the first grammar is inferred (see figure 4, performance after 100,000 transitions). Finally, as already stated we have experimented with a tabular example in Towers of Hanoi where grammar macro-actions are also without pre-training - see new appendix item F.\n\nBest wishes,\nThe authors.", "Dear reviewer 3,\n\nWe are very delighted and thankful for your assessment. \n\nWe do agree that a detailed comparison with traditional HRL algorithms may be useful. During the development of this work we found it very challenging to do so under fair circumstances. Both Feudal Networks as well as h-DQNs require significant amounts of user-defined specifications/hyperparameters (such as sub-goals and hierarchy definition) and often may not be trained in a fully end-to-end fashion. Therefore, we decided to focus on an “ablation” comparison with DDQN and SAC with frame-skipping (i.e. the “naive” grammar of primitive actions that correspond to length 4 macro-actions). \n\nRegarding the use of macro-actions to improve sample efficiency: The baseline comparison as well as ablation studies try to address these issues and provide more insights. Could you be so kind as to clarify which aspects exactly remain unclear? \n\nFinally, yes, we do have preliminary results for a sparse rewards environment, namely for 5-disk Towers of Hanoi (see newly added appendix item F). The agent only receives a positive reward when achieving the final state. The results so far are only for the tabular case and without HAR or “Abandon Ship”. In our experience, the grammar macros not only propagate value information further back into the past, but also allow the agent to explore parts of the state space more efficiently.  We also believe that refining value estimates & efficient exploration are by no means orthogonal to each other. From the figure it also becomes apparent that the agent is able to amplify initial successful trajectories by encoding the action sequences in a grammar. Thereby, an action grammar provides an action representation & an effective form of memory.\n\nBest wishes & thank you for your time,\nThe authors.", "Dear reviewer 2,\n\nThank you very much for your time, consideration and detailed review.  \nWe apologize for any writing errors and have corrected the mentioned mistakes (see updated submission document). We fully agree that the HRL sub-field of maco-actions dates back a lot longer than the literature cited in this submission. We have now revised the paper to address this; see section 2 with literature comparison. Here is a small excerpt from the new addition:\n\n“[...]Identification of suitable low level sub-policies poses a key challenge to HRL. \nCurrent approaches can be grouped into three main pillars:\nGraph theoretic (Hengst et al., 2002; Mannor et al., 2004; Simsek et al., 2004) and visitation-based (Stolle et al. 2002) approaches aim to identify bottlenecks within the state space. Bottlenecks are regions in the state space which characterize successful trajectories. This work, on the other hand, identifies patterns solely in the action space and does not rely on reward-less exploration of the state space. Furthermore, the proposed action grammar framework defines a set of macro-actions as opposed to full option-specific sub-policies. Thereby, it is less expressive but more sample-efficient to infer.\nGradient-based approaches, on the other hand, discover parametrized temporally-extended actions by iteratively optimizing an objective function such as the estimated expected value of the log likelihood with respect to the latent variables in a probabilistic setting (Daniel et al., 2016) or the expected cumulative reward in a policy gradient context (Bacon et al., 2017; Smith et al., 2018). Grammar induction, on the other hand, infers patterns without supervision solely based on a compression objective. The resulting parse tree provides an interpretable structure for the distilled skill set.\nFurthermore, recent approaches (Vezhnevets et al., 2017; Florensa et al., 2017) attempt to split the goal declaration and goal achievement across different stages and layers of the learned architecture. Usually, the top level of the hierarchy specifies goals in the environment while the lower levels have to achieve such. Again, such architectures lack sample efficiency and easy interpretation. The context-free grammar-based approach, on the other hand, is a symbolic method that requires few rollout traces and generalizes to more difficult task-settings. . [...]”\n\nThe reviewer brings up the concern that the inferred grammar is crudely flattened into a straight hierarchy. Thereby, the notion of production rules & sub-policies are lost. We have a different view on this: Firstly, all of the production rules may easily be recovered and identified during execution time. Thereby, the interpretation of a grammar-inferred rule of temporally-extended actions does not get lost. Furthermore, as reviewer 3 has highlighted, a deep hierarchy of policies is not required in order to obtain an effective action space of temporally-extended skills. We also want to highlight the additional novel introduction of “Hindsight Action Replay” which we believe to be of general interest to the HRL community of its own merit. \n\nAll in all we hope to have addressed some of the productive comments and will attempt to address any further concerns and questions in future work. We thank the reviewer for all their input and advice, and hope that the body of follow-up work is going to come closer to our aspirations. \n\nBest wishes and again thank you for your time,\nThe authors.", "The authors propose a method for learning macro-actions in a multi-step manner, where Sequitur, a grammar calculator, is leveraged together with an entropy-minimisation based strategy to find relevant macro-actions. The authors propose a system to bootstrap the weights of these macro-actions when increasing the policy's action space, and a system to increase the amount of data (and bias it towards macro-actions) used to learn a policy for when conditioned on this increased action-space. The authors test against a subset of the Arcade Learning Environment suite.\n\nOverall, I'm conflicted by this paper. On one hand, the framework is interesting, and their method involves the usage and exploration of quite a few nice ideas; on the other hand, (a) the quality of the scientific contribution is hard to judge considering the significant differences between the proposed baselines and and their methods, and (b) the experimental section doesn't provide a lot of qualitative analysis and signal wrt. each component.\n\nFurthermore, I have the following issues / questions:\n\n1. I'm not convinced that the usage of Sequitur to build the macro-actions is sufficient to declare this work novel wrt. other macro-action papers. Sequitur usage in this case seems to be particularly overkill, since ultimately all that the method seems to be doing is finding frequent sequences of actions, which can be done quite fast (at least given the amount of training steps) simply using search and pattern matching. From my point of view, there doesn't seem to be a lot in that work that exploits the fact that the macro-actions are constructed as a \"grammar\" (beyond, maybe, HAR)\n\n2. The Abandon Ship heuristics is effectively a fixed termination policy, which makes the entire setup somewhat similar to options. In this case, what is traded is learning complexity for a hyperparameter and a significant restriction in how the macro-actions terminate. Did you attempt to learn this function at all? Do you have any insights / experiments that might show how the heuristics behaves with changing values of $z$? Would it be possible to plot the distribution of attempted vs executed move lengths rather than then averages (since I doubt they would be normally distributed)?\n\n3. Given points 1 and 2, the literature review is lacking - there's a lot of prior work done on macro-actions in both RL and robotics (planning, HRI, ...) that goes well beyond the few recent papers mentioned by the authors, and I think it might be necessary to mention work on options where the termination function is structured / biased in some way.\n\n4. I have some doubt the experimental setup for DDQN fairly gives a fair assessment of the method. When using a pretrained features, the problem becomes significantly easier, and thus AG-DDQN potentially doesn't need to deal with the problem of learning extremely bad / noisy macro-actions. I would love to see the method trained for a more reasonable amount of frames without pre-training. Also, did the 8 / 20 atari games get chosen randomly, or were they picked based on some environment features?\n\n5. How do the Q-values for the policy evolve with training time? The proposed methods seem to somewhat imply that the action space grows unboundedly, which might seriously destroy the policy for tasks that require much longer training. Would it be possible to add a paragraph about how the policies evolve in at least some of these environments? Are macro-actions used most of the times after some full iterations? How many <learning -> action distillation> iterations are actually done in the current experiments?\n\nAt this point, I cannot recommend the acceptance of this work, however I'd be willing to reconsider my rating if the authors address the above points.\n", "This paper introduced a way to combine actions into meta-actions through action grammar. The authors trained agents that executes both primitive actions and meta-actions, resulting in better performance on Atari games. Specifically, meta-actions are generated after a period of training from collected greedy action sequences by finding repeated sub-sequences of actions. Several tricks are used to speed up learning and to make the framework more flexible. The most effective one is HAR (hindsight action replay), without which the agent's performance reduces to that of the baseline.\n\nOverall, this paper could be a great contribution for the following reasons: \n1. The paper is well written, with clear performance advantages over the baseline. \n2. The paper provides a different perspective for HRL research, namely that we might not need to have a hierarchical policy to benefit from hierarchical actions that spans over many timesteps. \n3. From this paper's ablation study for HAR, it seems to suggest that even with similar experiences, one can get better performance by substituting actions with temporally abstracted actions, propagating value function errors further back in time. If so, this work can serve as a novel counterexample to the claim made in Nachum et al., 2019.\n\nThe authors may want to address the following:\n1. They may want to compare and contrast to other works in HRL that also does temporally abstracted actions. e.g. h-DQN, Feudal networks. Or even to repeating the same action N times-- a simple trick commonly used in Atari  -- which can be seen as a very naive form of action grammar.\n2. The main claim that having Action Grammar improves sample efficiency is not proved clearly. Apart from the ablation study, it's not immediately clear whether sticking to sub-sequence of actions are inherently beneficial for exploration, or that the agent somehow learned faster with the same set of samples collected.\n3. It seems that the algorithm may be the most effective in areas where a baseline algorithm can learn to perform at least some meaningful action sequences already. Otherwise the Action Grammar may not extract meaningful subsequences. Has the algorithm been tried on sparse-reward games?\n", "This paper proposes the use of macro (i.e. aggregated) actions to address reinforcement learning tasks. The inspiration presented is from hierarchical grammar representations, and the method is tested on a subset of Atari games. The paper is overall well written, although many paragraph demonstrate a level of polish inadequate for a top level submission (repetitions, typos, etc.).\nThe main idea pursued in the work is extremely interesting and with likely important implications to recent DRL. The concept though is far from new: a quick search for \"macro action reinforcement learning\" points to a NIPS '99 paper from J. Randlov, though on top of my mind there should be even older work on the topic.\nThe perspective proposed of considering macro actions as atoms in a grammar is certainly intriguing, but the work proposed does not develop the concept. The macro actions are identified as patterns in action sequences, then built in straight hierarchies, without any distinction in type of atoms nor any rule to effectively make up a grammar.\nThe related work section is extremely lacking, with no work older than 2016. The introduction presents more background, marginally older than that (up to 2012), when grammars make for an entire field of study with decades of history.\nThe process is interesting and incorporates plenty of useful experience, which I would personally be glad to see published, although in the current context is insufficient as stand-alone contribution.\nOn a more personal note, I suggest the authors not to get discouraged, as I strongly believe such an avenue of research is worthy investigating. A few research questions which I think should be asked are:\n- Are the agents actually learning to play the game? Just render the game with one of your best players. For example, achieving a score of 360 on Qbert barely takes constant down input, and the fact that comparable scores have been published before is of no support.\n- Are long action sequences always useful? For Qbert for example an average move length of 8 learned from an initial, untrained policy, is sufficient to get off the screen consistently. While the Abandon Ship protocol can mitigate this, the RL exploration phase is done by random action selection (consider explicit exploration instead), and the action space grows fast from the small initial 6 actions with the addition of all the macro actions, possibly limiting the exploration capability and biasing towards the use of longer macro actions even when sub-optimal.\n- Mitigate the claims. I would love to \"eventually help make RL a universally practical and useful tool in modern society\", but unfortunately I think no single contribution can today make such a claim.", "Dear Authors,\n\n\nI have thoroughly read through the paper.  It is quite interesting.  I have a few questions regarding the feasibility of the proposed method.  \n\nFirst, according to the ablation study presented in Fig. 5, it seems that only HAR brings impact on the curves.  The other methods presented in Fig. 5, in contrast, do not seem to provide significant improvements (e.g., action balanced replay buffer, abandon ship, transfer learning, etc.).  This ablation study seems to reveal that the action balanced replay buffer, abandon ship, and transfer learning approaches discussed in the paper do not actually affect the performance.   I am wondering if the authors could provide stronger experimental results and more detailed explanation to justify the necessity of these approaches?\n\nAccording to the paper, the proposed method only presents results for 0.3M.  For most contemporary DRL papers in the literature, the training procedure is typically performed for 10M or above, while 0.3M seems to be relatively short.  Please note that 0.3M time steps of training can not sufficiently represent the capability of a training method.  For many cases, learning curves rise after 1M or even 5M time steps (e.g., http://htmlpreview.github.io/?https://github.com/openai/baselines/blob/master/benchmarks_atari10M.htm).  For a fair comparison with the existing contemporary DRL approaches, I suggest the authors to extend the experimental results to 10M, which is more appropriate.\n\nThe third questions is regarding the action space.  Based on the statements presented in the paper, it seems that the action space of the agent grows with time (i.e., more and more macro actions are added to the action space of the agent.).  With a huge action space containing only  a constant number of primitive actions, it seems that the agent has a higher chance to select macro actions instead of its primitive actions.  I am wondering if the authors could provide the frequency of the macro actions used by the policy (after training)?  In addition, as the action space grows over time, the training difficulty also increases accordingly, indicating that the learning curves may become harder and harder to rise.   This is the other reason why I would like to request the authors to provide the training curves for up to 10M time steps to justify the effectiveness of the proposed methodology.  Moreover, it would be more appropriate to show the growing trend of the action space as well as the final size of it.  \n\nIt would be nice if the authors could address my concerns regarding the proposed approaches and experimental results presented in this paper.  \n\nThank you very much.\n\n\nBest regards,\nChristopher"], "review_score_variance": 8.666666666666666, "summary": "The topic of macro-actions/hierarchical RL is an important one and the perspective this paper takes on this topic by drawing parallels with action grammars is intriguing. However, some more work is needed to properly evaluate the significance. In particular, a better evaluation of the strengths and weaknesses of the method would improve this paper a lot.", "paper_id": "iclr_2020_SJxDKerKDS", "label": "train", "paper_acceptance": "reject"}
{"source_documents": ["Modern machine learning systems such as deep neural networks are often highly over-parameterized so that they can fit the noisy training data exactly, yet they can still achieve small test errors in practice. In this paper, we study this \"benign overfitting\" phenomenon of the maximum margin classifier for linear classification problems. Specifically, we consider data generated from sub-Gaussian mixtures, and provide a tight risk bound for the maximum margin linear classifier in the over-parameterized setting. Our results precisely characterize the condition under which benign overfitting can occur in linear classification problems, and improve on previous work. They also have direct implications for over-parameterized logistic regression. \n", " Thank you for addressing my concerns. I am keeping my score. ", " Thank you for your further comment and question.\n\n1. Regarding point 5, in your \"corrected\" condition above, the LHS should be squared, $|| \\mathbf{\\mu} ||_2^2 \\geq C || \\mathbf{\\mu} ||\\_{\\mathbf{\\Sigma}}$?\n\nSorry for the typo. It should be $|| \\mathbf{\\mu} ||_2^2 \\geq C || \\mathbf{\\mu} ||\\_{\\mathbf{\\Sigma}} $, as is used in line 527 in the supplementary material. To verify that this condition $|| \\mathbf{\\mu} ||_2^2 \\geq C || \\mathbf{\\mu} ||\\_{\\mathbf{\\Sigma}} $ is indeed scale invariant: Under our model assumption, $\\mathbf{x} = \\mathbf{\\mu} + \\mathbf{q}$, and $\\mathbf{\\Sigma} = \\mathbb{E} [  \\mathbf{q}  \\mathbf{q}^\\top ]$. Therefore when $\\mathbf{x} $ is multiplied by $c$, $\\mathbf{\\mu}$ is also scaled by $c$, and $\\mathbf{\\Sigma}$ is scaled by $c^2$. Therefore the LHS $|| \\mathbf{\\mu} ||_2^2$ is scaled by $c^2$. Meanwhile, the RHS $C || \\mathbf{\\mu} ||\\_{\\mathbf{\\Sigma}} = C \\sqrt{ \\mathbf{\\mu}^\\top \\mathbf{\\Sigma} \\mathbf{\\mu} }$ is also scaled by $c^2$. \n\n\n2. Regarding point 3, are there any known results on estimating the mean vector in the regime considered in this paper (e.g., $d \\geq n^2$  in the isotropic case)?\n\nWe would like to clarify that we do not need to bound the estimation error of the sample mean estimator directly in that regime. Instead, we only need to keep a “sample average” term when deriving the centered SVM predictor, and then bound the risk jointly. \n\nHere is some intuition. Suppose that the labels $y = +1$ and $y = -1$ correspond to the Gaussian distributions with mean vectors $\\mathbf{\\mu}_1$ and $\\mathbf{\\mu}_2$ respectively, and let $\\tilde{\\mathbf{\\mu}} =( \\mathbf{\\mu}_1 - \\mathbf{\\mu}_2 ) / 2$. For simplicity, let us assume here that there are exactly $n/2$ data points from each class. Then, we can consider a centered SVM on the modified dataset with the $i$-th input being\n\n$\\tilde{\\mathbf{x}}_i := \\mathbf{x}_i - \\frac{1}{n} \\sum\\_{i'=1}^n \\mathbf{x}\\_{i'} = y\\_i \\cdot \\tilde{\\mathbf{\\mu}} + \\mathbf{q}\\_i - \\frac{1}{n} \\sum\\_{i'=1}^n \\mathbf{q}\\_{i'}  =  y\\_i \\cdot \\tilde{\\mathbf{\\mu}} + \\tilde{\\mathbf{q}}\\_i.$\n\nFrom the above calculation, we can see that the distribution of the training data points $\\\\{ \\tilde{\\mathbf{x}}_i \\\\}\\_{i=1}^n$ can still be expressed in a similar form as in the paper. The major difference is that in this new setting, $\\tilde{\\mathbf{q}}\\_{i}$ and $\\tilde{\\mathbf{q}}\\_{i'}$ are correlated and no longer independent even when $ i \\neq i’ $. Nevertheless, many of the key lemmas in our analysis such as Lemma A.4 and Lemma A.5 can still hold because they actually do not require such independence. Some other lemmas may need to be reproved with extra efforts. \n\nThis setting is definitely interesting but beyond the focus of our paper. We are happy to briefly comment on it in the future work section.\n", " Regarding point 5, in your \"corrected\" condition above, the LHS should be squared, $\\lVert \\mu \\rVert_2^2 \\geq C \\lVert \\mu \\rVert_\\Sigma$?\n\nRegarding point 3, are there any known results on estimating the mean vector in the regime considered in this paper (e.g., $d\\geq n^2$ in the isotropic case)?", " Thank you for your constructive review. We address your comments as follows.\n\n1. How fundamental is the covariance matrix's trace lower bound assumption(s)? Why is trace meaningful here?\n\nOur analysis is based on certain concentration inequalities for the data Gram matrix, and the eigenvalues of the Gram matrix concentrates around $\\mathrm{tr}( \\mathbf{\\Sigma} )$ (Lemma A.4). Intuitively, $\\mathrm{tr}( \\mathbf{\\Sigma} )$ serves as a natural surrogate of the dimension in the anisotropic setting, which enables us to study the infinite-dimensional maximum margin classifier. This is the main reason that our theorems require such conditions on the trace of $\\mathbf{\\Sigma}$. Assuming lower bounds of $\\mathrm{tr}( \\mathbf{\\Sigma} )$ ensures that we are indeed studying the over-parameterized setting (e.g., $d \\gg n$).\n\n2. What kinds of covariance matrices fail the conditions on $\\mathrm{tr}( \\mathbf{\\Sigma} )$ in Theorem 3.1? Do they have a flat spectrum? Does the sample complexity n limit how flat the spectrum can be? \n(For example, Theorem 3.1 requires $\\mathrm{tr}( \\mathbf{\\Sigma} ) / || \\mathbf{\\Sigma} ||_F \\geq n$ amongst other constraints. Is this a reasonable way to interpret the hard constraint? What does this really say about $\\mathbf{\\Sigma}$?)\n\nGiven that the dimension $d$ is large enough, and that the largest eigenvalue of $\\mathbf{\\Sigma}$ is of constant order, the flatter the spectrum is, the easier for the condition in Theorem 3.1 to be satisfied. An example when the condition fails is when the eigenvalues of $\\mathbf{\\Sigma}$ decay very fast. For example, when $\\lambda_k = k^{-\\alpha}$ with $\\alpha > 1$, the condition cannot be satisfied for large $n$.\n\nIt is true that part of the condition can be interpreted as $\\mathrm{tr}( \\mathbf{\\Sigma} ) / || \\mathbf{\\Sigma} ||_F \\geq n $. Note that $ \\mathrm{tr}( \\mathbf{\\Sigma} ) / || \\mathbf{\\Sigma} ||_F $ achieves its maximum when all eigenvalues of $\\mathbf{\\Sigma}$ are equal. Moreover, when all eigenvalues are approximately equal,  $ \\mathrm{tr}( \\mathbf{\\Sigma} ) / || \\mathbf{\\Sigma} ||_F $ is of order $\\sqrt{d}$, in which case the condition can be interpreted as $d > n^2$.\n\n3. 4 and 5, Suggestions for the revision: \n- Remove the line about logistic regression from the abstract.\n- Mention in the body of the paper that there are experiments in the appendix.\n- Presentation of the proof in the appendix.\n\nThank you for your suggestions. We will revise the paper accordingly, and improve the presentation of the proof in the appendix.\n\n6. Add error bars in the experiments.\n\nThank you for your suggestion. We will add the error bar in the plots.\n\n7.  Typos and smaller confusions\n\nThank you for pointing these out. We will fix them in the final version.\n", " Thank you for your helpful comments. Our detailed responses to your questions are as follows.\n\n1. The results are somewhat incremental as compared to previous work on the subject\n\nCompared with previous work, our paper gives a tighter risk upper bound, and gives the first lower bound of its kind, which demonstrates that our risk upper bound is tight. Moreover, our result covers the anisotropic setting, and is proved based on certain novel proof techniques, including the application of the polarization identity to establish equivalence between the maximum margin classifier and the minimum norm interpolator.\n\n2. In the introduction, the presented results are not scale invariant. \n\nIn the introduction, for simplicity, the bounds mentioned in the first bullet of contributions (lines 50-56) are for the setting where $\\mathbf{\\Sigma} = \\mathbf{I}$. Therefore, you can roughly treat $d$ as $|| \\mathbf{\\Sigma} ||_F^2$. Based on this, you can see that these bounds here are indeed scale invariant, as scaling the input $\\mathbf{x}$ does not change the results.\n\nIn terms of the conditions presented in the third bullet (lines 60-68), they are derived for the specific setting where $\\lambda_k = k^{-\\alpha}$. In other words, we directly assume the scale of $\\mathbf{\\Sigma}$. Therefore these conditions are not meant to be scale invariant.\n\nWe will clarify this in the revised version. \n\n3. How do the bounds change when the data model is not centered, and the SVM classifier has an additional bias parameter.\n\nWhen the data model is not centered, we can either center the data using the sample mean, or introduce a bias term in the maximum margin classifier. In both cases, the analysis will be a little more involved, but we believe it won’t change the results significantly, and the rates in the risk bounds should remain the same.\n\n4. Formal definition of the sub-gaussian/exponential norms. Adding related references\n\nThanks for pointing it out. We will give the formal definition of sub-gaussian/exponential norms in the revision, and add the related references.\n\n5. Is the condition $|| \\mathbf{\\mu} ||_2 \\geq C || \\mathbf{\\Sigma} ||_2 $ in Lemma 4.4 correct? It is not scale invariant.\n\nThanks for pointing it out. This is a typo. This condition should be $|| \\mathbf{\\mu} ||_2 \\geq C || \\mathbf{\\mu} ||\\_{ \\mathbf{\\Sigma} } $. We will fix it in the revision. Note that this is only a typo in the lemma statement, and there is no need to change the proof, as the current proof of Lemma 4.4 is only based on the assumption that $|| \\mathbf{\\mu} ||_2 \\geq C || \\mathbf{\\mu} ||\\_{\\mathbf{\\Sigma}} $ (line 527 in the supplementary material).\n", " Thank you for your insightful comments. Below are our answers to your questions. \n\n1. Theorems are for the extremely over-parameterized setting ($d>n^2$ for isotropic data). Is it a necessary condition? \n\nIt is true that for isotropic Gaussians our theorem requires $d > n^2$. This is partly due to our proof technique that relies on the equivalence between the maximum margin classifier and the minimum norm interpolator. To our knowledge, our condition is among the mildest in the existing results on benign overfitting of maximum margin classifiers on sub-Gaussian mixture data. However, $d>n^2$ may not be a necessary condition for benign overfitting. In fact, we tend to believe that benign-overfitting can still occur under a mild over-parameterized setting (say, \nd≥100n). In order to show this, a different proof technique that does not rely on the equivalence result may be needed.\n\n2. Extension to the setting of deep learning\n\nOur current result is only for linear models and relies on the maximum margin predictor in the parameter space. It serves as the first step to study more complicated models such as neural networks, and we hope that the insights and some of the mathematical analysis can be carried over to deep learning. Given the empirically observed benign overfitting phenomenon in deep learning, there is a hope that similar results can be proved for deep learning.  In fact, for neural networks with homogeneous activation functions, it has been proved in [16] that gradient descent minimizing the logistic loss will also converge to the maximum margin solution, or more precisely a KKT point of a maximum margin problem. Therefore, intuitively speaking, the extension of our results to deep learning can potentially rely on the maximum margin predictor (minimum-norm interpolator) defined in neural network function space instead of the linear function space. \n", " Thank you for your supportive comments. We address your questions as follows.\n\n1. Related work on high dimensional Gaussian Mixtures.\n\nWe appreciate your suggestion. We will discuss these relevant papers in the revision.\n\n2. Reliance on the equivalence result. The $d > n^2$ condition under the isotropic setting... far from a complete picture about max margin classifier \n\nIt is true that our analysis relies on the equivalence between the maximum margin classifier and the minimum norm interpolator, and we need $d>n^2$ in the isotropic Gaussian mixture setting. Nevertheless, our condition is still among the mildest in existing results on benign overfitting of maximum margin classifiers on sub-Gaussian mixtures.\n\nWe agree that $n/d = \\mathrm{constant}$ is an interesting setting. However, we believe that the setting $d = \\omega(n)$ is equally interesting, as it represents the heavily over-parameterized case. We are aware of some recent papers studying the $n/d = \\mathrm{constant}$ setting under the assumption that the input data are generated from a single Gaussian distribution (e.g., arXiv:1911.01544). That analysis relies on subtle properties of random matrices. It is not clear whether a similar analysis can be applied to a much broader class of sub-Gaussian mixtures. Indeed, a complete picture that unifies the $n/d = \\mathrm{constant}$ setting and the $d = \\omega(n)$ setting is still missing. Our paper does not aim to give such a unified theory. Instead, we aim at giving a tight characterization of the risk for the maximum margin classifier in the $d = \\omega(n)$ setting. We will comment on the $n/d = constant$ setting in the revision and leave it as a future work direction.\n", "This paper studied the classification error of max margin linear classifier in high dimensional Sub-Gaussian mixtures. Tight upper and lower bounds are derived, which generalizes the prior works on Gaussian Mixtures.\n  This paper studied the classification error of max margin linear classifier in high dimensional Sub-Gaussian mixtures. This is an important problem, due to its connection to the implicit bias of first order algorithms, interpolation and the double descent phenomena. \n\nTight upper and lower bounds are derived, which generalizes the prior works on Gaussian Mixtures. Overall, this paper is well-written, the results are solid and I enjoyed reading it. A few comments are in order:\n\n(1) It might be better to expand the prior works section to include some relevant papers on high dimensional Gaussian Mixture. For example, the exp(- |mu|^4/(|mu|^2+d/n)) -type risk bounds also appeared in high dimensional Gaussian clustering literature, e.g.\n\nhttps://arxiv.org/pdf/1812.08078.pdf\n\nhttps://arxiv.org/pdf/2006.14062.pdf\n\nand many other references therein.\n\n\n\n(2) A major limitation I see from this work, is that its analysis heavily relies on the equivalence between max margin classifier and min-norm interpolator, see [19]. As the authors already mentioned in this paper (e.g. corollary 3.3), for example, in the text book example of isotropic Gaussian mixture with a constant separation, this can only happen when d > n^2, or the number of samples n is at most sqrt(d). In contrast, the n/d = constant setting is much more standard, and in certain sense more interesting. In my opinion, the results in this paper are far from a complete picture about max margin classifier - In other words, they are really about min-norm interpolator, which happens to be equivalent to max-margin classifier in the small sample regime (and therefore the title is a little bit misleading). \n\nOverall, my recommendation is weak accept - any comments and clarifications from the authors are welcome. -", "This paper proves new risk bounds for overfitting in the over-parameterized linear classification setting when the data is generated by a sub-Gaussian mixture. In certain regimes, the results show that overfitting can be benign, that is, overfitting still leads to a good test error. An important property of the proposed upper bound on the risk is that it is not directly dependent on the dimension, but rather on certain norms of the covariance matrix. This is useful because, as shown in the paper, if the eigenvalues of the covariance matrix decay fast enough, then requirements for benign overfitting become dimension independent.  This paper proves new risk bounds for overfitting in the over-parameterized linear classification setting when the data is generated by a sub-Gaussian mixture. In certain regimes, the results show that overfitting can be benign, that is, overfitting still leads to a good test error. An important property of the proposed upper bound on the risk is that it is not directly dependent on the dimension, but rather on certain norms of the covariance matrix. This is useful because, as shown in the paper, if the eigenvalues of the covariance matrix decay fast enough, then requirements for benign overfitting become dimension independent.\n\nThis paper is good step towards understanding benign overfitting, even though the paper only considers linear classification in sub-Gaussian mixture setting. Corollary 3.5 proves that even if the eigenvalues of the covariance matrix do not decay very fast, the requirement for small test error scale sub-linearly with dimension. In particular, as long as the the number of samples is a large enough constant, and the means of the classes are separated by $d^{1/4}$, the risk of maximum margin classifier is $o(1)$. \n\nOverall, the paper is well written, and the usefulness of the theorem is well illustrated with examples. The intuition is explained well and the theorem proof in the main paper is also helpful.\n\nMy first concern about the results is that the requirement $\\text{tr}(\\mathbf{\\Sigma})\\geq C \\max (n^{3/2}||\\mathbf{\\Sigma}||_2,n||\\mathbf{\\Sigma}||_F)$, which in the isotropic Gaussian case implies that the theorem requires $d\\geq n^2$. Thus, the result is only applicable to extremely over-parameterized setting. Is this requirement a necessary condition for such results? In the mild over-parameterized setting (say $d\\geq 100 n$), would overfitting no-longer be benign? This can be the case, for example, if we keep the data distribution fixed and keep increasing the number of samples $n$.\n\nAnother concern that I have is whether these results can be transferred to the setting of deep learning. In the linear classification setting considered in this paper, the results are essentially for the maximum margin classifier, with the motivation being that asymptotically, SGD converges to the maximum margin classifier. This is not the case for deep learning, where if we just benignly overfit the data, then there still are adversarial examples. Hence, overfitting in deep learning does not lead to maximum margin classifiers usually. Given that benign overfitting still leads to a good test accuracy for deep networks, an important question is whether the explanation offered by this paper is still valid for deep learning. Please see the main review.", "The paper studies risk bounds for the hard-margin SVM binary classification problem in the over-parametrized regime, where the number of samples is much smaller than the dimension of the instance space. It is assumed that the samples follow a distribution corresponding to a mixture of two sub-gaussian components, one component for each label. These components are assumed to have the same covariance matrix Sigma and opposing means mu and -mu. The main results of the paper are upper and lower bounds for the risk of the SVM classifier as trained on the data. By previous results, this also has implications to the minimum norm solution learned by gradient decent. The derived bounds are tighter than previous bounds and extend to the anisotropic setting. The lower bound derived matches the upper bound in some regimes of the parameters, suggesting optimality. They also discuss in some detail the role played by the eigenvalues of Sigma in the risk bounds, showing a nice separation to two regimes.  The paper is well written and easy to follow and the results seem technically correct. The results, however, are somewhat incremental as compared to previous work on the subject, hence my relatively low score.\n\nSome questions/comments: \n\n- In the introduction, the results are presented without the dependence on $\\Sigma$, stating rates that are not scale invariant as one would expect.\n\n- You assume that the distribution is centered. How does your bounds change when one also needs to estimate the center of the mixture, or equivalently, learn an additional bias parameter in the SVM classifier.\n\n- Please give the formal definition of the sub-gaussian/exponential norms, or at least state an appropriate reference where they are mentioned (line 104).\n\n- Is the condition $||\\mu||_2 \\geq C||\\Sigma||_2$ in Lemma 4.4 correct? It is not scale invariant.\n Yes.", "The paper studies maximum-margin classifiers in the setting where the dimension of the data is much larger than the number of samples. This data is assumed to come from a mixture of two subgaussians under a standard generative model. Notably, prior work shows that max-margin classifiers (i.e. Support Vector Machines) return the same estimator as the minimum-norm solution to an underconstrained least squares problem. The research mostly pertains to analyzing that least squares solution.\n\nThe paper focuses on providing sharper guarantees on when exactly the SVM is equivalent to the LS solution, and when this LS solution has low risk/misclassification probability. Their guarantees depend exclusively on the sub-gaussian data's true covariance matrix, the distance between the two subgaussian modes, and the number of samples. Notably, this does *not* depend explicitly on the dimension of the data. So, we can better understand how the spectrum of the covariance matrix impacts the misclassification probability.\n\nSome complementary lower bounds are given.  ### Overall Impression\nThis paper was a pleasure to read, and I confidently recommend it for publication.\nFurther, I verified much of the math in the appendix and found no errors.\n\n### Detailed Review\n\nThe setting of the paper is interesting. The focus on characterizing as-much-as-possible in terms of the covariance matrix is solid and pays off well. The structure of the paper is very clear and easy to read, providing intuitive interpretations of the core theorems along with a high-level overview of the proof.\n\nThe background information in the introduction is also well presented. While I am familiar with all the technical tools used in this paper, I had not read papers on benign overfitting in the past. Nevertheless, I was able to quickly understand the core ideas and felt comfortable reading this paper.\n\nI do have some questions and feedback for the authors:\n1. How fundamental is the covariance matrix's trace lower bound assumption(s)? You mention that the core theorems only assume the trace is large enough, but that the risk bound itself does not depend directly on this trace. Do we have reason to think this trace assumption is fundamental or necessary? Of all metrics, why is trace meaningful here?\n1. Intuitively parse the hard constraints on the trace of the covariance matrix in Theorem 3.1 [Line 167]. What kinds of covariance matrices fail this check? Do they have a flat spectrum? Does the sample complexity $n$ limit how flat the spectrum can be? [Lines 209 - 233] get close to discussing this, but fall short of explaining what covariance matrices / sub-gaussian structures fail this test, and how that depends on $n$.\n    - For example, Theorem 3.1 requires $n \\leq \\frac{tr(\\Sigma)}{\\\\|\\Sigma\\\\|_F}$ amongst other constraints. Is this a reasonable way to interpret the hard constraint? What does this really say about $\\Sigma$?\n1. Consider removing the line about logistic regression from your abstract. You don't really provide any new guarantee there.\n1. You should mention in the body of the paper that there are experiments in the appendix.\n1. The appendix is ordered in a hard-to-read way. I constantly found myself flipping back-and-forth between pages to remember what Lemma X.X refers to. Further, finding the proof of a specific lemma takes a while. Lastly, the order of proofs makes it hard to jump to the proofs that interest me. I would recommend a few changes:\n    - At the beginning of *every* proof in the appendix, fully restate the lemma/theorem/corollary.\n    - When you state a lemma without proof, link to its proof.\n    - List all defined symbols at the beginning or end of the appendix. There's a lot of symbols, and some like $\\varepsilon_\\lambda$ are defined within some lemma statements, which is easy to skip over on accident.\n    - Consider just reordering the entire appendix to be fully bottom-up? Just start with the lemmas that requires no other lemmas, and build up from there?\n1. All experiments in this paper would benefit from error bars. Consider highlighting the area between $25^{th}$ and $75^{th}$ quartiles on each plot? Figure 2(d) at $\\\\|\\mu\\\\|_2^2 = 4,6$ might be a bit non-linear, and some error bars would dissuade my concern, for example. It would also resolve the weird bump at $\\\\|\\mu\\\\|=2,\\alpha=0.8$ on Figure 2(b).\n\nSome typos and smaller confusions:\n1. [Line 64] \"our result shows that to achieve $o(1)$ population risk\" If $n$ is constant, what is this little-o depending on? $d$?\n1. [Line 293] The details are in Appendix A.1, not Appendix 4.1.\n1. [Line 319] Consider trying to unify the $Q=Z\\Lambda^{1/2}V^\\intercal$ notation with the content of [Line 120]. Maybe introduce the $Z$ notation there as an equivalent characterization? It's a little confusing as written, since this notation hasn't been introduced earlier, so it takes extra effort to verify. It would be more clear if I thought about this characterization when $Q$ was first defined and fresher in my memory.\n1. [Line 605] Consider justifying $\\\\|\\mu\\\\|_2^2 \\geq \\mu^\\intercal Q^\\intercal (QQ^\\intercal)^{-1} Q\\mu$, which I believe follows from $Q^\\intercal (QQ^\\intercal)^{-1} Q$ being a hat matrix?\n1. [Line 612] I don't think $v_i$ was defined anywhere.\n1. [Line 642] Unmatched parenthesis in the definition of $J_2$.\n1. [Line 733] \"the dependency of\" should be removed\n1. [Line 747] Should read \"holds\" not \"hold\" The limitations discussed are fine and sufficient. Admittedly, they are not particularly creative or interesting.\n\nSocietal Impact: Not applicable."], "review_score_variance": 1.5, "summary": "This paper is favored by two reviewers and is acceptable by all. Reviewers agree that it is clearly written and organized, even enjoyable to read, and that the results are solid on their own. Throughout the various exchanges between reviewers and authors, some valid suggestions emerged that may strengthen the paper, for instance:\n\n* Discussing the possibility that d > n^2 is required and situating that relative to assumptions in related work (from discussion with GwQP and nXks), namely highlighting the sense in which this is comparatively mild.\n\n* Considering commenting on how to approach an extension to data that is not centered (from discussion with 1YJ4). This may be more of an optional suggestion, but again may be useful to understanding the setup and analysis.\n\nOverall the paper makes a number of technical contributions to the actively developing topic of studying benign overfitting. Together with mostly favorable reviews, I recommend it for acceptance.", "paper_id": "nips_2021_ChWy1anEuow", "label": "train", "paper_acceptance": "accept"}
