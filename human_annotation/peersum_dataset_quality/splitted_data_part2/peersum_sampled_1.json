{"source_documents": ["Normalizing flows are invertible neural networks with tractable change-of-volume terms, which allow optimization of their parameters to be efficiently performed via maximum likelihood. However, data of interest are typically assumed to live in some (often unknown) low-dimensional manifold embedded in a high-dimensional ambient space. The result is a modelling mismatch since -- by construction -- the invertibility requirement implies high-dimensional support of the learned distribution. Injective flows, mappings from low- to high-dimensional spaces, aim to fix this discrepancy by learning distributions on manifolds, but the resulting volume-change term becomes more challenging to evaluate. Current approaches either avoid computing this term entirely using various heuristics, or assume the manifold is known beforehand and therefore are not widely applicable. Instead, we propose two methods to tractably calculate the gradient of this term with respect to the parameters of the model, relying on careful use of automatic differentiation and techniques from numerical linear algebra. Both approaches perform end-to-end nonlinear manifold learning and density estimation for data projected onto this manifold. We study the trade-offs between our proposed methods, empirically verify that we outperform approaches ignoring the volume-change term by more accurately learning manifolds and the corresponding distributions on them, and show promising results on out-of-distribution detection. Our code is available at https://github.com/layer6ai-labs/rectangular-flows.\n", "This paper addresses the problem of constructing injective normalizing flows, which connect some low-dimensional space with the data manifold of interest in the high-dimensional space. Typically injective (or rectangular) flows are composed of two square flows with the “upsampling” padding layer between them. Because the volume change term in the case of injective flows is seemingly intractable, current approaches find workarounds by training these two square flows separately step-by-step. Though this decision has certain drawbacks and may result in suboptimal performance. Current work suggests to employ numerical linear algebra methods to make volume change computation tractable. Experiments demonstrate that taking into account volume сhanging term improves the generation results both for synthetic and real-life data. Interestingly, it also improves out of distribution detection score, allowing the model trained on FashionMNItST to assign lower probability for the MNIST data samples.  - Paper is clearly written, exposition is coherent with all the  preliminaries and motivations for the technical decisions in place. Derivations are plain and comprehensible. \n\n- While injective flows as a class were described before, previous heuristics for implementing them were oversimplified and in the best case suboptimal. Thus making volume-change tractable for the injective flows seems to be an important step forward. \n\n- While (as stated in the paper) conjugate gradients were previously applied to the calculation of the volume change for the square flows, authors are the first to adopt them to the rectangular flows and Jacobian-Transpose Jacobian product calculation. Thus the novelty in this sense is moderate. Any comments?\n\n- Experiments on the synthetic and tabular data are convincing and clear demonstrate, that two-step method leads to learning incorrect probability distribution. On the Fashion MNIST FID improvements are quite notable. \n\n- Though, for me, the fact that increasing the number of steps and thus decreasing variance leads to worse results is a bit puzzling, and thus I would like to see more experiments in this direction, which would clarify this question.  Any comments?\n\n- Out-of-distribution experiments also are promising and hint that low-dimensional manifold is a important inductive bias. \n\n- Overall the result on the toy data and simple image data are quite convincing, though until it is verified on the real-life level of complexity data the broader significance is under question. Thus I would also l like to see some experiments on CIFAR to get sure.  Any comments? The authors adequately addressed the limitations.", " The authors addressed my comments. I think that despite the drawbacks mentioned by other reviewers, the paper has practical merits, and can be accepted. I decided to increase my score.", " I thank the authors for the detailed response to my comments.\nAfter reading the other reviews and the authors' rebuttal, I am keeping my score and recommend accepting the paper, provided that the authors will incorporate the changes in the revision as they stated in the rebuttal.", " We kindly remind the reviewers about our rebuttal, in which we believe we have addressed their main concerns, including results on CIFAR-10 and clarifying perceived inconsistencies of Figure 1. Please let us know if there are any additional questions, comments, or updated views on our work.", " Thank you for your review. Please see points 1, 2, and 3 in our general rebuttal for answers to several of the points you raised. We address the rest of your points below:\n1. On novelty: As mentioned in point 1 of our general rebuttal, we believe that part of the method and thus its novelty might have been misunderstood, particularly given some of the additional questions about jvps and vjps (which we address below). The novelty of the exact method does not come from doing an exact computation, rather it stems from \\textit{how} this exact computation is carried out. Naively using backpropagation (vjps) to construct the Jacobian for the exact computation is unnecessarily expensive, and we show that using forward-mode AD (jvps) -- which is not the default, nor even efficiently implemented in PyTorch -- results in the same exact computation being performed much faster.\n2. On AD methods not being clearly explained: first, we point out that we actually have a brief overview of AD, both forward- and backward-mode in appendix C, and we also cite [1], a very comprehensive survey on the topic. We compute both vjps and jvps in standard ways; these methods are well established within the literature and we believe that our explanations in appendix C are already more than usually presented when using these methods, and a thorough explanation of them falls outside of the scope of our paper. For vjps we use PyTorch's bakpropagation implementation, and for jvps we use our custom implementation. We achieve this by having layers and nonlinearities not only take $x$ as input, but also a vector $v$, and outputting $Jv$ in addition to the usual output.\n3. On lines 164-165: We believe that \"since our likelihoods are Radon-Nikodym derivatives with respect to the Riemannian measure on $\\mathcal{M}_\\theta$, different values of $\\theta$ will result in different dominating measures\" is a mathematically precise and unambiguous statement. The intuitive meaning of this is that the support changes as $\\theta$ changes, with the implication being that one should be careful when comparing likelihoods for different values of $\\theta$, even within the same architecture (which is why we did not compare against the baseline using likelihoods). A formal explanation of these concepts falls outside the scope of our paper, but we refer the reviewer to [3] for a formal treatment of probability, which includes Radon-Nikodym derivatives, and to [4] for a definition of Riemannian measures.\n4. On equation (12) recovering the correct gradient: The fact that the $J_\\theta^T J_\\theta \\epsilon$ terms are computed using vjps and jvps (with respect to inputs) does not imply that $J_\\theta^T J_\\theta \\epsilon$ is not computed exactly, or that its gradient (with respect to $\\theta$) cannot be computed; it is only specifying _how_ we compute $J_\\theta^T J_\\theta \\epsilon$. Thus, using vjps and jvps does in no way result in obtaining incorrect gradients with respect to $\\theta$. Once again, we refer to appendix C and to [1] for AD details.\n5. On the paragraph starting in line 323: As mentioned in the paper and also pointed out by reviewer __zGxK__, we hypothesize that RNFs can help remedy OoD issues through the correct inductive bias of having the intrinsic data dimension being lower than the ambient dimension. We believe this is a reasonable hypothesis, and is clearly stated as a conjecture, and a theoretical verification falls outside the scope of our paper. We also point out that this is actually the only conjecture we put forward in this paragraph, when we say \"this seems to be a property of RNFs rather than our training method\" we are only referring to the fact that baseline RNFs-TS also had strong OoD empirical performance. We will clarify this by publication time.\n6. On the KKT conditions not holding: We point out that the KKT conditions do hold. The constrained optimization objective is:\n\n  $$\n\\text{argmax} _\\phi \\sum _{i=1}^n \\left\\\\{\\log p _Z\\left(g _\\phi^\\dagger (x _i )\\right)  - \\log\\left|\\det \\mathbf J [h _\\eta]\\left(g _\\phi^\\dagger(x _i)\\right)\\right|-\\frac{1}{2}\\log \\det J _\\theta^\\top(x _i)J _\\theta(x _i)\\right\\\\}\n  $$\n\n$\\text{subject to} \\displaystyle \\sum_{i=1}^n \\left|\\left|x_i - f_\\theta\\left(f_\\theta^\\dagger(x_i)\\right)\\right|\\right|_2^2 \\leq \\lambda,$\nwhere $\\lambda > 0$ is a hyperparameter which implicitly defines $\\beta$ from equation (8) in the paper. Rather than treat $\\lambda$ as the hyperparameter, we treat $\\beta$ directly as the hyperparameter and optimize the Lagrangian (equation (8)), which is a standard use of the KKT conditions in machine learning. We will include this in the appendix, and point to [2] for a thorough treatment of the KKT conditions.\n\n[1] Automatic Differentiation in Machine Learning: A Survey, Baydin et al.\n\n[2] Constrained Optimization and Lagrange Multiplier Methods, Bertsekas\n\n[3] Probability and Measure, Billingsley\n\n[4] Intrinsic Statistics on Riemannian Manifolds: Basic Tools for Geometric Measurements, Pennec", " Thank you for your review. Please see point 3 in our general rebuttal for an answer to your question about failed runs on simulated data. We will incorporate all your minor comments in a revised version of the manuscript by publication, and address some of your other points below:\n\n1. On only learning densities supported on manifolds which are homeomorphic to $\\mathbb{R}^d$ being a notable limitation: While we agree this is a limitation of our method and believe this should be addressed in future work (all of which we mentioned in our manuscript), we think it's also fair to highlight that almost all current models suffer from similar topological mismatches. For example, GANs, by mapping $\\mathbb{R}^d$ through a continuous function cannot model supports with more than a single connected component (since continuous images of connected sets are connected). Similarly, all VAE and EBM methods we are aware of are $\\mathbb{R}^D$-supported, resulting in supports (trivially) homeomorphic to $\\mathbb{R}^D$, which we believe is even worse. The only work we are aware of considering these topological pathologies is CIFs [1], which provides a more numerically stable approximation of topologically-misspecified supports over baseline square flows, but nonetheless still results in a support defined over $\\mathbb{R}^D$.\nAs mentioned in our paper, we believe that potentially integrating CIFs within our framework to alleviate numerical issues arising from modelling the manifold as being homeomorphic to $\\mathbb{R}^d$ is a promising direction of future work.\n2. While indeed having higher manifold dimension makes our method more computationally expensive, most high-dimensional datasets of interest will exhibit strong low-dimensional structure, and often have significantly fewer factors of variation than they do features.\n3. We treated $d$ as a hyperparameter for MNIST, see Table 5 in the appendix for all the hyperparameters that we considered. In particular, for $d$, we considered 10, 15, 20, and 30. We chose these values based on typically-selected values for the latent dimension of VAEs, which we believe are an adequate proxy for the number of factors of variation, or intrinsic dimension, of the data. We will clarify this in the main manuscript.\n\n[1] Relaxing Bijectivity Constraints with Continuously Indexed Normalizing Flows, Cornish et al.", " Thank you for your review. Please see point 2 in our general rebuttal for answers to some of your experimental concerns. Could you please elaborate on what is meant by our results being \"not strcitly positive\"? We believe we have improved over the two-step baseline in every dataset we tried.", " Thank you for your review. Please see points 1 and 2 in our general rebuttal for answers to several of the points you raised. As to why increasing $K$ led to worse results, we first clarify that $K$ is the number of Hutchinson samples used in our stochastic estimator, and not the number of flow steps, in case this caused confusion (although perhaps this was not what was meant by \"increasing number of steps\"). We do explain this in line 314, but agree that it warrants more emphasis and will add it by publication: since we used $K=1$ to tune hyperparameters (providing the baseline with the same amount of tuning) \nand then used the same hyperparameters for $K=4$ and the exact method, the selected hyperparameters seem to favour $K=1$. ", " Thank you for your review. Please see points 1, 2 and 3 in our general rebuttal for answers to most of the points you raised. We would like to highlight that, while the baseline performed better than our method for OoD detection on FMNIST/MNIST, the results were close, and we used our exact, forward-mode AD-based method to compute log likelihoods for the baseline at test time. Thus, the baseline results for OoD detection are only realizable thanks to our contributions (indeed, [1] does not report OoD results).\n\n[1] Flows for Simultaneous Manifold Learning and Density Estimation, Brehmer and Cranmer", " We thank the reviewers for their comments and feedback on our work, as well as for finding it \"well-written, timely, and interesting\" (__Jpwi__), an \"important step forward\" (__zGxK__), and a \"solid contribution\" (__mod8__ and __Jpwi__). Below we address high-level points and questions shared by several reviewers, and answer more specific points to each reviewer individually.\n\n1. __On novelty__ (reviewers __mod8__, __zGxK__, __b6vS__): We respectfully contest claims that our only novelty is the application and not our logdet (and logdet's gradient) estimators. As we point out in the paper, logdet approximations and CG have been used before in the context of NFs, but not in the context of rectangular flows, which require additional contribution to be efficient. For example, our solution involving forward-mode AD results in the first reported experiments where the logdet term is included in the objective of injective flows. For a further example, the paper mentioned by reviewer __mod8__ [1] (which we will cite and discuss in the paper) does indeed approximate a logdet, but uses power series -- which as discussed in our paper, results in biased estimates -- and does not require the use of forward-mode AD, in addition to only being applied to square flows. We stand behind our claim that the careful use of AD, both for our exact and stochastic methods, is novel in the context of rectangular flows. In particular, we believe reviewer __b6vS__'s comment that the exact method is straightforward might come from a misunderstanding about how we use forward-mode AD (see the individual answer to reviewer __b6vS__ for more detail).\n2. __On experimental choices and results__ (reviewers __mod8__, __zGxK__, __1ptY__, __b6vS__): We have only compared against the method of [2] since our method is designed to train the exact same model with an improved objective. Fair comparisons are thus straightforward as the exact same architecture can be used, thus allowing to see the effect of changing the objective. Comparing against, for example [3] as suggested by reviewer __mod8__, would confound comparisons and thus not allow to compare the objectives themselves.\nFurthermore, the comment by reviewer __1ptY__ that we only compared on three datasets has neglected to acknowledge the four tabular datasets in section 5.2.\nThat being said, we thank the reviewers for their suggestions and agree there is some instructive value in other comparisons, and will include comparisons against square flows by publication time if asked by the reviewers, although we also note that [2] already showed improved performance of rectangular flows (with two-step training, over which we improve upon) over square flows.\nAdditionally, we have run our method on CIFAR-10 since submission, and found that our best configuration (using the exact method) outperformed the two-step baseline's best configuration in FID score, $643$ vs $731$ (as in our other comparisons, the same architectures are used and both methods receive comparable amounts of tuning).\nAlso, as per reviewer __1ptY__'s suggestion, we have ran $d=2$ and plotted the manifold and observed a clear structure of similar images being close-by. We cannot share images in the rebuttal, but will also include this by publication time, along with the CIFAR-10 results.\nFinally, reviewer __b6vS__ mentions \"unexpected behavior on MNIST/FMNIST\" and our results show a convincing improvement on both of those datasets; perhaps they can elaborate on what this behaviour is?\n3. __On perceived inconsistencies of Figure 1__ (reviewers __mod8__, __Jpwi__, __b6vS__): We do not believe that the results from Figure 1 are particularly inconsistent, neither for our method nor for the baseline.\nNeural networks have complicated optimization landscapes and sometimes fail, and we included failed runs in the appendix in the interest of scientific integrity, even if they are not representative of most runs. Of the $24$ runs we had for each method, ours correctly recovered the manifold $17$ times, and the correct density on it $12$ times (we visually assessed correctness); whereas the baseline recovered the correct manifold $15$ times and the right distribution on it _only_ $2$ times. Additionally, these $24$ runs were done over different hyperparameters (the same hyperparameters were considered for both methods), and it is thus to be expected that some of these are just bad hyperparameter choices.\nAdditionally, these failures were even rarer in other experiments. We will clarify this point in the manuscript by publication time, and will happily include all these runs in the appendix if requested by the reviewers.\n\n[1] Invertible Residual Networks, Behrmann et al.\n\n[2] Flows for Simultaneous Manifold Learning and Density Estimation, Brehmer and Cranmer\n\n[3] What Regularized Auto-Encoders Learn from the Data Generating Distribution, Alain and Bengio", "The paper addresses the challenge of learning a normalizing flow, under the assumption that the data probability function is concentrated on a low dimensional manifold. Utilizing the manifold assumption introduces a challenge in computing the volume change term. This paper suggests addressing this computational challenge in two ways: i) If the manifold dimension is low enough, it is reasonable to calculate the map differential exactly based on AD forward mode; ii) alternatively, an unbiased stochastic estimator for the log det can be used.\n  The paper addresses the challenge of learning normalizing flows. Previous work has recently suggested incorporating the manifold assumption into the normalizing flow [1].  The loss that follows from the manifold normalizing flows in [1] depends on a volume change term, a term that [1] did not compute directly. The current paper claims that there is an advantage in optimizing directly the loss involving the volume change term, and suggests two options to compute it. Even though there is not a lot of novelty in the techniques related to the suggested computations, I mark this paper as a solid contribution to the ongoing effort of learning normalizing flows and rate it with a positive rating. Detailed comments are provided next. \n\nGenerally, the paper is well written and easy to follow. The introduction and the method presentation are of good quality. I appreciate the author's effort to include all the relevant details to the method, in addition to some issues that were considered in devising the method. This is also reflected in the experiments section, where details about failed experiments are included.\n\nThe main contribution of the paper is that in fact the loss suggested in [1] can be computed directly in some cases or stochastically estimated. It seems to me that the stochastic estimation of the logdet is not novel, see [2]. Also, the exact computation is based on the forward mode AD, which is a known technique. \n\nI am concerned about the inconsistency in the results of figure 1. Do the authors have some thoughts on what can be done? This should be in the main text. \n\nThe experiments show an advantage over [1], except for the OOD experiment. However, why other alternatives have not been considered? In table 3 it would be important to see results from other square flow models (without the manifold assumption), or contractive auto-encoders (that are known to approximate the derivative of the data log probability function [3]). As the current method cannot run on more complex datasets like CIFAR, these additional comparisons can some light if this line of works is worth pursuing. \n\n[1] : Flows for simultaneous manifold learning and density estimation, Brehmer and  Cranmer.\n[2] : Invertible Residual Networks, Behrmann et al.\n[3] : What Regularized Auto-Encoders Learn from the Data Generating Distribution, Alain and Bengio, Yes", "This paper is motivated by the desire to circumvent a problematic requirement in Normalizing Flows (NFs) on the function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^D$ that produces the pushforward measure --- namely, that $f$ must be strictly invertible. This is difficult for generative models, since the dimension of the latent space $d$ is usually much smaller than that of the target data space $D$, i.e. $d << D$. The invertibility requirement centers around the change-of-variables formula, which is crucial for computing the cost function in NFs, and which requires computing the Jacobian of $f^{-1}$. To enable the use of NFs on latent/data spaces where $d << D$, the authors propose \"Rectangular\" NFs (RNFs). The name comes from the fact that the Jacobian of $f$ is now rectangular in the $d << D$ case. This involves the use of a differential geometric variant of the change-of-variables formula, and is much more computationally taxing, but provides superior density estimation performance to previous work.\n  The authors explore a very natural and compelling question that arises from the invertibility requirement (of the aforementioned $f$) in NFs, when applying NFs to generative modeling. \n\n$f$, which can be thought of as the \"decoder\" or \"generator\" network, often maps from some lower dimensional latent space to a higher dimensional data space. If $f$ and the latent space are suitably well behaved, it would be reasonable to assume that, even though $f$ is not invertible, its restriction to the data manifold $\\mathcal{M}$ could be. Intuitively, if $f$ is injective, this suggests that there does still exist a tractable way to compute the pushforward density, and one wonders if there is a more general case of the change of variables formula that can be used.\n\nTo this end the authors demonstrate that, while this general form does exist, the computation of its terms (namely, the inverse of the Jacobian) is unfortunately quite heavy and numerically unstable, and moreover, stochastic estimations of this Jacobian via random projections is noisy. \n\nThough these are not strictly positive results, they highlight important obstacles to further progress in an area of very active research. Moreover, the work is presented in a lucid and understandable manner, and the limitations of the current method are clearly delineated. For these reasons, I am inclined to accept the paper. There is limited experimental validation of the proposed technique. The authors compare their method to only one other work, on three datasets: a synthetic circle dataset, MNIST, and FMNIST (they mention having tried CIFAR10 and SVHN, but were unable to tune their model in a reasonable amount of time). Even with MNIST and FMNIST, it could be informative to train an NF with a 2-dimensional latent space and visualize manifold to see if some structure of the dataset were preserved.\n", "The paper presents new methods for estimating densities using rectangular normalizing flows. Rectangular normalizing flows, unlike squared normalizing flows, allow considering injective instead of bijective flows. This property is exploited for estimating the density of data residing on a low (d) dimensional manifold in R^D, for D>d. Existing works using rectangular flows to estimate densities supported on low dimensional manifolds assume that the manifold is known, and therefore are restricted to tori, spheres, etc. In contrast, this work does not make this assumption and shows that density estimation for injective flows based on ML is tractable. Two methods are proposed, which are based on incorporating and estimating the “Jacobian-transpose-Jacobian” in the objective. One is based on an exact derivation, which is computationally demanding. The other is based on stochastic gradient estimates. Experimental results on a simulation and a couple of datasets are shown, demonstrating the advantage of the proposed methods compared to an existing method as well as the tradeoff between memory and variance incurred by the two proposed methods.  This is a well-written, timely, and interesting paper. It addresses an important problem and presents a solid contribution. \nThe work has several notable limitations:\n- It can only learn densities supported on manifolds homeomorphic to R^d. \n- It is computationally expensive and feasible only when the manifold dimension is low.\n- Partly as a result of the limitation above, the experimental study is not impressive as it includes only relatively simple examples. \nI commend the authors for clearly putting these limitations forward in a nice discussion in Section 6, which also highlights some intriguing results and the gained insights. \nDespite its shortcomings, I believe the paper improves existing methods and advances this line of work, and I would be happy to see it published.\n\nMinor comments:\n- A clear description of the two algorithms would help (even if put in the appendix due to length limitations).\n- More details on the choice of the number of gradients to estimate K (in the stochastic variant) would help practitioners.\n- In Appendix F, failed runs on the simulated data are presented, which raises two questions: is there an explanation for the failures? can the results be quantified (over many runs)?  \n- The FID score, which is used to evaluate the results in Sec 5.2, is defined only in the appendix. I believe that some details, or a basic definition, need to appear in the main paper.\n- In the experimental study on MNIST (Sec 5.3), the dimension d is set to 20. Why? Was it obtained empirically? Is it based on previous works? Is this a result of the computational limitation? Please elaborate.\n The authors very nicely discuss the limitations of their work. \nI do not foresee any potential negative societal impact", "The task of interest is to simultaneously learn a manifold (with a fixed predefined dimensionality) and the density on it, from training data. The proposed method, rectangular flows, is developed based on an existing work (Brehmer and Cranmer [6]) that used a two-step training procedure (for manifold matching and density matching, respectively). The main contributions include: (1) revealing that the two-step training can be improved by joint training; (2) a new technique to enable computing the Jacobian-transpose-Jacobian term of the joint training objective with better efficiency. Experiments on simulated data and MNIST/FMNIST are conducted to demonstrate the proposed method.  Originality. The presented method in Section 4.2, for calculating the volume-change term, is believed new; it's a novel combination of existing techniques. \n\nQuality. The presented method is technically sound. Some empirical observations are not discussed/analyzed in detail. For example, the inconsistent results on simulated data, and the unexpected behavior on MNIST/FMNIST.\n\nClarity. The writing is OK, but can be certainly improved; for example, to highlight the underlying logic, the main contributions over existing methods, algorithms that show the big picture and/or the details (like VJP/JVP), and so on.\n\nSignificance. The contributions might be incremental; the presented method enables joint training for existing work, but the corresponding importance is not well demonstrated with serious experiments and/or analysis.\n\nOther comments:\n\nLine 48, the exact method is quite straightforward. I don't think it is a contribution, so no credit for it.\n\nEq (8). Since beta is just treated as a hyperparameter, the KKT conditions are not met or used. Please clarify.\n\nLines 164-165. What's the meaning of the statements here?\n\nHow to calculate a VJP or a JVP? It's not clear in the manuscript.\n\nEq (12). Since the (J^T J eps) term of the loss is calculated via VJP/JVP, how do you guarantee the correct gradient wrt theta?\n\nLine 323. Why RNFs can remedy the phenomenon? Also, too many conjectures are made in this paragraph without solid analysis.\n Yes."], "review_score_variance": 1.2, "summary": "Congratulations on the acceptance of your paper! Please incorporate changes, edits and additional promised experiments from \"Author Discussion\" in the final paper/appendix.", "paper_id": "nips_2021_lmOF2OxxSz", "label": "train", "paper_acceptance": "accept"}
{"source_documents": ["Effective defense of deep neural networks against adversarial attacks remains a challenging problem, especially under white-box attacks.\n      In this paper, we develop a new generative cleaning network  with quantized nonlinear transform  for effective defense of deep neural networks.  The generative cleaning network, equipped with a trainable quantized nonlinear  transform block, is able to destroy the sophisticated noise pattern of adversarial attacks and recover the original image content. The generative cleaning network and attack detector network are jointly trained using adversarial learning  to minimize both perceptual loss and adversarial loss. Our extensive experimental results demonstrate that our approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. For example, it improves the classification accuracy for white-box attacks upon the second best method by more than 40\\% on the SVHN dataset and more than 20\\% on the challenging CIFAR-10 dataset. ", "\nWe really appreciate your valuable comment! \n\nBPDA is an attack method. In the original paper of BPDA, they provided results on MINST, CIFAR-10, and ImageNet. Here is the reason that we only had BPDA defense results on the CIFAR-10. (1) We found that only one defense paper had results on MINST, so we did not provide comparisons on MINST. (2) For the ImageNet, it is too huge and extremely time consuming for us to run all of these experiments. (3) In recently defense papers, only the STL method from CVPR 2019 provided results on BPDA, which we have included it in the paper and our algorithm significantly outperforms it.  \nWe used the standard attack-defense evaluation package called AdverTorch which provides implementations of major attack methods, including BPDA.\n\nWe set 10 attack iterations as the BPDA baseline setting. In addition, the large iterations and large epsilon evaluation of BPDA attacks and are also presented in Figure 3 and Figure 4, which show the proposed method is able to effective defend BPDA attacks and outperform other methods. \n\nWe did used the adversarial samples for training the generator and discriminator. But we did not re-train the target classifier and made no modification to the target classifier.", "Thanks. It appears then that BPDA at least is needed for accurately evaluating adversarial robustness of your model. My understanding is that Table 2 is the main place where you compare performance against BPDA to other models. Can you clarify how many BPDA iterations was used for this table? Am I missing any other place where the BPDA comparison is performed? In general more info about the implementation of BPDA would be helpful since it is the main relevant metric in the paper.\n\nI also had a question regarding the training procedure; a contrast you make with other methods is that they perform adversarial training whereas your method does not, but it seems that the generator-discriminator algorithm used at training time is essentially doing adversarial training (but on the generator rather than the final classifier). Is this correct? Or if this is wrong could you explain why?", "We really appreciate your insightful comment! \n\nWe forgot to plot more points on the curve. Yes, even with epsilon = 0.2, the accuracy with BPDA attack becomes 1.35%, far below 10%, according to our evaluation. However, with PGD at epsilon=0.5, our accuracy is 12.63%, which is slightly above 10% as you mentioned. This is because our nonlinear transform layer has disrupted the gradient propagation behavior of PGD. Because of this, the attacked image by PGD is messed up, instead of being totally random. When we set the epsilon to 0.6, the accuracy drops to near 10%. This shows BPDA is more effective than PGD with our defense. Hope this has addressed your concern. We will update the figure in the paper shortly to include these additional points on the curve, plus the large-epsilon BPDA curve. Thank you very much!", "Thank you for including these figures. If I understand figure 4 (right panel) correctly, the purported accuracy of your method at epsilon=0.4 is above 15%, and extrapolating to epsilon=0.5 it looks like it should be above 10%. However, at epsilon=0.5 an adversary could map every single image to be uniformly gray (all pixels = 0.5), which gives an upper bound on possible performance of 10% since there are 10 classes. (This is likely a very loose upper bound.) So it is concerning that the proposed method appears to overcome this fundamental limit, and calls into question the reliability of the evaluation. It would be helpful if you could comment on this.", "Dear Reviewer 3, as suggested by you, we have updated the paper to include the large-iterations and large-epsilon PGD attacks of our method. We also compare our algorithm against PNI and vanilla adv. training method. Please see Figure 4. We can see that our method is able to surve the large-iteration PGD attacks and significantly outperform the other two methods. Also, our method performs much better than the other two methods during large-epsilon attacks with epsilon going to 1, as you suggested. Please review. Thank you!", "Dear Reviewer, Thank you so much for the quick response and appreciate your valueable suggestion! We are updating the paper to include these two figures, as you suggested. We will upload the paper soon and let you know once it is updated. ", "Hello,\n\nI do not understand your response:\n\n\"As we can see from our paper, we followed exactly the same evaluation procedure and used the same datasets as the paper mentioned by the reviewer.\"\n\nEither I am confused or this is false, as I brought up two explicit best practices you did not follow: a large enough number of PGD iterations, and checking that accuracy goes to zero for large epsilon. These do not seem to be in the paper, if I have missed them please tell me where they are. If you are claiming that you performed them but did not include them in the paper, then please update the paper to include them so that I can look at the numbers and re-assess.\n\n\"Figure 3 shows the large number of BPDA iterations. Due to the page limitation, we did not include the figure for larger number of PGD iterations, since BPDA is a more powerful attack method than PGD. \"\n\nBPDA is not uniformly stronger than PGD; it is more finicky, and so while it sometimes works when PGD fails it is important to also use PGD. Moreover it is not clear that 100 iterations is enough for BPDA due to its more finicky nature. In any case, I would like to see PGD vs. number of iterations with number of iterations going at least up to 100 and preferably higher.\n\n\"We could not include the figure for attacks with large epsilon since this figure is not very critical and many recent papers chose not to include it due to page limitations. \"\n\nIf you are pressed for space I recommend removing the black-box evaluation (since black-box accuracy is mostly meaningless) to make room for more careful checks of the white-box evaluation. But regardless of space limitations, the large epsilon sanity check is essential for assessing the method.", "As we can see from our paper, we followed exactly the same evaluation procedure and used the same datasets as the paper mentioned by the reviewer. Following existing papers recently published in ICLR/CVPR/ICCV/ECCV 2017-2019, we used the advTorch standard package to generate all attacks on our method.\n\nFigure 3 shows the large number of BPDA iterations. Due to the page limitation, we did not include the figure for larger number of PGD iterations, since BPDA is a more powerful attack method than PGD. \n\nWe could not include the figure for attacks with large epsilon since this figure is not very critical and many recent papers chose not to include it due to page limitations. \n\nFollowing recently published paper, we have included the most comprehensive performance comparison results in the paper. We have demonstrated  that our method significantly outperforms existing state-of-the-art methods.", "We sincerely thank the Reviewer for the positive feedback and high recommendation of our paper!\n\nThanks for pointing out this! In this paper, we use the benchmark datasets and compare our results against the results published in existing papers. \n\nUnfortunately, not all published papers provided complete results for both of these datasets. For example, some papers did not have results on SVHN. In  this case, we left them empty. \n\nWe chose not to re-implement existing methods since it will be very hard to re-produce their exact results, which might lead to unfair performance comparisons.  \n\nDuring our experiments, we tried our best to compare with as many methods as possible if they have published results on the dataset.", "We sincerely thank Reviewer 1 for the positive and encouraging comments!\n\nThe reviewer suggests that we need to provide theoretical analysis or proof why the proposed deep neural network defense method is working. We must admit that this is really hard. The deep learning research community is still working very hard to establish theoretical analysis or proof for deep neural networks, which is however is very challenging. \n\nHowever, our proposed method is based on data-driven deep learning. All the defense networks are trained based on the loss functions. So, if the loss function converges, the proposed method is achieving the target performance. \n\nFollowing existing state-of-the-art methods published in recent ICLR/CVPR/ICCV/ECCV papers, we use the benchmark their datasets and standard evaluation protocols to demonstrate that our proposed method is significantly outperforming existing methods.  \n\nFor the experiments, we have results on two datasets, CIFAR-10 and SVHN with two different attack modes: white-box attack and black-box attacks. Sorry for the confusion. We will better organize these experimental results.\n\nFor the attacks, we follow the standard procedure used in all existing methods. Specifically, we use advTorch evaluation package to generate all attacks. ", "This paper proposes a method for adversarial defense based on generative cleaning.\n\nThe paper does not follow any of the best practices for evaluating adversarial robustness, e.g. in these two papers:\n\"On Evaluating Adversarial Robustness\" https://arxiv.org/abs/1902.06705\n\"Obfuscated Gradients Give a False Sense of Security\" https://arxiv.org/abs/1802.00420\n\nFor instance the paper does not use a large number of PGD iterations (10 is too small) and does not check that accuracies go to zero for large epsilon (an important sanity check to reveal gradient masking). In the one place where a larger number of attack iterations is used (100 for BPDA) the gap with adversarial training mostly vanishes.\n\nIn the absence of these best practices it is impossible to assess the validity of the results, so the paper should be rejected.", "This paper proposes a new method to defend a neural network agains adversarial attacks (both white-box and black-box attacks). By jointly training a Generative Cleaning Network with quantized nonlinear transform, and a Detector Network, the proposed cleans the incoming attacked image and correctly classifies its true label. The authors use state-of-the-art attack methods on various models, and the proposed model consistently outperforms all baseline models, even dramatically outperforming them for some specific attack methods.\n\nComment:\nIs there a reason the authors did not test the same set of attack methods for SVHN as they did for CIFAR-10?", "This paper developed a method for defending deep neural networks against adversarial attacks based on generative cleaning networks with quantized nonlinear transform. The network is claimed to recover the original image while cleaning up the residual attack noise. The authors developed a detector network, which serves as the dual network of the target classifier network to be defended, to detect if the image is clean or being attacked. This detector network and the generative cleaning network are jointly trained with adversarial learning so that the detector network cannot find any attack noise in the output image of generative cleaning network. The experimental results demonstrated that the proposed approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. \n\nA few comments: \n\n1. It does not provide theoretical reasons why the prosed method can defend against those attacks. \n\n2. The experiments are a bit messy and the attacks' setup need to improve. \n\n3. The proposed defense showed only empirical results against the target attack. It seems to provide no theoretical / provable guarantees. \n\n\n"], "review_score_variance": 8.666666666666666, "summary": "This paper presents a method to defend neural networks from adversarial attack. The proposed generative cleaning network has a trainable quantization module which is claimed to be able to eliminate adversarial noise and recover the original image. \nAfter the intensive interaction with authors and discussion, one expert reviewer (R3) admitted that the experimental procedure basically makes sense and increased the score to Weak Reject. Yet, R3 is still not satisfied with some details such as the number of BPDA iterations, and more importantly, concludes that the meaningful numbers reported in the paper show only small gains, making the claim of the paper less convincing. As authors seem to have less interest in providing theoretical analysis and support, this issue is critical for decision, and there was no objection from other reviewers. After carefully reading the paper myself, I decided to support the opinion and therefore would like to recommend rejection. \n", "paper_id": "iclr_2020_SkxOhANKDr", "label": "train", "paper_acceptance": "reject"}
{"source_documents": ["In this paper, we present a new strategy to prove the convergence of Deep Learning architectures to a zero training (or even testing) loss by gradient flow. Our analysis is centered on the notion of Rayleigh quotients in order to prove Kurdyka-Lojasiewicz inequalities for a broader set of neural network architectures and loss functions. We show that Rayleigh quotients provide a unified view for several convergence analysis techniques in the literature. Our strategy produces a proof of convergence for various examples of parametric learning. In particular, our analysis does not require the number of parameters to tend to infinity, nor the number of samples to be finite, thus extending to test loss minimization and beyond the over-parameterized regime.", " I agree with that there is a gap from the presented results to deep networks but it is still a very insightful work. I am looking forward to your follow-up version. ", " The comments/concerns/suggestions have been reflected in the rebuttal version. Also thanks for highlighting the updates for easier follow-up.\n\nImportant notice:\nYou should double-check page limit but I don't think you can go over 9 page limit in rebuttal, and it may cause rejection!\n", " Thank you for your time and (very) thorough reading of all proof details.\nWe have updated our submission to include your corrections (pdf in updated supplementary material, changes in red).\n\nWe have fixed all the typos and mistakes you found.\nA discussion of previous results on logistic regression has also been added to Section 4.3, and a reference with several examples of use of KL inequalities to Section 3.\n\nRegarding your first question on $\\mathcal{L}(\\theta) - \\varepsilon$, the issue was that we operated under the (unstated) assumption that $\\mathcal{L}(\\theta) \\geq \\varepsilon$ corresponding to non-null loss in Proposition 3.1, which was a mistake on our part. For succinctness and readability, we have modified the inequality to use the positive part $\\left(\\mathcal{L}(\\theta) - \\varepsilon\\right)_+$ which is null when $\\mathcal{L}(\\theta) \\leq \\varepsilon$, and thus fixes the issue with minimal changes.\n\nThe property $\\theta(\\mathbb{R}_+) \\subseteq \\mathcal{B}(\\theta_0, R)$ was not a claim but an assumption of our statement. In the final version, we have removed this assumption and strengthened our proposition, which should lift this concern as well. We have modified the proof to give a convergence rate as requested, and assume you will find the idea very unsurprising for it almost exactly matches your suggestion.\n\nWe have added a discussion of $\\kappa$-isolation and $\\varepsilon$-separation in Appendix A.5.5, please let us know if it answers your questions.\nThe property you singled out on L590 of the first submission is indeed not a direct consequence of the lemma, but is easily checked by the same proof technique.\nWe have also added the missing reference on extraction of sequences converging almost surely. Convergence of positive variables in expectation to zero implies convergence in $L_1$, which in turn implies convergence in probability, from which almost sure convergence follows up to extraction.\n\nWe are sorry for the readability issues in Sections 4.2 and 4.5.\nWe hope the idea for the convergence on the lemniscate was clear despite your trouble with Prop 4.2, for we found this low-dimensional counterexample enlightening. The periodic signal recovery on the other hand is significantly harder to follow, but demonstrates that a good understanding of the problem (materialized here by knowledge of an approximate singular vector decomposition of the network map differential, leveraged with Prop 3.6) is key to reach proofs in the regime with near-optimal parameter count, which seems to be the most interesting. This could also serve as a bridge to signal processing results, to show that this line of work is not restricted to deep learning. Any suggested changes or general ideas you may have to improve the proposition readability or high-level exposition of these two sections is of course very welcome.\n", " \nThank you for your time and dedication to this submission. We have updated it to address the concerns you raised (pdf in updated supplementary material, changes in red).\n\nWe will discuss the major problem (Weakness 1) last.\nFirst, thank you very much for the references on the convergence speed of the logistic regression, we have updated Section 4.3 to properly credit these works, together with comments on the differences between these bounds and the result presented. We are not aware of any more precise results on the convergence of logistic regression than those newly inserted. Please let us know if you think of any other relevant results that we have also missed, we will further update this section if this is the case.\nThe consistency with the $\\mathcal{O}(1/t)$ asymptotic behavior is checked in Appendix A.5.4, the link between the (unquantified) separability assumption and our formulation is now discussed in Appendix A.5.5.\nWe have also added an explicit comment on $(1/n)$-isolation for finite datasets of size $n$ after Proposition 4.5.\n\nRegarding the general readability of the paper, we have added a notation table in Appendix A.1, we hope this helps with the reading.\nWe found further simplification of notations in early sections hard to incorporate. Since the extension to infinite data is not standard, it requires some care in the definitions to have well-formed statements.\nIn particular, the use of the $\\mathcal{D}$-seminorm, though hard to get used to, makes much more explicit the geometric intuition in the following, with functions treated as vectors with angles between them for instance.\nWe believe this choice of notation is an opportunity to steer the discussion towards more geometrical arguments that will ultimately be easier to understand.\nChanges we could consider to lighten notations are inlining the definition of the $\\mathcal{D}$-seminorm everywhere to remove that definition, at the risk of erasing the geometric intuition and lenghtening some statements, along with the removal of the primal definition of $K_\\theta$ and discussion of the difference with the dual $K_\\theta^\\star$, to shorten that definition.\nIf you feel that these (or other changes you may suggest) would significantly improve the readability of this submission, please let us know, but please also keep in mind that the page limit and the conciseness that it implies are a major constraint of ours.\nAny suggestions regarding specific simplifications or preferred choices of notation for the later sections are also welcome.\n\nRegarding the major limitation, identified as Weakness 1 in the review, we have strengthened Proposition 4.6 to lift the initialization-ball assumption. We can now prove (with high probability) convergence to an arbitrarily low loss value under no additional assumption. We believe this clears the concern regarding concrete new results on non-linear networks. \nIt remains that this work is restricted to continuous-time noiseless differentiable optimization, as observed in the review, but we argue that this is a good first step towards a stronger theory.", " \nThank you for your time and thoughtful comments.\nWe have updated our submission to address your concerns (pdf in updated supplementary material, changes in red).\n\nRegarding minor concerns, we have added a notation table in Appendix A.1, and hope this helps with the readability, please let us know if you find that other shorthands deserve to be added to this table or need more attention. We took $\\mu$-uniform-conditioning of ${K_\\theta^\\star}$ to mean that the lowest eigenvalue is at least $\\mu$, i.e. ${K_\\theta^\\star} \\succeq \\mu$ (as positive semi-definite matrices). We have removed this term from the introduction to avoid misunderstandings, and replaced it with a more explicit description.\n\nThe question of the gap from presented results to deep networks is harder to address.\nAs a first and high-level answer, we use the term \"deep learning\" in this sentence in a broader sense, to mean essentially gradient-based iterative training of machine learning models in the massive-data regime, which is a looser definition but arguably compatible with a more modern use of the term, although not directly connected to the layer-depth of the model which initially led to this choice of terminology. In this broad sense, we believe that any theory powerful enough to explain the vanishing optimization error of deep learning will have to recover nearly-trivial systems such as the lemniscate as subcases. This example shares some properties with layer-deep architectures that rule out both parameter-convex and definite-NTK theories. We argue not that this approach will surely lead to convergence proofs of deep architectures, but only that previous approaches won't and show why. Our approach is then in a way incomplete by construction, but can be viewed as the easy case that later theories must recover as a subcase to stand a chance, which substantially shrinks the search space for such theories.\n\nRegarding a stricter interpretation of the term \"deep\" learning, and the road to transformers and the likes,\nwe still have high hopes for this line of work.\nWe have started exploring some ideas to extend the result from two-layer networks to deeper multi-layer perceptrons, but don't have fully fleshed-out results at the moment that could be added to this submission. Should this submission be accepted, we will consider extending it with more examples in the direction you suggest in a follow-up journal version, to strengthen this argument.", " This paper proposes a new insight in proving the convergence of the training of deep learning. Taking advantage of  the notion of Rayleigh quotients, the authors show that  KL inequalities for a broader set of neural network architectures and loss functions can be obtained. Moreover, several illustrative examples are provided to demonstrate their idea. In particular, their analysis does not require over-parameterization. My detailed comments are given as below.\n\nStrengths:\n\n1 This is a theoretical article on the convergence of the GD for deep learning. This topic has been intensively studied recently. Most of previous works require the over-parameterization. But this work is different, and it presents a new strategy in proving the convergence based on the Rayleigh quotients and the KL inequalities. This innovative idea has intrigued me, and I feel that this article contributes significantly to  the understanding of training dynamics of NNs.\n\n2 One of main limitations of previous works is that their arguments require the positiveness of the NTK which further requires the over-parameterization. This paper points out that  one can bound the Rayleigh quotient of the gradient instead of lower bounding the eigenvalues of the NTK. Furthermore, the authors show the connection between the Rayleigh quotient and the KL inequalities which ensure the convergence.\n\n3 This paper is very well written and easy to read. Several case studies are presented in Sec 4.These cases are common and representative, and make their ideas very convincing. Moreover, the proof seems clean. Although I did not thoroughly examine the proof, it provided me with several fresh perspectives.\n\nWeaknesses:\n\nThis paper argues that the proposed new strategy can be used to prove the convergence of deep learning architectures to a zero training\".  However, only two-layer NNs are discussed in case studies. Can you provide some cases to support your argument on ``the deep architectures\"?  Minor issues:\n\n1 A table of notations is preferred.\n\n2 What does the ``uniform conditioning\" (in Line 19) mean? YES", " This paper presents a strategy to prove the loss convergence of gradient flow for training loss of the form $\\ell(F(\\theta))$, where $F$ is the vector consisting of neural net outputs on each data point, and $\\ell$ maps the outputs to the final loss value (e.g., $\\ell$ can be the MSE to target values). Previously NTK-based analysis assumes that the smallest eigenvalue of the NTK matrix $K = DF \\cdot DF^\\top$ is bounded from below, then proves the loss convergence through establishing the PL condition under this assumption. In this paper, the authors avoid the assumption on the smallest eigenvalue and instead point out a proof strategy based on Rayleigh quotients of the NTK matrix and KL condition. This strategy is then applied to prove loss convergence bounds for linear regression, linear logistic regression, and some partial results are also given for two-layer net with squared loss. ### Strengths:\n\n1. The proposed proof strategy does not require the smallest eigenvalue of the NTK matrix to be lower bounded, allowing analysis when the number of data is more than the number of parameters.\n2. Applying the proposed proof strategy gives an alternative way to understand the loss convergence bound for linear regression.\n\n### Weaknesses:\n\n1. While it sounds more promising to establish loss convergence bounds beyond the over-parameterized regime, this paper does not prove any concrete theorem on nonlinear neural nets with fewer parameters than the number of data. In Sections 4.4 and 4.5, the authors give some partial results establishing loss convergence under the assumption that the gradient flow does not move far from the initial point. This assumption weakens the strength of the results because it seems to be a more interesting and important question that why gradient flow does not intend to move far away in search of a better solution. In other words, it is unclear how useful the proposed proof strategy is in terms of overcoming the various difficulties in analyzing loss convergence beyond over-parameterized regime.\n2. The results on linear logistic regression are not well contextualized relative to prior works. The authors claim that the loss convergence bound in this case is new (Line 97), but it is unclear how it compares with previous works such as the $O(1/t)$ bound in Theorem 5 of [1] for binary classification (see also Appendix E for the multi-class case). These two bounds should be the same when $t \\to \\infty$, so the authors should compare them carefully in the non-asymptotic case.\n3. The presentation of the current paper is not very understandable, since there are too many notations and the notations vary in different settings. I spent hours reading this paper to understand and memorize all these notations just to understand the basic idea. One main issue is that the definitions in Sections 2 and 3 include too many technical details that are unrelated to the idea of bounding Rayleigh quotients, e.g., the concept of D-seminorm is somewhat unnecessary because one can just replace the D-seminorm of $\\nabla \\ell_f$ with an expectation over the dataset, which is more accessible to the readers. To improve the presentation, I recommend the authors to either reduce the number of notations or add a warmup section that illustrates the proof strategy in a simple case (e.g., linear regression) before introducing the general setup.\n\n[1]. https://www.jmlr.org/papers/volume19/18-188/18-188.pdf 1. I wonder if the authors have some concrete examples showing that loss convergence bounds can indeed be deduced for nonlinear neural nets beyond the over-parameterized regime (without extra assumptions).\n2. How does the loss convergence bound in the linear logistic regression case compare with prior works? How much is known about the loss convergence rate of linear logistic regression in the literature?\n3. When does the isolation assumption hold in Proposition 4.5? The major limitation has already been mentioned in Weakness 1. Besides that, this paper also has some limitations that are common in many optimization papers: (1) the dynamics of SGD and gradient flow can be very different; (2) the loss function can be non-smooth.", " This work investigates the convergence of gradient flow to zero training (and test) loss. The main idea is to derive Kurdyka-Lojasiewicz inequality which is accomplished by lower bounding Rayleigh quotients, and several showcases of common learning problems have been analyzed by this technique.  Overall the paper is well-written and the main messages are easy to follow. However I had problems in understanding section 4.5 and its proofs, as well as prop 4.2, and couldn't check their correctness. Other results, except for prop 4.6 which might need slight modification discussed later, seem to be correct. The main weakness can be seen in discussing the conditions under which prop 4.6 and prop 4.5 hold, as addressed in below.\n\nI like the argument that Prop 3.1 makes the loss evolution more tractable using the function $\\phi$. Indeed in line 164 you provide $\\phi$ when PL condition is satisfied and further in section 4.3 provide example for multi-class logistic regression. In Prop 3.3 you argue that functional-space KL inequality is easier to obtain and together with Rayleigh quotient bound they imply parametric-space KL inequality, hence able to use the convergence result of Prop 3.1. The techniques employed throughout Prop 3.4-3.6 to split the Rayleigh quotient bound seems powerful and its first simple use is shown in linear models with quadratic loss in Prop 4.1.\n\nI would also like to know about any previous work that analyzes Cross-entropy minimization with linear models, assuming you are not the first paper to consider it. \n\nOverall the references include major important works in related areas, and some of them deal with Gradient Descent and overparameterized models. Please pinpoint the works that do not have overparametrized assumptions and if possible, compare your work to them. For example in line 150, you mention that \"this was introduced as an extension to PL inequality for linear convergence ...\". Please include as many references as possible that revolve around using inequalities in Kurdyka [1998]. One suggestion is to defer the proof of prop 4.1 to appendix so that you have more space. Typo:\nline 319, \"be compact\". line 703, \"has a ...\". line 730, equation (1), it should be \"l.h.s $\\leq \\frac{\\eta}{2}$\". in equation just before line 737, it might be $\\frac{\\eta}{2} + \\frac{R}{\\sqrt{k}}$, which yields $\\sqrt{k}\\geq 2R/\\eta$. line 360, \"there there\". line 545, there might be an error in calculating first component of the gradient.\n\nQuestions and suggestions:\n\n1- Proof and statement of Prop 4.6 needs revision. First I can't find a proof that $\\theta(\\mathbb{R}_+)\\subseteq \\mathcal{B}(\\theta_0,R)$ if $\\theta$ is a gradient flow, and in fact you just assume this condition holds in line 745 without proving it and further in line 328 you defer its proof to future work. Please indicate what is the difficulty in proving it. Most importantly, in the beginning of page 27, the conclusion in the 3rd line $\\cdots \\geq \\frac{1}{\\lVert \\nu_0 \\rVert^2}(\\mathcal{L}(\\theta_0) +0 - \\epsilon)^2 $ does not seem correct because $a\\geq b $ does not imply $a \\geq b$. However it seems that there is a fix for that because you just need to consider $\\lVert \\nabla \\mathcal{L}(w,a) \\rVert_2 \\geq \\cdots \\geq \\frac{1}{\\lVert \\nu_0 \\rVert}(\\mathcal{L}-\\epsilon)$. Even with this change, the argument in line 751 needs explanation on how you use prop 3.1 to show $\\overset{\\sim}{\\mathcal{L}}(\\theta)$ converges to zero. One way I can think of is to use $\\phi(u)=-\\frac{1}{u}$ which is a strictly increasing differentiable function on positive real numbers and $\\phi'(u)=\\frac{1}{u^2}$ and it gives $\\lVert \\nabla \\tilde{\\mathcal{L}}(\\theta) \\rVert^2 \\geq \\mu \\tilde{\\mathcal{L}}(\\theta)^2$ which is true here as you assume $\\mathcal{L}(\\theta_t)\\geq \\epsilon$. Hence by showing that $\\mathcal{L}(\\theta_t)$ converges to $\\eta\\leq \\epsilon$, it follows that $\\mathcal{L}(\\theta_t) - \\epsilon$ is negative for large enough t, and hence $a \\geq b$ does not imply $a^2 \\geq b^2$ with $a=\\lVert \\nabla \\mathcal{L}(\\theta) \\rVert$ and $b=\\kappa (\\mathcal{L}(\\theta) - \\epsilon)$. Finally I would like to know about rate of convergence by using Prop 3.1. It seems to me Prop 3.1 can only be used to show contradiction, i.e. $\\eta\\leq\\epsilon$, and I'm wondering if $\\lVert \\nabla \\mathcal{L}(\\theta) \\rVert \\geq \\kappa (\\mathcal{L}(\\theta) - \\epsilon) $ can be used with prop 3.1 to give any rate of convergence.\n\n2- (when) is the $\\kappa$-isolated condition in line 307 satisfied? I couldn’t find how it is proved anywhere in the Proof section, and stating under which conditions it is satisfied seems crucial. Also please discuss how fast is the convergence rate of prop 4.5.\n\n3- It is true that existing a \"non-zero\" $\\epsilon$-separating ray leads to unique label in softmax classifier and also interesting that separating ray with zero loss implies dirac labels. Please cite any existing work that considers this separating ray assumption, assuming you are not the first one.\n\n4- in line 590, is the conclusion $\\nabla \\ell(u): x\\rightarrow \\nabla \\ell_x(u(x))$ a result of Lemma A.1?\n\n5- in line 583, please explain more thoroughly how \"convergence of positive r.v.s in expectation to zero leads to some subsequence converging a.e. to zero\".\n\n The limitations concerning conditions under which prop 4.6 holds is discussed in lines 328-331. However I couldn't find how $\\kappa$-isolated in line 307 and $\\epsilon$-separating ray in line 309 are satisfied. It is strongly suggested to discuss these in the appendix."], "review_score_variance": 2.0, "summary": "This paper proposes a new method for proving the convergence of gradient flow to zero loss by leveraging Rayleigh quotients to establish KL inequalities, a strategy that can apply even without overparameterization. The reviewers found the paper to be well written and generally easy to follow, despite a few concerns about burdensome notation. The discussion highlighted a few minor technical issues which were addressed in the revision. Overall the consensus is that this paper provides valuable new tools and the results will be of interest to the theory community, so I recommend acceptance.\n", "paper_id": "nips_2022_pl279jU4GOu", "label": "train", "paper_acceptance": "Accept"}
