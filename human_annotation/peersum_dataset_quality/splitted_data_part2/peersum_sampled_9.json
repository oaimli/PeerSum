{"source_documents": ["Partial-label learning (PLL) is a peculiar weakly-supervised learning task where the training samples are generally associated with a set of candidate labels instead of single ground truth. While a variety of label disambiguation methods have been proposed in this domain, they normally assume a class-balanced scenario that may not hold in many real-world applications. Empirically, we observe degenerated performance of the prior methods when facing the combinatorial challenge from the long-tailed distribution and partial-labeling. In this work, we first identify the major reasons that the prior work failed. We subsequently propose SoLar, a novel Optimal Transport-based framework that allows to refine the disambiguated labels towards matching the marginal class prior distribution. SoLar additionally incorporates a new and systematic mechanism for estimating the long-tailed class prior distribution under the PLL setup. Through extensive experiments, SoLar exhibits substantially superior results on standardized benchmarks compared to the previous state-of-the-art PLL methods.", " Thank the authors for addressing all my concerns. I think this is a good paper and I will keep my score.", " Dear reviewer,\n\nIt seems that the author rebuttal directly addresses what you have identified as weaknesses of the paper: that the SOTA PLL methods already handle the long-tailed setting adequately, that they only compare to PRODEN empirically. They have also included an appendix showing experimental results on the benchmark PLL datasets you suggested.\n\nCould you please respond to the author's rebuttal, saying whether or not (and why) they have adequately addressed your concerns, and whether your rating of the paper has changed?\n\nThanks!", " My questions are well addressed and also vote for acceptance.", " Dear reviewers,\n\nAs we move closer to the end of the discussion panel, we haven't heard back from the reviewer. Please let us know if you have any further questions regarding our rebuttal. \n\nSincerely,\n\nAuthors", " We sincerely appreciate all reviewers for their great efforts in providing constructive and valuable feedback. We are pleased that the reviewers find our work **well-written** (R3), **theoretically solid** (**R1,R2**), and **experimentally comprehensive** (**R1,R2**). Moreover, we are more than encouraged that the reviewers agree that our work makes **an important attempt towards the imbalanced PLL problem** (**R1,R2**). \n\nWe have addressed the reviewers’ comments and concerns in individual responses to each reviewer. The reviews allowed us to strengthen our manuscript and the changes made are summarized below:\n\n- [R1] Updated the results of all baselines with mixup and consistency training (See Table 1,2)\n\n- [R1] Added new ablation studies on the $lambda$ parameter (See Appendix C.4 and Figure 3 (b))\n\n- [R1] Revised the caption of Figure 1 for a clear description of distribution estimation\n\n- [R2] Added new results on the real-world long-tailed SUN397 dataset (See Section 4.4 and Appendix C.1)\n\n- [R2] Corrected all the typos and mistakes\n\n- [R2] Added new results on SoLar with candidate counts initialization\n\n- [R3] Added new results on the PLL benchmark datasets (See Appendix C.6)\n\n- [R3] Added two SOTA PLL methods VALEN (NeurIPS’21) and PLDA (SIGKDD’22) for performance comparisons (See Table 1,2 and Appendix C.6)\n\n(* As abbreviations, we refer to reviewers **HWkc** as R1, **PJgg** as R2, and **nPgC** as R3 respectively.)\n", " **“Limitation: The motivation is not convincing as stated in the weakness.”**\n\n**A:** With the answers we posted above to address the reviewer’s concern, we want to reiterate the motivation to develop the SoLar approach. First, we claim that the real-world PLL scenarios often bear a long-tailed prediction distribution, which is also partially verified by the datasets recommended by the reviewer. Based on this observation, we identify the challenges and the degenerated effect of current PLL methods. In that regard, we intend to establish a novel and challenging scenario, blending PLL and long-tailed distribution, which is very much untapped by the prior work. Therefore, we propose SoLar, an optimal transport-based label refinery method, as a viable solution that demonstrates state-of-the-art performances on a variety of benchmarks.\n", " Thanks very much for your comments and suggestions. Below we address the feedback and comments in detail:\n\n**Q1. The paper claims that \"existing PLL methods have been commonly driven by the assumption that training data consists of class-balanced distribution\", which is not true as many SOTA PLL methods do not have the assumption, and they can deal with the imbalanced-class problem naturally embedded in the disambiguation.**\n\n**A:** The reviewer raises concern about the above claim. On that, we first want to point out that the most advanced approaches (i.e. the SOTA methods) — including PRODEN, PICO, and newly proposed VALEN [1] — have treated all labels as equal during the core label disambiguation process, which we may deem this as an implicit assumption of class-balancing. \n\nSecondly, while a few prior works may not bear a balanced-class assumption from a methodological perspective, they certainly have not put much emphasis on the class-imbalanced scenarios towards the development of their methods. \n\nThirdly, ubiquitous with the prior work, this lack of consideration of the class-imbalanced distribution has indeed led to poor performances as shown in our main table in Section 4.\n\nTo the best of our knowledge, there is no prior work that has systematically studied PLL under class-imbalanced settings like ours. In this regard, we believe our work can trigger a new round of thinking in the community.\n\n**Q2.The paper claims that \"we find that current best-performing PLL methods display degenerated performance in the long-tailed setting. This happens because the predictions of pseudo-labeling—a core component that PLL methods rely on—can be largely biased towards the head and majority classes.\" However, many SOTA methods are not pseudo-labeling-based methods, and the authors only present one PLL method, i. e., PRODEN, to display the degenerated performance in the long-tailed setting, which is not convincing.**\n\n**A:** Indeed, we demonstrate the degenerated performances of PRODEN qualitatively in Figure 1 as a visual example. However, we kindly disagree with the reviewer because we quantitatively evaluate the degenerated performances of an extensive number of PLL methods, including PiCO (ICLR’22), LWS (ICML’21), PRODEN (ICML’20), and CC (NeurIPS’20), in Table 1, 2, and Figure 1. With these results, we further notably conduct a few-shot experiment, displayed in Table 2, where we evidently locate the poor performance onto the tail labels in the long-tailed setup. Besides, as per the reviewer’s request, we supplement our paper with some newer PLL methods — VALEN [1] (NeurIPS’21) and PLDA [3] (SIGKDD’ 22) — where SoLar still performs at best.\n \n**Q3. The authors should also conduct experiments to show that their motivation works on benchmark PLL datasets, such as lost/BirdSong/Soccer Player/Yahoo!News. The authors should compare more SOTA PLL methods.**\n\n**A:** This is a very helpful suggestion. We chose CIFAR-10, CIFAR-100, and CUB-200 as our benchmarking datasets in our original submission, mainly due to that we closely follow the prior published work in ICLR, NeurIPS, and ICML, such as PiCO paper, PRODEN paper, etc. \n\nHowever, we do believe that the suggested benchmarks provided by the reviewer are helpful in particular these datasets have a wider spectrum of modalities. We supplement a series of experiments to show the effectiveness of our SoLar approach on these datasets, demonstrated in Appendix C.6. Besides, we find these real-world datasets are all naturally imbalanced, which accords with our motivation that ambiguous labels are more likely to occur on long-tailed datasets (also mentioned in our Conclusion). With almost no change to the SoLar setting, our approach still gets established as the one of most advantageous methods on these supplemented datasets.\n\nLast but not least, as we also mentioned in the previous response, we added the comparison of SoLar with more SOTA PLL methods, including VALEN [1] (NeurIPS’21), IPAL [2], and PLDA [3] (SIGKDD’22). The results can be found in Tables 1, 2, 12, and Appendix C.6, where SoLar maintains its significant advantages.\n\n**References:**\n\n[1] Xu N, Qiao C, Geng X, et al. Instance-dependent partial label learning. Advances in Neural Information Processing Systems, 2021, 34: 27119-27130.\n\n[2] Zhang M L, Yu F. Solving the partial label learning problem: An instance-based approach. Twenty-fourth international joint conference on artificial intelligence, 2015.\n\n[3] Wei Wang and Min-Ling Zhang. Partial label learning with discrimination augmentation. SIGKDD, 2022", " Thanks very much for your insightful comments and suggestions! We have properly revised our paper based on your reviews and below are our detailed responses.\n\n**Q1. Can SoLar get similar performance improvement on real-world long-tailed datasets?**\n\n**A:** That’s a good point, thank you! We additionally conducted experiments on the SUN397 dataset and we refer the reviewer to Table 4 and Appendix C.1 for more details. The training set contains 88,904 images with an imbalanced ratio of 2311/50=46.22. From Table 4, we can observe that SoLar still outperforms the baselines by a substantial margin. Besides, we also conducted a new series of experiments on classical PLL datasets which are naturally imbalanced; see Appendix C.6. We can observe that SoLar achieves better results than the baselines. These results clearly verify the effectiveness of SoLar on real-world long-tailed datasets.\n \n**Q2. Typos: 'Goup of Labels' in Figures and Wrong figure caption in Figure 3, CIFAR-10-LT rather than CIFAR100-H-LT.**\n\n**A:** We are sorry for the lack of carefulness. We have double-checked our manuscript and fixed all typos and mistakes.\n \n**Q3. Can we count the number of candidate labels as a rough estimation of the class prior?**\n\n**A:**  The answer is affirmative. To verify it clearly, we experimented with SoLar by initializing $r_j$ with the proportion of candidate counts, and the results are reported in Table 3 (the variant *SoLar w Cand-Est*) in the revised version. It can be shown that the performances of *SoLar w Cand-Est* favorably match the original SoLar. \n", " We are glad that you enjoyed our work! We have updated our manuscript based on the comments of the reviewers.\n\n**Q1. I'd like to see how much the strong CC method could benefit from the representation training technique.**\n\n**A:** We have tested the performance of all the baselines and a new baseline VALEN [1] with mixup (MU) and consistency training (CT) techniques. The results are reported in Table 1,2 in the revised manuscript (without changing the name of these methods). From the two labels, we can observe that most methods can benefit from the representation learning techniques. But, SoLar still achieves substantially better performance than them, which further highlights the effectiveness of SoLar. \n\n**Q2. It is not clear why the proposed sample selection mechanism helps preserve the label distribution.**\n\n**A:** We apologize for the confusion! Recall that we anchor the total sample number to be $k=\\min(\\lceil r_j\\rho|B|\\rceil, |B_j|)$. For the first term, let us consider an ideal setup where both the estimated class prior and model predictions are accurate. Obviously, each label would be assigned $r_j|B|$ examples. Thus, we anchor the first term to be $\\rho$-proportional to this ideal setting. The second term indicates that the selected are conducted within the subset $B_j$ where examples have the $j$-th class as their pseudo label assignment. Through the Sinkhorn label refinery procedure, the pseudo labels are enforced to match the class prior $r_i$ and the size of subsets $|B_j|$ roughly follows distribution $r_i$. Thus, we say the proposed sample selection procedure approximately preserves the class prior distribution.\n\n**Q3. Why does the relaxed problem guarantee converge? Does Solar always run this relaxed version of Sinkhorn-Knopp?**\n\n**A:** Note that the modified matrix $M+\\epsilon$ is entry-wise positive. Henceforth, the convergence property of our relaxed version of Sinkhorn-Knopp is guaranteed by the Sinkhorn’s Theorem [2] as follows\n\n> If $\\mathbf{A}$ is an $n × n$ matrix with strictly positive elements, then there exist diagonal matrices $\\mathbf{D}_1$ and $\\mathbf{D}_2$ with strictly positive diagonal elements such that $\\mathbf{D}_1\\mathbf{A}\\mathbf{D}_2$ is doubly stochastic. The matrices $\\mathbf{D}_1$ and $\\mathbf{D}_2$ are unique modulo multiplying the first matrix by a positive number and dividing the second one by the same number.\n\nThe Sinkhorn-Knopp algorithm is one classic algorithm to approach this double stochastic matrix with a convergence guarantee [3].\nWe believe the unrelaxed version can result in a better pseudo-label assignment. Hence, in our implementation, we run this relaxed version only when the original version fails. Empirically, this relaxed version will be called only in the first few epochs.\n\n**Q4.How is gamma in the Sinknhorn-Knopp affect the performance?**\n\n**A:** If we get the reviewer right, the ''gamma'' here may refer to the ''lambda'' In the Sinkhorn-Knopp algorithm. We add a new set of experiments for ablation on $\\lambda$ as shown in Appendix B.5. In general, a too small $\\lambda$ results in poor label assignment as the optimal transport objective becomes hard to be resolved. With a too large $\\lambda$, our objective is over-smoothed and thus the resultant solution may deviate from the true one, slightly degrading the performance. We empirically found that $\\lambda=3$ is a proper choice.\n\nFor the $\\gamma$ parameter, which controls the skewness of the datasets, we refer the reviewer to Table 1 for details. All methods perform worse when $\\gamma$ becomes larger, regarding more severe label skewness. But, our SoLar consistently outperforms under varying $\\gamma$ values.\n\n**Q5.How does the class distribution estimate for PRODEN in Figure 1?**\n\n**A:** For PRODEN, we count training samples grouped by the predicted labels (categorical) and report the proportion of the counts. We have updated the caption of Figure 1 to make it more transparent. \n\n**References:**\n\n[1] Xu N, Qiao C, Geng X, et al. Instance-dependent partial label learning[J]. Advances in Neural Information Processing Systems, 2021, 34: 27119-27130.\n\n[2] https://en.wikipedia.org/wiki/Sinkhorn%27s_theorem\n\n[3] Sinkhorn, Richard, & Knopp, Paul. (1967). \"Concerning nonnegative matrices and doubly stochastic matrices\". Pacific J. Math. 21, 343–348.\n", " This paper proposes an optimal transport framework Solar for imbalanced partial label learning where each data point has multiple candidate labels and the latent label distribution is skewed. The key of Solar is to refine the pseudo-labels by a constraint of within-candidate allocation and class prior matching. In parallel, a moving-average-based class prior algorithm is proposed to generate class prior for the aforementioned constraints. Experiment results on CIFAR-LT and CUB-200 datasets validate the effectiveness of Solar in improving few-shot accuracy. Strengths:\n\n1.The long-tailed learning problem is important but is generally overlooked in the PLL problem. This work systematically investigated the imbalanced PLL problem by identifying the failure of existing PLL methods and a new Solar framework, which can inspire future research in this field. \n\n2.Solid theoretical analysis is given. The overall objective guarantees to recover the true classifier at a population level. The detailed derivation of the Sinkhorn-Knopp algorithm is also provided.\n\n3.The experiments are sufficient. The ablation study strongly supports the effectiveness of the label refinery procedure. Reproducibility is ensured as the source code was submitted.\n\nWeaknesses:\n\n\n1.In experiments, the PRODEN method also uses mixup and consistency training techniques for fair comparisons. What about other competitive baselines? I'd like to see how much the strong CC method could benefit from the representation training technique.\n\n2.It is not clear why the proposed sample selection mechanism helps preserve the label distribution.\n\n3.In App. B.2, a relaxed solution of Sinkhorn-Knopp algorithm is proposed. Why the relaxed problem guarantees to converge?Does Solar always run this relaxed version of Sinkhorn-Knopp?\n\n4.How is gamma in the Sinknhorn-Knopp affect the performance?\n\n5.How does the class distribution estimate for PRODEN in Figure 1? Please see my main review. Societal Impacts: The main negative impact is lower annotation costs may decrease the requirement for annotator employment. \n\nLimitations: The experiments need to be further improved.\n", " This work approaches the class-imbalanced partial-label learning problem where each training example is assigned a set of candidate labels. Efforts are made to address two technical challenges. First, even the best-performing PLL method fails to identify the true labels from the candidate labels, because the pseudo-labels are largely biased toward head labels. To address this problem, an optimal transport-based objective is proposed to enforce constraints on pseudo-labels to match the class prior. Second, this work proposes a new class prior estimation algorithm along with a sample selection mechanism for practical training. Experiments validate the effectiveness of the proposed method. Pros:\n\n(1) Most studies about PLL assume the actual label distribution is balanced, which may not hold in practice. This work reveals the leading cause of performance degradation of the SOTA PLL methods long-tailed PLL problem and proposes a principled OT-based framework to resolve this. It is a serious attempt that makes PLL more suitable for real-world applications. \n\n(2) The iterative class prior estimation technique is novel, as estimating the true class prior without an additional validation set is intractable for any weakly-supervised learning problems. Other imbalanced weakly-supervised learning algorithms can potentially benefit from this technique.\n\n(3) Theoretical results show the consistency of the proposed OT objective.\n\n(4) Experiments are thorough and the proposed SoLar algorithm shows impressive performance. \n\n\n\nCons:\n\n(1) The main weakness is the evaluation was conducted on CIFAR datasets. With long-tailed sampling, the resulting data examples are typically sparse. Can SoLar get similar performance improvement on real-world long-tailed datasets?\n\n(2) Typos: 'Goup of Labels' in Figures. \n\n(3) Wrong figure caption in Figure 3, CIFAR-10-LT rather than CIFAR100-H-LT.\n I noticed that the class prior vector r is initialized as uniform. But, why can't we count the number of candidate labels as a rough estimation of the class prior? It is typically close to the true class prior and may perform better. It would be appreciated if experimental results on large-scale LTL datasets are provided.\n\nPotential negative societal impacts are discussed in the Appendix and the PLL framework may cause potential employment destruction.\n", " This paper concentrates on how to perform disambiguation in partial label learning with the long-tailed label distribution considered. The authors propose an Optimal Transport-based framework called SoLar that allows to refine the disambiguated labels towards matching the marginal class prior distribution. They conduct experiments on several benchmarks to demonstrate the superiority of the proposed method. Strengths: \nThe paper is well-written and easy to follow. The proposed method seems to be technically reasonable. \n\nWeaknesses: \nThe motivation is not convincing. On the one hand, the paper claims that \"existing PLL methods have been commonly driven by the assumption that training data consists of class-balanced distribution\", which is not true as many SOTA PLL methods do not have the assumption, and they can deal with the imbalanced-class problem natually embedded in the disambiguation. On the other hand, the paper claims that \"we find that current best-performing PLL methods display degenerated performance in the long-tailed setting. This happens because the predictions of pseudo-labeling—a core component that PLL methods rely on—can be largely biased towards the head and majority classes.\" However, many SOTA methods are not pseudo-labeling based methods, and the authors only present one PLL mehtod, i. e., PRODEN, to display the degenerated performance in the long-tailed setting, which is not convincing.  The authors should compare more SOTA PLL methods except for some specific pseudo-labeling-based method to verify their point that the performance will degenerate in the long-tailed setting. The authors should also conduct experiments to show that their motivation works on benchmark PLL datasets, such as lost/BirdSong/Soccer Player/Yahoo!News. \n\n The motivation is not convincing as stated in the weakness. \nThe novelty and the techquie seem to be limited.\nThe experiments have some limitations. For example, experiments on many PLL benchmark datasets (such as lost/BirdSong/Soccer Player/Yahoo!News ) are not conducted, and many SOTA methods are not compared, for example, Instance-Dependent Partial Label Learning, NeurIPS 2021.\n\n"], "review_score_variance": 8.0, "summary": "Most approaches to partial-label learning (PLL) tasks assume the label distribution is balanced, which may not hold in practice. This paper provides a principled optimal transport-based framework to resolve the issues with performance degradation caused by skewed label distributions in PLL. The reviewers found that the work makes a theoretically solid and experimentally comprehensive attempt towards solving this practical and under-addressed problem.", "paper_id": "nips_2022_wUUutywJY6", "label": "train", "paper_acceptance": "Accept"}
{"source_documents": ["We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.", "The paper describes a number of modifications of GAN training that enable synthesis of high-resolution images. The modifications also support more automated longer-term training, and increasing variability in the results.\n\nThe key modification is progressive growing. First, a GAN is trained for image synthesis at very low resolution. Then a layer that refines the resolution is progressively faded in. (More accurately, a corresponding pair of layers, one in the generation and one in the discriminator.) This progressive fading in of layers is repeated, one octave at a time, until the desired resolution is reached.\n\nAnother modification reported in the paper is a simple parameter-free minibatch summary statistic feature that is reported to increase variation. Finally, the paper describes simple schemes for initialization and feature normalization that are reported to be more effective than commonly used initializers and batchnorm.\n\nIt's a very nice paper. It does share the \"bag of tricks\" nature of many GAN papers, but as such it is better than most of the lot. I appreciate that some of the tricks actually simplify training, and most are conceptually reasonable. The paper is also very well written.\n\nMy quibbles are minor. First, I would discuss [Huang et al., CVPR 2017] and the following paper more prominently:\n\n[Zhang et al., ICCV 2017] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, and D. Metaxas. StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks. In ICCV, 2017.\n\nI couldn't find a discussion of [Huang et al., CVPR 2017] at all, although it's in the bibliography. (Perhaps I overlooked the discussion.) And [Zhang et al., ICCV 2017] is quite closely related, since it also tackles high-resolution synthesis via multi-scale refinement. These papers don't diminish the submission, but they should be clearly acknowledged and the contribution of the submission relative to these prior works should be discussed.\n\nAlso, [Rabin et al., 2011] is cited in Section 5 but I couldn't find it in the bibliography.\n", "Before the actual review I must mention that the authors provide links in the paper that immediately disclose their identity (for instance, the github link). This is a violation of double-blindness, and in any established double-blind conference this would be a clear reason for automatic rejection. In case of ICLR, double-blindness is new and not very well described in the call for papers, so I guess it’s up to ACs/PCs to decide. I would vote for rejection. I understand in the age of arxiv and social media double-blindness is often violated in some way, but here the authors do not seem to care at all. \n\n—\n\nThe paper proposes a collections of techniques for improving the performance of Generative Adversarial Networks (GANs). The key contribution is a principled multi-scale approach, where in the process of training both the generator and the discriminator are made progressively deeper and operate on progressively larger images. The proposed version of GANs allows generating images of high resolution (up to 1024x1024) and high visual quality.\n\nPros:\n1) The visual quality of the results is very good, both on faces and on objects from the LSUN dataset. This is a large and clear improvement compared to existing GANs.\n2) The authors perform a thorough quantitative evaluation, demonstrating the value of the proposed approach. They also introduce a new metric - Sliced Wasserstein Distance.\n3) The authors perform an ablation study illustrating the value of each of the proposed modifications.\n\nCons:\n1) The paper only shows results on image generation from random noise. The evaluation of this task is notoriously difficult, up to impossible (Theis et al., ICLR 2016). The authors put lots of effort in the evaluation, but still:\n- it is unclear what is the average quality of the samples - a human study might help\n- it is unclear to which extent the images are copied from the training set.  The authors show some nearest neighbors from the training set, but very few and in the pixel space, which is known to be pointless (again, Theis et al. 2016). Interpolations in the latent space is a good experiment, but in fact the interpolations do not look that great on LSUN\n- it is unclear if the model covers the full diversity of images (mode collapse)\nIt would be more convincing to demonstrate some practical results, for instance inpainting, superresolution, unsupervised or semi-supervised learning, etc.\n2) The general idea of multi-scale generation is not new, and has been investigated for instance in LapGAN (Denton et al., ICLR 2015) or StackGAN (Zhang et al., ICCV2017, arxiv 2017). The authors should properly discuss this. \n3) The authors mention “unhealthy competition” between the discriminator and the generator several times, but it is not quite clear what exactly they mean - a more specific definition would be useful.\n\n(This conclusion does not take the anonymity violation into account. Because of the violation I believe the paper should be rejected. Of course I am open to discussions with ACs/PCs.) \nTo conclude, the paper demonstrates a breakthrough in the quality and resolution of images generated with a GAN. The experimental evaluation is thorough, to the degree allowed by the poorly defined task of generating images from random noise. Results on some downstream tasks, such as inpainting, image processing or un-/semi-supervised learning would make the paper more convincing. Still, the paper should definitely be accepted for publication. Normally, I would give the paper a rating of 8.", "This paper proposes a number of ideas for improving GANs for image generation, highlighting in particular a curriculum learning strategy to progressively increase the resolution of the generated images, resulting in GAN generators capable of producing samples with unprecedented resolution and visual fidelity.\n\n\nPros:\n\nThe paper is well-written and the results speak for themselves! Qualitatively they’re an impressive and significant improvement over previous results from GANs and other generative models.  The latent space interpolations shown in the video (especially on CelebA-HQ) demonstrate that the generator can smoothly transition between modes and convince me that it isn’t simply memorizing the training data. (Though I think this issue could be addressed a bit better -- see below.) Though quantification of GAN performance is difficult and rapidly evolving, there is a lot of quantitative analysis all pointing to significant improvements over previous methods.\n\nA number of new tricks are proposed, with the ablation study (tab 1 + fig 3) and learning curves (fig 4) giving insight into their effects on performance.  Though the field is moving quickly, I expect that several of these tricks will be broadly adopted in future work at least in the short to medium term.\n\nThe training code and data are released.\n\n\nCons/Suggestions:\n\nIt would be nice to see overfitting addressed and quantified in some way.  For example, the proposed SWD metric could be recomputed both for the training and for a held-out validation/test set, with the difference between the two scores measuring the degree of overfitting.  Similarly, Danihelka et al. [1] show that an independently trained Wasserstein critic (with one critic trained on G samples vs. train samples, and another trained on G samples vs. val samples) can be used to measure overfitting.  Another way to go could be to generate a large number of samples and show the nearest neighbor for a few training set samples and for a few val set samples.  Doing this in pixel space may not work well especially at the higher resolutions, but maybe a distance function in the space of some high-level hidden layer of a trained discriminator could show good semantic nearest neighbors.\n\nThe proposed SWD metric is interesting and computationally convenient, but it’s not clear to me that it’s an improvement over previous metrics like the independent Wasserstein critic proposed in [1].  In particular the use of 7x7 patches would seem to limit the metric’s ability to capture the extent to which global structure has been learned, even though the patches are extracted at multiple levels of the Laplacian pyramid.\n\nThe ablation study (tab 1 + fig 3) leaves me somewhat unsure which tricks contribute the most to the final performance improvement over previous work.  Visually, the biggest individual improvement is easily when going from (c) to (d), which adds the “Revised training parameters”, with the improvement from (a) to (b) which adds the highlighted progressive training schedule appearing relatively minor in comparison.  However, I realize the former large improvement is due to the arbitrary ordering of the additions in the ablation study, with the small minibatch addition in (c) crippling results on its own.  Ablation studies with large numbers of tweaks are always difficult and this one is welcome and useful despite the ambiguity.\n\nOn a related note, it would be nice if there were more details on the “revised training hyperparameters” improvement ((d) in the ablation study) -- which training hyperparameters are adjusted, and how?\n\n“LAPGAN” (Denton et al., 2015) should be cited as related work: LAPGAN’s idea of using a separate generator/discriminator at each level of a Laplacian pyramid conditioned on the previous level is quite relevant to the progressive training idea proposed here.  Currently the paper is only incorrectly cited as “DCGAN” in a results table -- this should be fixed as well.\n\n\nOverall, this is a well-written paper with striking results and a solid effort to analyze, ablate, and quantify the effect of each of the many new techniques proposed. It’s likely that the paper will have a lot of impact on future GAN work.\n\n\n[1] Danihelka et al., “Comparison of Maximum Likelihood and GAN-based training of Real NVPs” https://arxiv.org/abs/1705.05263", "I totally agree with the last point: it would have been great if the organizers provided a more detailed CFP and recommended best practices.\n\nHowever, I disagree with the other two points:\nFirst, I know arxiv, talks, blogs, etc are permitted. But directly linking the author list from the paper is generally not. \nSecond, there are ways to host data anonymously and many ICLR authors (including myself) found some. If in doubt, the right way would be to ask the organizers, not breach the anonymity directly.\n\nIn the end, the decision is on ACs and PCs.", "We have uploaded a new revision of the paper, addressing the concerns brought up in the reviews. The detailed list of changes is as follows:\n\n- Revise the nearest neighbors in Figure 10 by using VGG feature-space distance and showing 5 best matches for each generated image.\n- Report average CIFAR10 inception score over 10 random initializations in Table 3, in addition to the highest achieved score.\n- Add discussion of [Denton et al. 2015], [Huang et al. 2016], and [Zhang et al. 2017] in Section 2.\n- Fix [Anonymous 2017], [Rabin et al. 2011], and [Radford et al. 2015] references.\n- Update \"(h) Converged\" case in Table 1 and Figure 3, as well as LSUN images in Figures 12-17 using networks that were trained longer.\n- Report SWD numbers for CelebA-HQ and LSUN categories in Figures 11-17.\n- Typo fixes and minor clarifications.\n\nWe would like to thank the reviewers again for useful feedback.", "We will fix all the references and add related discussion to the paper. \n\nWe have, in fact, obtained more sensible nearest-neighbor results using a feature-space distance metric for image comparison. We will update Fig. 10 accordingly and include multiple nearest neighbors for each generated image. The conclusion still stands that the generated images have no obvious source images in the training set. The hyperparameter changes related to Table 1 (d) are listed in Appendix A.2. \n\nWe acknowledge R1’s concerns about anonymity and feel a few words are in order.\n\nFirst, the call for papers explicitly states that arXiv and other such public forums are permitted. While we agree that full anonymity is valuable, we feel that one cannot realistically expect to achieve it perfectly, because so many potential reviewers subscribe to the arXiv announce list and articles from that list are inevitably discussed in social media.\n\nSecond, the OpenReview submission site does not allow supplemental videos or code, forcing one to use services like YouTube and GitHub, neither of which allows anonymous submissions. In our opinion, that leaves two possibilities: 1) fake accounts, or 2) breach of anonymity. We thought about this long and hard and chose #2 because #1 seems fraught with many more problems -- and would perhaps also seem like a strange requirement. While we anonymized the paper and the video to the extent possible within these bounds, we regrettably forgot a full author list in the readme at GitHub. We sincerely apologize for this oversight.\n\nIn order to avoid this kind of awkwardness in the future, we feel that explicit guidance in the CFP -- including suggested best practices for submitting videos, code, and data -- would be helpful and greatly facilitate the review process.", "We apologize that the source code is somewhat convoluted in this regard.\n\nOur implementation performs weight initialization in two distinct phases. It first initializes the weights using Lasagne's standard He initializer and then rescales them in accordance to Section 4.1. For example, consider the following line in network.py (http://bit.ly/2zykV3P#L471):\n\n471  net = PN(BN(WS(Conv2DLayer(net, name='G1b', num_filters=nf(1), filter_size=3, pad=1, nonlinearity=act, W=iact))))\n\nHere, we create a standard Conv2DLayer and apply equalized learning rate (WS) as well as pixelwise feature vector normalization (PN) on top of it. Note that batch normalization (BN) is disabled in most of our experiments. When the Conv2DLayer is first instantiated, the weights are initialized according to W=iact, which in turn is defined as lasagne.init.HeNormal('relu') on lines 459 and 32. We apply equalized learning rate by latching a custom WScaleLayer (line 278) onto the Conv2DLayer. When the WScaleLayer is instantiated, it estimates the elementwise standard deviation of the weights and normalizes them accordingly:\n\n281  W = incoming.W.get_value()\n282  scale = np.sqrt(np.mean(W ** 2))\n283  incoming.W.set_value(W / scale)\n\nThe value on line 281 corresponds to $\\hat{w}_i$ in the paper, line 282 corresponds to $c$, and line 283 to $w_i$. In other words, this part of the code undoes the effect of He's initializer and brings the weights back to trivial N(0,1) initialization.", "In Section 4.1 you mention that you are initializing the network weights by sampling from the normal distribution. In your code, it appears you are using the stock Lasagne weight initialization, which uses the Xavier Glorot uniform distribution. Or has this changed in some newer version of Lasagne?", "1.\nYes, assuming that the question refers to the exponential running average that we use for visualizing the generated images.\n\nWe have observed that the best results are generally obtained using a relatively high learning rate, which in turn leads to significant variation in terms of network weights between consecutive training iterations. Any instantaneous snapshot of the generator is likely to be slightly off or exaggerated in terms of various image features such as color, brightness, sharpness, shape of the mouth, amount of hair, color of the eyes, etc. The exponential running average reduces this variation, leading to considerably more consistent results.\n\nIntuitively speaking, we can say that the generator and discriminator are constantly exploring a large neighborhood of different solutions around the current average solution, even though the average solution itself evolves relatively slowly. According to our experiments, such exploration seems to be highly beneficial in terms of eventually converging towards a good local optimum.\n\n2.\nWe have tried increasing the minibatch size in our CIFAR-10 runs, but we have not observed an increase in the inception scores.\n\nThe performance degradation associated with small minibatch size is largely limited to configurations that rely heavily on batch normalization (rows a-c in Table 1). Perhaps surprisingly, we have observed that smaller minibatches actually produce slightly better results in configurations where batch normalization is not present (rows d-h).\n\n3.\nWe did explore different network architectures in the early stages of the project. In general, it does not seem to make a big difference whether we start at 2x2, 4x4, 8x8, or 16x16 resolution. We chose 4x4 mainly because it is the most natural fit for our specific network architecture. We have also observed that it is beneficial to have roughly the same structure and capacity in both networks, as well as matching upsampling and downsampling operators.\n", "1. Was performance degrading actually observed when smoothing was not used?\n\n2. It seems that the reported Inception score of CIFAR is based on minibatch size of 16 because you wanted to show that the performance degrading of small minibatch could be remedied by that. Have you found a significant increase in score when the size is 64 and when you use all the techniques you used to remedy the aforementioned performance degrading? If so, I think the degree of the increase in score would indicate the size of this bottleneck. \n\n3. Did you do any other architecture exploration such as beginning from first block having 2x2 output instead of 4x4?"], "review_score_variance": 10.888888888888891, "summary": "The main contribution of the paper is a technique for training GANs which consists in progressively increasing the resolution of generated images by gradually enabling layers in the generator and the discriminator. The method is novel, and outperforms the state of the art in adversarial image generation both quantitatively and qualitatively. The evaluation is carried out on several datasets; it also contains an ablation study showing the effect of contributions (I recommend that the authors follow the suggestions of AnonReviewer2 and further improve it). Finally, the source code is released which should facilitate the reproducibility of the results and further progress in the field.\n\nAnonReviewer1 has noted that the authors have revealed their names through GitHub, thus violating the double-blind submission requirement of ICLR; if not for this issue, the reviewer’s rating would have been 8. While these concerns should be taken very seriously, I believe that in this particular case the paper should still be accepted for the following reasons:\n1) the double blind rule is new for ICLR this year, and posting the paper on arxiv is allowed;\n2) the author list has been revealed through the supplementary material (Github page) rather than the paper itself;\n3) all reviewers agree on the high impact of the paper, so having it presented and discussed at the conference would be very useful for the community.", "paper_id": "iclr_2018_Hk99zCeAb", "label": "train", "paper_acceptance": "accepted-oral-papers"}
{"source_documents": ["A weakly supervised learning based clustering framework is proposed in this paper. As the core of this framework, we introduce a novel multiple instance learning task based on a bag level label called unique class count (ucc), which is the number of unique classes among all instances inside the bag. In this task, no annotations on individual instances inside the bag are needed during training of the models. We mathematically prove that with a perfect ucc classifier, perfect clustering of individual instances inside the bags is possible even when no annotations on individual instances are given during training. We have constructed a neural network based ucc classifier and experimentally shown that the clustering performance of our framework with our weakly supervised ucc classifier is comparable to that of fully supervised learning models where labels for all instances are known. Furthermore, we have tested the applicability of our framework to a real world task of semantic segmentation of breast cancer metastases in histological lymph node sections and shown that the performance of our weakly supervised framework is comparable to the performance of a fully supervised Unet model.", "\n2) \"UCC model uses bags of sizes from 1 to 4. Assuming uniform distribution, 25% of the samples are fully labeled. The semi-supervised methods from Table 6 how many labeled samples do they use? Having that small bag samples the problem is quite easy. Moreover, during training, I guess that the same sample can go to different bags. Is this right? If so, the annotation time would be very big. And also, one could trivially get the full label of each sample.\"\n\nAssuming that the main concern of the reviewer was related to number of instances inside the bags, i.e. bag sizes, we have addressed the reviewer's concerns in here. If the reviewer has any other points to be clarified, we will be happy to address those points as well.\n\n- \"UCC model uses bags of sizes from 1 to 4. (...) Having that small bag samples the problem is quite easy.\"\n--> We apologize for misunderstanding. We didn't specify the number of instances inside the bags. Considering the reviewer's concern, we have updated Appendix C.2 to include number of instances in each bag for each dataset. For MNIST and CIFAR10 datasets, bags with 32 instances and for CIFAR100 dataset, bags with 128 instances were used in all experiments. We thank the reviewer for addressing this point.\n\n- \"Assuming uniform distribution, 25% of the samples are fully labeled.\"\n--> During training, labels of individual instances inside the bags are unknown (Sec. 3 Objective, Sec. 3.1 Definition 1). Even if a bag is a $ucc1$ bag, only thing we know is that all the instances inside the bag belongs to the same class, but the class is unknown. The unknown class, for example, can be anything in $\\{0,1,\\cdots,9\\}$ for MNIST. Hence, we don't have any 'fully labeled' instances, i.e. we don't know the labels of individual instances.\n\n- \"The semi-supervised methods from Table 6 how many labeled samples do they use?\"\n--> The semi-supervised models in Table 1 (previously in Table 6) use 100 instances with labels for MNIST dataset. Please note that MNIST is such an easy dataset that 100 instances with labels are enough to boost the performance of semi-supervised models. On the other hand, they use 10% of instances with labels for CIFAR10 dataset, which is not a small portion.\n\n- \"Moreover, during training, I guess that the same sample can go to different bags. Is this right?\"\n--> The reviewer is correct: one instance may appear in more than one bag since inputs to $ucc$ models were sampled from the power sets of MNIST, CIFAR10 and CIFAR100 datasets, i.e. $2^{{\\mathcal{X}}_{mnist,tr}}$, $2^{{\\mathcal{X}}_{cifar10,tr}}$ and $2^{{\\mathcal{X}}_{cifar100,tr}}$, during training (Sec. 4.1 paragraph 2).\n\n- \"If so, the annotation time would be very big. And also, one could trivially get the full label of each sample.\"\n--> We like to emphasize that $UCC$ model is a typical example of Multiple Instance Learning (MIL) paradigm and it is different than the traditional supervised classification paradigm, where individual instances are labeled.\n\nIt is important to state that in almost all MIL problems, bag level labels are readily available and task is to infer instance level labels. Trying to obtain bag level labels from instance level labels is either not possible (instance level labels are not known) or not feasible (i.e. directly obtaining bag level labels is much cheaper). For example, in our histopathology example, labelling an image with $ucc1$ or $ucc2$ label is much cheaper than annotating each individual pixel (instance) inside the image.\n\nWe used MNIST and CIFAR datasets in our experiments and obtained bag level labels from instance labels while preparing MIL dataset. However, this was just to construct controlled datasets to analyze the characteristics of our models properly. Moreover, individual instance labels were never used during training of our models.\n\n4) \"Once trained on a dataset the method can perform clustering on classes (classify) better than a fully unsupervised clustering algorithm and worse than a fully supervised model.\"\n--> We like to highlight that clustering was performed on the features extracted by feature extractor module $\\bar{\\theta}_{feature}$ of trained models (Sec. 3.3).", "\n1) \"The authors use KDE method to estimate the distribution of extracted features as the input of distribution regression module, but it is not clear how the description of KDE method updates on the parameters of the autoencoder.\"\n\n--> Assuming that the concern of the reviewer was related to the weight updates of the ${\\theta_{feature}}$ module, which is encoder part of autoencoder branch. We will explain the weight update procedure in here. If the reviewer has any other points to be clarified, we will be happy to address those points as well.\n\nConsidering the reviewer's concern, we have updated Appendix A.1 to include the weight update procedure for the feature extractor module ${\\theta_{feature}}$, which is summarised here as well.\n\nFeature extractor module ${\\theta_{feature}}$ is shared by both autoencoder branch and $ucc$ branch in our model. During back-propagation phase of the end-to-end training process, the weight updates of ${\\theta_{feature}}$ comprise the gradients coming from both branches (Equation (2)). Gradients coming from autoencoder branch follow the traditional neural network back-propagation flow through the convolutional and fully connected layers. Different than that, gradients coming from $ucc$ branch (Equation (3)) also back-propagate through the custom KDE layer according to the differential equation in Equation (4), derivation of which has been given in Appendix A.1.\n\n(1) $Loss\\ =\\ \\alpha \\underbrace{Loss_{ucc}}_{ \\substack{\\text{ucc} \\\\ \\text{loss}} }\\ +\\ (1-\\alpha)\\underbrace{Loss_{ae}}_{ \\substack{\\text{autoencoder} \\\\ \\text{loss}} } \\text{ where } \\alpha \\in [0,1]$\n\n(2) $\\underbrace{\\frac{\\partial Loss}{\\partial {\\theta_{feature}}}}_{ \\substack{\\text{gradients} \\\\ \\text{for} \\\\ \\theta_{feature}} }\\ =\\ \\alpha \\underbrace{\\frac{\\partial Loss_{ucc}}{\\partial {\\theta_{feature}}}}_{ \\substack{\\text{gradients} \\\\ \\text{from} \\\\ \\text{ucc branch}} }\\ +\\ (1-\\alpha) \\underbrace{\\frac{\\partial Loss_{ae}}{\\partial {\\theta_{feature}}}}_{\\substack{\\text{gradients} \\\\ \\text{from} \\\\ \\text{autoencoder branch}}}$\n\n(3) $\\frac{\\partial Loss_{ucc}}{\\partial {\\theta_{feature}}}=\\frac{\\partial Loss_{ucc}}{\\partial h_{\\sigma_\\zeta}} \\times \\underbrace{\\frac{\\partial h_{\\sigma_\\zeta}}{\\partial f_{\\sigma_\\zeta}}}_{ \\substack{\\text{back-propagation} \\\\ \\text{through} \\\\ \\text{KDE layer}} } \\times \\frac{\\partial f_{\\sigma_\\zeta}}{\\partial {\\theta_{feature}}}$\n\n(4) $\\frac{\\partial h^j_{\\sigma_\\zeta}(v)}{\\partial f^{j,i}_{\\sigma_\\zeta}} = \\frac{1}{ |\\sigma_\\zeta| } \\frac{\\left(v-f^{j,i}_{\\sigma_\\zeta}\\right)}{ {{\\sigma}^2} {\\sqrt{2\\pi{\\sigma}^2}} } e^{-\\frac{1}{2{\\sigma}^2}\\left(v-f^{j,i}_{\\sigma_\\zeta}\\right)^2}$\n\n$\\sigma_\\zeta$: a set of instances\n\n$h^j_{\\sigma_\\zeta}(v)$: distribution of $j^{th}$ feature obtained over the instances of set $\\sigma_\\zeta$ at the output of KDE layer\n\n$f^{j,i}_{\\sigma_\\zeta}$: $j^{th}$ feature of $i^{th}$ instance in set $\\sigma_\\zeta$ extracted by ${\\theta_{feature}}$\n\n$\\sigma$: standard deviation of Gaussian kernel in KDE module\n", "Thank you very much for your valuable time, insightful comments and suggestions. Please find our answers to your questions below. We have updated our paper based on suggestions and comments of the reviewers. The summary of updates in the paper are listed in a separate comment on top of the page. If you have any further comments or suggestions on the updated version of our paper, we will be glad to improve on them. Thanks.\n\n1) \"The connection of the proposed UCC approach to the motivating histopath example should be explicitly stated upfront to help the reader understand how this method could be used and how it makes sense!\"\n\n--> Following the reviewer's suggestion, we have stated the connection of the proposed UCC approach to the motivating histopathology example by introducing our formulation of semantic segmentation task in proposed framework. We have updated the second paragraph in page 2 starting with \"Our weakly supervised clustering framework is illustrated in Figure 1. It consists ...\".\n\n2) \"It seems that the KDE element of the UCC model was chosen in part to enable the theoretical analysis? If so, this should be clearly stated to help the reader understand the design rationale.\"\n\n--> This is a great observation and the reviewer is correct. One of the main reasons in choosing KDE module was to enable the theoretical analysis. Kernel density estimation has decomposability property, which was used in the proofs of propositions. Although this was highlighted at the beginning of Appendix B, we unfortunately did not include this point in the main text. Thanks for raising this point, we have updated Sec 3.2.1 accordingly.\n\n3) \"It seems that a different approach than the KDE layer could have been taken- this should be added to the ablation experiments\"\n\n--> That is true. Instead of KDE layer, for example, an \"averaging layer\" [1], which gives the average value of feature scores accross the instances inside the bag,  could have been used in our proposed architecture while still all of the propositions in the paper are valid. We chose KDE over \"averaging layer\" since we simply wanted to provide the distribution regression module with the full distribution information rather than point estimates. Our concern was that two different distributions may have the same point estimates, which would make the training process harder.\n\nWe designed and conducted a complete set of experiments with KDE module to compare different models in our paper. However, as the reviewer suggested, it could be good to see the performance of other approaches and compare with the performance of KDE layer. Although a complete set of experiments with different MIL pooling layers, different models and different datasets is not feasible to be done by rebuttal deadline, we agree with the reviewer on exploring the alternatives as much as we can within the rebuttal period. Hence, we have started to conduct experiments with \"averaging layer\" in some of our models. We have presented the results of completed experiments in Appendix C.6 and we will update this part once the results are available.\n\n[1] Xinggang Wang, Yongluan Yan, Peng Tang, Xiang Bai, and Wenyu Liu. Revisiting multiple instance neural networks. Pattern Recognition, 74:15–24, 2018.\n", "Thank you very much for your valuable time, insightful comments and suggestions. Please find our answers to your questions below. We have updated our paper based on suggestions and comments of the reviewers. The summary of updates in the paper are listed in a separate comment on top of the page. If you have any further comments or suggestions on the updated version of our paper, we will be glad to improve on them. Thanks.\n\n1) \"The authors use KDE method to estimate the distribution of extracted features as the input of distribution regression module, but it is not clear how the description of KDE method updates on the parameters of the autoencoder.\"\n\n--> Awaiting reviewer's reply to our request!\n      |--> Please see our response below.\n\n2) \"The proofs of some propositions in Appendix B are very obvious and redundant. The authors’ proofs are not enough to prove whether the designed UCC classifier is perfect.\"\n\n--> The reviewer is correct and we totally agree with the reviewer. Our proofs do not guarantee to find a perfect classifier (if a perfect classifier exists), nor do they prove UCC model is a universal approximator [1]. We will be glad to explore any idea and suggestion on this. However, from our propositions, we know that a perfect ucc classifier, theoretically, guarantees the perfect clustering of individual instances and in Figure 3, we experimentally showed that as ucc accuracy increases (i.e. as the classifier gets closer to the perfect), the clustering accuracy increases. Indeed, UCC model seems to be able to approximate the perfect ucc classifier in practice.\n\n[1] Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4 (2):251–257, 1991.\n", "Thank you very much for your valuable time, insightful comments and suggestions. Please find our answers to your questions below. We have updated our paper based on suggestions and comments of the reviewers. The summary of updates in the paper are listed in a separate comment on top of the page. If you have any further comments or suggestions on the updated version of our paper, we will be glad to improve on them. Thanks.\n\n1) \"The main table of the paper (Table 1) compares the results of the proposed methods only with unsupervised methods and a fully supervised one. The results are reasonable. However, not a fair comparison. More fair comparisons can be found in Table 6 of appendix C.6 page 22 where the method is compared to semi-supervised methods. In this case, the proposed method performs worse than the semisupervised methods.\"\n\n--> Considering the reviewer's concern, in Table 1, we have included semi-supervised models as well. Previously, only the unsupervised and fully supervised models were included in Table 1 since they set the lower bound and upper bound on the performance scale bar, respectively. The aim was to show that UCC models performed somewhere in between those lower and upper bounds. Actually, the performance of UCC models were explicitly compared with the performance of semi-supervised models in the main text (Sec 4.4 Labels on Intances, paragraph 2) and the reader was referred to Appendix C.6 for detailed comparison due to space limitations. Now, everything is included in the main text.\n\n2) \"UCC model uses bags of sizes from 1 to 4. Assuming uniform distribution, 25\\% of the samples are fully labeled. The semi-supervised methods from Table 6 how many labeled samples do they use? Having that small bag samples the problem is quite easy. Moreover, during training, I guess that the same sample can go to different bags. Is this right? If so, the annotation time would be very big. And also, one could trivially get the full label of each sample.\"\n\n--> Awaiting reviewer's reply to our request!\n      |--> Please see our response below.\n\n3) \"The article is excessively long. 10 pages for the main article and 12 extra pages for supplementary material which are needed for understanding the paper. Many of the definitions are redundant and there is excessive detail in explanations that can be expressed more simply.\"\n\n--> Following the reviewer's comment, we have gone through our paper again and moved some of the propositions, which are obvious and used in the proof of other propositions, from Sec 3.2.2 to Appendix B. We keep working on our paper to improve it further during the rebuttal period.\n\n4) \"Once trained on a dataset the method can perform clustering on classes (classify) better than a fully unsupervised clustering algorithm and worse than a fully supervised model.\"\n\n--> Awaiting reviewer's reply to our request!\n      |--> Please see our response below.", "The updates made in the paper during rebuttal period are summarised in here!\n\n- We have stated the connection of the proposed UCC approach to the motivating histopathology example by introducing our formulation of semantic segmentation task in proposed framework. We have updated the second paragraph in page 2 starting with \"Our weakly supervised clustering framework is illustrated in Figure 1. It consists ...\".\n\n- We have updated Sec 3.2.1 to highlight that KDE module also enables our theoretical analysis beyond providing us with some other unique advantages.\n\n- We have moved some of the propositions, which are obvious and used in the proof of other propositions, from Sec 3.2.2 to Appendix B.\n\n- We have included semi-supervised models in Table 1 and removed from Appendix C.6\n\n- We have updated Appendix A.1 to include weight update procedure for feature extractor module ${\\theta_{feature}}$.\n\n- We have updated Appendix C.2 to include number of instances in each bag for each dataset.\n\n- We have devoted Appendix C.6 to include the clustering accuracy results of UCC models with \"averaging layer\" as an alternative to KDE layer.\n", "Thank you very much for your valuable time and detailed review.\n\nIn order to address the comments/concerns in the review in a better way, we want to clarify a few points in the review.\n\nCould you please kindly help us to understand the following statements from your review better? Thanks.\n\n1) \"UCC model uses bags of sizes from 1 to 4.\"\nDoes that mean the number of instances inside a bag is either 1 or 2 or 3 or 4?\n\n2) \"Having that small bag samples the problem is quite easy.\"\nWhat does \"small bag samples\" refer to?\n\n3) \"Once trained on a dataset the method can perform clustering on classes (classify) better than a fully unsupervised clustering algorithm and worse than a fully supervised model.\"\nWhat does \"perform clustering on classes (classify)\" mean?", "Thank you very much for your valuable time and detailed review.\n\nIn order to address the comments/concerns in the review in a better way, we want to clarify a few points in the review. \n\nCould you please kindly elaborate on the following statement from your review to help us to understand it better? Thanks.\n       \"it is not clear how the description of KDE method updates on the parameters of the autoencoder.\"\n", "The authors present a novel weakly-supervised multiple instance learning model based on a bag level label called unique class count(UCC). The core content of the method is to learn mapping between bags and their associated bag level ucc labels and then to predict the ucc labels of unseen bags. \nPositive:\n(1) The authors use the unique class count(UCC) in the bag as a weak, bag level label to design a deep learning based UCC model for extracting features and clustering the individual instances in the unseen bags. They also construct a new optimization objective function by combining ucc loss and autoencoder loss. This is a novel weakly-supervised clustering method rarely seen.\n(2) In this paper, a large number of experiments show the effectiveness of the algorithm in different data sets and semantic segmentation of breast cancer metastases.\nNegative:\n(1) The authors use KDE method to estimate the distribution of extracted features as the input of distribution regression module, but it is not clear how the description of KDE method updates on the parameters of the auto-encoder.\n(2) The proofs of some propositions in Appendix B are very obvious and redundant. The authors’ proofs are not enough to prove whether the designed UCC classifier is perfect.", "This paper proposes a MIL clustering method. The proposed MIL setup is called \"unique class count (ucc)\", this is, for a bag os samples ucc is the number of clusters in the bag. The method learns the features of the samples using two losses: an autoender loss and the ucc loss. Once trained on a dataset the method can perform clustering on classes (classify) better than a fully unsupervised clustering algorithm and worse than a fully supervised model. The method is evaluated on MNIST, CIFAR10, CIFAR100  and on binary breast cancer segmentation.\n\nThe main table of the paper (Table 1) compares the results of the proposed methods only with unsupervised methods and a fully supervised one. The results are reasonable. However, not a fair comparison. More fair comparisons can be found in Table 6 of appendix C.6 page 22 where the method is compared to semi-supervised methods. In this case, the proposed method performs worse than the semisupervised methods.\n\nUCC model uses bags of sizes from 1 to 4. Assuming uniform distribution, 25% of the samples are fully labeled. The semi-supervised methods from Table 6 how many labeled samples do they use? Having that small bag samples the problem is quite easy. Moreover, during training, I guess that the same sample can go to different bags. Is this right? If so, the annotation time would be very big. And also, one could trivially get the full label of each sample.\n\nThe article is excessively long. 10 pages for the main article and 12 extra pages for supplementary material which are needed for understanding the paper. Many of the definitions are redundant and there is excessive detail in explanations that can be expressed more simply. \n\n", "This paper proposes a new type of weakly supervised clustering / multiple instance learning (MIL) problem in which bags of instances (data points) are labeled with a \"unique class count (UCC)*, rather than any bag-level or instance-level labels.  For example, a histopathology slide (the bag), consisting of many individual pixels to be labeled (the instances) could be labeled at the bag level only with UCC = 1 (for only healthy or only metastatic) or UCC = 2 (for mixed / border case).  The paper then proposes an approach for clustering instances based on the following two-step approach: (1) a UCC model is trained to predict the UCC given an input bag, and (2) the features of this learned UCC model are used in an unsupervised clustering algorithm to the get the instance-level clusters / labels.  The paper also provides a theoretical argument for why this approach is feasible.\n\nOverall this paper proposes a (1) novel and creative approach that is well validated both (2) theoretically and (3) empirically, and relevant to a real-world problem, and thus I believe should be accepted.  In slightly more detail:\n\n- (1) MIL where we are given bag-level labels only is a well-studied problem that occurs in many real world settings, such as the histopathology one used in this paper.  However to this reader's knowledge, this is a new variant that is both creative and motivated by an actual real-world study, which is exciting and alone warrants presentation at the conference in my opinion.\n\n- (2) The theoretical treatment is high-level, but still serves a clear purpose of establishing feasibility of the proposed method- this modest and appropriate purpose serves the paper well.\n\n- (3) The empirical results are thorough---e.g. the use of the two loss components for the UCC model are appropriately ablated, there are a range of baseline approaches compared to, multiple evaluation points are provided (i.e. both UCC prediction and final clustering metrics), and a real world use case is presented--and the results are impressive.\n\nSome comments to improve the paper:\n- The connection of the proposed UCC approach to the motivating histopath example should be explicitly stated upfront to help the reader understand how this method could be used and how it makes sense!\n- It seems that the KDE element of the UCC model was chosen in part to enable the theoretical analysis?  If so, this should be clearly stated to help the reader understand the design rationale.\n- Either way, it seems that a different approach than the KDE layer could have been taken- this should be added to the ablation experiments"], "review_score_variance": 8.666666666666666, "summary": "The paper proposes a weakly supervised learning algorithm, motivated by its application to histopathology. Similar to the multiple instance learning scenario, labels are provided for bags of instances. However instead of a single (binary) label per bag, the paper introduces the setting where the training algorithm is provided with the number of classes in the bag (but not which ones). Careful empirical experiments on semantic segmentation of histopathology data, as well as simulated labelling from MNIST and CIFAR demonstrate the usefulness of the method. The proposed approach is similar in spirit to works such as learning from label proportions and UU learning (both which solve classification tasks).\nhttp://www.jmlr.org/papers/volume10/quadrianto09a/quadrianto09a.pdf\nhttps://arxiv.org/abs/1808.10585\n\nThe reviews are widely spread, with a low confidence reviewer rating (1). However it seems that the high confidence reviewers are also providing higher scores and better comments. The authors addressed many of the reviewer comments, and seeked clarification for certain points, but the reviewers did not engage further during the discussion period.\n\nThis paper provides a novel weakly supervised learning setting, motivated by a real world semantic segmentation task, and provides an algorithm to learn from only the number of classes per bag, which is demonstrated to work on empirical experiments. It is a good addition to the ICLR program.", "paper_id": "iclr_2020_B1xIj3VYvr", "label": "train", "paper_acceptance": "accept-poster"}
